[
  {
    "id": 45717250,
    "name": "tensorflow",
    "full_name": "tensorflow/tensorflow",
    "description": "An Open Source Machine Learning Framework for Everyone",
    "html_url": "https://github.com/tensorflow/tensorflow",
    "clone_url": "https://github.com/tensorflow/tensorflow.git",
    "owner_login": "tensorflow",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/15658638?v=4",
    "stargazers_count": 191065,
    "watchers_count": 191065,
    "forks_count": 74759,
    "open_issues_count": 1509,
    "size": 1186553,
    "language": "C++",
    "languages": {
      "C++": 100386972,
      "Python": 45864984,
      "MLIR": 11211596,
      "HTML": 7662661,
      "Starlark": 7336994,
      "Go": 2173960,
      "C": 1250883,
      "Java": 1178899,
      "Jupyter Notebook": 805762,
      "Shell": 662640,
      "Objective-C++": 279706,
      "Objective-C": 169983,
      "CMake": 150395,
      "Smarty": 138594,
      "Swift": 81677,
      "Dockerfile": 34389,
      "C#": 13585,
      "Batchfile": 12126,
      "Ruby": 8898,
      "Perl": 7536,
      "Roff": 5034,
      "Linker Script": 4568,
      "Cython": 3899,
      "Makefile": 2845,
      "CSS": 2761,
      "Vim Snippet": 58
    },
    "topics": [
      "deep-learning",
      "deep-neural-networks",
      "distributed",
      "machine-learning",
      "ml",
      "neural-network",
      "python",
      "tensorflow"
    ],
    "license_name": "Apache License 2.0",
    "created_at": "2015-11-07T01:19:20+00:00",
    "updated_at": "2025-08-06T01:46:18+00:00",
    "pushed_at": "2025-08-06T02:10:33+00:00",
    "contributors_count": 100,
    "readme_length": 11899,
    "readme_content": "<div align=\"center\">\n  <img src=\"https://www.tensorflow.org/images/tf_logo_horizontal.png\">\n</div>\n\n[![Python](https://img.shields.io/pypi/pyversions/tensorflow.svg)](https://badge.fury.io/py/tensorflow)\n[![PyPI](https://badge.fury.io/py/tensorflow.svg)](https://badge.fury.io/py/tensorflow)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4724125.svg)](https://doi.org/10.5281/zenodo.4724125)\n[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/1486/badge)](https://bestpractices.coreinfrastructure.org/projects/1486)\n[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/tensorflow/tensorflow/badge)](https://securityscorecards.dev/viewer/?uri=github.com/tensorflow/tensorflow)\n[![Fuzzing Status](https://oss-fuzz-build-logs.storage.googleapis.com/badges/tensorflow.svg)](https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&can=1&q=proj:tensorflow)\n[![Fuzzing Status](https://oss-fuzz-build-logs.storage.googleapis.com/badges/tensorflow-py.svg)](https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&can=1&q=proj:tensorflow-py)\n[![OSSRank](https://shields.io/endpoint?url=https://ossrank.com/shield/44)](https://ossrank.com/p/44)\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v1.4%20adopted-ff69b4.svg)](CODE_OF_CONDUCT.md)\n\n**`Documentation`** |\n------------------- |\n[![Documentation](https://img.shields.io/badge/api-reference-blue.svg)](https://www.tensorflow.org/api_docs/) |\n\n[TensorFlow](https://www.tensorflow.org/) is an end-to-end open source platform\nfor machine learning. It has a comprehensive, flexible ecosystem of\n[tools](https://www.tensorflow.org/resources/tools),\n[libraries](https://www.tensorflow.org/resources/libraries-extensions), and\n[community](https://www.tensorflow.org/community) resources that lets\nresearchers push the state-of-the-art in ML and developers easily build and\ndeploy ML-powered applications.\n\nTensorFlow was originally developed by researchers and engineers working within\nthe Machine Intelligence team at Google Brain to conduct research in machine\nlearning and neural networks. However, the framework is versatile enough to be\nused in other areas as well.\n\nTensorFlow provides stable [Python](https://www.tensorflow.org/api_docs/python)\nand [C++](https://www.tensorflow.org/api_docs/cc) APIs, as well as a\nnon-guaranteed backward compatible API for\n[other languages](https://www.tensorflow.org/api_docs).\n\nKeep up-to-date with release announcements and security updates by subscribing\nto\n[announce@tensorflow.org](https://groups.google.com/a/tensorflow.org/forum/#!forum/announce).\nSee all the [mailing lists](https://www.tensorflow.org/community/forums).\n\n## Install\n\nSee the [TensorFlow install guide](https://www.tensorflow.org/install) for the\n[pip package](https://www.tensorflow.org/install/pip), to\n[enable GPU support](https://www.tensorflow.org/install/gpu), use a\n[Docker container](https://www.tensorflow.org/install/docker), and\n[build from source](https://www.tensorflow.org/install/source).\n\nTo install the current release, which includes support for\n[CUDA-enabled GPU cards](https://www.tensorflow.org/install/gpu) *(Ubuntu and\nWindows)*:\n\n```\n$ pip install tensorflow\n```\n\nOther devices (DirectX and MacOS-metal) are supported using\n[Device Plugins](https://www.tensorflow.org/install/gpu_plugins#available_devices).\n\nA smaller CPU-only package is also available:\n\n```\n$ pip install tensorflow-cpu\n```\n\nTo update TensorFlow to the latest version, add `--upgrade` flag to the above\ncommands.\n\n*Nightly binaries are available for testing using the\n[tf-nightly](https://pypi.python.org/pypi/tf-nightly) and\n[tf-nightly-cpu](https://pypi.python.org/pypi/tf-nightly-cpu) packages on PyPI.*\n\n#### *Try your first TensorFlow program*\n\n```shell\n$ python\n```\n\n```python\n>>> import tensorflow as tf\n>>> tf.add(1, 2).numpy()\n3\n>>> hello = tf.constant('Hello, TensorFlow!')\n>>> hello.numpy()\nb'Hello, TensorFlow!'\n```\n\nFor more examples, see the\n[TensorFlow Tutorials](https://www.tensorflow.org/tutorials/).\n\n## Contribution guidelines\n\n**If you want to contribute to TensorFlow, be sure to review the\n[Contribution Guidelines](CONTRIBUTING.md). This project adheres to TensorFlow's\n[Code of Conduct](CODE_OF_CONDUCT.md). By participating, you are expected to\nuphold this code.**\n\n**We use [GitHub Issues](https://github.com/tensorflow/tensorflow/issues) for\ntracking requests and bugs, please see\n[TensorFlow Forum](https://discuss.tensorflow.org/) for general questions and\ndiscussion, and please direct specific questions to\n[Stack Overflow](https://stackoverflow.com/questions/tagged/tensorflow).**\n\nThe TensorFlow project strives to abide by generally accepted best practices in\nopen-source software development.\n\n## Patching guidelines\n\nFollow these steps to patch a specific version of TensorFlow, for example, to\napply fixes to bugs or security vulnerabilities:\n\n*   Clone the TensorFlow repository and switch to the appropriate branch for\n    your desired version\u2014for example, `r2.8` for version 2.8.\n*   Apply the desired changes (i.e., cherry-pick them) and resolve any code\n    conflicts.\n*   Run TensorFlow tests and ensure they pass.\n*   [Build](https://www.tensorflow.org/install/source) the TensorFlow pip\n    package from source.\n\n## Continuous build status\n\nYou can find more community-supported platforms and configurations in the\n[TensorFlow SIG Build Community Builds Table](https://github.com/tensorflow/build#community-supported-tensorflow-builds).\n\n### Official Builds\n\nBuild Type                    | Status                                                                                                                                                                           | Artifacts\n----------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------\n**Linux CPU**                 | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-cc.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-cc.html)           | [PyPI](https://pypi.org/project/tf-nightly/)\n**Linux GPU**                 | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-gpu-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-gpu-py3.html) | [PyPI](https://pypi.org/project/tf-nightly-gpu/)\n**Linux XLA**                 | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-xla.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-xla.html)         | TBA\n**macOS**                     | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/macos-py2-cc.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/macos-py2-cc.html)     | [PyPI](https://pypi.org/project/tf-nightly/)\n**Windows CPU**               | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-cpu.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-cpu.html)       | [PyPI](https://pypi.org/project/tf-nightly/)\n**Windows GPU**               | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-gpu.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-gpu.html)       | [PyPI](https://pypi.org/project/tf-nightly-gpu/)\n**Android**                   | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/android.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/android.html)               | [Download](https://bintray.com/google/tensorflow/tensorflow/_latestVersion)\n**Raspberry Pi 0 and 1**      | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py3.html)           | [Py3](https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp34-none-linux_armv6l.whl)\n**Raspberry Pi 2 and 3**      | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py3.html)           | [Py3](https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp34-none-linux_armv7l.whl)\n**Libtensorflow MacOS CPU**   | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/macos/latest/macos_cpu_libtensorflow_binaries.tar.gz) [Official GCS](https://storage.googleapis.com/tensorflow/)\n**Libtensorflow Linux CPU**   | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/ubuntu_16/latest/cpu/ubuntu_cpu_libtensorflow_binaries.tar.gz) [Official GCS](https://storage.googleapis.com/tensorflow/)\n**Libtensorflow Linux GPU**   | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/ubuntu_16/latest/gpu/ubuntu_gpu_libtensorflow_binaries.tar.gz) [Official GCS](https://storage.googleapis.com/tensorflow/)\n**Libtensorflow Windows CPU** | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/windows/latest/cpu/windows_cpu_libtensorflow_binaries.tar.gz) [Official GCS](https://storage.googleapis.com/tensorflow/)\n**Libtensorflow Windows GPU** | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/windows/latest/gpu/windows_gpu_libtensorflow_binaries.tar.gz) [Official GCS](https://storage.googleapis.com/tensorflow/)\n\n## Resources\n\n*   [TensorFlow.org](https://www.tensorflow.org)\n*   [TensorFlow Tutorials](https://www.tensorflow.org/tutorials/)\n*   [TensorFlow Official Models](https://github.com/tensorflow/models/tree/master/official)\n*   [TensorFlow Examples](https://github.com/tensorflow/examples)\n*   [TensorFlow Codelabs](https://codelabs.developers.google.com/?cat=TensorFlow)\n*   [TensorFlow Blog](https://blog.tensorflow.org)\n*   [Learn ML with TensorFlow](https://www.tensorflow.org/resources/learn-ml)\n*   [TensorFlow Twitter](https://twitter.com/tensorflow)\n*   [TensorFlow YouTube](https://www.youtube.com/channel/UC0rqucBdTuFTjJiefW5t-IQ)\n*   [TensorFlow model optimization roadmap](https://www.tensorflow.org/model_optimization/guide/roadmap)\n*   [TensorFlow White Papers](https://www.tensorflow.org/about/bib)\n*   [TensorBoard Visualization Toolkit](https://github.com/tensorflow/tensorboard)\n*   [TensorFlow Code Search](https://cs.opensource.google/tensorflow/tensorflow)\n\nLearn more about the\n[TensorFlow Community](https://www.tensorflow.org/community) and how to\n[Contribute](https://www.tensorflow.org/community/contribute).\n\n## Courses\n\n* [Coursera](https://www.coursera.org/search?query=TensorFlow)\n* [Udacity](https://www.udacity.com/courses/all?search=TensorFlow)\n* [Edx](https://www.edx.org/search?q=TensorFlow)\n\n## License\n\n[Apache License 2.0](LICENSE)\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 155220641,
    "name": "transformers",
    "full_name": "huggingface/transformers",
    "description": "\ud83e\udd17 Transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training. ",
    "html_url": "https://github.com/huggingface/transformers",
    "clone_url": "https://github.com/huggingface/transformers.git",
    "owner_login": "huggingface",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/25720743?v=4",
    "stargazers_count": 147934,
    "watchers_count": 147934,
    "forks_count": 29918,
    "open_issues_count": 1938,
    "size": 344549,
    "language": "Python",
    "languages": {
      "Python": 69333515,
      "Cuda": 204021,
      "Dockerfile": 43058,
      "C++": 19093,
      "C": 7703,
      "Makefile": 4377,
      "Cython": 3635,
      "Shell": 1838,
      "Jsonnet": 937
    },
    "topics": [
      "audio",
      "deep-learning",
      "deepseek",
      "gemma",
      "glm",
      "hacktoberfest",
      "llm",
      "machine-learning",
      "model-hub",
      "natural-language-processing",
      "nlp",
      "pretrained-models",
      "python",
      "pytorch",
      "pytorch-transformers",
      "qwen",
      "speech-recognition",
      "transformer",
      "vlm"
    ],
    "license_name": "Apache License 2.0",
    "created_at": "2018-10-29T13:56:00+00:00",
    "updated_at": "2025-08-06T02:06:14+00:00",
    "pushed_at": "2025-08-05T23:48:07+00:00",
    "contributors_count": 100,
    "readme_length": 17297,
    "readme_content": "<!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n<p align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg\">\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\">\n    <img alt=\"Hugging Face Transformers Library\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\" width=\"352\" height=\"59\" style=\"max-width: 100%;\">\n  </picture>\n  <br/>\n  <br/>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://huggingface.com/models\"><img alt=\"Checkpoints on Hub\" src=\"https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen\"></a>\n    <a href=\"https://circleci.com/gh/huggingface/transformers\"><img alt=\"Build\" src=\"https://img.shields.io/circleci/build/github/huggingface/transformers/main\"></a>\n    <a href=\"https://github.com/huggingface/transformers/blob/main/LICENSE\"><img alt=\"GitHub\" src=\"https://img.shields.io/github/license/huggingface/transformers.svg?color=blue\"></a>\n    <a href=\"https://huggingface.co/docs/transformers/index\"><img alt=\"Documentation\" src=\"https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online\"></a>\n    <a href=\"https://github.com/huggingface/transformers/releases\"><img alt=\"GitHub release\" src=\"https://img.shields.io/github/release/huggingface/transformers.svg\"></a>\n    <a href=\"https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md\"><img alt=\"Contributor Covenant\" src=\"https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg\"></a>\n    <a href=\"https://zenodo.org/badge/latestdoi/155220641\"><img src=\"https://zenodo.org/badge/155220641.svg\" alt=\"DOI\"></a>\n</p>\n\n<h4 align=\"center\">\n    <p>\n        <b>English</b> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hans.md\">\u7b80\u4f53\u4e2d\u6587</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hant.md\">\u7e41\u9ad4\u4e2d\u6587</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ko.md\">\ud55c\uad6d\uc5b4</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_es.md\">Espa\u00f1ol</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ja.md\">\u65e5\u672c\u8a9e</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_hd.md\">\u0939\u093f\u0928\u094d\u0926\u0940</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ru.md\">\u0420\u0443\u0441\u0441\u043a\u0438\u0439</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_pt-br.md\">Portugu\u00eas</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_te.md\">\u0c24\u0c46\u0c32\u0c41\u0c17\u0c41</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_fr.md\">Fran\u00e7ais</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_de.md\">Deutsch</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_vi.md\">Ti\u1ebfng Vi\u1ec7t</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ar.md\">\u0627\u0644\u0639\u0631\u0628\u064a\u0629</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ur.md\">\u0627\u0631\u062f\u0648</a> |\n    </p>\n</h4>\n\n<h3 align=\"center\">\n    <p>State-of-the-art pretrained models for inference and training</p>\n</h3>\n\n<h3 align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/transformers_as_a_model_definition.png\"/>\n</h3>\n\n\nTransformers acts as the model-definition framework for state-of-the-art machine learning models in text, computer \nvision, audio, video, and multimodal model, for both inference and training. \n\nIt centralizes the model definition so that this definition is agreed upon across the ecosystem. `transformers` is the \npivot across frameworks: if a model definition is supported, it will be compatible with the majority of training \nframeworks (Axolotl, Unsloth, DeepSpeed, FSDP, PyTorch-Lightning, ...), inference engines (vLLM, SGLang, TGI, ...),\nand adjacent modeling libraries (llama.cpp, mlx, ...) which leverage the model definition from `transformers`.\n\nWe pledge to help support new state-of-the-art models and democratize their usage by having their model definition be\nsimple, customizable, and efficient.\n\nThere are over 1M+ Transformers [model checkpoints](https://huggingface.co/models?library=transformers&sort=trending) on the [Hugging Face Hub](https://huggingface.com/models) you can use.\n\nExplore the [Hub](https://huggingface.com/) today to find a model and use Transformers to help you get started right away.\n\n## Installation\n\nTransformers works with Python 3.9+ [PyTorch](https://pytorch.org/get-started/locally/) 2.1+, [TensorFlow](https://www.tensorflow.org/install/pip) 2.6+, and [Flax](https://flax.readthedocs.io/en/latest/) 0.4.1+.\n\nCreate and activate a virtual environment with [venv](https://docs.python.org/3/library/venv.html) or [uv](https://docs.astral.sh/uv/), a fast Rust-based Python package and project manager.\n\n```py\n# venv\npython -m venv .my-env\nsource .my-env/bin/activate\n# uv\nuv venv .my-env\nsource .my-env/bin/activate\n```\n\nInstall Transformers in your virtual environment.\n\n```py\n# pip\npip install \"transformers[torch]\"\n\n# uv\nuv pip install \"transformers[torch]\"\n```\n\nInstall Transformers from source if you want the latest changes in the library or are interested in contributing. However, the *latest* version may not be stable. Feel free to open an [issue](https://github.com/huggingface/transformers/issues) if you encounter an error.\n\n```shell\ngit clone https://github.com/huggingface/transformers.git\ncd transformers\n\n# pip\npip install .[torch]\n\n# uv\nuv pip install .[torch]\n```\n\n## Quickstart\n\nGet started with Transformers right away with the [Pipeline](https://huggingface.co/docs/transformers/pipeline_tutorial) API. The `Pipeline` is a high-level inference class that supports text, audio, vision, and multimodal tasks. It handles preprocessing the input and returns the appropriate output.\n\nInstantiate a pipeline and specify model to use for text generation. The model is downloaded and cached so you can easily reuse it again. Finally, pass some text to prompt the model.\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(task=\"text-generation\", model=\"Qwen/Qwen2.5-1.5B\")\npipeline(\"the secret to baking a really good cake is \")\n[{'generated_text': 'the secret to baking a really good cake is 1) to use the right ingredients and 2) to follow the recipe exactly. the recipe for the cake is as follows: 1 cup of sugar, 1 cup of flour, 1 cup of milk, 1 cup of butter, 1 cup of eggs, 1 cup of chocolate chips. if you want to make 2 cakes, how much sugar do you need? To make 2 cakes, you will need 2 cups of sugar.'}]\n```\n\nTo chat with a model, the usage pattern is the same. The only difference is you need to construct a chat history (the input to `Pipeline`) between you and the system.\n\n> [!TIP]\n> You can also chat with a model directly from the command line.\n> ```shell\n> transformers chat Qwen/Qwen2.5-0.5B-Instruct\n> ```\n\n```py\nimport torch\nfrom transformers import pipeline\n\nchat = [\n    {\"role\": \"system\", \"content\": \"You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986.\"},\n    {\"role\": \"user\", \"content\": \"Hey, can you tell me any fun things to do in New York?\"}\n]\n\npipeline = pipeline(task=\"text-generation\", model=\"meta-llama/Meta-Llama-3-8B-Instruct\", torch_dtype=torch.bfloat16, device_map=\"auto\")\nresponse = pipeline(chat, max_new_tokens=512)\nprint(response[0][\"generated_text\"][-1][\"content\"])\n```\n\nExpand the examples below to see how `Pipeline` works for different modalities and tasks.\n\n<details>\n<summary>Automatic speech recognition</summary>\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(task=\"automatic-speech-recognition\", model=\"openai/whisper-large-v3\")\npipeline(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}\n```\n\n</details>\n\n<details>\n<summary>Image classification</summary>\n\n<h3 align=\"center\">\n    <a><img src=\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\"></a>\n</h3>\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(task=\"image-classification\", model=\"facebook/dinov2-small-imagenet1k-1-layer\")\npipeline(\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\")\n[{'label': 'macaw', 'score': 0.997848391532898},\n {'label': 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n  'score': 0.0016551691805943847},\n {'label': 'lorikeet', 'score': 0.00018523589824326336},\n {'label': 'African grey, African gray, Psittacus erithacus',\n  'score': 7.85409429227002e-05},\n {'label': 'quail', 'score': 5.502637941390276e-05}]\n```\n\n</details>\n\n<details>\n<summary>Visual question answering</summary>\n\n\n<h3 align=\"center\">\n    <a><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg\"></a>\n</h3>\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(task=\"visual-question-answering\", model=\"Salesforce/blip-vqa-base\")\npipeline(\n    image=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg\",\n    question=\"What is in the image?\",\n)\n[{'answer': 'statue of liberty'}]\n```\n\n</details>\n\n## Why should I use Transformers?\n\n1. Easy-to-use state-of-the-art models:\n    - High performance on natural language understanding & generation, computer vision, audio, video, and multimodal tasks.\n    - Low barrier to entry for researchers, engineers, and developers.\n    - Few user-facing abstractions with just three classes to learn.\n    - A unified API for using all our pretrained models.\n\n1. Lower compute costs, smaller carbon footprint:\n    - Share trained models instead of training from scratch.\n    - Reduce compute time and production costs.\n    - Dozens of model architectures with 1M+ pretrained checkpoints across all modalities.\n\n1. Choose the right framework for every part of a models lifetime:\n    - Train state-of-the-art models in 3 lines of code.\n    - Move a single model between PyTorch/JAX/TF2.0 frameworks at will.\n    - Pick the right framework for training, evaluation, and production.\n\n1. Easily customize a model or an example to your needs:\n    - We provide examples for each architecture to reproduce the results published by its original authors.\n    - Model internals are exposed as consistently as possible.\n    - Model files can be used independently of the library for quick experiments.\n\n<a target=\"_blank\" href=\"https://huggingface.co/enterprise\">\n    <img alt=\"Hugging Face Enterprise Hub\" src=\"https://github.com/user-attachments/assets/247fb16d-d251-4583-96c4-d3d76dda4925\">\n</a><br>\n\n## Why shouldn't I use Transformers?\n\n- This library is not a modular toolbox of building blocks for neural nets. The code in the model files is not refactored with additional abstractions on purpose, so that researchers can quickly iterate on each of the models without diving into additional abstractions/files.\n- The training API is optimized to work with PyTorch models provided by Transformers. For generic machine learning loops, you should use another library like [Accelerate](https://huggingface.co/docs/accelerate).\n- The [example scripts](https://github.com/huggingface/transformers/tree/main/examples) are only *examples*. They may not necessarily work out-of-the-box on your specific use case and you'll need to adapt the code for it to work.\n\n## 100 projects using Transformers\n\nTransformers is more than a toolkit to use pretrained models, it's a community of projects built around it and the\nHugging Face Hub. We want Transformers to enable developers, researchers, students, professors, engineers, and anyone\nelse to build their dream projects.\n\nIn order to celebrate Transformers 100,000 stars, we wanted to put the spotlight on the\ncommunity with the [awesome-transformers](./awesome-transformers.md) page which lists 100\nincredible projects built with Transformers.\n\nIf you own or use a project that you believe should be part of the list, please open a PR to add it!\n\n## Example models\n\nYou can test most of our models directly on their [Hub model pages](https://huggingface.co/models).\n\nExpand each modality below to see a few example models for various use cases.\n\n<details>\n<summary>Audio</summary>\n\n- Audio classification with [Whisper](https://huggingface.co/openai/whisper-large-v3-turbo)\n- Automatic speech recognition with [Moonshine](https://huggingface.co/UsefulSensors/moonshine)\n- Keyword spotting with [Wav2Vec2](https://huggingface.co/superb/wav2vec2-base-superb-ks)\n- Speech to speech generation with [Moshi](https://huggingface.co/kyutai/moshiko-pytorch-bf16)\n- Text to audio with [MusicGen](https://huggingface.co/facebook/musicgen-large)\n- Text to speech with [Bark](https://huggingface.co/suno/bark)\n\n</details>\n\n<details>\n<summary>Computer vision</summary>\n\n- Automatic mask generation with [SAM](https://huggingface.co/facebook/sam-vit-base)\n- Depth estimation with [DepthPro](https://huggingface.co/apple/DepthPro-hf)\n- Image classification with [DINO v2](https://huggingface.co/facebook/dinov2-base)\n- Keypoint detection with [SuperPoint](https://huggingface.co/magic-leap-community/superpoint)\n- Keypoint matching with [SuperGlue](https://huggingface.co/magic-leap-community/superglue_outdoor)\n- Object detection with [RT-DETRv2](https://huggingface.co/PekingU/rtdetr_v2_r50vd)\n- Pose Estimation with [VitPose](https://huggingface.co/usyd-community/vitpose-base-simple)\n- Universal segmentation with [OneFormer](https://huggingface.co/shi-labs/oneformer_ade20k_swin_large)\n- Video classification with [VideoMAE](https://huggingface.co/MCG-NJU/videomae-large)\n\n</details>\n\n<details>\n<summary>Multimodal</summary>\n\n- Audio or text to text with [Qwen2-Audio](https://huggingface.co/Qwen/Qwen2-Audio-7B)\n- Document question answering with [LayoutLMv3](https://huggingface.co/microsoft/layoutlmv3-base)\n- Image or text to text with [Qwen-VL](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct)\n- Image captioning [BLIP-2](https://huggingface.co/Salesforce/blip2-opt-2.7b)\n- OCR-based document understanding with [GOT-OCR2](https://huggingface.co/stepfun-ai/GOT-OCR-2.0-hf)\n- Table question answering with [TAPAS](https://huggingface.co/google/tapas-base)\n- Unified multimodal understanding and generation with [Emu3](https://huggingface.co/BAAI/Emu3-Gen)\n- Vision to text with [Llava-OneVision](https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf)\n- Visual question answering with [Llava](https://huggingface.co/llava-hf/llava-1.5-7b-hf)\n- Visual referring expression segmentation with [Kosmos-2](https://huggingface.co/microsoft/kosmos-2-patch14-224)\n\n</details>\n\n<details>\n<summary>NLP</summary>\n\n- Masked word completion with [ModernBERT](https://huggingface.co/answerdotai/ModernBERT-base)\n- Named entity recognition with [Gemma](https://huggingface.co/google/gemma-2-2b)\n- Question answering with [Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)\n- Summarization with [BART](https://huggingface.co/facebook/bart-large-cnn)\n- Translation with [T5](https://huggingface.co/google-t5/t5-base)\n- Text generation with [Llama](https://huggingface.co/meta-llama/Llama-3.2-1B)\n- Text classification with [Qwen](https://huggingface.co/Qwen/Qwen2.5-0.5B)\n\n</details>\n\n## Citation\n\nWe now have a [paper](https://www.aclweb.org/anthology/2020.emnlp-demos.6/) you can cite for the \ud83e\udd17 Transformers library:\n```bibtex\n@inproceedings{wolf-etal-2020-transformers,\n    title = \"Transformers: State-of-the-Art Natural Language Processing\",\n    author = \"Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R\u00e9mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\",\n    month = oct,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.emnlp-demos.6\",\n    pages = \"38--45\"\n}\n```\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 65600975,
    "name": "pytorch",
    "full_name": "pytorch/pytorch",
    "description": "Tensors and Dynamic neural networks in Python with strong GPU acceleration",
    "html_url": "https://github.com/pytorch/pytorch",
    "clone_url": "https://github.com/pytorch/pytorch.git",
    "owner_login": "pytorch",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/21003710?v=4",
    "stargazers_count": 92101,
    "watchers_count": 92101,
    "forks_count": 24865,
    "open_issues_count": 16712,
    "size": 1088908,
    "language": "Python",
    "languages": {
      "Python": 73723327,
      "C++": 42406155,
      "Cuda": 3694703,
      "C": 1829643,
      "Objective-C++": 1391588,
      "CMake": 822366,
      "Shell": 444633,
      "Assembly": 336439,
      "Starlark": 330132,
      "Metal": 307974,
      "HIP": 287597,
      "GLSL": 204578,
      "Jupyter Notebook": 186191,
      "JavaScript": 92859,
      "Java": 87332,
      "Batchfile": 78530,
      "Objective-C": 58643,
      "Dockerfile": 34037,
      "Makefile": 12990,
      "PowerShell": 7509,
      "Thrift": 7059,
      "GDB": 653,
      "Linker Script": 473,
      "HTML": 384,
      "Smarty": 376,
      "Vim Script": 154
    },
    "topics": [
      "autograd",
      "deep-learning",
      "gpu",
      "machine-learning",
      "neural-network",
      "numpy",
      "python",
      "tensor"
    ],
    "license_name": "Other",
    "created_at": "2016-08-13T05:26:41+00:00",
    "updated_at": "2025-08-06T01:40:03+00:00",
    "pushed_at": "2025-08-06T02:11:00+00:00",
    "contributors_count": 100,
    "readme_length": 27015,
    "readme_content": "![PyTorch Logo](https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/pytorch-logo-dark.png)\n\n--------------------------------------------------------------------------------\n\nPyTorch is a Python package that provides two high-level features:\n- Tensor computation (like NumPy) with strong GPU acceleration\n- Deep neural networks built on a tape-based autograd system\n\nYou can reuse your favorite Python packages such as NumPy, SciPy, and Cython to extend PyTorch when needed.\n\nOur trunk health (Continuous Integration signals) can be found at [hud.pytorch.org](https://hud.pytorch.org/ci/pytorch/pytorch/main).\n\n<!-- toc -->\n\n- [More About PyTorch](#more-about-pytorch)\n  - [A GPU-Ready Tensor Library](#a-gpu-ready-tensor-library)\n  - [Dynamic Neural Networks: Tape-Based Autograd](#dynamic-neural-networks-tape-based-autograd)\n  - [Python First](#python-first)\n  - [Imperative Experiences](#imperative-experiences)\n  - [Fast and Lean](#fast-and-lean)\n  - [Extensions Without Pain](#extensions-without-pain)\n- [Installation](#installation)\n  - [Binaries](#binaries)\n    - [NVIDIA Jetson Platforms](#nvidia-jetson-platforms)\n  - [From Source](#from-source)\n    - [Prerequisites](#prerequisites)\n      - [NVIDIA CUDA Support](#nvidia-cuda-support)\n      - [AMD ROCm Support](#amd-rocm-support)\n      - [Intel GPU Support](#intel-gpu-support)\n    - [Get the PyTorch Source](#get-the-pytorch-source)\n    - [Install Dependencies](#install-dependencies)\n    - [Install PyTorch](#install-pytorch)\n      - [Adjust Build Options (Optional)](#adjust-build-options-optional)\n  - [Docker Image](#docker-image)\n    - [Using pre-built images](#using-pre-built-images)\n    - [Building the image yourself](#building-the-image-yourself)\n  - [Building the Documentation](#building-the-documentation)\n    - [Building a PDF](#building-a-pdf)\n  - [Previous Versions](#previous-versions)\n- [Getting Started](#getting-started)\n- [Resources](#resources)\n- [Communication](#communication)\n- [Releases and Contributing](#releases-and-contributing)\n- [The Team](#the-team)\n- [License](#license)\n\n<!-- tocstop -->\n\n## More About PyTorch\n\n[Learn the basics of PyTorch](https://pytorch.org/tutorials/beginner/basics/intro.html)\n\nAt a granular level, PyTorch is a library that consists of the following components:\n\n| Component | Description |\n| ---- | --- |\n| [**torch**](https://pytorch.org/docs/stable/torch.html) | A Tensor library like NumPy, with strong GPU support |\n| [**torch.autograd**](https://pytorch.org/docs/stable/autograd.html) | A tape-based automatic differentiation library that supports all differentiable Tensor operations in torch |\n| [**torch.jit**](https://pytorch.org/docs/stable/jit.html) | A compilation stack (TorchScript) to create serializable and optimizable models from PyTorch code  |\n| [**torch.nn**](https://pytorch.org/docs/stable/nn.html) | A neural networks library deeply integrated with autograd designed for maximum flexibility |\n| [**torch.multiprocessing**](https://pytorch.org/docs/stable/multiprocessing.html) | Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training |\n| [**torch.utils**](https://pytorch.org/docs/stable/data.html) | DataLoader and other utility functions for convenience |\n\nUsually, PyTorch is used either as:\n\n- A replacement for NumPy to use the power of GPUs.\n- A deep learning research platform that provides maximum flexibility and speed.\n\nElaborating Further:\n\n### A GPU-Ready Tensor Library\n\nIf you use NumPy, then you have used Tensors (a.k.a. ndarray).\n\n![Tensor illustration](./docs/source/_static/img/tensor_illustration.png)\n\nPyTorch provides Tensors that can live either on the CPU or the GPU and accelerates the\ncomputation by a huge amount.\n\nWe provide a wide variety of tensor routines to accelerate and fit your scientific computation needs\nsuch as slicing, indexing, mathematical operations, linear algebra, reductions.\nAnd they are fast!\n\n### Dynamic Neural Networks: Tape-Based Autograd\n\nPyTorch has a unique way of building neural networks: using and replaying a tape recorder.\n\nMost frameworks such as TensorFlow, Theano, Caffe, and CNTK have a static view of the world.\nOne has to build a neural network and reuse the same structure again and again.\nChanging the way the network behaves means that one has to start from scratch.\n\nWith PyTorch, we use a technique called reverse-mode auto-differentiation, which allows you to\nchange the way your network behaves arbitrarily with zero lag or overhead. Our inspiration comes\nfrom several research papers on this topic, as well as current and past work such as\n[torch-autograd](https://github.com/twitter/torch-autograd),\n[autograd](https://github.com/HIPS/autograd),\n[Chainer](https://chainer.org), etc.\n\nWhile this technique is not unique to PyTorch, it's one of the fastest implementations of it to date.\nYou get the best of speed and flexibility for your crazy research.\n\n![Dynamic graph](https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/dynamic_graph.gif)\n\n### Python First\n\nPyTorch is not a Python binding into a monolithic C++ framework.\nIt is built to be deeply integrated into Python.\nYou can use it naturally like you would use [NumPy](https://www.numpy.org/) / [SciPy](https://www.scipy.org/) / [scikit-learn](https://scikit-learn.org) etc.\nYou can write your new neural network layers in Python itself, using your favorite libraries\nand use packages such as [Cython](https://cython.org/) and [Numba](http://numba.pydata.org/).\nOur goal is to not reinvent the wheel where appropriate.\n\n### Imperative Experiences\n\nPyTorch is designed to be intuitive, linear in thought, and easy to use.\nWhen you execute a line of code, it gets executed. There isn't an asynchronous view of the world.\nWhen you drop into a debugger or receive error messages and stack traces, understanding them is straightforward.\nThe stack trace points to exactly where your code was defined.\nWe hope you never spend hours debugging your code because of bad stack traces or asynchronous and opaque execution engines.\n\n### Fast and Lean\n\nPyTorch has minimal framework overhead. We integrate acceleration libraries\nsuch as [Intel MKL](https://software.intel.com/mkl) and NVIDIA ([cuDNN](https://developer.nvidia.com/cudnn), [NCCL](https://developer.nvidia.com/nccl)) to maximize speed.\nAt the core, its CPU and GPU Tensor and neural network backends\nare mature and have been tested for years.\n\nHence, PyTorch is quite fast \u2014 whether you run small or large neural networks.\n\nThe memory usage in PyTorch is extremely efficient compared to Torch or some of the alternatives.\nWe've written custom memory allocators for the GPU to make sure that\nyour deep learning models are maximally memory efficient.\nThis enables you to train bigger deep learning models than before.\n\n### Extensions Without Pain\n\nWriting new neural network modules, or interfacing with PyTorch's Tensor API was designed to be straightforward\nand with minimal abstractions.\n\nYou can write new neural network layers in Python using the torch API\n[or your favorite NumPy-based libraries such as SciPy](https://pytorch.org/tutorials/advanced/numpy_extensions_tutorial.html).\n\nIf you want to write your layers in C/C++, we provide a convenient extension API that is efficient and with minimal boilerplate.\nNo wrapper code needs to be written. You can see [a tutorial here](https://pytorch.org/tutorials/advanced/cpp_extension.html) and [an example here](https://github.com/pytorch/extension-cpp).\n\n\n## Installation\n\n### Binaries\nCommands to install binaries via Conda or pip wheels are on our website: [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)\n\n\n#### NVIDIA Jetson Platforms\n\nPython wheels for NVIDIA's Jetson Nano, Jetson TX1/TX2, Jetson Xavier NX/AGX, and Jetson AGX Orin are provided [here](https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-10-now-available/72048) and the L4T container is published [here](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/l4t-pytorch)\n\nThey require JetPack 4.2 and above, and [@dusty-nv](https://github.com/dusty-nv) and [@ptrblck](https://github.com/ptrblck) are maintaining them.\n\n\n### From Source\n\n#### Prerequisites\nIf you are installing from source, you will need:\n- Python 3.9 or later\n- A compiler that fully supports C++17, such as clang or gcc (gcc 9.4.0 or newer is required, on Linux)\n- Visual Studio or Visual Studio Build Tool (Windows only)\n\n\\* PyTorch CI uses Visual C++ BuildTools, which come with Visual Studio Enterprise,\nProfessional, or Community Editions. You can also install the build tools from\nhttps://visualstudio.microsoft.com/visual-cpp-build-tools/. The build tools *do not*\ncome with Visual Studio Code by default.\n\nAn example of environment setup is shown below:\n\n* Linux:\n\n```bash\n$ source <CONDA_INSTALL_DIR>/bin/activate\n$ conda create -y -n <CONDA_NAME>\n$ conda activate <CONDA_NAME>\n```\n\n* Windows:\n\n```bash\n$ source <CONDA_INSTALL_DIR>\\Scripts\\activate.bat\n$ conda create -y -n <CONDA_NAME>\n$ conda activate <CONDA_NAME>\n$ call \"C:\\Program Files\\Microsoft Visual Studio\\<VERSION>\\Community\\VC\\Auxiliary\\Build\\vcvarsall.bat\" x64\n```\n\nA conda environment is not required.  You can also do a PyTorch build in a\nstandard virtual environment, e.g., created with tools like `uv`, provided\nyour system has installed all the necessary dependencies unavailable as pip\npackages (e.g., CUDA, MKL.)\n\n##### NVIDIA CUDA Support\nIf you want to compile with CUDA support, [select a supported version of CUDA from our support matrix](https://pytorch.org/get-started/locally/), then install the following:\n- [NVIDIA CUDA](https://developer.nvidia.com/cuda-downloads)\n- [NVIDIA cuDNN](https://developer.nvidia.com/cudnn) v8.5 or above\n- [Compiler](https://gist.github.com/ax3l/9489132) compatible with CUDA\n\nNote: You could refer to the [cuDNN Support Matrix](https://docs.nvidia.com/deeplearning/cudnn/backend/latest/reference/support-matrix.html) for cuDNN versions with the various supported CUDA, CUDA driver, and NVIDIA hardware.\n\nIf you want to disable CUDA support, export the environment variable `USE_CUDA=0`.\nOther potentially useful environment variables may be found in `setup.py`.  If\nCUDA is installed in a non-standard location, set PATH so that the nvcc you\nwant to use can be found (e.g., `export PATH=/usr/local/cuda-12.8/bin:$PATH`).\n\nIf you are building for NVIDIA's Jetson platforms (Jetson Nano, TX1, TX2, AGX Xavier), Instructions to install PyTorch for Jetson Nano are [available here](https://devtalk.nvidia.com/default/topic/1049071/jetson-nano/pytorch-for-jetson-nano/)\n\n##### AMD ROCm Support\nIf you want to compile with ROCm support, install\n- [AMD ROCm](https://rocm.docs.amd.com/en/latest/deploy/linux/quick_start.html) 4.0 and above installation\n- ROCm is currently supported only for Linux systems.\n\nBy default the build system expects ROCm to be installed in `/opt/rocm`. If ROCm is installed in a different directory, the `ROCM_PATH` environment variable must be set to the ROCm installation directory. The build system automatically detects the AMD GPU architecture. Optionally, the AMD GPU architecture can be explicitly set with the `PYTORCH_ROCM_ARCH` environment variable [AMD GPU architecture](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html#supported-gpus)\n\nIf you want to disable ROCm support, export the environment variable `USE_ROCM=0`.\nOther potentially useful environment variables may be found in `setup.py`.\n\n##### Intel GPU Support\nIf you want to compile with Intel GPU support, follow these\n- [PyTorch Prerequisites for Intel GPUs](https://www.intel.com/content/www/us/en/developer/articles/tool/pytorch-prerequisites-for-intel-gpus.html) instructions.\n- Intel GPU is supported for Linux and Windows.\n\nIf you want to disable Intel GPU support, export the environment variable `USE_XPU=0`.\nOther potentially useful environment variables may be found in `setup.py`.\n\n#### Get the PyTorch Source\n\n```bash\ngit clone https://github.com/pytorch/pytorch\ncd pytorch\n# if you are updating an existing checkout\ngit submodule sync\ngit submodule update --init --recursive\n```\n\n#### Install Dependencies\n\n**Common**\n\n```bash\nconda install cmake ninja\n# Run this command from the PyTorch directory after cloning the source code using the \u201cGet the PyTorch Source\u201c section below\npip install -r requirements.txt\n```\n\n**On Linux**\n\n```bash\npip install mkl-static mkl-include\n# CUDA only: Add LAPACK support for the GPU if needed\n# magma installation: run with active conda environment. specify CUDA version to install\n.ci/docker/common/install_magma_conda.sh 12.4\n\n# (optional) If using torch.compile with inductor/triton, install the matching version of triton\n# Run from the pytorch directory after cloning\n# For Intel GPU support, please explicitly `export USE_XPU=1` before running command.\nmake triton\n```\n\n**On MacOS**\n\n```bash\n# Add this package on intel x86 processor machines only\npip install mkl-static mkl-include\n# Add these packages if torch.distributed is needed\nconda install pkg-config libuv\n```\n\n**On Windows**\n\n```bash\npip install mkl-static mkl-include\n# Add these packages if torch.distributed is needed.\n# Distributed package support on Windows is a prototype feature and is subject to changes.\nconda install -c conda-forge libuv\n```\n\n#### Install PyTorch\n\n**On Linux**\n\nIf you're compiling for AMD ROCm then first run this command:\n\n```bash\n# Only run this if you're compiling for ROCm\npython tools/amd_build/build_amd.py\n```\n\nInstall PyTorch\n\n```bash\nexport CMAKE_PREFIX_PATH=\"${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}\"\npython -m pip install --no-build-isolation -v -e .\n```\n\n**On macOS**\n\n```bash\npython -m pip install --no-build-isolation -v -e .\n```\n\n**On Windows**\n\nIf you want to build legacy python code, please refer to [Building on legacy code and CUDA](https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#building-on-legacy-code-and-cuda)\n\n**CPU-only builds**\n\nIn this mode PyTorch computations will run on your CPU, not your GPU.\n\n```cmd\npython -m pip install --no-build-isolation -v -e .\n```\n\nNote on OpenMP: The desired OpenMP implementation is Intel OpenMP (iomp). In order to link against iomp, you'll need to manually download the library and set up the building environment by tweaking `CMAKE_INCLUDE_PATH` and `LIB`. The instruction [here](https://github.com/pytorch/pytorch/blob/main/docs/source/notes/windows.rst#building-from-source) is an example for setting up both MKL and Intel OpenMP. Without these configurations for CMake, Microsoft Visual C OpenMP runtime (vcomp) will be used.\n\n**CUDA based build**\n\nIn this mode PyTorch computations will leverage your GPU via CUDA for faster number crunching\n\n[NVTX](https://docs.nvidia.com/gameworks/content/gameworkslibrary/nvtx/nvidia_tools_extension_library_nvtx.htm) is needed to build Pytorch with CUDA.\nNVTX is a part of CUDA distributive, where it is called \"Nsight Compute\". To install it onto an already installed CUDA run CUDA installation once again and check the corresponding checkbox.\nMake sure that CUDA with Nsight Compute is installed after Visual Studio.\n\nCurrently, VS 2017 / 2019, and Ninja are supported as the generator of CMake. If `ninja.exe` is detected in `PATH`, then Ninja will be used as the default generator, otherwise, it will use VS 2017 / 2019.\n<br/> If Ninja is selected as the generator, the latest MSVC will get selected as the underlying toolchain.\n\nAdditional libraries such as\n[Magma](https://developer.nvidia.com/magma), [oneDNN, a.k.a. MKLDNN or DNNL](https://github.com/oneapi-src/oneDNN), and [Sccache](https://github.com/mozilla/sccache) are often needed. Please refer to the [installation-helper](https://github.com/pytorch/pytorch/tree/main/.ci/pytorch/win-test-helpers/installation-helpers) to install them.\n\nYou can refer to the [build_pytorch.bat](https://github.com/pytorch/pytorch/blob/main/.ci/pytorch/win-test-helpers/build_pytorch.bat) script for some other environment variables configurations\n\n```cmd\ncmd\n\n:: Set the environment variables after you have downloaded and unzipped the mkl package,\n:: else CMake would throw an error as `Could NOT find OpenMP`.\nset CMAKE_INCLUDE_PATH={Your directory}\\mkl\\include\nset LIB={Your directory}\\mkl\\lib;%LIB%\n\n:: Read the content in the previous section carefully before you proceed.\n:: [Optional] If you want to override the underlying toolset used by Ninja and Visual Studio with CUDA, please run the following script block.\n:: \"Visual Studio 2019 Developer Command Prompt\" will be run automatically.\n:: Make sure you have CMake >= 3.12 before you do this when you use the Visual Studio generator.\nset CMAKE_GENERATOR_TOOLSET_VERSION=14.27\nset DISTUTILS_USE_SDK=1\nfor /f \"usebackq tokens=*\" %i in (`\"%ProgramFiles(x86)%\\Microsoft Visual Studio\\Installer\\vswhere.exe\" -version [15^,17^) -products * -latest -property installationPath`) do call \"%i\\VC\\Auxiliary\\Build\\vcvarsall.bat\" x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%\n\n:: [Optional] If you want to override the CUDA host compiler\nset CUDAHOSTCXX=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.27.29110\\bin\\HostX64\\x64\\cl.exe\n\npython -m pip install --no-build-isolation -v -e .\n```\n\n**Intel GPU builds**\n\nIn this mode PyTorch with Intel GPU support will be built.\n\nPlease make sure [the common prerequisites](#prerequisites) as well as [the prerequisites for Intel GPU](#intel-gpu-support) are properly installed and the environment variables are configured prior to starting the build. For build tool support, `Visual Studio 2022` is required.\n\nThen PyTorch can be built with the command:\n\n```cmd\n:: CMD Commands:\n:: Set the CMAKE_PREFIX_PATH to help find corresponding packages\n:: %CONDA_PREFIX% only works after `conda activate custom_env`\n\nif defined CMAKE_PREFIX_PATH (\n    set \"CMAKE_PREFIX_PATH=%CONDA_PREFIX%\\Library;%CMAKE_PREFIX_PATH%\"\n) else (\n    set \"CMAKE_PREFIX_PATH=%CONDA_PREFIX%\\Library\"\n)\n\npython -m pip install --no-build-isolation -v -e .\n```\n\n##### Adjust Build Options (Optional)\n\nYou can adjust the configuration of cmake variables optionally (without building first), by doing\nthe following. For example, adjusting the pre-detected directories for CuDNN or BLAS can be done\nwith such a step.\n\nOn Linux\n\n```bash\nexport CMAKE_PREFIX_PATH=\"${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}\"\nCMAKE_ONLY=1 python setup.py build\nccmake build  # or cmake-gui build\n```\n\nOn macOS\n\n```bash\nexport CMAKE_PREFIX_PATH=\"${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}\"\nMACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ CMAKE_ONLY=1 python setup.py build\nccmake build  # or cmake-gui build\n```\n\n### Docker Image\n\n#### Using pre-built images\n\nYou can also pull a pre-built docker image from Docker Hub and run with docker v19.03+\n\n```bash\ndocker run --gpus all --rm -ti --ipc=host pytorch/pytorch:latest\n```\n\nPlease note that PyTorch uses shared memory to share data between processes, so if torch multiprocessing is used (e.g.\nfor multithreaded data loaders) the default shared memory segment size that container runs with is not enough, and you\nshould increase shared memory size either with `--ipc=host` or `--shm-size` command line options to `nvidia-docker run`.\n\n#### Building the image yourself\n\n**NOTE:** Must be built with a docker version > 18.06\n\nThe `Dockerfile` is supplied to build images with CUDA 11.1 support and cuDNN v8.\nYou can pass `PYTHON_VERSION=x.y` make variable to specify which Python version is to be used by Miniconda, or leave it\nunset to use the default.\n\n```bash\nmake -f docker.Makefile\n# images are tagged as docker.io/${your_docker_username}/pytorch\n```\n\nYou can also pass the `CMAKE_VARS=\"...\"` environment variable to specify additional CMake variables to be passed to CMake during the build.\nSee [setup.py](./setup.py) for the list of available variables.\n\n```bash\nmake -f docker.Makefile\n```\n\n### Building the Documentation\n\nTo build documentation in various formats, you will need [Sphinx](http://www.sphinx-doc.org)\nand the pytorch_sphinx_theme2.\n\nBefore you build the documentation locally, ensure `torch` is\ninstalled in your environment. For small fixes, you can install the\nnightly version as described in [Getting Started](https://pytorch.org/get-started/locally/).\n\nFor more complex fixes, such as adding a new module and docstrings for\nthe new module, you might need to install torch [from source](#from-source).\nSee [Docstring Guidelines](https://github.com/pytorch/pytorch/wiki/Docstring-Guidelines)\nfor docstring conventions.\n\n```bash\ncd docs/\npip install -r requirements.txt\nmake html\nmake serve\n```\n\nRun `make` to get a list of all available output formats.\n\nIf you get a katex error run `npm install katex`.  If it persists, try\n`npm install -g katex`\n\n> [!NOTE]\n> If you installed `nodejs` with a different package manager (e.g.,\n> `conda`) then `npm` will probably install a version of `katex` that is not\n> compatible with your version of `nodejs` and doc builds will fail.\n> A combination of versions that is known to work is `node@6.13.1` and\n> `katex@0.13.18`. To install the latter with `npm` you can run\n> ```npm install -g katex@0.13.18```\n\n> [!NOTE]\n> If you see a numpy incompatibility error, run:\n> ```\n> pip install 'numpy<2'\n> ```\n\nWhen you make changes to the dependencies run by CI, edit the\n`.ci/docker/requirements-docs.txt` file.\n\n#### Building a PDF\n\nTo compile a PDF of all PyTorch documentation, ensure you have\n`texlive` and LaTeX installed. On macOS, you can install them using:\n\n```\nbrew install --cask mactex\n```\n\nTo create the PDF:\n\n1. Run:\n\n   ```\n   make latexpdf\n   ```\n\n   This will generate the necessary files in the `build/latex` directory.\n\n2. Navigate to this directory and execute:\n\n   ```\n   make LATEXOPTS=\"-interaction=nonstopmode\"\n   ```\n\n   This will produce a `pytorch.pdf` with the desired content. Run this\n   command one more time so that it generates the correct table\n   of contents and index.\n\n> [!NOTE]\n> To view the Table of Contents, switch to the **Table of Contents**\n> view in your PDF viewer.\n\n\n### Previous Versions\n\nInstallation instructions and binaries for previous PyTorch versions may be found\non [our website](https://pytorch.org/get-started/previous-versions).\n\n\n## Getting Started\n\nThree pointers to get you started:\n- [Tutorials: get you started with understanding and using PyTorch](https://pytorch.org/tutorials/)\n- [Examples: easy to understand PyTorch code across all domains](https://github.com/pytorch/examples)\n- [The API Reference](https://pytorch.org/docs/)\n- [Glossary](https://github.com/pytorch/pytorch/blob/main/GLOSSARY.md)\n\n## Resources\n\n* [PyTorch.org](https://pytorch.org/)\n* [PyTorch Tutorials](https://pytorch.org/tutorials/)\n* [PyTorch Examples](https://github.com/pytorch/examples)\n* [PyTorch Models](https://pytorch.org/hub/)\n* [Intro to Deep Learning with PyTorch from Udacity](https://www.udacity.com/course/deep-learning-pytorch--ud188)\n* [Intro to Machine Learning with PyTorch from Udacity](https://www.udacity.com/course/intro-to-machine-learning-nanodegree--nd229)\n* [Deep Neural Networks with PyTorch from Coursera](https://www.coursera.org/learn/deep-neural-networks-with-pytorch)\n* [PyTorch Twitter](https://twitter.com/PyTorch)\n* [PyTorch Blog](https://pytorch.org/blog/)\n* [PyTorch YouTube](https://www.youtube.com/channel/UCWXI5YeOsh03QvJ59PMaXFw)\n\n## Communication\n* Forums: Discuss implementations, research, etc. https://discuss.pytorch.org\n* GitHub Issues: Bug reports, feature requests, install issues, RFCs, thoughts, etc.\n* Slack: The [PyTorch Slack](https://pytorch.slack.com/) hosts a primary audience of moderate to experienced PyTorch users and developers for general chat, online discussions, collaboration, etc. If you are a beginner looking for help, the primary medium is [PyTorch Forums](https://discuss.pytorch.org). If you need a slack invite, please fill this form: https://goo.gl/forms/PP1AGvNHpSaJP8to1\n* Newsletter: No-noise, a one-way email newsletter with important announcements about PyTorch. You can sign-up here: https://eepurl.com/cbG0rv\n* Facebook Page: Important announcements about PyTorch. https://www.facebook.com/pytorch\n* For brand guidelines, please visit our website at [pytorch.org](https://pytorch.org/)\n\n## Releases and Contributing\n\nTypically, PyTorch has three minor releases a year. Please let us know if you encounter a bug by [filing an issue](https://github.com/pytorch/pytorch/issues).\n\nWe appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion.\n\nIf you plan to contribute new features, utility functions, or extensions to the core, please first open an issue and discuss the feature with us.\nSending a PR without discussion might end up resulting in a rejected PR because we might be taking the core in a different direction than you might be aware of.\n\nTo learn more about making a contribution to Pytorch, please see our [Contribution page](CONTRIBUTING.md). For more information about PyTorch releases, see [Release page](RELEASE.md).\n\n## The Team\n\nPyTorch is a community-driven project with several skillful engineers and researchers contributing to it.\n\nPyTorch is currently maintained by [Soumith Chintala](http://soumith.ch), [Gregory Chanan](https://github.com/gchanan), [Dmytro Dzhulgakov](https://github.com/dzhulgakov), [Edward Yang](https://github.com/ezyang), and [Nikita Shulga](https://github.com/malfet) with major contributions coming from hundreds of talented individuals in various forms and means.\nA non-exhaustive but growing list needs to mention: [Trevor Killeen](https://github.com/killeent), [Sasank Chilamkurthy](https://github.com/chsasank), [Sergey Zagoruyko](https://github.com/szagoruyko), [Adam Lerer](https://github.com/adamlerer), [Francisco Massa](https://github.com/fmassa), [Alykhan Tejani](https://github.com/alykhantejani), [Luca Antiga](https://github.com/lantiga), [Alban Desmaison](https://github.com/albanD), [Andreas Koepf](https://github.com/andreaskoepf), [James Bradbury](https://github.com/jekbradbury), [Zeming Lin](https://github.com/ebetica), [Yuandong Tian](https://github.com/yuandong-tian), [Guillaume Lample](https://github.com/glample), [Marat Dukhan](https://github.com/Maratyszcza), [Natalia Gimelshein](https://github.com/ngimel), [Christian Sarofeen](https://github.com/csarofeen), [Martin Raison](https://github.com/martinraison), [Edward Yang](https://github.com/ezyang), [Zachary Devito](https://github.com/zdevito). <!-- codespell:ignore -->\n\nNote: This project is unrelated to [hughperkins/pytorch](https://github.com/hughperkins/pytorch) with the same name. Hugh is a valuable contributor to the Torch community and has helped with many things Torch and PyTorch.\n\n## License\n\nPyTorch has a BSD-style license, as found in the [LICENSE](LICENSE) file.\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 343965132,
    "name": "ML-For-Beginners",
    "full_name": "microsoft/ML-For-Beginners",
    "description": "12 weeks, 26 lessons, 52 quizzes, classic Machine Learning for all",
    "html_url": "https://github.com/microsoft/ML-For-Beginners",
    "clone_url": "https://github.com/microsoft/ML-For-Beginners.git",
    "owner_login": "microsoft",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/6154722?v=4",
    "stargazers_count": 76264,
    "watchers_count": 76264,
    "forks_count": 16953,
    "open_issues_count": 13,
    "size": 749196,
    "language": "HTML",
    "languages": {
      "HTML": 69312552,
      "Jupyter Notebook": 5468130,
      "Python": 54534,
      "Vue": 3931,
      "JavaScript": 1960,
      "Dockerfile": 1371,
      "CSS": 394
    },
    "topics": [
      "data-science",
      "education",
      "machine-learning",
      "machine-learning-algorithms",
      "machinelearning",
      "machinelearning-python",
      "microsoft-for-beginners",
      "ml",
      "python",
      "r",
      "scikit-learn",
      "scikit-learn-python"
    ],
    "license_name": "MIT License",
    "created_at": "2021-03-03T01:34:05+00:00",
    "updated_at": "2025-08-06T02:10:08+00:00",
    "pushed_at": "2025-08-05T08:45:12+00:00",
    "contributors_count": 100,
    "readme_length": 23028,
    "readme_content": "[![GitHub license](https://img.shields.io/github/license/microsoft/ML-For-Beginners.svg)](https://github.com/microsoft/ML-For-Beginners/blob/master/LICENSE)\n[![GitHub contributors](https://img.shields.io/github/contributors/microsoft/ML-For-Beginners.svg)](https://GitHub.com/microsoft/ML-For-Beginners/graphs/contributors/)\n[![GitHub issues](https://img.shields.io/github/issues/microsoft/ML-For-Beginners.svg)](https://GitHub.com/microsoft/ML-For-Beginners/issues/)\n[![GitHub pull-requests](https://img.shields.io/github/issues-pr/microsoft/ML-For-Beginners.svg)](https://GitHub.com/microsoft/ML-For-Beginners/pulls/)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)\n\n[![GitHub watchers](https://img.shields.io/github/watchers/microsoft/ML-For-Beginners.svg?style=social&label=Watch)](https://GitHub.com/microsoft/ML-For-Beginners/watchers/)\n[![GitHub forks](https://img.shields.io/github/forks/microsoft/ML-For-Beginners.svg?style=social&label=Fork)](https://GitHub.com/microsoft/ML-For-Beginners/network/)\n[![GitHub stars](https://img.shields.io/github/stars/microsoft/ML-For-Beginners.svg?style=social&label=Star)](https://GitHub.com/microsoft/ML-For-Beginners/stargazers/)\n\n[![](https://dcbadge.vercel.app/api/server/ByRwuEEgH4)](https://discord.gg/zxKYvhSnVp?WT.mc_id=academic-000002-leestott)\n\n# Machine Learning for Beginners - A Curriculum\n\n> \ud83c\udf0d Travel around the world as we explore Machine Learning by means of world cultures \ud83c\udf0d\n\nCloud Advocates at Microsoft are pleased to offer a 12-week, 26-lesson curriculum all about **Machine Learning**. In this curriculum, you will learn about what is sometimes called **classic machine learning**, using primarily Scikit-learn as a library and avoiding deep learning, which is covered in our [AI for Beginners' curriculum](https://aka.ms/ai4beginners). Pair these lessons with our ['Data Science for Beginners' curriculum](https://aka.ms/ds4beginners), as well!\n\nTravel with us around the world as we apply these classic techniques to data from many areas of the world. Each lesson includes pre- and post-lesson quizzes, written instructions to complete the lesson, a solution, an assignment, and more. Our project-based pedagogy allows you to learn while building, a proven way for new skills to 'stick'.\n\n**\u270d\ufe0f Hearty thanks to our authors** Jen Looper, Stephen Howell, Francesca Lazzeri, Tomomi Imura, Cassie Breviu, Dmitry Soshnikov, Chris Noring, Anirban Mukherjee, Ornella Altunyan, Ruth Yakubu and Amy Boyd\n\n**\ud83c\udfa8 Thanks as well to our illustrators** Tomomi Imura, Dasani Madipalli, and Jen Looper\n\n**\ud83d\ude4f Special thanks \ud83d\ude4f to our Microsoft Student Ambassador authors, reviewers, and content contributors**, notably Rishit Dagli, Muhammad Sakib Khan Inan, Rohan Raj, Alexandru Petrescu, Abhishek Jaiswal, Nawrin Tabassum, Ioan Samuila, and Snigdha Agarwal\n\n**\ud83e\udd29 Extra gratitude to Microsoft Student Ambassadors Eric Wanjau, Jasleen Sondhi, and Vidushi Gupta for our R lessons!**\n\n# Getting Started\n\nFollow these steps:\n1. **Fork the Repository**: Click on the \"Fork\" button at the top-right corner of this page.\n2. **Clone the Repository**:   `git clone https://github.com/microsoft/ML-For-Beginners.git`\n\n> [find all additional resources for this course in our Microsoft Learn collection](https://learn.microsoft.com/en-us/collections/qrqzamz1nn2wx3?WT.mc_id=academic-77952-bethanycheum)\n\n\n**[Students](https://aka.ms/student-page)**, to use this curriculum, fork the entire repo to your own GitHub account and complete the exercises on your own or with a group:\n\n- Start with a pre-lecture quiz.\n- Read the lecture and complete the activities, pausing and reflecting at each knowledge check.\n- Try to create the projects by comprehending the lessons rather than running the solution code; however that code is available in the `/solution` folders in each project-oriented lesson.\n- Take the post-lecture quiz.\n- Complete the challenge.\n- Complete the assignment.\n- After completing a lesson group, visit the [Discussion Board](https://github.com/microsoft/ML-For-Beginners/discussions) and \"learn out loud\" by filling out the appropriate PAT rubric. A 'PAT' is a Progress Assessment Tool that is a rubric you fill out to further your learning. You can also react to other PATs so we can learn together.\n\n> For further study, we recommend following these [Microsoft Learn](https://docs.microsoft.com/en-us/users/jenlooper-2911/collections/k7o7tg1gp306q4?WT.mc_id=academic-77952-leestott) modules and learning paths.\n\n**Teachers**, we have [included some suggestions](for-teachers.md) on how to use this curriculum.\n\n---\n\n## Video walkthroughs\n\nSome of the lessons are available as short form video. You can find all these in-line in the lessons, or on the [ML for Beginners playlist on the Microsoft Developer YouTube channel](https://aka.ms/ml-beginners-videos) by clicking the image below.\n\n[![ML for beginners banner](./ml-for-beginners-video-banner.png)](https://aka.ms/ml-beginners-videos)\n\n---\n\n## Meet the Team\n\n[![Promo video](ml.gif)](https://youtu.be/Tj1XWrDSYJU \"Promo video\")\n\n**Gif by** [Mohit Jaisal](https://linkedin.com/in/mohitjaisal)\n\n> \ud83c\udfa5 Click the image above for a video about the project and the folks who created it!\n\n---\n\n## Pedagogy\n\nWe have chosen two pedagogical tenets while building this curriculum: ensuring that it is hands-on **project-based** and that it includes **frequent quizzes**. In addition, this curriculum has a common **theme** to give it cohesion.\n\nBy ensuring that the content aligns with projects, the process is made more engaging for students and retention of concepts will be augmented. In addition, a low-stakes quiz before a class sets the intention of the student towards learning a topic, while a second quiz after class ensures further retention. This curriculum was designed to be flexible and fun and can be taken in whole or in part. The projects start small and become increasingly complex by the end of the 12-week cycle. This curriculum also includes a postscript on real-world applications of ML, which can be used as extra credit or as a basis for discussion.\n\n> Find our [Code of Conduct](CODE_OF_CONDUCT.md), [Contributing](CONTRIBUTING.md), and [Translation](TRANSLATIONS.md) guidelines. We welcome your constructive feedback!\n\n## Each lesson includes\n\n- optional sketchnote\n- optional supplemental video\n- video walkthrough (some lessons only)\n- pre-lecture warmup quiz\n- written lesson\n- for project-based lessons, step-by-step guides on how to build the project\n- knowledge checks\n- a challenge\n- supplemental reading\n- assignment\n- post-lecture quiz\n\n> **A note about languages**: These lessons are primarily written in Python, but many are also available in R. To complete an R lesson, go to the `/solution` folder and look for R lessons. They include an .rmd extension that represents an **R Markdown** file which can be simply defined as an embedding of `code chunks` (of R or other languages) and a `YAML header` (that guides how to format outputs such as PDF) in a `Markdown document`. As such, it serves as an exemplary authoring framework for data science since it allows you to combine your code, its output, and your thoughts by allowing you to write them down in Markdown. Moreover, R Markdown documents can be rendered to output formats such as PDF, HTML, or Word.\n\n> **A note about quizzes**: All quizzes are contained in [Quiz App folder](./quiz-app/), for 52 total quizzes of three questions each. They are linked from within the lessons but the quiz app can be run locally; follow the instruction in the `quiz-app` folder to locally host or deploy to Azure.\n\n| Lesson Number |                             Topic                              |                   Lesson Grouping                   | Learning Objectives                                                                                                             |                                                              Linked Lesson                                                               |                        Author                        |\n| :-----------: | :------------------------------------------------------------: | :-------------------------------------------------: | ------------------------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------: | :--------------------------------------------------: |\n|      01       |                Introduction to machine learning                |      [Introduction](1-Introduction/README.md)       | Learn the basic concepts behind machine learning                                                                                |                                             [Lesson](1-Introduction/1-intro-to-ML/README.md)                                             |                       Muhammad                       |\n|      02       |                The History of machine learning                 |      [Introduction](1-Introduction/README.md)       | Learn the history underlying this field                                                                                         |                                            [Lesson](1-Introduction/2-history-of-ML/README.md)                                            |                     Jen and Amy                      |\n|      03       |                 Fairness and machine learning                  |      [Introduction](1-Introduction/README.md)       | What are the important philosophical issues around fairness that students should consider when building and applying ML models? |                                              [Lesson](1-Introduction/3-fairness/README.md)                                               |                        Tomomi                        |\n|      04       |                Techniques for machine learning                 |      [Introduction](1-Introduction/README.md)       | What techniques do ML researchers use to build ML models?                                                                       |                                          [Lesson](1-Introduction/4-techniques-of-ML/README.md)                                           |                    Chris and Jen                     |\n|      05       |                   Introduction to regression                   |        [Regression](2-Regression/README.md)         | Get started with Python and Scikit-learn for regression models                                                                  |         <ul><li>[Python](2-Regression/1-Tools/README.md)</li><li>[R](2-Regression/1-Tools/solution/R/lesson_1.html)</li></ul>         |      <ul><li>Jen</li><li>Eric Wanjau</li></ul>       |\n|      06       |                North American pumpkin prices \ud83c\udf83                |        [Regression](2-Regression/README.md)         | Visualize and clean data in preparation for ML                                                                                  |          <ul><li>[Python](2-Regression/2-Data/README.md)</li><li>[R](2-Regression/2-Data/solution/R/lesson_2.html)</li></ul>          |      <ul><li>Jen</li><li>Eric Wanjau</li></ul>       |\n|      07       |                North American pumpkin prices \ud83c\udf83                |        [Regression](2-Regression/README.md)         | Build linear and polynomial regression models                                                                                   |        <ul><li>[Python](2-Regression/3-Linear/README.md)</li><li>[R](2-Regression/3-Linear/solution/R/lesson_3.html)</li></ul>        |      <ul><li>Jen and Dmitry</li><li>Eric Wanjau</li></ul>       |\n|      08       |                North American pumpkin prices \ud83c\udf83                |        [Regression](2-Regression/README.md)         | Build a logistic regression model                                                                                               |     <ul><li>[Python](2-Regression/4-Logistic/README.md) </li><li>[R](2-Regression/4-Logistic/solution/R/lesson_4.html)</li></ul>      |      <ul><li>Jen</li><li>Eric Wanjau</li></ul>       |\n|      09       |                          A Web App \ud83d\udd0c                          |           [Web App](3-Web-App/README.md)            | Build a web app to use your trained model                                                                                       |                                                 [Python](3-Web-App/1-Web-App/README.md)                                                  |                         Jen                          |\n|      10       |                 Introduction to classification                 |    [Classification](4-Classification/README.md)     | Clean, prep, and visualize your data; introduction to classification                                                            | <ul><li> [Python](4-Classification/1-Introduction/README.md) </li><li>[R](4-Classification/1-Introduction/solution/R/lesson_10.html)  | <ul><li>Jen and Cassie</li><li>Eric Wanjau</li></ul> |\n|      11       |             Delicious Asian and Indian cuisines \ud83c\udf5c             |    [Classification](4-Classification/README.md)     | Introduction to classifiers                                                                                                     | <ul><li> [Python](4-Classification/2-Classifiers-1/README.md)</li><li>[R](4-Classification/2-Classifiers-1/solution/R/lesson_11.html) | <ul><li>Jen and Cassie</li><li>Eric Wanjau</li></ul> |\n|      12       |             Delicious Asian and Indian cuisines \ud83c\udf5c             |    [Classification](4-Classification/README.md)     | More classifiers                                                                                                                | <ul><li> [Python](4-Classification/3-Classifiers-2/README.md)</li><li>[R](4-Classification/3-Classifiers-2/solution/R/lesson_12.html) | <ul><li>Jen and Cassie</li><li>Eric Wanjau</li></ul> |\n|      13       |             Delicious Asian and Indian cuisines \ud83c\udf5c             |    [Classification](4-Classification/README.md)     | Build a recommender web app using your model                                                                                    |                                              [Python](4-Classification/4-Applied/README.md)                                              |                         Jen                          |\n|      14       |                   Introduction to clustering                   |        [Clustering](5-Clustering/README.md)         | Clean, prep, and visualize your data; Introduction to clustering                                                                |         <ul><li> [Python](5-Clustering/1-Visualize/README.md)</li><li>[R](5-Clustering/1-Visualize/solution/R/lesson_14.html)         |      <ul><li>Jen</li><li>Eric Wanjau</li></ul>       |\n|      15       |              Exploring Nigerian Musical Tastes \ud83c\udfa7              |        [Clustering](5-Clustering/README.md)         | Explore the K-Means clustering method                                                                                           |           <ul><li> [Python](5-Clustering/2-K-Means/README.md)</li><li>[R](5-Clustering/2-K-Means/solution/R/lesson_15.html)           |      <ul><li>Jen</li><li>Eric Wanjau</li></ul>       |\n|      16       |        Introduction to natural language processing \u2615\ufe0f         |   [Natural language processing](6-NLP/README.md)    | Learn the basics about NLP by building a simple bot                                                                             |                                             [Python](6-NLP/1-Introduction-to-NLP/README.md)                                              |                       Stephen                        |\n|      17       |                      Common NLP Tasks \u2615\ufe0f                      |   [Natural language processing](6-NLP/README.md)    | Deepen your NLP knowledge by understanding common tasks required when dealing with language structures                          |                                                    [Python](6-NLP/2-Tasks/README.md)                                                     |                       Stephen                        |\n|      18       |             Translation and sentiment analysis \u2665\ufe0f              |   [Natural language processing](6-NLP/README.md)    | Translation and sentiment analysis with Jane Austen                                                                             |                                            [Python](6-NLP/3-Translation-Sentiment/README.md)                                             |                       Stephen                        |\n|      19       |                  Romantic hotels of Europe \u2665\ufe0f                  |   [Natural language processing](6-NLP/README.md)    | Sentiment analysis with hotel reviews 1                                                                                         |                                               [Python](6-NLP/4-Hotel-Reviews-1/README.md)                                                |                       Stephen                        |\n|      20       |                  Romantic hotels of Europe \u2665\ufe0f                  |   [Natural language processing](6-NLP/README.md)    | Sentiment analysis with hotel reviews 2                                                                                         |                                               [Python](6-NLP/5-Hotel-Reviews-2/README.md)                                                |                       Stephen                        |\n|      21       |            Introduction to time series forecasting             |        [Time series](7-TimeSeries/README.md)        | Introduction to time series forecasting                                                                                         |                                             [Python](7-TimeSeries/1-Introduction/README.md)                                              |                      Francesca                       |\n|      22       | \u26a1\ufe0f World Power Usage \u26a1\ufe0f - time series forecasting with ARIMA |        [Time series](7-TimeSeries/README.md)        | Time series forecasting with ARIMA                                                                                              |                                                 [Python](7-TimeSeries/2-ARIMA/README.md)                                                 |                      Francesca                       |\n|      23       |  \u26a1\ufe0f World Power Usage \u26a1\ufe0f - time series forecasting with SVR  |        [Time series](7-TimeSeries/README.md)        | Time series forecasting with Support Vector Regressor                                                                           |                                                  [Python](7-TimeSeries/3-SVR/README.md)                                                  |                       Anirban                        |\n|      24       |             Introduction to reinforcement learning             | [Reinforcement learning](8-Reinforcement/README.md) | Introduction to reinforcement learning with Q-Learning                                                                          |                                             [Python](8-Reinforcement/1-QLearning/README.md)                                              |                        Dmitry                        |\n|      25       |                 Help Peter avoid the wolf! \ud83d\udc3a                  | [Reinforcement learning](8-Reinforcement/README.md) | Reinforcement learning Gym                                                                                                      |                                                [Python](8-Reinforcement/2-Gym/README.md)                                                 |                        Dmitry                        |\n|  Postscript   |            Real-World ML scenarios and applications            |      [ML in the Wild](9-Real-World/README.md)       | Interesting and revealing real-world applications of classical ML                                                               |                                             [Lesson](9-Real-World/1-Applications/README.md)                                              |                         Team                         |\n|  Postscript   |            Model Debugging in ML using RAI dashboard          |      [ML in the Wild](9-Real-World/README.md)       | Model Debugging in Machine Learning using Responsible AI dashboard components                                                              |                                             [Lesson](9-Real-World/2-Debugging-ML-Models/README.md)                                              |                         Ruth Yakubu                       |\n\n> [find all additional resources for this course in our Microsoft Learn collection](https://learn.microsoft.com/en-us/collections/qrqzamz1nn2wx3?WT.mc_id=academic-77952-bethanycheum)\n\n## Offline access\n\nYou can run this documentation offline by using [Docsify](https://docsify.js.org/#/). Fork this repo, [install Docsify](https://docsify.js.org/#/quickstart) on your local machine, and then in the root folder of this repo, type `docsify serve`. The website will be served on port 3000 on your localhost: `localhost:3000`.\n\n## PDFs\n\nFind a pdf of the curriculum with links [here](https://microsoft.github.io/ML-For-Beginners/pdf/readme.pdf).\n\n## Help Wanted\n\nWould you like to contribute a translation? Please read our [translation guidelines](TRANSLATIONS.md) and add a templated issue to manage the workload [here](https://github.com/microsoft/ML-For-Beginners/issues).\n\n## \ud83c\udf92 Other Courses \n\nOur team produces other courses! Check out:\n\n- [Generative AI for Beginners](https://aka.ms/genai-beginners)\n- [Generative AI for Beginners .NET](https://github.com/microsoft/Generative-AI-for-beginners-dotnet)\n- [Generative AI with JavaScript](https://github.com/microsoft/generative-ai-with-javascript)\n- [AI for Beginners](https://aka.ms/ai-beginners)\n- [Data Science for Beginners](https://aka.ms/datascience-beginners)\n- [ML for Beginners](https://aka.ms/ml-beginners)\n- [Cybersecurity for Beginners](https://github.com/microsoft/Security-101) \n- [Web Dev for Beginners](https://aka.ms/webdev-beginners)\n- [IoT for Beginners](https://aka.ms/iot-beginners)\n- [XR Development for Beginners](https://github.com/microsoft/xr-development-for-beginners)\n- [Mastering GitHub Copilot for Paired Programming](https://github.com/microsoft/Mastering-GitHub-Copilot-for-Paired-Programming)\n- [Mastering GitHub Copilot for C#/.NET Developers](https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers)\n- [Choose Your Own Copilot Adventure](https://github.com/microsoft/CopilotAdventures)\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 10744183,
    "name": "netdata",
    "full_name": "netdata/netdata",
    "description": "The fastest path to AI-powered full stack observability, even for lean teams.",
    "html_url": "https://github.com/netdata/netdata",
    "clone_url": "https://github.com/netdata/netdata.git",
    "owner_login": "netdata",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/43390781?v=4",
    "stargazers_count": 75339,
    "watchers_count": 75339,
    "forks_count": 6098,
    "open_issues_count": 226,
    "size": 236727,
    "language": "C",
    "languages": {
      "C": 24453503,
      "Go": 6270344,
      "JavaScript": 1007485,
      "Shell": 891232,
      "Jupyter Notebook": 826673,
      "Python": 801537,
      "C++": 534780,
      "CMake": 236182,
      "HTML": 225570,
      "Rust": 159380,
      "CSS": 86498,
      "Batchfile": 12367,
      "Yacc": 7186,
      "Rich Text Format": 6985,
      "POV-Ray SDL": 6860,
      "PowerShell": 6115,
      "Dockerfile": 5247,
      "Makefile": 3014
    },
    "topics": [
      "ai",
      "alerting",
      "cncf",
      "data-visualization",
      "database",
      "devops",
      "docker",
      "grafana",
      "influxdb",
      "kubernetes",
      "linux",
      "machine-learning",
      "mcp",
      "mongodb",
      "monitoring",
      "mysql",
      "netdata",
      "observability",
      "postgresql",
      "prometheus"
    ],
    "license_name": "GNU General Public License v3.0",
    "created_at": "2013-06-17T18:39:10+00:00",
    "updated_at": "2025-08-06T00:26:27+00:00",
    "pushed_at": "2025-08-06T00:26:22+00:00",
    "contributors_count": 100,
    "readme_length": 26186,
    "readme_content": "<p align=\"center\">\n<a href=\"https://www.netdata.cloud#gh-light-mode-only\">\n  <img src=\"https://www.netdata.cloud/img/readme-images/netdata_readme_logo_light.png\" alt=\"Netdata\" width=\"300\"/>\n</a>\n<a href=\"https://www.netdata.cloud#gh-dark-mode-only\">\n  <img src=\"https://www.netdata.cloud/img/readme-images/netdata_readme_logo_dark.png\" alt=\"Netdata\" width=\"300\"/>\n</a>\n</p>\n<h3 align=\"center\">X-Ray Vision for your infrastructure!</h3>\n<h4 align=\"center\">Every Metric, Every Second. No BS.</h4>\n\n<br />\n<p align=\"center\">\n  <a href=\"https://github.com/netdata/netdata/\"><img src=\"https://img.shields.io/github/stars/netdata/netdata?style=social\" alt=\"GitHub Stars\"></a>\n  <br />\n  <a href=\"https://app.netdata.cloud/spaces/netdata-demo?utm_campaign=github_readme_demo_badge\"><img src=\"https://img.shields.io/badge/Live%20Demo-green\" alt=\"Live Demo\"></a>\n  <a href=\"https://github.com/netdata/netdata/releases/latest\"><img src=\"https://img.shields.io/github/release/netdata/netdata.svg\" alt=\"Latest release\"></a>\n  <a href=\"https://github.com/netdata/netdata-nightlies/releases/latest\"><img src=\"https://img.shields.io/github/release/netdata/netdata-nightlies.svg\" alt=\"Latest nightly build\"></a>\n  <br/>\n  <a href=\"https://community.netdata.cloud\"><img alt=\"Discourse topics\" src=\"https://img.shields.io/discourse/topics?server=https%3A%2F%2Fcommunity.netdata.cloud%2F&logo=discourse&label=discourse%20forum\"></a>\n  <a href=\"https://github.com/netdata/netdata/discussions\"><img alt=\"GitHub Discussions\" src=\"https://img.shields.io/github/discussions/netdata/netdata?logo=github&label=github%20discussions\"></a>\n  <br/>\n  <a href=\"https://bestpractices.coreinfrastructure.org/projects/2231\"><img src=\"https://bestpractices.coreinfrastructure.org/projects/2231/badge\" alt=\"CII Best Practices\"></a>\n  <a href=\"https://scan.coverity.com/projects/netdata-netdata?tab=overview\"><img alt=\"Coverity Scan\" src=\"https://img.shields.io/coverity/scan/netdata\"></a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://registry.my-netdata.io/#menu_netdata_submenu_registry\"><img src=\"https://registry.my-netdata.io/api/v3/badge.svg?chart=netdata.registry_entries&dimensions=persons&label=user%20base&units=M&value_color=blue&precision=2&divide=1000000&options=unaligned&tier=1&v44\" alt=\"User base\"></a>\n  <a href=\"https://registry.my-netdata.io/#menu_netdata_submenu_registry\"><img src=\"https://registry.my-netdata.io/api/v3/badge.svg?chart=netdata.registry_entries&dimensions=machines&label=servers%20monitored&units=M&divide=1000000&value_color=orange&precision=2&options=unaligned&tier=1&v44\" alt=\"Servers monitored\"></a>\n  <a href=\"https://registry.my-netdata.io/#menu_netdata_submenu_registry\"><img src=\"https://registry.my-netdata.io/api/v3/badge.svg?chart=netdata.registry_sessions&label=sessions%20served&units=M&value_color=yellowgreen&precision=2&divide=1000000&options=unaligned&tier=1&v44\" alt=\"Sessions served\"></a>\n  <a href=\"https://hub.docker.com/r/netdata/netdata\"><img src=\"https://registry.my-netdata.io/api/v3/badge.svg?chart=dockerhub.pulls_sum&divide=1000000&precision=1&units=M&label=docker+hub+pulls&options=unaligned&tier=1&v44\" alt=\"Docker Hub pulls\"></a>\n</p>\n<p align=\"center\"><b>Visit our <a href=\"https://www.netdata.cloud\">Home Page</a></b></p>\n\n<hr class=\"solid\">\n\nMENU: **[WHO WE ARE](#who-we-are)** | **[KEY FEATURES](#key-features)** | **[GETTING STARTED](#getting-started)** | **[HOW IT WORKS](#how-it-works)** | **[FAQ](#faq)** | **[DOCS](#book-documentation)** | **[COMMUNITY](#tada-community)** | **[CONTRIBUTE](#pray-contribute)** | **[LICENSE](#scroll-license)**\n\n\n> [!WARNING]\n> People **get addicted to Netdata.**\n> Once you use it on your systems, *there's no going back.*\n\n[![Platforms](https://img.shields.io/badge/Platforms-Linux%20%7C%20macOS%20%7C%20FreeBSD%20%7C%20Windows-blue)]()\n\n---\n\n## WHO WE ARE\n\nNetdata is an open-source, real-time infrastructure monitoring platform. Monitor, detect, and act across your entire infrastructure.\n\n**Core Advantages:**\n\n* **Instant Insights** \u2013 With Netdata you can access per-second metrics and visualizations.\n* **Zero Configuration** \u2013 You can deploy immediately without complex setup.\n* **ML-Powered** \u2013 You can detect anomalies, predict issues, and automate analysis.\n* **Efficient** \u2013 You can monitor with minimal resource usage and maximum scalability.\n* **Secure & Distributed** \u2013 You can keep your data local with no central collection needed.\n\nWith Netdata, you get real-time, per-second updates. Clear **insights at a glance**, no complexity.\n\n<details>\n  <summary><strong>All heroes have a great origin story. Click to discover ours.</strong></summary>\n  <br/>\n\nIn 2013, at the company where Costa Tsaousis was COO, a significant percentage of their cloud-based transactions failed silently, severely impacting business performance.\n\nCosta and his team tried every troubleshooting tool available at the time. None could identify the root cause. As Costa later wrote:\n\n\u201c*I couldn\u2019t believe that monitoring systems provide so few metrics and with such low resolution, scale so badly, and cost so much to run.*\u201d\n\nFrustrated, he decided to build his own monitoring tool, starting from scratch.\n\nThat decision led to countless late nights and weekends. It also sparked a fundamental shift in how infrastructure monitoring and troubleshooting are approached, both in method and in cost.\n</details>\n\n### Most Energy-Efficient Monitoring Tool\n\n<p align=\"center\">\n<a href=\"https://www.ivanomalavolta.com/files/papers/ICSOC_2023.pdf#gh-dark-mode-only\">\n  <img src=\"https://github.com/netdata/netdata/assets/139226121/7118757a-38fb-48d7-b12a-53e709a8e8c0\" alt=\"Energy Efficiency\" width=\"800\"/>\n</a>\n<a href=\"https://www.ivanomalavolta.com/files/papers/ICSOC_2023.pdf#gh-light-mode-only\">\n  <img src=\"https://github.com/netdata/netdata/assets/139226121/4f64cbb6-05e4-48e3-b7c0-d1b79e37e219\" alt=\"Energy efficiency\" width=\"800\"/>\n</a>\n</p>\n\nAccording to the [University of Amsterdam study](https://www.ivanomalavolta.com/files/papers/ICSOC_2023.pdf), Netdata is the most energy-efficient tool for monitoring Docker-based systems. The study also shows Netdata excels in CPU usage, RAM usage, and execution time compared to other monitoring solutions.\n\n---\n\n## Key Features\n\n| Feature                    | Description                               | What Makes It Unique                                     |\n|----------------------------|-------------------------------------------|----------------------------------------------------------|\n| **Real-Time**              | Per-second data collection and processing | Works in a beat \u2013 click and see results instantly        |\n| **Zero-Configuration**     | Automatic detection and discovery         | Auto-discovers everything on the nodes it runs           |\n| **ML-Powered**             | Unsupervised anomaly detection            | Trains multiple ML models per metric at the edge         |\n| **Long-Term Retention**    | High-performance storage                  | ~0.5 bytes per sample with tiered storage for archiving  |\n| **Advanced Visualization** | Rich, interactive dashboards              | Slice and dice data without query language               |\n| **Extreme Scalability**    | Native horizontal scaling                 | Parent-Child centralization with multi-million samples/s |\n| **Complete Visibility**    | From infrastructure to applications       | Simplifies operations and eliminates silos               |\n| **Edge-Based**             | Processing at your premises               | Distributes code instead of centralizing data            |\n\n> [!NOTE]  \n> Want to put Netdata to the test against Prometheus?\n> Explore the [full comparison](https://www.netdata.cloud/blog/netdata-vs-prometheus-2025/).\n\n---\n\n## Netdata Ecosystem\n\nThis three-part architecture enables you to scale from single nodes to complex multi-cloud environments:\n\n| Component         | Description                                                                                                                                                 | License                                         |\n|-------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------|\n| **Netdata Agent** | \u2022 Core monitoring engine<br>\u2022 Handles collection, storage, ML, alerts, exports<br>\u2022 Runs on servers, cloud, K8s, IoT<br>\u2022 Zero production impact            | [GPL v3+](https://www.gnu.org/licenses/gpl-3.0) |\n| **Netdata Cloud** | \u2022 Enterprise features<br>\u2022 User management, RBAC, horizontal scaling<br>\u2022 Centralized alerts<br>\u2022 Free community tier<br>\u2022 No metric storage centralization |                                                 |\n| **Netdata UI**    | \u2022 Dashboards and visualizations<br>\u2022 Free to use<br>\u2022 Included in standard packages<br>\u2022 Latest version via CDN                                             | [NCUL1](https://app.netdata.cloud/LICENSE.txt)  |\n\n## What You Can Monitor\n\nWith Netdata you can monitor all these components across platforms:\n\n|                                                                                                   Component |              Linux               | FreeBSD | macOS |                      Windows                      |\n|------------------------------------------------------------------------------------------------------------:|:--------------------------------:|:-------:|:-----:|:-------------------------------------------------:|\n|                             **System Resources**<small><br/>CPU, Memory and system shared resources</small> |               Full               |   Yes   |  Yes  |                        Yes                        |\n|                                **Storage**<small><br/>Disks, Mount points, Filesystems, RAID arrays</small> |               Full               |   Yes   |  Yes  |                        Yes                        |\n|                                 **Network**<small><br/>Network Interfaces, Protocols, Firewall, etc</small> |               Full               |   Yes   |  Yes  |                        Yes                        |\n|                        **Hardware & Sensors**<small><br/>Fans, Temperatures, Controllers, GPUs, etc</small> |               Full               |  Some   | Some  |                       Some                        |\n|                                       **O/S Services**<small><br/>Resources, Performance and Status</small> | Yes<small><br/>`systemd`</small> |    -    |   -   |                         -                         |\n|                                      **Processes**<small><br/>Resources, Performance, OOM, and more</small> |               Yes                |   Yes   |  Yes  |                        Yes                        |\n|                                                                             System and Application **Logs** | Yes<small><br/>`systemd`-journal |    -    |   -   | Yes<small><br/>`Windows Event Log`, `ETW`</small> |\n|                                 **Network Connections**<small><br/>Live TCP and UDP sockets per PID</small> |               Yes                |    -    |   -   |                         -                         |\n|                               **Containers**<small><br/>Docker/containerd, LXC/LXD, Kubernetes, etc</small> |               Yes                |    -    |   -   |                         -                         |\n|                                 **VMs** (from the host)<small><br/>KVM, qemu, libvirt, Proxmox, etc</small> | Yes<small><br/>`cgroups`</small> |    -    |   -   |         Yes<small><br/>`Hyper-V`</small>          |\n|                       **Synthetic Checks**<small><br/>Test APIs, TCP ports, Ping, Certificates, etc</small> |               Yes                |   Yes   |  Yes  |                        Yes                        |\n| **Packaged Applications**<small><br/>nginx, apache, postgres, redis, mongodb,<br/>and hundreds more</small> |               Yes                |   Yes   |  Yes  |                        Yes                        |\n|                              **Cloud Provider Infrastructure**<small><br/>AWS, GCP, Azure, and more</small> |               Yes                |   Yes   |  Yes  |                        Yes                        |\n|                       **Custom Applications**<small><br/>OpenMetrics, StatsD and soon OpenTelemetry</small> |               Yes                |   Yes   |  Yes  |                        Yes                        |\n\nOn Linux, you can continuously monitor all kernel features and hardware sensors for errors, including Intel/AMD/Nvidia GPUs, PCI AER, RAM EDAC, IPMI, S.M.A.R.T, Intel RAPL, NVMe, fans, power supplies, and voltage readings.\n\n---\n\n## Getting Started\n\nYou can install Netdata on all major operating systems. To begin:\n\n### 1. Install Netdata\n\nChoose your platform and follow the installation guide:\n\n* [Linux Installation](https://learn.netdata.cloud/docs/installing/one-line-installer-for-all-linux-systems)\n* [macOS](https://learn.netdata.cloud/docs/installing/macos)\n* [FreeBSD](https://learn.netdata.cloud/docs/installing/freebsd)\n* [Windows](https://learn.netdata.cloud/docs/netdata-agent/installation/windows)\n* [Docker Guide](/packaging/docker/README.md)\n* [Kubernetes Setup](https://learn.netdata.cloud/docs/installation/install-on-specific-environments/kubernetes)\n\n> [!NOTE]\n> You can access the Netdata UI at `http://localhost:19999` (or `http://NODE:19999` if remote).\n\n### 2. Configure Collectors\n\nNetdata auto-discovers most metrics, but you can manually configure some collectors:\n\n* [All collectors](https://learn.netdata.cloud/docs/data-collection/)\n* [SNMP monitoring](https://learn.netdata.cloud/docs/data-collection/monitor-anything/networking/snmp)\n\n### 3. Configure Alerts\n\nYou can use hundreds of built-in alerts and integrate with:\n\n`email`, `Slack`, `Telegram`, `PagerDuty`, `Discord`, `Microsoft Teams`, and more.\n\n> [!NOTE]  \n> Email alerts work by default if there's a configured MTA.\n\n### 4. Configure Parents\n\nYou can centralize dashboards, alerts, and storage with Netdata Parents:\n\n* [Streaming Reference](https://learn.netdata.cloud/docs/streaming/streaming-configuration-reference)\n\n> [!NOTE]  \n> You can use Netdata Parents for central dashboards, longer retention, and alert configuration.\n\n### 5. Connect to Netdata Cloud\n\n[Sign in to Netdata Cloud](https://app.netdata.cloud/sign-in) and connect your nodes for:\n\n* Access from anywhere\n* Horizontal scalability and multi-node dashboards\n* UI configuration for alerts and data collection\n* Role-based access control\n* Free tier available\n\n> [!NOTE]  \n> Netdata Cloud is optional. Your data stays in your infrastructure.\n\n## Live Demo Sites\n\n<p align=\"center\">\n  <b>See Netdata in action</b><br/>\n  <a href=\"https://frankfurt.netdata.rocks\"><b>FRANKFURT</b></a> |\n  <a href=\"https://newyork.netdata.rocks\"><b>NEWYORK</b></a> |\n  <a href=\"https://atlanta.netdata.rocks\"><b>ATLANTA</b></a> |\n  <a href=\"https://sanfrancisco.netdata.rocks\"><b>SANFRANCISCO</b></a> |\n  <a href=\"https://toronto.netdata.rocks\"><b>TORONTO</b></a> |\n  <a href=\"https://singapore.netdata.rocks\"><b>SINGAPORE</b></a> |\n  <a href=\"https://bangalore.netdata.rocks\"><b>BANGALORE</b></a>\n  <br/>\n  <i>These demo clusters run with default configuration and show real monitoring data.</i>\n  <br/>\n  <i>Choose the instance closest to you for the best performance.</i>\n</p>\n\n---\n\n## How It Works\n\nWith Netdata you can run a modular pipeline for metrics collection, processing, and visualization.\n\n```mermaid\nflowchart TB\n  A[Netdata Agent]:::mainNode\n  A1(Collect):::green --> A\n  A2(Store):::green --> A\n  A3(Learn):::green --> A\n  A4(Detect):::green --> A\n  A5(Check):::green --> A\n  A6(Stream):::green --> A\n  A7(Archive):::green --> A\n  A8(Query):::green --> A\n  A9(Score):::green --> A\n\n  classDef green fill:#bbf3bb,stroke:#333,stroke-width:1px,color:#000\n  classDef mainNode fill:#f0f0f0,stroke:#333,stroke-width:1px,color:#333\n```\n\nWith each Agent you can:\n\n1. **Collect** \u2013 Gather metrics from systems, containers, apps, logs, APIs, and synthetic checks.\n2. **Store** \u2013 Save metrics to a high-efficiency, tiered time-series database.\n3. **Learn** \u2013 Train ML models per metric using recent behavior.\n4. **Detect** \u2013 Identify anomalies using trained ML models.\n5. **Check** \u2013 Evaluate metrics against pre-set or custom alert rules.\n6. **Stream** \u2013 Send metrics to Netdata Parents in real time.\n7. **Archive** \u2013 Export metrics to Prometheus, InfluxDB, OpenTSDB, Graphite, and others.\n8. **Query** \u2013 Access metrics via an API for dashboards or third-party tools.\n9. **Score** \u2013 Use a scoring engine to find patterns and correlations across metrics.\n\n> [!NOTE]  \n> Learn more: [Netdata's architecture](https://learn.netdata.cloud/docs/netdata-agent/#distributed-observability-pipeline)\n\n## Agent Capabilities\n\nWith the Netdata Agent, you can use these core capabilities out-of-the-box:\n\n| Capability                   | Description                                                                                                                                   |\n|------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------|\n| **Comprehensive Collection** | \u2022 800+ integrations<br>\u2022 Systems, containers, VMs, hardware sensors<br>\u2022 OpenMetrics, StatsD, and logs<br>\u2022 OpenTelemetry support coming soon |\n| **Performance & Precision**  | \u2022 Per-second collection<br>\u2022 Real-time visualization with 1-second latency<br>\u2022 High-resolution metrics                                       |\n| **Edge-Based ML**            | \u2022 ML models trained at the edge<br>\u2022 Automatic anomaly detection per metric<br>\u2022 Pattern recognition based on historical behavior             |\n| **Advanced Log Management**  | \u2022 Direct systemd-journald and Windows Event Log integration<br>\u2022 Process logs at the edge<br>\u2022 Rich log visualization                         |\n| **Observability Pipeline**   | \u2022 Parent-Child relationships<br>\u2022 Flexible centralization<br>\u2022 Multi-level replication and retention                                          |\n| **Automated Visualization**  | \u2022 NIDL data model<br>\u2022 Auto-generated dashboards<br>\u2022 No query language needed                                                                |\n| **Smart Alerting**           | \u2022 Pre-configured alerts<br>\u2022 Multiple notification methods<br>\u2022 Proactive detection                                                           |\n| **Low Maintenance**          | \u2022 Auto-detection<br>\u2022 Zero-touch ML<br>\u2022 Easy scalability<br>\u2022 CI/CD friendly                                                                 |\n| **Open & Extensible**        | \u2022 Modular architecture<br>\u2022 Easy to customize<br>\u2022 Integrates with existing tools                                                             |\n\n---\n\n## CNCF Membership\n\n<p align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/cncf/artwork/master/other/cncf/horizontal/white/cncf-white.svg\">\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/cncf/artwork/master/other/cncf/horizontal/color/cncf-color.svg\">\n    <img alt=\"CNCF Logo\" src=\"https://raw.githubusercontent.com/cncf/artwork/master/other/cncf/horizontal/color/cncf-color.svg\" width=\"300\">\n  </picture>\n  <br />\n  Netdata actively supports and is a member of the Cloud Native Computing Foundation (CNCF).<br />\n  It is one of the most starred projects in the <a href=\"https://landscape.cncf.io/?item=observability-and-analysis--observability--netdata\">CNCF landscape</a>.\n</p>\n\n---\n\n## FAQ\n\n<details>\n<summary><strong>Is Netdata secure?</strong></summary>\n<br/>\n\nYes. Netdata follows [OpenSSF best practices](https://bestpractices.coreinfrastructure.org/en/projects/2231), has a security-first design, and is regularly audited by the community.\n\n* [Security design](https://learn.netdata.cloud/docs/security-and-privacy-design)\n* [Security policies and advisories](https://github.com/netdata/netdata/security)\n\n</details>\n\n<details>\n<summary><strong>Does Netdata use a lot of resources?</strong></summary>\n<br/>\n\nNo. Even with ML and per-second metrics, Netdata uses minimal resources.\n\n* \\~5% CPU and 150MiB RAM by default on production systems\n* <1% CPU and \\~100MiB RAM when ML and alerts are disabled and using ephemeral storage\n* Parents scale to millions of metrics per second with appropriate hardware\n\n> You can use the **Netdata Monitoring** section in the dashboard to inspect its resource usage.\n\n</details>\n\n<details>\n<summary><strong>How much data retention is possible?</strong></summary>\n<br/>\n\nAs much as your disk allows.\n\nWith Netdata you can use tiered retention:\n\n* Tier 0: per-second resolution\n* Tier 1: per-minute resolution\n* Tier 2: per-hour resolution\n\nThese are queried automatically based on the zoom level.\n</details>\n\n<details>\n<summary><strong>Can Netdata scale to many servers?</strong></summary>\n<br/>\n\nYes. With Netdata you can:\n\n* Scale horizontally with many Agents\n* Scale vertically with powerful Parents\n* Scale infinitely via Netdata Cloud\n\n> You can use Netdata Cloud to merge many independent infrastructures into one logical view.\n\n</details>\n\n<details>\n<summary><strong>Is disk I/O a concern?</strong></summary>\n<br/>\n\nNo. Netdata minimizes disk usage:\n\n* Metrics are flushed to disk every 17 minutes, spread out evenly\n* Uses direct I/O and compression (ZSTD)\n* Can run entirely in RAM or stream to a Parent\n\n> You can use `alloc` or `ram` mode for no disk writes.\n\n</details>\n\n<details>\n<summary><strong>How is Netdata different from Prometheus + Grafana?</strong></summary>\n<br/>\n\nWith Netdata you get a complete monitoring solution\u2014not just tools.\n\n* No manual setup or dashboards needed\n* Built-in ML, alerts, dashboards, and correlations\n* More efficient and easier to deploy\n\n> [Performance comparison](https://blog.netdata.cloud/netdata-vs-prometheus-performance-analysis/)\n\n</details>\n\n<details>\n<summary><strong>How is Netdata different from commercial SaaS tools?</strong></summary>\n<br/>\n\nWith Netdata you can store all metrics on your infrastructure\u2014no sampling, no aggregation, no loss.\n\n* High-resolution metrics by default\n* ML per metric, not shared models\n* Unlimited scalability without skyrocketing cost\n\n</details>\n\n<details>\n<summary><strong>Can Netdata run alongside Nagios, Zabbix, etc.?</strong></summary>\n<br/>\n\nYes. You can use Netdata together with traditional tools.\n\nWith Netdata you get:\n\n* Real-time, high-resolution monitoring\n* Zero configuration and auto-generated dashboards\n* Anomaly detection and advanced visualization\n\n</details>\n\n<details>\n<summary><strong>What if I feel overwhelmed?</strong></summary>\n<br/>\n\nYou can start small:\n\n* Use the dashboard's table of contents and search\n* Explore anomaly scoring (\"AR\" toggle)\n* Create custom dashboards in Netdata Cloud\n\n> [Docs and guides](https://learn.netdata.cloud/guides)\n\n</details>\n\n<details>\n<summary><strong>Do I have to use Netdata Cloud?</strong></summary>\n<br/>\n\nNo. Netdata Cloud is optional.\n\nNetdata works without it, but with Cloud you can:\n\n* Access remotely with SSO\n* Save dashboard customizations\n* Configure alerts centrally\n* Collaborate with role-based access\n\n</details>\n\n<details>\n<summary><strong>What telemetry does Netdata collect?</strong></summary>\n<br/>\n\nAnonymous telemetry helps improve the product. You can disable it:\n\n* Add `--disable-telemetry` to the installer, or\n* Create `/etc/netdata/.opt-out-from-anonymous-statistics` and restart Netdata\n\n> Telemetry helps us understand usage, not track users. No private data is collected.\n\n</details>\n\n<details>\n<summary><strong>Who uses Netdata?</strong></summary>\n<br/>\n\nYou'll join users including:\n\n* Major companies (Amazon, ABN AMRO Bank, Facebook, Google, IBM, Intel, Netflix, Samsung)\n* Universities (NYU, Columbia, Seoul National, UCL)\n* Government organizations worldwide\n* Infrastructure-intensive organizations\n* Technology operators\n* Startups and freelancers\n* SysAdmins and DevOps professionals\n\n</details>\n\n---\n\n## \\:book: Documentation\n\nVisit [Netdata Learn](https://learn.netdata.cloud) for full documentation and guides.\n\n> [!NOTE]  \n> Includes deployment, configuration, alerting, exporting, troubleshooting, and more.\n\n---\n\n## \\:tada: Community\n\nJoin the Netdata community:\n\n* [Discord](https://discord.com/invite/2mEmfW735j)\n* [Forum](https://community.netdata.cloud)\n* [GitHub Discussions](https://github.com/netdata/netdata/discussions)\n\n> [!NOTE]  \n> [Code of Conduct](https://github.com/netdata/.github/blob/main/CODE_OF_CONDUCT.md)\n\nFollow us on:\n[Twitter](https://twitter.com/netdatahq) | [Reddit](https://www.reddit.com/r/netdata/) | [YouTube](https://www.youtube.com/c/Netdata) | [LinkedIn](https://www.linkedin.com/company/netdata-cloud/)\n\n---\n\n## \\:pray: Contribute\n\nWe welcome your contributions.\n\nWays you help us stay sharp:\n\n* Share best practices and monitoring insights\n* Report issues or missing features\n* Improve documentation\n* Develop new integrations or collectors\n* Help users in forums and chats\n\n> [!NOTE]  \n> [Contribution guide](https://github.com/netdata/.github/blob/main/CONTRIBUTING.md)\n\n---\n\n## \\:scroll: License\n\nThe Netdata ecosystem includes:\n\n* **Netdata Agent** \u2013 Open-source core (GPLv3+). **Includes** data collection, storage, ML, alerting, APIs and **redistributes** several other open-source tools and libraries.\n    * [Netdata Agent License](https://github.com/netdata/netdata/blob/master/LICENSE)\n    * [Netdata Agent Redistributed](https://github.com/netdata/netdata/blob/master/REDISTRIBUTED.md)\n* **Netdata UI** \u2013 Closed-source but free to use with Netdata Agent and Cloud. Delivered via CDN. It integrates third-party open-source components.\n    * [Netdata Cloud UI License](https://app.netdata.cloud/LICENSE.txt)\n    * [Netdata UI third-party licenses](https://app.netdata.cloud/3D_PARTY_LICENSES.txt)\n* **Netdata Cloud** \u2013 Closed-source, with free and paid tiers. Adds remote access, SSO, scalability.\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 101138315,
    "name": "d2l-zh",
    "full_name": "d2l-ai/d2l-zh",
    "description": "\u300a\u52a8\u624b\u5b66\u6df1\u5ea6\u5b66\u4e60\u300b\uff1a\u9762\u5411\u4e2d\u6587\u8bfb\u8005\u3001\u80fd\u8fd0\u884c\u3001\u53ef\u8ba8\u8bba\u3002\u4e2d\u82f1\u6587\u7248\u88ab70\u591a\u4e2a\u56fd\u5bb6\u7684500\u591a\u6240\u5927\u5b66\u7528\u4e8e\u6559\u5b66\u3002",
    "html_url": "https://github.com/d2l-ai/d2l-zh",
    "clone_url": "https://github.com/d2l-ai/d2l-zh.git",
    "owner_login": "d2l-ai",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/43974506?v=4",
    "stargazers_count": 71384,
    "watchers_count": 71384,
    "forks_count": 11778,
    "open_issues_count": 112,
    "size": 316965,
    "language": "Python",
    "languages": {
      "Python": 427646,
      "HTML": 68515,
      "TeX": 61055,
      "Shell": 10935
    },
    "topics": [
      "book",
      "chinese",
      "computer-vision",
      "deep-learning",
      "machine-learning",
      "natural-language-processing",
      "notebook",
      "python"
    ],
    "license_name": "Apache License 2.0",
    "created_at": "2017-08-23T04:40:24+00:00",
    "updated_at": "2025-08-06T01:53:54+00:00",
    "pushed_at": "2024-07-30T09:32:19+00:00",
    "contributors_count": 100,
    "readme_length": 2801,
    "readme_content": "# \u52a8\u624b\u5b66\u6df1\u5ea6\u5b66\u4e60\uff08Dive into Deep Learning\uff0cD2L.ai\uff09\n\n[\u7b2c\u4e8c\u7248\uff1azh.D2L.ai](https://zh.d2l.ai)  | [\u7b2c\u4e00\u7248\uff1azh-v1.D2L.ai](https://zh-v1.d2l.ai/) |  \u5b89\u88c5\u548c\u4f7f\u7528\u4e66\u4e2d\u6e90\u4ee3\u7801\uff1a [\u7b2c\u4e8c\u7248](https://zh.d2l.ai/chapter_installation/index.html) [\u7b2c\u4e00\u7248](https://zh-v1.d2l.ai/chapter_prerequisite/install.html)\n\n<h5 align=\"center\"><i>\u7406\u89e3\u6df1\u5ea6\u5b66\u4e60\u7684\u6700\u4f73\u65b9\u6cd5\u662f\u5b66\u4ee5\u81f4\u7528\u3002</i></h5>\n\n<p align=\"center\">\n  <img width=\"200\"  src=\"static/frontpage/_images/eq.jpg\">\n  <img width=\"200\"  src=\"static/frontpage/_images/figure.jpg\">\n  <img width=\"200\"  src=\"static/frontpage/_images/code.jpg\">\n  <img width=\"200\"  src=\"static/frontpage/_images/notebook.gif\">\n</p>\n\n\u672c\u5f00\u6e90\u9879\u76ee\u4ee3\u8868\u4e86\u6211\u4eec\u7684\u4e00\u79cd\u5c1d\u8bd5\uff1a\u6211\u4eec\u5c06\u6559\u7ed9\u8bfb\u8005\u6982\u5ff5\u3001\u80cc\u666f\u77e5\u8bc6\u548c\u4ee3\u7801\uff1b\u6211\u4eec\u5c06\u5728\u540c\u4e00\u4e2a\u5730\u65b9\u9610\u8ff0\u5256\u6790\u95ee\u9898\u6240\u9700\u7684\u6279\u5224\u6027\u601d\u7ef4\u3001\u89e3\u51b3\u95ee\u9898\u6240\u9700\u7684\u6570\u5b66\u77e5\u8bc6\uff0c\u4ee5\u53ca\u5b9e\u73b0\u89e3\u51b3\u65b9\u6848\u6240\u9700\u7684\u5de5\u7a0b\u6280\u80fd\u3002\n\n\u6211\u4eec\u7684\u76ee\u6807\u662f\u521b\u5efa\u4e00\u4e2a\u4e3a\u5b9e\u73b0\u4ee5\u4e0b\u76ee\u6807\u7684\u7edf\u4e00\u8d44\u6e90\uff1a\n1. \u6240\u6709\u4eba\u5747\u53ef\u5728\u7f51\u4e0a\u514d\u8d39\u83b7\u53d6\uff1b\n1. \u63d0\u4f9b\u8db3\u591f\u7684\u6280\u672f\u6df1\u5ea6\uff0c\u4ece\u800c\u5e2e\u52a9\u8bfb\u8005\u5b9e\u9645\u6210\u4e3a\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528\u79d1\u5b66\u5bb6\uff1a\u65e2\u7406\u89e3\u6570\u5b66\u539f\u7406\uff0c\u53c8\u80fd\u591f\u5b9e\u73b0\u5e76\u4e0d\u65ad\u6539\u8fdb\u65b9\u6cd5\uff1b\n1. \u5305\u542b\u53ef\u8fd0\u884c\u7684\u4ee3\u7801\uff0c\u4e3a\u8bfb\u8005\u5c55\u793a\u5982\u4f55\u5728\u5b9e\u9645\u4e2d\u89e3\u51b3\u95ee\u9898\u3002\u8fd9\u6837\u4e0d\u4ec5\u76f4\u63a5\u5c06\u6570\u5b66\u516c\u5f0f\u5bf9\u5e94\u6210\u5b9e\u9645\u4ee3\u7801\uff0c\u800c\u4e14\u53ef\u4ee5\u4fee\u6539\u4ee3\u7801\u3001\u89c2\u5bdf\u7ed3\u679c\u5e76\u53ca\u65f6\u83b7\u53d6\u7ecf\u9a8c\uff1b\n1. \u5141\u8bb8\u6211\u4eec\u548c\u6574\u4e2a\u793e\u533a\u4e0d\u65ad\u5feb\u901f\u8fed\u4ee3\u5185\u5bb9\uff0c\u4ece\u800c\u7d27\u8ddf\u4ecd\u5728\u9ad8\u901f\u53d1\u5c55\u7684\u6df1\u5ea6\u5b66\u4e60\u9886\u57df\uff1b\n1. \u7531\u5305\u542b\u6709\u5173\u6280\u672f\u7ec6\u8282\u95ee\u7b54\u7684\u8bba\u575b\u4f5c\u4e3a\u8865\u5145\uff0c\u4f7f\u5927\u5bb6\u53ef\u4ee5\u76f8\u4e92\u7b54\u7591\u5e76\u4ea4\u6362\u7ecf\u9a8c\u3002\n\n<h5 align=\"center\">\u5c06\u672c\u4e66\uff08\u4e2d\u82f1\u6587\u7248\uff09\u7528\u4f5c\u6559\u6750\u6216\u53c2\u8003\u4e66\u7684\u5927\u5b66</h5>\n<p align=\"center\">\n  <img width=\"400\"  src=\"https://d2l.ai/_images/map.png\">\n</p>\n\n\u5982\u679c\u672c\u4e66\u5bf9\u4f60\u6709\u5e2e\u52a9\uff0c\u8bf7Star (\u2605) \u672c\u4ed3\u5e93\u6216\u5f15\u7528\u672c\u4e66\u7684\u82f1\u6587\u7248\uff1a\n\n```\n@book{zhang2023dive,\n    title={Dive into Deep Learning},\n    author={Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},\n    publisher={Cambridge University Press},\n    note={\\url{https://D2L.ai}},\n    year={2023}\n}\n```\n\n## \u672c\u4e66\u7684\u82f1\u6587\u7248\n\n\u867d\u7136\u7eb8\u8d28\u4e66\u5df2\u51fa\u7248\uff0c\u4f46\u6df1\u5ea6\u5b66\u4e60\u9886\u57df\u4f9d\u7136\u5728\u8fc5\u901f\u53d1\u5c55\u3002\u4e3a\u4e86\u5f97\u5230\u6765\u81ea\u66f4\u5e7f\u6cdb\u7684\u82f1\u6587\u5f00\u6e90\u793e\u533a\u7684\u5e2e\u52a9\uff0c\u4ece\u800c\u63d0\u5347\u672c\u4e66\u8d28\u91cf\uff0c\u672c\u4e66\u7684\u65b0\u7248\u5c06\u7ee7\u7eed\u7528\u82f1\u6587\u7f16\u5199\uff0c\u5e76\u642c\u56de\u4e2d\u6587\u7248\u3002\n\n\u6b22\u8fce\u5173\u6ce8\u672c\u4e66\u7684[\u82f1\u6587\u5f00\u6e90\u9879\u76ee](https://github.com/d2l-ai/d2l-en)\u3002\n\n## \u4e2d\u82f1\u6587\u6559\u5b66\u8d44\u6e90\n\n\u52a0\u5dde\u5927\u5b66\u4f2f\u514b\u5229\u5206\u6821 2019 \u5e74\u6625\u5b66\u671f [*Introduction to Deep Learning* \u8bfe\u7a0b](http://courses.d2l.ai/berkeley-stat-157/index.html)\u6559\u6750\uff08\u540c\u65f6\u63d0\u4f9b\u542b\u6559\u5b66\u89c6\u9891\u5730\u5740\u7684[\u4e2d\u6587\u7248\u8bfe\u4ef6](https://github.com/d2l-ai/berkeley-stat-157/tree/master/slides-zh)\uff09\u3002\n\n## \u5b66\u672f\u754c\u63a8\u8350\n\n> <p>\"Dive into this book if you want to dive into deep learning!\"</p>\n> <b>&mdash; \u97e9\u5bb6\u709c\uff0cACM \u9662\u58eb\u3001IEEE \u9662\u58eb\uff0c\u7f8e\u56fd\u4f0a\u5229\u8bfa\u4f0a\u5927\u5b66\u9999\u69df\u5206\u6821\u8ba1\u7b97\u673a\u7cfb Michael Aiken Chair \u6559\u6388</b>\n\n> <p>\"This is a highly welcome addition to the machine learning literature.\"</p>\n> <b>&mdash; Bernhard Sch\u00f6lkopf\uff0cACM \u9662\u58eb\u3001\u5fb7\u56fd\u56fd\u5bb6\u79d1\u5b66\u9662\u9662\u58eb\uff0c\u5fb7\u56fd\u9a6c\u514b\u65af\u2022\u666e\u6717\u514b\u7814\u7a76\u6240\u667a\u80fd\u7cfb\u7edf\u9662\u9662\u957f</b>\n\n> <p>\"\u4e66\u4e2d\u4ee3\u7801\u53ef\u8c13\u2018\u6240\u5b66\u5373\u6240\u7528\u2019\u3002\"</p>\n> <b>&mdash; \u5468\u5fd7\u534e\uff0cACM \u9662\u58eb\u3001IEEE \u9662\u58eb\u3001AAAS \u9662\u58eb\uff0c\u5357\u4eac\u5927\u5b66\u8ba1\u7b97\u673a\u79d1\u5b66\u4e0e\u6280\u672f\u7cfb\u4e3b\u4efb</b>\n\n> <p>\"\u8fd9\u672c\u4e66\u53ef\u4ee5\u5e2e\u52a9\u6df1\u5ea6\u5b66\u4e60\u5b9e\u8df5\u8005\u5feb\u901f\u63d0\u5347\u81ea\u5df1\u7684\u80fd\u529b\u3002\"</p>\n> <b>&mdash; \u5f20\u6f7c\uff0cASA \u9662\u58eb\u3001IMS \u9662\u58eb\uff0c\u9999\u6e2f\u79d1\u6280\u5927\u5b66\u8ba1\u7b97\u673a\u7cfb\u548c\u6570\u5b66\u7cfb\u6559\u6388</b>\n\n## \u5de5\u4e1a\u754c\u63a8\u8350\n\n> <p>\"\u4e00\u672c\u4f18\u79c0\u7684\u6df1\u5ea6\u5b66\u4e60\u6559\u6750\uff0c\u503c\u5f97\u4efb\u4f55\u60f3\u4e86\u89e3\u6df1\u5ea6\u5b66\u4e60\u4f55\u4ee5\u5f15\u7206\u4eba\u5de5\u667a\u80fd\u9769\u547d\u7684\u4eba\u5173\u6ce8\u3002\"</p>\n> <b>&mdash; \u9ec4\u4ec1\u52cb\uff0cNVIDIA\u521b\u59cb\u4eba & CEO</b>\n\n> <p>\"\u300a\u52a8\u624b\u5b66\u6df1\u5ea6\u5b66\u4e60\u300b\u662f\u6700\u9002\u5408\u5de5\u4e1a\u754c\u7814\u53d1\u5de5\u7a0b\u5e08\u5b66\u4e60\u7684\u3002\u6211\u6beb\u65e0\u4fdd\u7559\u5730\u5411\u5e7f\u5927\u7684\u8bfb\u8005\u4eec\u5f3a\u70c8\u63a8\u8350\u3002\"</p>\n> <b>&mdash; \u4f59\u51ef\uff0c\u5730\u5e73\u7ebf\u516c\u53f8\u521b\u59cb\u4eba & CEO</b>\n\n> <p>\"\u5f3a\u70c8\u63a8\u8350\u8fd9\u672c\u4e66\uff01\u6211\u7279\u522b\u8d5e\u8d4f\u8fd9\u79cd\u624b\u8111\u4e00\u4f53\u7684\u5b66\u4e60\u65b9\u5f0f\u3002\"</p>\n> <b>&mdash; \u6f06\u8fdc\uff0c\u590d\u65e6\u5927\u5b66\u201c\u6d69\u6e05\u201d\u6559\u6388\u3001\u4eba\u5de5\u667a\u80fd\u521b\u65b0\u4e0e\u4ea7\u4e1a\u7814\u7a76\u9662\u9662\u957f</b>\n\n> <p>\"\u300a\u52a8\u624b\u5b66\u6df1\u5ea6\u5b66\u4e60\u300b\u662f\u4e00\u672c\u5f88\u5bb9\u6613\u8ba9\u5b66\u4e60\u8005\u4e0a\u763e\u7684\u4e66\u3002\"</p>\n> <b>&mdash; \u6c88\u5f3a\uff0c\u5c06\u95e8\u521b\u6295\u521b\u59cb\u5408\u4f19\u4eba</b>\n\n## \u8d21\u732e\n\n\u611f\u8c22[\u793e\u533a\u8d21\u732e\u8005\u4eec](https://github.com/d2l-ai/d2l-zh/graphs/contributors)\u4e3a\u6bcf\u4e00\u4f4d\u8bfb\u8005\u6539\u8fdb\u8fd9\u672c\u5f00\u6e90\u4e66\u3002\n\n[\u5982\u4f55\u8d21\u732e](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/contributing.html) | [\u81f4\u8c22](https://zh.d2l.ai/chapter_preface/index.html) | [\u8ba8\u8bba\u6216\u62a5\u544a\u95ee\u9898](https://discuss.d2l.ai/c/chinese-version/16) | [\u5176\u4ed6](INFO.md)\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 71583602,
    "name": "cs-video-courses",
    "full_name": "Developer-Y/cs-video-courses",
    "description": "List of Computer Science courses with video lectures.",
    "html_url": "https://github.com/Developer-Y/cs-video-courses",
    "clone_url": "https://github.com/Developer-Y/cs-video-courses.git",
    "owner_login": "Developer-Y",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/22699926?v=4",
    "stargazers_count": 69516,
    "watchers_count": 69516,
    "forks_count": 9358,
    "open_issues_count": 2,
    "size": 847,
    "language": null,
    "languages": {},
    "topics": [
      "algorithms",
      "bioinformatics",
      "computational-biology",
      "computational-physics",
      "computer-architecture",
      "computer-science",
      "computer-vision",
      "database-systems",
      "databases",
      "deep-learning",
      "embedded-systems",
      "machine-learning",
      "quantum-computing",
      "reinforcement-learning",
      "robotics",
      "security",
      "systems",
      "web-development"
    ],
    "license_name": null,
    "created_at": "2016-10-21T17:02:11+00:00",
    "updated_at": "2025-08-05T19:51:17+00:00",
    "pushed_at": "2025-07-30T17:26:44+00:00",
    "contributors_count": 100,
    "readme_length": 192437,
    "readme_content": "<!-- omit in toc -->\n# Computer Science courses with video lectures\n\n<!-- omit in toc -->\n## Introduction\n\n- Please check [NOTES](https://github.com/Developer-Y/cs-video-courses/blob/master/NOTES.md) for general information about this list.\n- Please refer [CONTRIBUTING.md](https://github.com/Developer-Y/cs-video-courses/blob/master/CONTRIBUTING.md) for contribution guidelines.\n- Please feel free to raise any genuine issue you may have, however, it has been noticed that few people open empty issues to raise their GitHub contribution on their account. Such spammers will be blocked. \n- You are welcome to contribute, please create PR for actual college/University level courses. Please do not add links for small MOOCs, basic tutorials, or advertisements for some sites/channels.\n\n------------------------------\n\nTable of Contents\n\n------------------------------\n\n- [Introduction to Computer Science](#introduction-to-computer-science)\n- [Data Structures and Algorithms](#data-structures-and-algorithms)\n- [Systems Programming](#systems-programming)\n  * [Operating Systems](#operating-systems)\n  * [Distributed Systems](#distributed-systems)\n  * [Real-Time Systems](#real-time-systems) \n- [Database Systems](#database-systems)\n- [Software Engineering](#software-engineering)\n  * [Object Oriented Design](#object-oriented-design)\n  * [Software Engineering](#software-engineering)\n  * [Software Architecture](#software-architecture)\n  * [Concurrency](#concurrency)\n  * [Mobile Application Development](#mobile-application-development)\n- [Artificial Intelligence](#artificial-intelligence)\n- [Machine Learning](#machine-learning)\n  * [Introduction to Machine Learning](#introduction-to-machine-learning)\n  * [Data Mining](#data-mining)\n  * [Probabilistic Graphical Modeling](#probabilistic-graphical-modeling)\n  * [Deep Learning](#deep-learning)\n  * [Reinforcement Learning](#reinforcement-learning)\n  * [Advanced Machine Learning](#advanced-machine-learning)\n  * [Natural Language Processing](#natural-language-processing)\n  * [Generative AI](#generative-ai)\n  * [Computer Vision](#computer-vision)\n  * [Time Series Analysis](#time-series-analysis)\n  * [Optimization](#optimization)\n  * [Unsupervised Learning](#unsupervised-learning)\n  * [Misc Machine Learning Topics](#misc-machine-learning-topics)\n- [Computer Networks](#computer-networks)\n- [Math for Computer Scientist](#math-for-computer-scientist)\n- [Web Programming and Internet Technologies](#web-programming-and-internet-technologies)\n- [Theoretical CS and Programming Languages](#theoretical-cs-and-programming-languages)\n- [Embedded Systems](#embedded-systems)\n- [Real time system evaluation](#real-time-system-evaluation)\n- [Computer Organization and Architecture](#computer-organization-and-architecture)\n- [Security](#security)\n- [Computer Graphics](#computer-graphics)\n- [Image Processing and Computer Vision](#image-processing-and-computer-vision)\n- [Computational Physics](#computational-physics)\n- [Computational Biology](#computational-biology)\n- [Quantum Computing](#quantum-computing)\n- [Robotics and Control](#robotics-and-control)\n- [Computational Finance](#computational-finance)\n- [Network Science](#network-science)\n- [Blockchain Development](#blockchain-development)\n- [Misc](#misc)\n\n<!-- omit in toc -->\n## Courses\n\n------------------------------\n\n### Introduction to Computer Science\n\n- [CS 10 - The Beauty and Joy of Computing - Spring 2015 - Dan Garcia - UC Berkeley InfoCoBuild](http://www.infocobuild.com/education/audio-video-courses/computer-science/cs10-spring2015-berkeley.html)\n- [6.0001 - Introduction to Computer Science and Programming in Python - MIT OCW](https://ocw.mit.edu/courses/6-0001-introduction-to-computer-science-and-programming-in-python-fall-2016/video_galleries/lecture-videos/)\n- [6.001 - Structure and Interpretation of Computer Programs, MIT](https://ocw.mit.edu/courses/6-001-structure-and-interpretation-of-computer-programs-spring-2005/video_galleries/video-lectures/)\n- [Introduction to Computational Thinking - MIT](https://computationalthinking.mit.edu/Fall22/)\n- [CS 50 - Introduction to Computer Science, Harvard University](https://online-learning.harvard.edu/course/cs50-introduction-computer-science) ([cs50.tv](http://cs50.tv/2017/fall/))\n- [CS50R - Introduction to Programming with R](https://cs50.harvard.edu/r/2024/) ([Lecture Videos](https://www.youtube.com/playlist?list=PLhQjrBD2T382yfNp_-xzX244d-O9W6YmD))\n- [CS 61A - Structure and Interpretation of Computer Programs [Python], UC Berkeley](https://cs61a.org/)\n- [CPSC 110 - Systematic Program Design [Racket], University of British Columbia](https://www.youtube.com/channel/UC7dEjIUwSxSNcW4PqNRQW8w/playlists?view=1&flow=grid&sort=da)\n- [CS50's Understanding Technology](https://www.youtube.com/playlist?list=PLhQjrBD2T382p8amnvUp1rws1p7n7gJ2p)\n- [CSE 142 Computer Programming I (Java Programming), Spring 2016 - University of Washington](https://courses.cs.washington.edu/courses/cse142/16sp/calendar.shtml)\n- [CS 1301 Intro to computing - Gatech](https://www.cc.gatech.edu/classes/AY2016/cs1301c_fall/)\n- [CS 106A - Programming Methodology, Stanford University](https://see.stanford.edu/Course/CS106A) ([Lecture Videos](https://www.youtube.com/playlist?list=PL84A56BC7F4A1F852))\n- [CS 106B - Programming Abstractions, Stanford University](https://see.stanford.edu/Course/CS106B) ([Lecture Videos](https://www.youtube.com/playlist?list=PLnfg8b9vdpLn9exZweTJx44CII1bYczuk))\n- [CS 106L - Standard C++ Programming](https://web.stanford.edu/class/cs106l/)([Lecture Videos](https://www.youtube.com/playlist?list=PLCgD3ws8aVdolCexlz8f3U-RROA0s5jWA))\n- [CS 106X - Programming Abstractions in C++](http://web.stanford.edu/class/cs106x/) ([Lecture Videos](https://www.youtube.com/playlist?list=PLrivl8gTKLcpIJ-ktHCxMEgWOn8LawYhb))\n- [CS 107 - Programming Paradigms, Stanford University](https://see.stanford.edu/Course/CS107)\n- [CmSc 150 - Introduction to Programming with Arcade Games, Simpson College](http://ProgramArcadeGames.com)\n- [IN2377 - Concepts of C++ programming (Winter 2023), TUM](https://live.rbg.tum.de/?year=2023&term=W&slug=cpp&view=3) ([Winter 2022](https://live.rbg.tum.de/?year=2022&term=W&slug=cpp&view=3)) ([Summer 2022](https://live.rbg.tum.de/?year=2022&term=S&slug=ccppprog&view=3)) ([Summer 2021](https://live.rbg.tum.de/?year=2021&term=S&slug=ccppprog&view=3))\n- [IN1503 - Advanced C++ Programming, TUM](https://live.rbg.tum.de/?year=2023&term=W&slug=AdvProg&view=3)\n- [LINFO 1104 - Paradigms of computer programming, Peter Van Roy, Universit\u00e9 catholique de Louvain, Belgium - EdX](https://www.youtube.com/playlist?list=PLw454N-VXALSIzIe_eL5U8L4S68v2X_ak)\n- [FP 101x - Introduction to Functional Programming, TU Delft](https://ocw.tudelft.nl/courses/introduction-to-functional-programming/)\n- [Introduction to Problem Solving and Programming - IIT Kanpur](https://nptel.ac.in/courses/106104074/)\n- [Introduction to programming in C - IIT Kanpur](https://nptel.ac.in/courses/106104128/)\n- [Programming in C++ - IIT Kharagpur](https://nptel.ac.in/courses/106105151/)\n- [Python Boot Camp Fall 2016 - Berkeley Institute for Data Science (BIDS)](https://www.youtube.com/playlist?list=PLKW2Azk23ZtSeBcvJi0JnL7PapedOvwz9)\n- [CS 101 - Introduction to Computer Science - Udacity](https://www.youtube.com/playlist?list=PLAwxTw4SYaPmjFQ2w9j05WDX8Jtg5RXWW)\n- [6.00SC - Introduction to Computer Science and Programming (Spring 2011) - MIT OCW](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-00sc-introduction-to-computer-science-and-programming-spring-2011/)\n- [6.00 - Introduction to Computer Science and Programming (Fall 2008) - MIT OCW](https://ocw.mit.edu/courses/6-00-introduction-to-computer-science-and-programming-fall-2008/video_galleries/video-lectures/)\n- [6.01SC - Introduction to Electrical Engineering and Computer Science I - MIT OCW](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-01sc-introduction-to-electrical-engineering-and-computer-science-i-spring-2011/)\n- [Modern C++ Course (2018) - Bonn University](https://www.youtube.com/playlist?list=PLgnQpQtFTOGR50iIOtO36nK6aNPtVq98C)\n- [Modern C++ (Lecture & Tutorials, 2020, Vizzo & Stachniss) - University of Bonn](https://www.youtube.com/playlist?list=PLgnQpQtFTOGRM59sr3nSL8BmeMZR9GCIA)\n- [UW Madison CS 368 C++ for Java Programmers Fall 2020, by Michael Doescher](https://www.youtube.com/playlist?list=PLXY5xcFHqg33srpQjC7q7jqITLxcErPCM)\n- [UW Madison CS 354 Machine Organization and Programming spring 2020, 2021, by Michael Doescher](https://www.youtube.com/playlist?list=PLXY5xcFHqg32r5MZ-HfpA2Tr8Ke2lDYwI)\n- [Cornell CS 1110 Introduction to Computing using Python fall 2020, by Walker White](https://www.cs.cornell.edu/courses/cs1110/2020fa/lessons/) ([Lecture Videos](https://vod.video.cornell.edu/channel/CS+1110+Fall+2020/179890731))\n- [Cornell ECE 4960 Computational and Software Engineering spring 2017, by Edwin Kan](https://www.youtube.com/playlist?list=PLcVqWUh-bHiFN2CY1KMTw0-L39iDXlemi)\n\n------------------------------\n\n### Data Structures and Algorithms\n\n- [ECS 36C - Data Structures and Algorithms (C++) - Spring 2020 - Jo\u00ebl Porquet-Lupine - UC Davis](https://lupteach.gitlab.io/courses/ucd-ecs36c/online/)\n- [Programming and Data Structures with Python, 2021-2022, Sem I - by Prof. Madhavan Mukund, CMI](https://www.cmi.ac.in/~madhavan/courses/python2021sep/)\n- [Graph Algorithms - Robert Sedgewick - Princeton University](https://www.youtube.com/watch?v=0qF7tPSQdCg)\n- [6.006 - Introduction to Algorithms, MIT OCW](https://ocw.mit.edu/courses/6-006-introduction-to-algorithms-spring-2020/video_galleries/lecture-videos/)\n- [MIT 6.006 Introduction to Algorithms, Spring 2020](https://www.youtube.com/playlist?list=PLUl4u3cNGP63EdVPNLG3ToM6LaEUuStEY)\n- [Algorithms: Design and Analysis 1 - Stanford University](https://www.youtube.com/playlist?list=PLXFMmlk03Dt7Q0xr1PIAriY5623cKiH7V)\n- [Algorithms: Design and Analysis 2 - Stanford University](https://www.youtube.com/playlist?list=PLXFMmlk03Dt5EMI2s2WQBsLsZl7A5HEK6)\n- [COS 226 Algorithms, Youtube, Princeton - by Robert Sedgewick and Kevin Wayne](https://www.youtube.com/watch?v=1QZDe28peZk&list=PLRdD1c6QbAqJn0606RlOR6T3yUqFWKwmX&index=1)\n- [CSE 331 Introduction to Algorithm Design and Analysis, SUNY University at Buffalo, NY - Fall 2017](http://www-student.cse.buffalo.edu/~atri/cse331/fall17/index.html) ([Lectures](https://www.youtube.com/playlist?list=PLZBCR-EGqNpoiHeO17FlLADJ38Kb3EiPU)) ([Homework Walkthroughs](https://www.youtube.com/playlist?list=PLZBCR-EGqNpoVyQCIUDHiXnL-zdFD_ixk))\n- [CSE 373 - Analysis of Algorithms, Stony Brook - Prof Skiena](http://www.cs.sunysb.edu/~algorith/video-lectures/)\n- [COP 3530 Data Structures and Algorithms, Prof Sahni, UFL](http://www.cise.ufl.edu/~sahni/cop3530/) ([Videos](http://www.cise.ufl.edu/academics/courses/preview/cop3530sahni/))\n- [CS225 - Data Structures - University of Illinois at Urbana-Champaign](https://cs.illinois.edu/courses/profile/CS225)([Video lectures](https://www.youtube.com/playlist?list=PLRdSp8jtJxBqG3KNQPKKB-0Z2hh9omoDo))\n- [CS2 - Data Structures and Algorithms - Richard Buckland - UNSW](https://www.youtube.com/playlist?list=PLE621E25B3BF8B9D1)\n- [Data Structures - Pepperdine University](https://itunes.apple.com/us/course/data-structures/id546468797)\n- [CS 161 - Design and Analysis of Algorithms, Prof. Tim Roughgarden, Stanford University](http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=IntroToAlgorithms)\n- [6.046J - Introduction to Algorithms - Fall 2005, MIT OCW](https://ocw.mit.edu/courses/6-046j-introduction-to-algorithms-sma-5503-fall-2005/)\n- [Introduction to Algorithms (Spring 2020), MIT OCW](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-006-introduction-to-algorithms-spring-2020/)\n- [6.046 - Design and Analysis of Algorithms, Spring 2015 - MIT OCW](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-design-and-analysis-of-algorithms-spring-2015/lecture-videos/)\n- [CS 473 - Algorithms - University of Illinois at Urbana-Champaign](https://courses.engr.illinois.edu/cs473/sp2016/lectures.html) ([Notes - Jeff Erickson](http://jeffe.cs.illinois.edu/teaching/algorithms/)) ([YouTube](https://www.youtube.com/playlist?list=PL0v718LJg-78SFq81e4kJh_rS8XbKZ7Kn))\n- [COMP300E - Programming Challenges, Prof Skiena, Hong Kong University of Science and Technology - 2009](https://www.youtube.com/playlist?list=PL07B3F10B48592010)\n- [16s-4102 - Algorithms, University of Virginia](http://www.cs.virginia.edu/~shelat/16s-4102/) ([Youtube](https://www.youtube.com/channel/UCxXYk53cSZof2bR_Ax0uJYQ/videos))\n- [CS 61B - Data Structures (Java) - UC Berkeley](https://inst.eecs.berkeley.edu/~cs61b/)([Youtube](https://www.youtube.com/watch?v=gG4--V_PpEk&list=PLjuu7kFWxFtZBm-5GifiVpqdAxeW7Hsax))\n- [CS 170 Algorithms - UCBerkeley](https://cs170.org/) [Fall 2019, Youtube](https://www.youtube.com/playlist?list=PLIygTcviGPKD4TU_QsvJI0G7QnrIS_7Wn) [Fall 2018, Youtube](https://www.youtube.com/watch?v=fd5P-8IQwMY&list=PLkFD6_40KJIx8lWWbE-Uk069aZ1R-W-VU&index=2&t=0s) [Fall 2018,Bilibili](https://www.bilibili.com/video/av43955743/?p=1) [2013 Bilibili](https://www.bilibili.com/video/av26670685/)\n- [CS 159 Data-Driven Algorithm Design - Caltech](https://sites.google.com/view/cs-159-spring-2020/lectures?authuser=0) [Spring 2020, Youtube](https://www.youtube.com/playlist?list=PLuz4CTPOUNi4Dz6zBPypcI8I3oJUjFKk4)\n- [ECS 122A - Algorithm Design and Analysis, UC Davis](http://web.cs.ucdavis.edu/~gusfield/cs122f10/videolist.html)\n- [CSE 373 - Data Structures and Algorithms, Winter 2024 - University of Washington](https://courses.cs.washington.edu/courses/cse373/24wi/) ([Winter 2024, Youtube](https://www.youtube.com/playlist?list=PLEcoVsAaONjd5n69K84sSmAuvTrTQT_Nl)) ([Spring 2023, Notes](https://courses.cs.washington.edu/courses/cse373/23sp/)) ([Spring 2023, Youtube](https://www.youtube.com/playlist?list=PLEcoVsAaONjfHSAbP1AsVjAxIOFue6uWh))\n- [CSEP 521 - Applied Algorithms, Winter 2013 - University of Washington](https://courses.cs.washington.edu/courses/csep521/13wi/) ([Videos](https://courses.cs.washington.edu/courses/csep521/13wi/video/))\n- [Data Structures And Algorithms - IIT Delhi](https://nptel.ac.in/courses/106102064/)\n- [Design and Analysis of Algorithms - IIT Bombay](https://nptel.ac.in/courses/106101060/)\n- [Programming, Data Structures and Algorithms - IIT Madras](https://nptel.ac.in/courses/106106127/)\n- [Design and Analysis of Algorithms - IIT Madras](https://nptel.ac.in/courses/106106131/)\n- [Fundamental Algorithms:Design and Analysis - IIT Kharagpur](https://nptel.ac.in/courses/106105157/)\n- [Programming and Data Structure - IIT Kharagpur](https://nptel.ac.in/courses/106105085/)\n- [Programming, Data structures and Algorithms - IIT Madras](https://nptel.ac.in/courses/106106133/)\n- [Programming, Data Structures and Algorithms in Python - IIT Madras](https://nptel.ac.in/courses/106106145/)\n- [Programming and Data structures (PDS) - IIT Madras](https://nptel.ac.in/courses/106106130/)\n- [COP 5536 Advanced Data Structures, Prof Sahni - UFL](http://www.cise.ufl.edu/~sahni/cop5536/index.html) ([Videos](http://www.cise.ufl.edu/academics/courses/preview/cop5536sahni/))\n- [CS 261 - A Second Course in Algorithms, Stanford University](http://theory.stanford.edu/~tim/w16/w16.html) ([Youtube](https://www.youtube.com/playlist?list=PLEGCF-WLh2RJh2yDxlJJjnKswWdoO8gAc))\n- [CS 224 - Advanced Algorithms, Harvard University](http://people.seas.harvard.edu/~minilek/cs224/fall14/index.html) ([Lecture Videos](http://people.seas.harvard.edu/~minilek/cs224/fall14/lec.html)) ([Youtube](https://www.youtube.com/playlist?list=PL2SOU6wwxB0uP4rJgf5ayhHWgw7akUWSf))\n- [CS 6150 - Advanced Algorithms (Fall 2016), University of Utah](https://www.youtube.com/playlist?list=PLbuogVdPnkCp8X9FHOglnLqFjyvqGLftx)\n- [CS 6150 - Advanced Algorithms (Fall 2017), University of Utah](https://www.youtube.com/playlist?list=PLbuogVdPnkCqS9Z419eky9m6gJP7zfhO9)\n- [ECS 222A - Graduate Level Algorithm Design and Analysis, UC Davis](http://web.cs.ucdavis.edu/~gusfield/cs222f07/videolist.html)\n- [6.851 - Advanced Data Structures, MIT](http://courses.csail.mit.edu/6.851/spring14/lectures/) ([MIT OCW](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-851-advanced-data-structures-spring-2012/lecture-videos/))\n- [6.854 - Advanced Algorithms, MIT](https://www.youtube.com/playlist?list=PL6ogFv-ieghdoGKGg2Bik3Gl1glBTEu8c) ([Prof. Karger lectures](https://www.youtube.com/channel/UCtv9PiQVUDzsT4yl7524DCg/videos))\n- [CS264 Beyond Worst-Case Analysis, Fall 2014 - Tim Roughgarden Lecture](http://theory.stanford.edu/~tim/f14/f14.html) ([Youtube](https://www.youtube.com/playlist?list=PLEGCF-WLh2RL8jsZpaf2tLHa5LotFEt5b))\n- [CS364A Algorithmic Game Theory, Fall 2013 - Tim Roughgarden Lectures](https://www.youtube.com/playlist?list=PLEGCF-WLh2RJBqmxvZ0_ie-mleCFhi2N4)\n- [CS364B Advanced Mechanism Design, Winter 2014 - Tim Roughgarden Lectures](https://www.youtube.com/playlist?list=PLEGCF-WLh2RI77PL4gwLld_OU9Zh3TCX9)\n- [Algorithms - Aduni](http://aduni.org/courses/algorithms/index.php?view=cw)\n- [6.889 - Algorithms for Planar Graphs and Beyond (Fall 2011) MIT](http://courses.csail.mit.edu/6.889/fall11/lectures/)\n- [6.890 Algorithmic Lower Bounds: Fun with Hardness Proofs - MIT OCW](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-890-algorithmic-lower-bounds-fun-with-hardness-proofs-fall-2014/)\n- [Computer Algorithms - 2 - IIT Kanpur](https://nptel.ac.in/courses/106104019/)\n- [Parallel Algorithm - IIT Kanpur](https://nptel.ac.in/courses/106104120/)\n- [Graph Theory - IISC Bangalore](https://nptel.ac.in/courses/106108054/)\n- [Data Structures - mycodeschool](https://www.youtube.com/watch?v=92S4zgXN17o&list=PL2_aWCzGMAwI3W_JlcBbtYTwiQSsOTa6P)\n- [Algorithmic Game Theory, Winter 2020/21 - Uni Bonn](https://www.youtube.com/playlist?list=PLyzcvvgje7aD_DjpmhFzQ9DVS8zzhrgp6)\n- [NETS 4120: Algorithmic Game Theory, Spring 2023 - UPenn](https://www.youtube.com/playlist?list=PLlIlhe_rS4U1MfB0NzG4IWb7CM0xKkx4d)\n- [Introduction to Game Theory and Mechanism Design - IIT Kanpur](https://www.youtube.com/playlist?list=PL3eEm6KzZ3lF2TlVOnPyJHyGWJhUogn-D)\n- [15-850 Advanced Algorithms - CMU Spring 2023](https://scs.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx#folderID=%2253c58248-7fd4-4f71-8774-af85013a570a%22&page=1)\n- [CS 270. Combinatorial Algorithms and Data Structures, Spring 2021](https://people.eecs.berkeley.edu/~prasad/spring2021.html) ([Youtube](https://www.youtube.com/playlist?list=PLfkeJ2f4i0AfWApBP8X8YvQfN4WbRQTC3))\n- [CMU 15 850 Advanced Algorithms spring 2023, by Anupam Gupta](https://scs.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx#folderID=%2253c58248-7fd4-4f71-8774-af85013a570a%22&page=1)\n- [UC Berkeley CS 294-165 Sketching Algorithms fall 2020, by Jelani Nelson](https://www.sketchingbigdata.org/fall20/lec/) ([Youtube](https://www.youtube.com/playlist?list=PLIygTcviGPKCdx1AVD-CAzNk5uXDu9wIA))\n- [UIUC CS 498 ABD / CS 598 CSC Algorithms for Big Data fall 2020, by Chandra Chekuri](https://www.youtube.com/playlist?list=PL682UO4IMem_OA8_wY3nnSDLOSWr3PgYa)\n- [Algorithms for Data Science spring 2021, by Anil Maheshwari ](https://people.scs.carleton.ca/~maheshwa/courses/ADS/ADS-S20.html)\n- [CMU 15 859 Algorithms for Big Data fall 2020, by David Woodruff](http://www.cs.cmu.edu/~dwoodruf/teaching/15859-fall20/index.html)\n- [CO 642 Graph Theory - University of Waterloo](https://www.youtube.com/playlist?list=PL2BdWtDKMS6mplieDd_vls0TBX9Fq2jht)\n- [COMS W4241 Numerical Algorithms spring 2006, by Henryk Wozniakowski - Columbia](https://www.youtube.com/playlist?list=PL682UO4IMem98vm26lNUJ0TV0-EFrcUJb)\n- [Bonn Algorithms and Uncertainty summer 2021, by Thomas Kesselheim](https://www.youtube.com/playlist?list=PLyzcvvgje7aDZRFMJZgaVgOW5t5KLvD1-)\n- [Harvard Information Theory 2022, by Gregory Falkovich](https://www.youtube.com/playlist?list=PLDEN2FPNHwVZKAFqfFl1b_NNAESTJwV9o)\n- [Math 510 - Linear Programming and Network Flows - Colorado State University](https://www.math.colostate.edu/~adams/teaching/math510fall2020/)\n- [LINFO 2266 Advanced Algorithms for Optimization 2021, by Pierre Schaus - UCLouvain](https://www.youtube.com/playlist?list=PL682UO4IMem-wgYnJl5yMswlNkve_8oGU)\n- [MIT 6.5210 / 6.854 / 18.415 Advanced Algorithms Fall 2013, 2020, 2021, 2022, by David Karger](https://6.5210.csail.mit.edu/materials) ([Spring 2016, by Ankur Moitra](https://www.youtube.com/playlist?list=PL6ogFv-ieghdoGKGg2Bik3Gl1glBTEu8c))\n- [CMU 10 801 Advanced Optimization and Randomized Algorithms spring 2014, by Suvrit Sra and Alex Smola](https://www.cs.cmu.edu/~suvrit/teach/)\n- [UC Santa Cruz CSE 101 Intro to Data Structures and Algorithms fall 2022, by Seshadhri Comandur](https://www.youtube.com/playlist?list=PLOQjlWvnI0fY1BCDxdiUSwkRHjnNI73G6) ([Fall 2020](https://www.youtube.com/playlist?list=PLOQjlWvnI0fZGffr1_MqCoaC5nUVtQIWz))\n- [UC Santa Cruz CSE 201 Analysis of Algorithms winter 2022, by Seshadhri Comandur](https://www.youtube.com/playlist?list=PLOQjlWvnI0fYmOmrAAN-g1d4nFB2uz6tU)\n- [UC Santa Cruz CSE 202 Combinatorial Algorithms spring 2021, by Seshadhri Comandur](https://www.youtube.com/playlist?list=PLOQjlWvnI0fbn9zAJfvJoQF1nc50KQR9g)\n- [UC Santa Cruz CSE 104, 204 Computational Complexity spring 2022, by Seshadhri Comandur](https://www.youtube.com/playlist?list=PLOQjlWvnI0fYMPFnJeVZ0kt4KPwWcbF0o) ([Fall 2020](https://www.youtube.com/playlist?list=PLOQjlWvnI0fas529oXenovd3MyafNQbKl))\n- [UC Santa Cruz CSE 290A Randomized Algorithms spring 2020, by Seshadhri Comandur](https://www.youtube.com/playlist?list=PLOQjlWvnI0faRpH2oJcyW4CuM5Clt8a2n)\n- [Algorithms for Big-Data (Fall 2020) - Saket Saurabh](https://sites.google.com/view/sakethome/teaching/algorithms-for-big-data-fall-2020)\n- [CS498ABD - Algorithms for Big Data - UIUC, Fall 2020](https://courses.engr.illinois.edu/cs498abd/fa2020/schedule.html)\n- [Advanced Data Structures](https://www.youtube.com/playlist?list=PLN-ShipRKQ0h6jIphD381pHdQtj_APRM8)\n- [CS60025 Algorithmic Game Theory - IIT KGP - Winter 2020](http://cse.iitkgp.ac.in/~palash/Courses/2020AlgorithmicGameTheory/agt2020.html)\n- [CS60083 Parameterized Algorithms - IIT KGP](http://cse.iitkgp.ac.in/~palash/Courses/2020ParameterizedAlgo/paramAlgo.html)\n- [Parameterized Complexity](https://sites.google.com/view/sakethome/teaching/parameterized-complexity)\n- [Structural Graph Theory - IIT Madras](https://www.youtube.com/playlist?list=PLtDHG-2klXcEedB8L-jjvb17OIUZbF3gW)\n- [Information Theory - IISC Bangalore](https://nptel.ac.in/courses/108/108/108108168/)\n\n\n\n\n------------------------------\n\n### Systems Programming\n\n- [15-213 Introduction to Computer Systems, Fall 2015  - CMU](https://scs.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx#folderID=%22b96d90ae-9871-4fae-91e2-b1627b43e25e%22&maxResults=150)\n- [Computer Systems: A programmer's Perspective](https://www.youtube.com/playlist?list=PLyboo2CCDSWnhzzzzDQ3OBPrRiIjl-aIE)\n- [CS361 - COMPUTER SYSTEMS - UIC](https://www.cs.uic.edu/~ckanich/cs361/f20/)\n- [CS 3650 - Computer Systems - Fall 2020 - Nat Tuck - NEU](https://web.archive.org/web/20210423030302/https://ntuck-neu.site/2020-09/cs3650/) ([Lectures - YouTube](https://www.youtube.com/playlist?list=PLtg_A_3rzLAtBuwQp6mA3WveYw9Q7GzIZ))\n- [CS 4400 \u2013 Computer Systems   Fall 2016 - UoUtah](https://www.eng.utah.edu/~cs4400/)\n- [Systems - Aduni](http://aduni.org/courses/systems/index.php?view=cw)\n- [CS110: Principles of Computer Systems - Stanford](https://web.stanford.edu/class/archive/cs/cs110/cs110.1202/)\n- #### **Operating Systems**\n  - [ECS 150 - Operating Systems and Systems Programming - Fall 2020 - Jo\u00ebl Porquet-Lupine - UC Davis](https://lupteach.gitlab.io/courses/ucd-ecs150/online/)\n  - [CS124 Operating Systems - California Institute of Technology, Fall 2018 - Youtube](https://www.youtube.com/playlist?list=PL3swII2vlVoVbav6FV98pidq6BsTN4u56)\n  - [CS 162 Operating Systems and Systems Programming, Spring 2015 - University of California, Berkeley](https://archive.org/details/ucberkeley-webcast-PL-XXv-cvA_iBDyz-ba4yDskqMDY6A1w_c?sort=titleSorter)\n  - [CS 4414 - Operating Systems, University of Virginia (rust-class)](http://rust-class.org/pages/classes.html)\n  - [CS 4414 Operating Systems, Fall 2018 - University of Virginia](https://www.cs.virginia.edu/~cr4bd/4414/F2018/schedule.html)\n  - [CSE 421/521 - Introduction to Operating Systems, SUNY University at Buffalo, NY - Spring 2016](https://www.ops-class.org/courses/buffalo/CSE421_Spring2016/) ([Lectures - YouTube](https://www.youtube.com/playlist?list=PLE6LEE8y2Jp-kbEcVR2W3vfx0Pdca0BD3)) ([Recitations 2016](https://www.youtube.com/playlist?list=PLE6LEE8y2Jp_YJn8wu9aJTPOgeWqiaJDF)) ([Assignment walkthroughs](https://www.youtube.com/playlist?list=PLE6LEE8y2Jp9PC8fyzc2meL4XvrVSyP8O))\n  - [CS 377 - Operating Systems, Fall 16 - Umass OS](https://www.youtube.com/playlist?list=PLacuG5pysFbDTmsCRGWsMW_PzIOpXnckw)\n  - [CS 577 - Operating Systems, Spring 20 - Umass OS](https://www.youtube.com/playlist?list=PLacuG5pysFbB2_z9EkSfQIjq3yNzy8igs)\n  - [6.828 - Operating System Engineering [Fall 2014]](https://www.youtube.com/playlist?list=PLfciLKR3SgqNJKKIKUliWoNBBH1VHL3AP)\n  - [6.S081 - Operating System Engineering [Fall 2020]](https://pdos.csail.mit.edu/6.828/2020/schedule.html)\n  - [CSE 30341 - Operating Systems, Spr 2008](https://www.youtube.com/playlist?list=PLAB7D5CA7E262B0E2)\n  - [CSEP 551 Operating Systems Autumn 2014 - University of Washington](https://courses.cs.washington.edu/courses/csep551/14au/video/)\n  - [Introduction to Operating Systems - IIT Madras](https://nptel.ac.in/courses/106106144/)\n  - [CS194 Advanced Operating Systems Structures and Implementation, Spring 2013 InfoCoBuild, UC Berkeley](http://www.infocobuild.com/education/audio-video-courses/computer-science/cs194-spring2013-berkeley.html)\n  - [CSE 60641 - Graduate Operating Systems, Fall 08](https://www.youtube.com/view_play_list?p=22B10D854588E20C)\n  - [Advanced Programming in the UNIX Environment](https://stevens.netmeister.org/631/)\n  - [Operating System - IIT Madras](https://www.youtube.com/playlist?list=PLZ2ps__7DhBYcwlZ7GPCBzbowmiiF4BYR)\n- #### **Distributed Systems**\n  - [CS 677 - Distributed Operating Systems, Spring 24 - Umass OS](https://www.youtube.com/playlist?list=PLacuG5pysFbBpWHfKUU9Dfdk8RmQ7B9EH)\n  - [CS 436 - Distributed Computer Systems - U Waterloo](https://www.youtube.com/playlist?list=PLawkBQ15NDEkDJ5IyLIJUTZ1rRM9YQq6N)\n  - [6.824 - Distributed Systems, Spring 2015 - MIT](https://www.youtube.com/playlist?list=PLkcQbKbegkMqiWf7nF8apfMRL4P4sw8UL)\n  - [6.824 Distributed Systems - Spring 2020 - MIT](https://pdos.csail.mit.edu/6.824/schedule.html) ([Youtube](https://www.youtube.com/playlist?list=PLrw6a1wE39_tb2fErI4-WkMbsvGQk9_UB))\n  - [Distributed Systems Lecture Series](https://www.youtube.com/playlist?list=PLeKd45zvjcDFUEv_ohr_HdUFe97RItdiB)\n  - [Distributed Algorithms, https://canvas.instructure.com/courses/902299](https://www.youtube.com/playlist?list=PL700757A5D4B3F368)\n  - [CSEP 552 - PMP Distributed Systems, Spring 2013 - University of Washington](https://courses.cs.washington.edu/courses/csep552/13sp/) ([Videos](https://courses.cs.washington.edu/courses/csep552/13sp/video/))\n  - [CSE 490H - Scalable Systems: Design, Implementation and Use of Large Scale Clusters, Autumn 2008 - University of Washington](https://courses.cs.washington.edu/courses/cse490h/08au/lectures.htm) ([Videos](https://courses.cs.washington.edu/courses/cse490h/08au/video.htm))\n  - [MOOC - Cloud Computing Concepts - UIUC](https://www.youtube.com/playlist?list=PLFd87qVsaLhOkTLvfp6MC94iFa_1c9wrU)\n  - [Distributed Systems (Prof. Pallab Dasgupta)](https://www.youtube.com/playlist?list=PLUJ7JmcrTifBROWODSG8wgyl20XgBuE-N)\n  - [EdX KTHx ID2203 Reliable Distributed Algorithms](https://www.youtube.com/playlist?list=PLx3mQFFeHPjndmQ0iP9j6C58b90hqGa0X)\n  - [Distributed Data Management - Technische Universit\u00e4t Braunschweig, Germany](http://www.ifis.cs.tu-bs.de/teaching/ss-15/ddm)\n  - [Information Retrieval and Web Search Engines - Technische Universit\u00e4t Braunschweig, Germany](http://www.ifis.cs.tu-bs.de/teaching/ws-1516/IRWS)\n  - [Middleware and Distributed Systems (WS 2009/10) - Dr. Martin von L\u00f6wis - HPI](https://www.tele-task.de/series/729/)\n  - [CSE 138 - Distributed Systems - UC Santa Cruz, Spring 2020](https://www.youtube.com/playlist?list=PLNPUF5QyWU8O0Wd8QDh9KaM1ggsxspJ31) ([2021](https://www.youtube.com/playlist?list=PLNPUF5QyWU8PydLG2cIJrCvnn5I_exhYx))\n  - [CMU 15 440 / 640 Distributed Systems Spring 2022, by Mahadev Satyanarayanan, Padmanabhan Pillai](https://www.youtube.com/playlist?list=PLIygTcviGPKAp30J9kcVW9jPzFC7Otpol)\n  - [UNC Comp533 - Distributed Systems Spring 2020](https://www.youtube.com/playlist?list=PLH5XTBxCO2hzgww9p5sew30lx3ngJsxcB)\n  - [Brown CSCI 1380 Distributed Computer Systems spring 2016, by Tom Doeppner & Rodrigo Fonseca](https://cs.brown.edu/courses/cs138/s16/syllabus.html)\n  - [Distributed Systems lecture series - Martin Kleppmann](https://www.youtube.com/playlist?list=PLeKd45zvjcDFUEv_ohr_HdUFe97RItdiB)\n  - [Distributed Algorithms - Jukka Suomela](https://www.youtube.com/playlist?list=PL2RY7P3JxZN8g9hFCasNqzuDhZbIbAj54)\n  - [Programming Parallel Computers - Jukka Suomela](https://www.youtube.com/playlist?list=PL2RY7P3JxZN-Pz1nwvnoJ9uEHmOmv4jmi)\n- #### **Real-Time Systems**\n  - [CPCS 663 - Real-Time Systems: Video Material - TAMU](http://faculty.cs.tamu.edu/bettati/Courses/663/Video/presentation.html)\n  - [Real Time Systems - IIT Kharagpur](https://nptel.ac.in/courses/106105036/)\n- [6.172 Performance Engineering of Software Systems - MIT OCW](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-172-performance-engineering-of-software-systems-fall-2018/)\n- [Performance Evaluation of Computer Systems - IIT Madras](https://nptel.ac.in/courses/106106048/)\n- [Storage Systems - IISC Bangalore](https://nptel.ac.in/courses/106108058/)\n- [MAP6264 - Queueing Theory - FAU](http://www.cse.fau.edu/~bob/courses/map6264/)([Video Lectures](https://vimeo.com/album/171324/))\n- [EE 380 Colloquium on Computer Systems - Stanford University](http://web.stanford.edu/class/ee380/) ([Lecture videos](https://www.youtube.com/playlist?list=PLoROMvodv4rMWw6rRoeSpkiseTHzWj6vu))\n\n------------------------------\n\n### Database Systems\n\n- [CMPSC 431W Database Management Systems, Fall 2015 - Penn State University](http://www.cse.psu.edu/~wul2/cmpsc431w/) [Lectures - YouTube](https://www.youtube.com/playlist?list=PLstRzn3gXZMdXqAiVJ1NN2CoyXHqma7pQ)\n- [CS121 - Introduction to Relational Database Systems, Fall 2016 - Caltech](http://users.cms.caltech.edu/~donnie/cs121/)\n- [CS 5530 - Database Systems, Spring 2016 - University of Utah](https://www.youtube.com/playlist?list=PLbuogVdPnkCrercQNP9tTsjjPdgRVYvC7)\n- [Distributed Data Management (WT 2018/19) - HPI University of Potsdam](https://www.tele-task.de/series/1224/)\n- [MOOC - Database Stanford Dbclass](https://www.youtube.com/playlist?list=PL6hGtHedy2Z4EkgY76QOcueU8lAC4o6c3)\n- [CSEP 544, Database Management Systems, Au 2015 - University of Washington](https://www.youtube.com/playlist?list=PLTPQEx-31JXjQYrUKvAjUTWgCYluHGs_L)\n- [Database Design - IIT Madras](https://nptel.ac.in/courses/106106093/)\n- [Fundamentals of Database Systems - IIT Kanpur](https://nptel.ac.in/courses/106104135/)\n- [Principles of Database Management, Bart Baesens](https://www.youtube.com/playlist?list=PLdQddgMBv5zEhlpqdiUcf9aTNEtmESgyl)\n- [FIT9003 Database Systems Design - Monash University](https://itunes.apple.com/us/podcast/fit9003-database-systems-design/id306569364)\n- [15-445 - Introduction to Database Systems, CMU](https://15445.courses.cs.cmu.edu/fall2022/) ([YouTube-2017](https://www.youtube.com/playlist?list=PLSE8ODhjZXjYutVzTeAds8xUt1rcmyT7x), [YouTube-2018](https://www.youtube.com/playlist?list=PLSE8ODhjZXja3hgmuwhf89qboV1kOxMx7), [YouTube-2019](https://www.youtube.com/playlist?list=PLSE8ODhjZXjbohkNBWQs_otTrBTrjyohi), [YouTube-2021](https://www.youtube.com/playlist?list=PLSE8ODhjZXjZaHA6QcxDfJ0SIWBzQFKEG), [YouTube-2022](https://www.youtube.com/playlist?list=PLSE8ODhjZXjaKScG3l0nuOiDTTqpfnWFf))\n- [15-721 - Database Systems, CMU](http://15721.courses.cs.cmu.edu/spring2017) ([YouTube-2017](https://www.youtube.com/playlist?list=PLSE8ODhjZXjYgTIlqf4Dy9KQpQ7kn1Tl0), [YouTube-2016](https://www.youtube.com/playlist?list=PLSE8ODhjZXjbisIGOepfnlbfxeH7TW-8O))\n- [15-721 Advanced Database Systems (Spring 2019) - CMU](https://www.youtube.com/playlist?list=PLSE8ODhjZXja7K1hjZ01UTVDnGQdx5v5U)\n- [CS122 - Relational Database System Implementation, Winter 2014-2015 - Caltech](http://users.cms.caltech.edu/~donnie/cs122/)\n- [CS 186 - Database Systems, UC Berkeley, Spring 2015](http://www.infocobuild.com/education/audio-video-courses/computer-science/cs186-spring2015-berkeley.html)\n- [CS 6530 - Graduate-level Database Systems, Fall 2016, University of Utah](https://www.cs.utah.edu/~lifeifei/cs6530/) ([Lectures - YouTube](https://www.youtube.com/playlist?list=PLbuogVdPnkCqwHUcieMrytP453Ep0y6eI))\n- [6.830/6.814 - Database Systems [Fall 2014]](https://www.youtube.com/playlist?list=PLfciLKR3SgqOxCy1TIXXyfTqKzX2enDjK)\n- [Informatics 1 - Data & Analysis 2014/15- University of Edinburgh](http://groups.inf.ed.ac.uk/vision/VIDEO/2014/da.htm)\n- [Database Management Systems, Aduni](http://aduni.org/courses/databases/index.php?view=cw)\n- [D4M - Signal Processing on Databases](https://ocw.mit.edu/resources/res-ll-005-d4m-signal-processing-on-databases-fall-2012/)\n- [In-Memory Data Management (2013)Prof. Hasso Plattner - HPI](https://open.hpi.de/courses/imdb2013/items/72j6pftms3dOSunM98JhfW)\n- [Distributed Data Management (WT 2019/20) - Dr. Thorsten Papenbrock - HPI](https://www.tele-task.de/series/1285/)\n- [CS122d - NoSQL Data Management (Spring 21) - Prof. Mike Carey - UC Irvine](https://uci.yuja.com/V/PlayList?node=9933576&a=1583628376&autoplay=1)\n\n------------------------------\n\n### Software Engineering\n\n- #### **Object Oriented Design**\n  - [ECE 462 Object-Oriented Programming using C++ and Java - Purdue](https://engineering.purdue.edu/OOSD/F2008/F2008.html)\n  - [Object-oriented Program Design and Software Engineering - Aduni](http://aduni.org/courses/java/index.php?view=cw)\n  - [OOSE - Object-Oriented Software Engineering, Dr. Tim Lethbridge](https://www.youtube.com/playlist?list=PL6iDJCG2nkhfNlig8NY5ePPfGvtQX6yLa)\n  - [Object Oriented Systems Analysis and Design (Systems Analysis and Design in a Changing World)](https://www.youtube.com/playlist?list=PL6XklZATqYx9dj72MKG6wLYjljeB2odra)\n  - [CS 251 - Intermediate Software Design (C++ version) - Vanderbilt University](https://www.youtube.com/playlist?list=PLZ9NgFYEMxp4ZsvD10uXmClGnukcu3Uff)\n  - [OOSE - Software Dev Using UML and Java](https://www.youtube.com/playlist?list=PLJ9pm_Rc9HesnkwKlal_buSIHA-jTZMpO)\n  - [Object-Oriented Analysis and Design - IIT Kharagpur](https://nptel.ac.in/courses/106105153/)\n  - [CS3 - Design in Computing - Richard Buckland UNSW](https://www.youtube.com/playlist?list=PL0C5D85DBA20E685C)\n  - [Informatics 1 - Object-Oriented Programming 2014/15- University of Edinburgh](http://groups.inf.ed.ac.uk/vision/VIDEO/2014/inf1op.htm)\n  - [Software Engineering with Objects and Components 2015/16- University of Edinburgh](http://groups.inf.ed.ac.uk/vision/VIDEO/2015/seoc.htm)\n- #### **Software Engineering**\n  - [Computer Science 169- Software Engineering - Spring 2015 - UCBerkeley](https://youtube.com/playlist?list=PLVEFwJhglgHJQEQ6RjMMjcclix94gp1k2)\n  - [Computer Science 169- Software Engineering - Fall 2019 - UCBerkeley](https://www.youtube.com/playlist?list=PLkFD6_40KJIxCKgzL0uysjsAtfY3JawLS)\n  - [CS 5150 -  Software Engineering, Fall 2014 - Cornell University](http://www.cs.cornell.edu/courses/cs5150/2014fa/materials.html)\n  - [Introduction to Service Design and Engineering - University of Trento, Italy](https://www.youtube.com/playlist?list=PLBdajHWwi0JCn87QuFT3e58mekU0-6WUT)\n  - [CS 164 Software Engineering - Harvard](https://www.youtube.com/watch?v=3zdfCR6c8vw&list=PLuhjguFxSeVLvKvWwTUIpVwXdLtZPX1ZS)\n  - [System Analysis and Design - IISC Bangalore](https://nptel.ac.in/courses/106108102/)\n  - [Software Engineering - IIT Bombay](https://nptel.ac.in/courses/106101061/)\n  - [Dependable Systems (SS 2014)- HPI University of Potsdam](https://www.tele-task.de/series/1005/)\n  - [Automated Software Testing - ETH Z\u00fcrich | Spring 2024](https://video.ethz.ch/lectures/d-infk/2024/spring/263-2815-00L/9c81df65-d04d-411a-bea4-cbd32eb249e5.html)\n  - [Software Testing - IIT Kharagpur](https://nptel.ac.in/courses/106105150/)\n  - [Software Testing - Udacity, course-cs258 | 2015](https://www.youtube.com/playlist?list=PLAwxTw4SYaPkWVHeC_8aSIbSxE_NXI76g)\n  - [Software Debugging - Udacity, course-cs259 | 2015](https://www.youtube.com/playlist?list=PLAwxTw4SYaPkxK63TiT88oEe-AIBhr96A)\n  - [Software Engineering - Bauhaus-Uni Weimar](https://www.youtube.com/watch?v=jouBM4AH0jw&list=PLjEglKdMOevU2STTGq79duxTXDFuO-k1H&index=2)\n  - [CMU 17-445 Software Engineering for AI-Enabled Systems summer 2020, by Christian Kaestner](https://www.youtube.com/playlist?list=PLDS2JMJnJzdkQPdkhcuwcbJpjB84g9ffX)\n- #### **Software Architecture**\n  - [CS 411 - Software Architecture Design - Bilkent University](http://video.bilkent.edu.tr/course_videos.php?courseid=10)\n  - [MOOC - Software Architecture & Design - Udacity](https://www.youtube.com/playlist?list=PLAwxTw4SYaPkMTetlG7xKWaI5ZAZFX8fL)\n  - [CS-310 Scalable Software Architectures](https://www.youtube.com/playlist?list=PLWl7jvxH18r0u5VRZsOjhghNXc_Ec4dZz)\n- #### **Concurrency**\n  - [CS176 - Multiprocessor Synchronization - Brown University](http://cs.brown.edu/courses/cs176/course_information.shtml) ([Videos from 2012](http://www.brown.edu/cis/sta/dev/herlihy_csci1760_fa12/#vid))\n  - [CS 282 (2014): Concurrent Java Network Programming in Android](https://www.youtube.com/playlist?list=PLZ9NgFYEMxp4KSJPUyaQCj7x--NQ6kvcX)\n  - [CSE P 506 \u2013 Concurrency, Spring 2011 - University of Washington](https://courses.cs.washington.edu/courses/csep506/11sp/Home.html) ([Videos](https://courses.cs.washington.edu/courses/csep506/11sp/Videos.html))\n  - [CSEP 524 - Parallel Computation - University of Washington](https://courses.cs.washington.edu/courses/csep524/10sp/) ([Videos](https://courses.cs.washington.edu/courses/csep524/10sp/lectures/video.html))\n  - [Parallel Programming Concepts (WT 2013/14) - HPI University of Potsdam](https://www.tele-task.de/series/977/)\n  - [Parallel Programming Concepts (WT 2012/13) - HPI University of Potsdam](https://www.tele-task.de/series/924/)\n  - [UIUC ECE 408 / CS 408 Applied Parallel Programming fall 2022, by Wen-mei Hwu, Sanjay Patel](https://www.youtube.com/playlist?list=PL6RdenZrxrw-UKfRL5smPfFFpeqwN3Dsz) ([Spring 2018](https://www.youtube.com/playlist?list=PLRRuQYjFhpmvu5ODQoY2l7D0ADgWEcYAX))\n  - [UIUC ECE 508 / CS 508 Manycore Parallel Algorithms spring 2019, by Wen-mei Hwu](https://www.youtube.com/playlist?list=PLRRuQYjFhpmspsME4LmLbuCG1VHbJmIcy)\n  - [UIUC CS 420 / ECE 492 / CSE 402 Introduction to Parallel Programming for Scientists and Engineers fall 2015, by Sanjay Kale](https://www.youtube.com/playlist?list=PL682UO4IMem9cAjfy_RPjAc6k-HPYpTa9)\n  - [Stanford CME 213 Introduction to Parallel Computing using MPI, openMP, and CUDA winter 2020, by Eric Darve](https://www.youtube.com/playlist?list=PLAtMgFDMfGy2mysjPHN_d1cf9sR1muRkq)\n- #### **Mobile Application Development**\n  - [MOOC Programming Mobile Applications for Android Handheld Systems - University of Maryland](https://www.youtube.com/playlist?list=PLkHsKoi6eZnwilGXUc95CqS7Vw4uLLDLG)\n  - [CS 193p - Developing Applications for iOS, Stanford University](https://cs193p.sites.stanford.edu/)\n  - [CS S-76 Building Mobile Applications - Harvard](http://cs76.tv/2013/summer/)\n  - [CS 251 (2015): Intermediate Software Design](https://www.youtube.com/playlist?list=PLZ9NgFYEMxp7lylj-XC8h1kjatOjbh9ne)\n  - [Android App Development for Beginners Playlist - thenewboston](https://www.youtube.com/playlist?list=PL6gx4Cwl9DGBsvRxJJOzG4r4k_zLKrnxl)\n  - [Android Application Development Tutorials - thenewboston](https://www.youtube.com/playlist?list=PL2F07DBCDCC01493A)\n  - [MOOC - Developing Android Apps - Udacity](https://www.youtube.com/playlist?list=PLAwxTw4SYaPnMwH5-FNkErnnq_aSy706S)\n  - [MOOC - Advanced Android App Development - Udacity](https://www.youtube.com/playlist?list=PLAwxTw4SYaPmETCT07vnDSiIaUBuyut0X)\n  - [CSSE490 Android Development Rose-Hulman Winter 2010-2011, Dave Fisher](https://www.youtube.com/playlist?list=PLF3EEB647F6B52F03)\n  - [iOS Course, Dave Fisher](https://www.youtube.com/playlist?list=PL96C635E4DCD393A8)\n  - [Developing iPad Applications for Visualization and Insight - Carnegie Mellon University](https://itunes.apple.com/us/course/developing-ipad-applications/id499050344)\n  - [Mobile Computing - IIT Madras](https://nptel.ac.in/courses/106106147/)\n  - [Mobile Information Systems - Bauhaus-Uni Weimar](https://www.youtube.com/watch?v=8EmbrZJwMOI&list=PLjEglKdMOevWv4zPW0diw7iJFdT7s4sTP)\n\n------------------------------\n\n### Artificial Intelligence\n\n- [CS50 - Introduction to Artificial Intelligence with Python (and Machine Learning), Harvard OCW](https://cs50.harvard.edu/ai/2023/)\n- [CS 188 - Introduction to Artificial Intelligence, UC Berkeley - Spring 2025, by John Canny, Oliver Grillmeyer](https://inst.eecs.berkeley.edu/~cs188/sp25/) ([Spring 2024](https://inst.eecs.berkeley.edu/~cs188/sp24/)) ([Spring 2023](https://www.youtube.com/playlist?list=PLp8QV47qJEg7WWVg_5eOECzVPpy23UjJz))\n- [6.034 Artificial Intelligence, MIT OCW](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/lecture-videos/)\n- [CS221: Artificial Intelligence: Principles and Techniques - Autumn 2019 - Stanford University](https://www.youtube.com/playlist?list=PLoROMvodv4rO1NB9TD4iUZ3qghGEGtqNX)\n- [15-780 - Graduate Artificial Intelligence, Spring 14, CMU](http://www.cs.cmu.edu/~zkolter/course/15-780-s14/lectures.html)\n- [CSE 592 Applications of Artificial Intelligence, Winter 2003 - University of Washington](https://courses.cs.washington.edu/courses/csep573/03wi/lectures/index.htm)\n- [CS322 - Introduction to Artificial Intelligence, Winter 2012-13 - UBC](http://www.cs.ubc.ca/~mack/CS322/) ([YouTube](https://www.youtube.com/playlist?list=PLDPnGbm0sUmpzvcGvktbz446SLdFbfZVU))\n- [CS 4804: Introduction to Artificial Intelligence, Fall 2016](https://www.youtube.com/playlist?list=PLUenpfvlyoa1iiSbGy9BBewgiXjzxVgBd)\n- [CS 5804: Introduction to Artificial Intelligence, Spring 2015](https://www.youtube.com/playlist?list=PLUenpfvlyoa0PB6_kqJ9WU7m6i6z1RhfJ)\n- [Artificial Intelligence, Fall 2023 - FAU](https://www.fau.tv/course/id/3595) ([Spring 2023](https://www.fau.tv/course/id/3386)) ([Fall 2022](https://www.fau.tv/course/id/3180)) ([Spring 2021](https://www.fau.tv/course/id/2095)) ([Fall 2020](https://www.fau.tv/course/id/1690)) ([Fall 2018](https://www.fau.tv/course/id/713)) ([Spring 2018](https://www.fau.tv/course/id/657))\n- [Artificial Intelligence - IIT Kharagpur](https://nptel.ac.in/courses/106105077/)\n- [Artificial Intelligence - IIT Madras](https://nptel.ac.in/courses/106106126/)\n- [Artificial Intelligence(Prof.P.Dasgupta) - IIT Kharagpur](https://nptel.ac.in/courses/106105079/)\n- [MOOC - Intro to Artificial Intelligence - Udacity](https://www.youtube.com/playlist?list=PLAwxTw4SYaPlqMkzr4xyuD6cXTIgPuzgn)\n- [MOOC - Artificial Intelligence for Robotics - Udacity](https://www.youtube.com/playlist?list=PLAwxTw4SYaPkCSYXw6-a_aAoXVKLDwnHK)\n- [Graduate Course in Artificial Intelligence, Autumn 2012 - University of Washington](https://www.youtube.com/playlist?list=PLbQ3Aya0VERDoDdbMogU9EASJGWris9qG)\n- [Agent-Based Systems 2015/16- University of Edinburgh](http://groups.inf.ed.ac.uk/vision/VIDEO/2015/abs.htm)\n- [Informatics 2D - Reasoning and Agents 2014/15- University of Edinburgh](http://groups.inf.ed.ac.uk/vision/VIDEO/2014/inf2d.htm)\n- [Artificial Intelligence - Hochschule Ravensburg-Weingarten](https://www.youtube.com/playlist?list=PL39B5D3AFC249556A)\n- [Deductive Databases and Knowledge-Based Systems - Technische Universit\u00e4t Braunschweig, Germany](http://www.ifis.cs.tu-bs.de/teaching/ws-1516/KBS)\n- [Artificial Intelligence: Knowledge Representation and Reasoning - IIT Madras](https://nptel.ac.in/courses/106106140/)\n- [Semantic Web Technologies by Dr. Harald Sack - HPI](https://www.youtube.com/playlist?list=PLoOmvuyo5UAeihlKcWpzVzB51rr014TwD)\n- [Knowledge Engineering with Semantic Web Technologies by Dr. Harald Sack - HPI](https://www.youtube.com/playlist?list=PLoOmvuyo5UAcBXlhTti7kzetSsi1PpJGR)\n- [T81-558: Applications of Deep Neural Networks by Jeff Heaton, 2022, Washington University in St. Louis](https://sites.wustl.edu/jeffheaton/t81-558/)\n- [MSU programming for AI](https://www.youtube.com/playlist?list=PLZ-krWGO-UEz84TseDMIlx2Set6xZp0YP)\n\n------------------------------\n\n### Machine Learning\n\n- #### **Introduction to Machine Learning**\n  - [Introduction to Machine Learning for Coders](https://course18.fast.ai/ml)\n  - [MOOC - Statistical Learning, Stanford University](http://www.dataschool.io/15-hours-of-expert-machine-learning-videos/)\n  - [Statistical Learning with Python - Stanford Online](https://www.youtube.com/playlist?list=PLoROMvodv4rPP6braWoRt5UCXYZ71GZIQ)\n  - [Foundations of Machine Learning Boot Camp, Berkeley Simons Institute](https://www.youtube.com/playlist?list=PLgKuh-lKre11GbZWneln-VZDLHyejO7YD)\n  - [CS 155 - Machine Learning & Data Mining, 2023 - Caltech](https://www.youtube.com/playlist?list=PLu5yXJ11s4r-nBlojSQC9seeUF1IxX-_z) ([Notes-2020](http://www.yisongyue.com/courses/cs155/2020_winter/)) ([YouTube-2020](https://www.youtube.com/playlist?list=PLuz4CTPOUNi67hPzb9zJXH1cbeN7LKNiD)) ([Notes-2019](http://www.yisongyue.com/courses/cs155/2019_winter/)) ([YouTube-2019](https://www.youtube.com/playlist?list=PLuz4CTPOUNi7r2trKGgwaedY17MADTay4)) ([Notes-2018](http://www.yisongyue.com/courses/cs155/2018_winter/)) ([YouTube-2018](https://www.youtube.com/playlist?list=PLuz4CTPOUNi644ypoxzP1frkPYVHdjDJU)) ([Notes-2017](http://www.yisongyue.com/courses/cs155/2017_winter/)) ([YouTube-2017](https://www.youtube.com/playlist?list=PLuz4CTPOUNi6BfMrltePqMAHdl5W33-bC)) ([Notes-2016](http://www.yisongyue.com/courses/cs155/2016_winter/)) ([YouTube-2016](https://www.youtube.com/playlist?list=PL5HdMttxBY0BVTP9y7qQtzTgmcjQ3P0mb))\n  - [CS 156 - Learning from Data, Caltech](https://work.caltech.edu/lectures.html)\n  - [10-601 - Introduction to Machine Learning (MS) - Tom Mitchell - 2015, CMU](http://www.cs.cmu.edu/~ninamf/courses/601sp15/lectures.shtml) ([YouTube](https://www.youtube.com/playlist?list=PLAJ0alZrN8rD63LD0FkzKFiFgkOmEtltQ))\n  - [10-601 Machine Learning | CMU | Fall 2017](https://www.youtube.com/playlist?list=PL7k0r4t5c10-g7CWCnHfZOAxLaiNinChk)\n  - [10-701 - Introduction to Machine Learning (PhD) - Tom Mitchell, Spring 2011, CMU](http://www.cs.cmu.edu/~tom/10701_sp11/lectures.shtml) ([Fall 2014](https://www.youtube.com/playlist?list=PL7y-1rk2cCsDZCVz2xS7LrExqidHpJM3B)) ([Spring 2015 by Alex Smola](https://www.youtube.com/playlist?list=PLZSO_6-bSqHTTV7w9u7grTXBHMH-mw3qn)) ([Fall 2020 by Ziv Bar-Joseph, Eric Xing](https://www.youtube.com/playlist?list=PLsWN0V-b507g7dbQTUvFkKZEqdHR5Fh4P))\n  - [10 - 301/601 - Introduction to Machine Learning - Fall 2023 - CMU](https://scs.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx#folderID=%22d5bf275d-ff88-4bf6-a865-b065010f55c2%22)\n  - [6.036 - Machine Learning, Broderick - MIT Fall 2020](https://www.youtube.com/playlist?list=PLxC_ffO4q_rW0bqQB80_vcQB09HOA3ClV)\n  - [Mediterranean Machine Learning summer school 2024](https://www.youtube.com/playlist?list=PLF-wkqRv4u1bV4Zd1UepYfyZXkv6Bz6ra) ([YouTube-2023](https://www.youtube.com/playlist?list=PLF-wkqRv4u1Y-Bret-wrcPypPCZ3Gg_3L)) ([YouTube-2022](https://www.youtube.com/playlist?list=PLF-wkqRv4u1agtfVaDsDUaMHmToP84Fk6)) ([YouTube-2021](https://www.youtube.com/playlist?list=PLF-wkqRv4u1YRbfnwN8cXXyrmXld-sked))\n  - [LxMLS Lisbon Machine Learning School 2024](https://www.youtube.com/playlist?list=PLQl_xdhSmQeh4eRfAwETbtJJLPKcDLrzw) ([YouTube-2023](https://www.youtube.com/playlist?list=PLQl_xdhSmQeikRCf-wJ8NCK51JOMHGOCP)) ([YouTube-2022](https://www.youtube.com/playlist?list=PLQl_xdhSmQejdxQL7qI5aJkLcAQJ68Abp)) ([YouTube-2021](https://www.youtube.com/playlist?list=PLQl_xdhSmQegzsLin54NbfePFAuTUEmUj)) ([YouTube-2020](https://www.youtube.com/playlist?list=PLQl_xdhSmQehE6aAk774yWMag4NNBGr5k))\n  - [Applied Machine Learning (Cornell Tech CS 5787, Fall 2020)](https://www.youtube.com/playlist?list=PL2UML_KCiC0UlY7iCQDSiGDMovaupqc83)\n  - [Stanford CS229: Machine Learning Course | Summer 2019 (Anand Avati)](https://www.youtube.com/playlist?list=PLoROMvodv4rNH7qL6-efu_q2_bPuy0adh) ([Spring 2022](https://www.youtube.com/playlist?list=PLoROMvodv4rNyWOpJg_Yh4NSqI4Z4vOYy))\n  - [CMS 165 Foundations of Machine Learning - 2019 - Caltech](http://tensorlab.cms.caltech.edu/users/anima/cms165-2019.html) ([Youtube](https://www.youtube.com/playlist?list=PLVNifWxslHCA5GUh0o92neMiWiQiGVFqp))\n  - [CMS 165 Foundations of Machine Learning and Statistical Inference - 2020 - Caltech](https://www.youtube.com/playlist?list=PLVNifWxslHCDlbyitaLLYBOAEPbmF1AHg)\n  - [Microsoft Research - Machine Learning Course](https://www.youtube.com/playlist?list=PL34iyE0uXtxo7vPXGFkmm6KbgZQwjf9Kf)\n  - [CS 446 - Machine Learning, Fall 2016, UIUC](https://www.youtube.com/playlist?list=PLQcasX5-oG91TgY6A_gz-IW7YSpwdnD2O)\n  - [undergraduate machine learning at UBC 2012, Nando de Freitas](https://www.youtube.com/playlist?list=PLE6Wd9FR--Ecf_5nCbnSQMHqORpiChfJf)\n  - [CS 229 - Machine Learning - Stanford University](https://see.stanford.edu/Course/CS229) ([Autumn 2018](https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU))\n  - [CS 189/289A Introduction to Machine Learning, Prof Jonathan Shewchuk - UCBerkeley](https://people.eecs.berkeley.edu/~jrs/189/)\n  - [CPSC 340: Machine Learning and Data Mining (2018) - UBC](https://www.youtube.com/playlist?list=PLWmXHcz_53Q02ZLeAxigki1JZFfCO6M-b)\n  - [CS391L Machine Learning, Spring 2025 - UT Austin](https://utcs-ml-course.github.io/main/Lectures/)\n  - [CS4780/5780 Machine Learning, Fall 2013 - Cornell University](http://www.cs.cornell.edu/courses/cs4780/2013fa/)\n  - [CS4780/5780 Machine Learning, Fall 2018 - Cornell University](http://www.cs.cornell.edu/courses/cs4780/2018fa/page18/index.html) ([Youtube](https://www.youtube.com/playlist?list=PLl8OlHZGYOQ7bkVbuRthEsaLr7bONzbXS))\n  - [CSE474/574 Introduction to Machine Learning - SUNY University at Buffalo](https://www.youtube.com/playlist?list=PLEQDy5tl3xkMzk_zlo2DPzXteCquHA8bQ)\n  - [CS 5350/6350 - Machine Learning, Spring 2024, University of Utah](https://svivek.com/teaching/machine-learning/spring2024/) ([Youtube](https://www.youtube.com/playlist?list=PLbuogVdPnkCr-ANNi5GZid3MvSkzm_wnM))\n  - [ECE 4252/8803 Fundamentals of Machine Learning (FunML), Spring 2024 - Georgia Tech](https://alregib.ece.gatech.edu/georgia-tech-courses/funml/)\n  - [ECE 5984 Introduction to Machine Learning, Spring 2015 - Virginia Tech](https://filebox.ece.vt.edu/~s15ece5984/)\n  - [CSx824/ECEx242 Machine Learning, Bert Huang, Fall 2015 - Virginia Tech](https://www.youtube.com/playlist?list=PLUenpfvlyoa0rMoE5nXA8kdctBKE9eSob)\n  - [STA 4273H - Large Scale Machine Learning, Winter 2015 - University of Toronto](http://www.cs.toronto.edu/~rsalakhu/STA4273_2015/lectures.html)\n  - [CSC 2515 Introduction to Machine Learning, Amir-massoud Farahmand, Fall 2021, University of Toronto](https://www.youtube.com/playlist?list=PLCveiXxL2xNZRg7PVp-JM4teSmaBETksy)\n  - [ECE 421 Introduction to Machine Learning, Amir Ashouri, Winter 2019, University of Toronto](https://www.youtube.com/playlist?list=PL-Mfq5QS-s8iS9XqKuApPE1TSlnZblFHF)\n  - [EECS 4404E/5327 Introduction to Machine Learning, Amir Ashouri, Fall 2019, York University](https://www.youtube.com/playlist?list=PL-Mfq5QS-s8horb94sQH4xcL85zDkpL9w)\n  - [CS 480/680 Introduction to Machine Learning, Gautam Kamath, University of Waterloo](http://www.gautamkamath.com/courses/CS480-sp2021.html) ([Spring 2021](https://www.youtube.com/playlist?list=PLmd_zeMNzSvSzRRc4Q29qEcpxbhdwjMOx))\n  - [CS 480/680 Introduction to Machine Learning, Kathryn Simone, University of Waterloo](https://github.com/kpc-simone/cs480-f24) ([Fall 2024](https://www.youtube.com/playlist?list=PLH84ETHrlsC8sbfb8WaOeXSx9ySH2Qoyc))\n  - [CS 485/685 Machine Learning, Shai Ben-David, University of Waterloo](https://www.youtube.com/channel/UCR4_akQ1HYMUcDszPQ6jh8Q/videos)\n  - [STAT 441/841 Classification Winter 2017 , Waterloo](https://www.youtube.com/playlist?list=PLehuLRPyt1HzXDemu7K4ETcF0Ld_B5adG)\n  - [10-605 - Machine Learning with Large Datasets, Fall 2016 - CMU](https://www.youtube.com/channel/UCIE4UdPoCJZMAZrTLuq-CPQ/videos)\n  - [Information Theory, Pattern Recognition, and Neural Networks - University of Cambridge](https://www.youtube.com/playlist?list=PLruBu5BI5n4aFpG32iMbdWoRVAA-Vcso6)\n  - [Pattern Analysis (2018) - FAU](https://www.fau.tv/course/id/655) ([Class 2017](https://www.fau.tv/course/id/544)) ([Class 2016](https://www.fau.tv/course/id/449)) ([Class 2015](https://www.fau.tv/course/id/355)) ([Class 2009](https://www.fau.tv/course/id/2))\n  - [Pattern Recognition (2020-2021) - FAU](https://www.fau.tv/course/id/1579) ([Class 2012-2013](https://www.fau.tv/course/id/173))\n  - [Beyond the Patterns (2020-2021) - FAU](https://www.fau.tv/course/id/1868)\n  - [Python and machine learning - Stanford Crowd Course Initiative](https://www.youtube.com/playlist?list=PLVxFQjPUB2cnYGZPAGG52OQc9SpWVKjjB)\n  - [MOOC - Machine Learning Part 1a - Udacity/Georgia Tech](https://www.youtube.com/playlist?list=PLAwxTw4SYaPl0N6-e1GvyLp5-MUMUjOKo) ([Part 1b](https://www.youtube.com/playlist?list=PLAwxTw4SYaPlkESDcHD-0oqVx5sAIgz7O) [Part 2](https://www.youtube.com/playlist?list=PLAwxTw4SYaPmaHhu-Lz3mhLSj-YH-JnG7) [Part 3](https://www.youtube.com/playlist?list=PLAwxTw4SYaPnidDwo9e2c7ixIsu_pdSNp))\n  - [Pattern Recognition Class (2012)- Universit\u00e4t Heidelberg](https://www.youtube.com/playlist?list=PLuRaSnb3n4kRDZVU6wxPzGdx1CN12fn0w)\n  - [Introduction to Machine Learning and Pattern Recognition - CBCSL OSU](https://www.youtube.com/playlist?list=PLcXJymqaE9PPGGtFsTNoDWKl-VNVX5d6b)\n  - [Introduction to Machine Learning - IIT Kharagpur](https://nptel.ac.in/courses/106105152/)\n  - [Introduction to Machine Learning - IIT Madras](https://nptel.ac.in/courses/106106139/)\n  - [Pattern Recognition - IISC Bangalore](https://nptel.ac.in/courses/117108048/)\n  - [Pattern Recognition and Application - IIT Kharagpur](https://nptel.ac.in/courses/117105101/)\n  - [Pattern Recognition - IIT Madras](https://nptel.ac.in/courses/106106046/)\n  - [Machine Learning Summer School 2013 - Max Planck Institute for Intelligent Systems T\u00fcbingen](https://www.youtube.com/playlist?list=PLqJm7Rc5-EXFv6RXaPZzzlzo93Hl0v91E)\n  - [Machine Learning - Professor Kogan (Spring 2016) - Rutgers](https://www.youtube.com/playlist?list=PLauepKFT6DK_1_plY78bXMDj-bshv7UsQ)\n  - [CS273a: Introduction to Machine Learning](http://sli.ics.uci.edu/Classes/2015W-273a) ([YouTube](https://www.youtube.com/playlist?list=PLkWzaBlA7utJMRi89i9FAKMopL0h0LBMk))\n  - [Machine Learning Crash Course 2015](https://www.youtube.com/playlist?list=PLyGKBDfnk-iD5dK8N7UBUFVVDBBtznenR)\n  - [COM4509/COM6509 Machine Learning and Adaptive Intelligence 2015-16](http://inverseprobability.com/mlai2015/)\n  - [Introduction to Machine Learning - Spring 2018 - ETH Zurich](https://www.youtube.com/playlist?list=PLzn6LN6WhlN273tsqyfdrBUsA-o5nUESV)\n  - [Machine Learning - Pedro Domingos- University of Washington](https://www.youtube.com/user/UWCSE/playlists?view=50&sort=dd&shelf_id=16)\n  - [CSE 446/546 - Machine Learning, Spring 2020 - University of Washington](https://courses.cs.washington.edu/courses/cse446/20sp/schedule/) ([Videos](https://www.youtube.com/playlist?list=PLrE1feouzSWr7LBFAeRZIb7CN9H6dB9Jt))\n  - [Machine Learning (COMP09012)](https://www.youtube.com/playlist?list=PLyH-5mHPFffFwz7Twap0XuVeUJ8vuco9t)\n  - [Probabilistic Machine Learning 2020 - University of T\u00fcbingen](https://www.youtube.com/playlist?list=PL05umP7R6ij1tHaOFY96m5uX3J21a6yNd)\n  - [Statistical Machine Learning 2020 - Ulrike von Luxburg - University of T\u00fcbingen](https://www.youtube.com/playlist?list=PL05umP7R6ij2XCvrRzLokX6EoHWaGA2cC)\n  - [COMS W4995 - Applied Machine Learning - Spring 2020 - Columbia University](https://www.cs.columbia.edu/~amueller/comsw4995s20/schedule/)\n  - [Machine Learning for Engineers 2022](https://apmonitor.com/pds) ([YouTube](https://www.youtube.com/watch?v=Gh5rbBLh4JY&list=PLLBUgWXdTBDg1K1bu60lHypSzSP-WSBmx))\n  - [10-418 / 10-618 (Fall 2019) Machine Learning for Structured Data](https://www.youtube.com/playlist?list=PL4CxkUJbvNVihRKP4bXufvRLIWzeS-ieP)\n  - [ORIE 4741/5741: Learning with Big Messy Data - Cornell](https://people.orie.cornell.edu/mru8/orie4741/lectures.html)\n  - [Machine Learning in IoT](https://www.youtube.com/playlist?list=PLeZoXD_TLsLbW_ILvL9TlhBYdW8wJyON-)\n  - [Stanford CS229M: Machine Learning Theory - Fall 2021](https://www.youtube.com/playlist?list=PLoROMvodv4rP8nAmISxFINlGKSK4rbLKh)\n  - [Intro to Machine Learning and Statistical Pattern Classification - Prof Sebastian Raschka](https://www.youtube.com/playlist?list=PLTKMiZHVd_2KyGirGEvKlniaWeLOHhUF3)\n  - [CMU's Multimodal Machine Learning course (11-777), Fall 2020](https://www.youtube.com/playlist?list=PL-Fhd_vrvisNup9YQs_TdLW7DQz-lda0G)\n  - [EE104: Introduction to Machine Learning - Stanford University](https://www.youtube.com/playlist?list=PLoROMvodv4rN_Uy7_wmS051_q1d6akXmK)\n  - [CPSC 330: Applied Machine Learning (2020) - UBC](https://www.youtube.com/playlist?list=PLWmXHcz_53Q2BXsWviGgEqdlSHmfsjSzC)\n  - [Machine Learning 2013 - Nando de Freitas, UBC](https://www.youtube.com/playlist?list=PLE6Wd9FR--EdyJ5lbFl8UuGjecvVw66F6)\n  - [Machine Learning, 2014-2015, University of Oxford](https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/)\n  - [10-702/36-702 - Statistical Machine Learning - Larry Wasserman, Spring 2016, CMU](https://www.stat.cmu.edu/~ryantibs/statml/) ([Spring 2015](https://www.youtube.com/playlist?list=PLjbUi5mgii6BWEUZf7He6nowWvGne_Y8r))\n  - [10-715 Advanced Introduction to Machine Learning - CMU](http://www.cs.cmu.edu/~bapoczos/Classes/ML10715_2015Fall/) ([YouTube](https://www.youtube.com/playlist?list=PL4DwY1suLMkcu-wytRDbvBNmx57CdQ2pJ))\n  - [CS 281B - Scalable Machine Learning, Alex Smola, UC Berkeley](http://alex.smola.org/teaching/berkeley2012/syllabus.html)\n  - [100 Days of Machine Learning - CampusX (Hindi)](https://www.youtube.com/playlist?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH)\n  - [CampusX Data Science Mentorship Program 2022-23 (Hindi)](https://www.youtube.com/playlist?list=PLKnIA16_RmvbAlyx4_rdtR66B7EHX5k3z)\n  - [Statistical Machine Learning - S2023 - Benyamin Ghojogh](https://www.youtube.com/playlist?list=PLPrxGIUWsqP2g7cpk0nFFt0c4aRcREq2s)\n  - [MIT 6.5940 EfficientML.ai Lecture, Fall 2023](https://www.youtube.com/playlist?list=PL80kAHvQbh-pT4lCkDT53zT8DKmhE0idB)\n  - [TinyML - Tiny Machine Learning at UPenn](https://www.youtube.com/playlist?list=PL7rtKJAz_mPe6kAbiH6Ucq02Vpa95qvBJ)\n  - [ECE 4760 (Digital Systems Design Using Microcontrollers) at Cornell for the Fall, 2022](https://www.youtube.com/playlist?list=PLDqMkB5cbBA5oDg8VXM110GKc-CmvUqEZ) ([Spring 2021](https://www.youtube.com/playlist?list=PLDqMkB5cbBA6FEJuj94gl-9vw8xcHu9Gp))\n  - [EfficientML.ai Lecture, Fall 2023, MIT 6.5940](https://www.youtube.com/playlist?list=PL80kAHvQbh-pT4lCkDT53zT8DKmhE0idB)\n  - [SFU CMPT 727 Statistical Machine Learning, by Maxwell Libbrecht](https://coursys.sfu.ca/2023sp-cmpt-727-g1/pages/) ([Spring 2023](https://www.youtube.com/playlist?list=PL_5SuHtr8fsrK9NqWWSL4YL8urMAHLsvU)) ([Spring 2022](https://www.youtube.com/playlist?list=PL_5SuHtr8fsp95AhIKeTHbpcVdhlhB9h6))\n  - [UC Berkeley CS 189 / 289A Introduction to Machine Learning fall 2023, by Jennifer Listgarten & Jitendra Malik](https://eecs189.org/)\n  - [UC Berkeley CS 189 Introduction to Machine Learning (CDSS offering) spring 2022, by Marvin Zhang](https://www.youtube.com/playlist?list=PLCuQm2FL98HTlRmlwMk2AuFEM9n1c06HE)\n  - [UC San Diego/edX DSE 220X Machine Learning Fundamentals, by Sanjoy Dasgupta](https://www.youtube.com/playlist?list=PLUPLKa8g2P75vrLVe6HKdgAwfuU89RfqB)\n  - [MIT 6.036 Introduction to Machine Learning spring 2019, by Leslie Kaelbling](https://www.youtube.com/playlist?list=PLQEw29vp6f1Ae9dp8vkKB8H6sF1PHvP5N)\n  - [LMU Munich Introduction to Machine Learning](https://slds-lmu.github.io/i2ml/)\n  - [CMU 15 388 / 15 688 Practical Data Science, by Zico Kolter](https://www.datasciencecourse.org/lectures/) ([Fall 2019](https://scs.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx#folderID=%22618ea253-ca45-4b14-9f1d-aab501543bd2%22)) ([Spring 2018](https://scs.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx#folderID=%22912b80a3-625d-405d-8905-a8620133666b%22))\n  - [UW Madison CS 320 Data Programming II spring 2021, by Tyler R. Caraza-Harter](https://tyler.caraza-harter.com/cs320/s21/schedule.html)\n  - [UC San Diego COGS9 Introduction to Data Science fall 2020, by Jason Fleischer](https://www.youtube.com/playlist?list=PLaaNbhBDEsoFarUB58v9s7pUVoyAahMeQ)\n  - [UCLA Stats 15 Introduction to Data Science fall 2022, by Miles Chen](https://www.youtube.com/playlist?list=PLIygTcviGPKCEzYDpJha6hP6ne-50sT1o)\n  - [UCLA Stats 21 Python and Other Technologies for Data Science spring 2024, by Miles Chen](https://www.youtube.com/playlist?list=PLIygTcviGPKAdsiprcbdk5HHPxS6PloWE) ([Spring 2021](https://www.youtube.com/playlist?list=PLKR7271tMEmgBPgu4LtjDhX3ywpTxda5g))\n  - [UCLA Stats C161/C261 Introduction to Pattern Recognition and Machine Learning winter 2024, by Arash Amini](https://www.youtube.com/playlist?list=PLN_qg0-2-0SxQ2vlXxlZVMKkt4gI1YYP8) ([Winter 2023](https://www.youtube.com/playlist?list=PLN_qg0-2-0SwLCXGUyM3FNSRwG6GNgONr))\n  - [UCLA Stats 231C Theories of Machine Learning spring 2022, by Arash Amini](https://www.youtube.com/playlist?list=PLN_qg0-2-0SxKyZLv_FotPDED5ET_rQmo)\n  - [MSU Machine Learning](https://www.youtube.com/watch?v=kMf0qDtQ_PM&list=PLZ-krWGO-UEyPHsZfOjYH03_TyIN2pPhl&pp=iAQB)\n  - [Data Science for Dynamical Systems, by Oliver Wallscheid & Sebastian Peitz](https://github.com/DS-4-DS/DS4DS_Course) ([YouTube](https://www.youtube.com/@DataScience4DynamicalSystems/playlists))\n  - [Cambridge Statistical Learning in Practice 2021, by Alberto J. Coca](https://www.youtube.com/playlist?list=PLn1JSlh3WT_b7sMBktkAgV9-cP052JFhb)\n  - [Data 8: The Foundations of Data Science - UC Berkeley](http://data8.org/) ([Spring 23](https://www.data8.org/sp23/)) ([Fall 22](https://www.data8.org/fa22/)) ([Spring 22](https://www.data8.org/sp22/)) ([Summer 17](http://data8.org/su17/))\n  - [Data 144: Foundations of Data Science spring 2021 - Vassar College](https://www.youtube.com/playlist?list=PLIygTcviGPKB9hHuywn56TraSMv2pRumr) ([Course materials](https://github.com/jwaterman/data144-materials-sp21))\n  - [CSE519 - Data Science Fall 2016 - Skiena, SBU](https://www.youtube.com/playlist?list=PLOtl7M3yp-DVBdLYatrltDJr56AKZ1qXo)\n  - [CS 109 Data Science, Harvard University](http://cs109.github.io/2015/pages/videos.html) ([YouTube](https://www.youtube.com/playlist?list=PLb4G5axmLqiuneCqlJD2bYFkBwHuOzKus))\n  - [6.0002 Introduction to Computational Thinking and Data Science - MIT OCW](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-0002-introduction-to-computational-thinking-and-data-science-fall-2016/lecture-videos/)\n  - [Data 100: Principles and Techniques of Data Science - UC Berkeley](https://ds100.org/fa24/) ([Fall 24](https://www.youtube.com/playlist?list=PLIygTcviGPKBzDKR72ILypzPQZ_Cz6HcH)) ([Spring 24](https://ds100.org/sp24/)) ([Summer 19](https://www.youtube.com/playlist?list=PLPHXc20GewP8J56CisONS_mFZWZAfa7jR))\n  - [Data 102 - Spring 21- UC Berkeley](https://data102.org/sp21/#lecture-week-14) ([YouTube](https://www.youtube.com/playlist?list=PLIygTcviGPKAEbY32OXjmkQbD0uRhge9Y))\n  - [Distributed Data Analytics (WT 2017/18) - HPI University of Potsdam](https://www.tele-task.de/series/1179/)\n  - [Data Profiling and Data Cleansing (WS 2014/15) - HPI University of Potsdam](https://www.tele-task.de/series/1027/)\n  - [CS 229r - Algorithms for Big Data, Harvard University](http://people.seas.harvard.edu/~minilek/cs229r/fall15/lec.html) ([Youtube](https://www.youtube.com/playlist?list=PL2SOU6wwxB0v1kQTpqpuu5kEJo2i-iUyf))\n  - [Algorithms for Big Data - IIT Madras](https://nptel.ac.in/courses/106106142/)\n  - [Python Data Science with the TCLab](https://github.com/APMonitor/data_science) ([YouTube](https://www.youtube.com/watch?v=pAgW_bZVo88&list=PLLBUgWXdTBDg1Qgmwt4jKtVn9BWh5-zgy))\n  - [Foundations of Data Analysis (Fall 2020)- University of Utah](https://www.youtube.com/playlist?list=PLbuogVdPnkCqB1sx1eheVmLtp2EN7osYt)\n\n  \n- #### **Data Mining**\n  - [CSEP 546, Data Mining - Pedro Domingos, Sp 2016 - University of Washington](https://courses.cs.washington.edu/courses/csep546/16sp/) ([YouTube](https://www.youtube.com/playlist?list=PLTPQEx-31JXgtDaC6-3HxWcp7fq4N8YGr))\n  - [CS 5140/6140 - Data Mining, Spring 2020, University of Utah by Prof. Jeff Phillips](https://users.cs.utah.edu/~jeffp/teaching/cs5140-S20/cs5140.html) ([Youtube](https://www.youtube.com/playlist?list=PLbuogVdPnkCrEf65zrd3J1UG3LT6TcDlt))\n  - [CS 5140/6140 - Data Mining, Spring 2023, University of Utah by Prof. Ana Marasovi\u0107](https://utah-data-mining-spring23.github.io/) ([Youtube](https://www.youtube.com/playlist?list=PLbuogVdPnkCrnLNqZPnTuG_s19TNDoad0))\n  - [CS 5955/6955 - Data Mining, University of Utah](http://www.cs.utah.edu/~jeffp/teaching/cs5955.html) ([YouTube](https://www.youtube.com/channel/UCcrlwW88yMcXujhGjSP2WBg/videos))\n  - [Statistics 202 - Statistical Aspects of Data Mining, Summer 2007 - Google](http://www.stats202.com/original_index.html) ([YouTube](https://www.youtube.com/playlist?list=PLFE776F2C513A744E))\n  - [MOOC - Text Mining and Analytics by ChengXiang Zhai](https://www.youtube.com/playlist?list=PLLssT5z_DsK8Xwnh_0bjN4KNT81bekvtt)\n  - [Information Retrieval SS 2014, iTunes - HPI](https://itunes.apple.com/us/itunes-u/information-retrieval-ss-2014/id874200291)\n  - [MOOC - Data Mining with Weka](https://www.youtube.com/playlist?list=PLm4W7_iX_v4NqPUjceOGd-OKNVO4c_cPD)\n  - [CS 290 DataMining Lectures](https://www.youtube.com/playlist?list=PLB4CCA346A5741C4C)\n  - [CS246 - Mining Massive Data Sets, Winter 2016, Stanford University](https://web.stanford.edu/class/cs246/) ([YouTube](https://www.youtube.com/channel/UC_Oao2FYkLAUlUVkBfze4jg/videos))\n  - [Information Retrieval - Spring 2018 - ETH Zurich](https://www.youtube.com/playlist?list=PLzn6LN6WhlN1ktkDvNurPSDwTQ_oGQisn)\n  - [Information Retrieval - WS 2022/23 - Universit\u00e4t Freiburg](https://ad-wiki.informatik.uni-freiburg.de/teaching/InformationRetrievalWS2223)\n  - [CAP6673 - Data Mining and Machine Learning - FAU](http://www.cse.fau.edu/~taghi/classes/cap6673/)([Video lectures](https://vimeo.com/album/1505953))\n  - [CS 412 - Introduction to Data Mining - UIUC](https://www.youtube.com/playlist?list=PLIygTcviGPKDZi44-yuH2XH9UaHdaJkxs)\n  - [CS 512 - Data Mining Principles - UIUC](https://github.com/spacemanidol/CS512DM/tree/main/lectures) ([YouTube](https://www.youtube.com/playlist?list=PLIygTcviGPKABUzEm9v1PuKLlb0AQ9tdH))\n  \n- #### **Probabilistic Graphical Modeling**\n  - [CS 6190 - Probabilistic Modeling, Spring 2016, University of Utah](https://www.youtube.com/playlist?list=PLbuogVdPnkCpvxdF-Gy3gwaBObx7AnQut)\n  - [10-708 - Probabilistic Graphical Models, Carnegie Mellon University](https://www.cs.cmu.edu/~epxing/Class/10708-20/lectures.html)\n  - [Probabilistic Graphical Models, Daphne Koller, Stanford University](http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=ProbabilisticGraphicalModels)\n  - [Probabilistic Graphical Models, Spring 2018 - Notre Dame](https://www.youtube.com/playlist?list=PLd-PuDzW85AcV4bgdu7wHPL37hm60W4RM)\n  \n- #### **Deep Learning**\n  - [Full Stack Deep Learning - Course 2022](https://www.youtube.com/watch?v=-Iob-FW5jVM&list=PL1T8fO7ArWleMMI8KPJ_5D5XSlovTW_Ur)\n  - [Full Stack Deep Learning - Course 2021](https://www.youtube.com/watch?v=fGxWfEuUu0w&list=PL1T8fO7ArWlcWg04OgNiJy91PywMKT2lv)\n  - [NYU Deep Learning Spring 2020](https://www.youtube.com/playlist?list=PLLHTzKZzVU9eaEyErdV26ikyolxOsz6mq)\n  - [NYU Deep Learning Spring 2021](https://www.youtube.com/playlist?list=PLLHTzKZzVU9e6xUfG10TkTWApKSZCzuBI)\n  - [6.S191: Introduction to Deep Learning - MIT](http://introtodeeplearning.com/)\n  - [Intro to Deep Learning and Generative Models Course - Prof Sebastian Raschka](https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51)\n  - [Deep Learning CMU](https://www.youtube.com/channel/UC8hYZGEkI2dDO8scT8C5UQA/videos)\n  - [CS231n Deep Learning for Computer Vision - Winter 2016 Andrej Karpathy - Stanford University](https://www.youtube.com/playlist?list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC)\n  - [Deep Learning: CS 182 Spring 2021](https://www.youtube.com/playlist?list=PL_iWQOsE6TfVmKkQHucjPAoRtIJYt8a5A)\n  - [10-414/714: Deep Learning Systems - CMU](https://dlsyscourse.org/lectures/) ([Youtube](https://www.youtube.com/@deeplearningsystemscourse1116/videos))\n  - [11-785: Introduction to Deep Learning - CMU](https://deeplearning.cs.cmu.edu/S24/index.html) ([Lectures - YouTube-2024](https://www.youtube.com/playlist?list=PLp-0K3kfddPxUJzAW0KxNNjGiK_hISFas), [Recitations - YouTube-2024](https://www.youtube.com/playlist?list=PLp-0K3kfddPzNnco9QQAoTx_sVhZgHK-n))\n  - [Part 1: Practical Deep Learning for Coders, v3 - fast.ai](https://course.fast.ai/)\n  - [Part 2: Deep Learning from the Foundations - fast.ai](https://course19.fast.ai/part2)\n  - [Deep learning at Oxford 2015 - Nando de Freitas](https://www.youtube.com/playlist?list=PLE6Wd9FR--EfW8dtjAuPoTuPcqmOV53Fu)\n  - [Self-Driving Cars \u2014 Andreas Geiger, 2021/22](https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/autonomous-vision/lectures/self-driving-cars/) ([YouTube](https://www.youtube.com/watch?v=wAaSJUAKPuY&list=PL05umP7R6ij321zzKXK6XCQXAaaYjQbzr))\n  - [6.S094: Deep Learning for Self-Driving Cars - MIT](https://www.youtube.com/playlist?list=PLrAXtmErZgOeiKm4sgNOknGvNjby9efdf)\n  - [CS294-129 Designing, Visualizing and Understanding Deep Neural Networks](https://bcourses.berkeley.edu/courses/1453965/pages/cs294-129-designing-visualizing-and-understanding-deep-neural-networks) ([YouTube](https://www.youtube.com/playlist?list=PLkFD6_40KJIxopmdJF_CLNqG3QuDFHQUm))\n  - [CS230: Deep Learning - Autumn 2018 - Stanford University](https://www.youtube.com/playlist?list=PLoROMvodv4rOABXSygHTsbvUz4G_YQhOb)\n  - [STAT-157 Deep Learning 2019 - UC Berkeley](https://www.youtube.com/playlist?list=PLZSO_6-bSqHQHBCoGaObUljoXAyyqhpFW)\n  - [Deep Learning, Stanford University](http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=DeepLearning)\n  - [MOOC - Neural Networks for Machine Learning, Geoffrey Hinton 2016 - Coursera](https://www.youtube.com/playlist?list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9)\n  - [Stat 946 Deep Learning - University of Waterloo](https://www.youtube.com/playlist?list=PLehuLRPyt1Hyi78UOkMPWCGRxGcA9NVOE)\n  - [EECS 298 Theory of Computational Neural Networks and Machine Learning (Fall 2020) - UC Irvine](https://grandcentral.eee.uci.edu/syllabus/download/F-2020/17815) ([YouTube](https://www.youtube.com/playlist?list=PLIygTcviGPKBAh1HWXPOI3Rw59WdRlJXu))\n  - [ECE 1508 Applied Deep Learning - University of Toronto](https://www.bereyhi.com/deep-learning) ([Winter 2025](https://www.youtube.com/playlist?list=PLcFgNUo9s_AgsMOnniTMIWmLpjj9vZ1Wm)) ([Fall 2024](https://www.youtube.com/playlist?list=PLcFgNUo9s_Ajz1l4rBDIApwdcEKgzoMko))\n  - [Neural networks class - Universit\u00e9 de Sherbrooke](http://info.usherbrooke.ca/hlarochelle/neural_networks/content.html) ([YouTube](https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH))\n  - [DLCV - Deep Learning for Computer Vision - UPC Barcelona](https://www.youtube.com/playlist?list=PL-5eMc3HQTBavDoZpFcX-bff5WgQqSLzR)\n  - [DLAI - Deep Learning for Artificial Intelligence @ UPC Barcelona](https://www.youtube.com/playlist?list=PL-5eMc3HQTBagIUjKefjcTbnXC0wXC_vd)\n  - [Neural Networks and Applications - IIT Kharagpur](https://nptel.ac.in/courses/117105084/)\n  - [UVA DEEP LEARNING COURSE](http://uvadlc.github.io/#lecture)\n  - [Deep Learning - Winter 2020-21 - T\u00fcbingen Machine Learning](https://www.youtube.com/playlist?list=PL05umP7R6ij3NTWIdtMbfvX7Z-4WEXRqD)\n  - [Geometric Deep Learning - AMMI](https://www.youtube.com/playlist?list=PLn2-dEmQeTfQ8YVuHBOvAhUlnIPYxkeu3)\n  - [Math for Deep Learning \u2014 Andreas Geiger](https://www.youtube.com/playlist?list=PL05umP7R6ij0bo4UtMdzEJ6TiLOqj4ZCm)\n  - [Applied Deep Learning 2022 - TU Wien](https://www.youtube.com/playlist?list=PLNsFwZQ_pkE_QaTwYxoTmmRJHtMXyIAU6)\n  - [Neural Networks: Zero to Hero - Andrej Karpathy](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)\n  - [CIS 522 - Deep Learning - U Penn](https://www.youtube.com/@cis522-deeplearning8/playlists)\n  - [UVA DEEP LEARNING COURSE](http://uvadlc.github.io/#lecture)\n  - [Deep Learning (Fall 2020) - FAU](https://www.fau.tv/course/id/1600) ([Spring 2020](https://www.fau.tv/course/id/925)) ([Fall 2019](https://www.fau.tv/course/id/849)) ([Spring 2019](https://www.fau.tv/course/id/758)) ([Fall 2018](https://www.fau.tv/course/id/701)) ([Spring 2018](https://www.fau.tv/course/id/662))\n  - [Deep Learning (Fall 2020) - Georgia Tech](https://www.youtube.com/playlist?list=PL-fZD610i7yB7gDnPDpFcKpHI9X8z3OQ7)\n  - [Mathematics of Deep Learning (2021) - FAU](https://www.fau.tv/course/id/878)\n  - [CS7015 - Deep Learning - Prof. Mitesh M. Khapra - IIT Madras](https://www.youtube.com/playlist?list=PLyqSpQzTE6M9gCgajvQbc68Hk_JKGBAYT)\n  - [ETH Z\u00fcrich | Deep Learning in Scientific Computing 2023](https://www.youtube.com/playlist?list=PLJkYEExhe7rYY5HjpIJbgo-tDZ3bIAqAm)\n  - [Applied Deep Learning Maziar Raissi](https://www.youtube.com/playlist?list=PLoEMreTa9CNmuxQeIKWaz7AVFd_ZeAcy4)\n  - [UC Berkeley CS 182 / 282a Deep Learning spring 2023, by Anant Sahai](https://www.youtube.com/playlist?list=PLIygTcviGPKAaj_UAJcazYN4964xZ7Lt1)\n  - [Foundations of Deep Learning - UMD](https://www.youtube.com/playlist?list=PLHgjs9ncvHi80UCSlSvQe-TK_uOyDv_Jf)\n  - [TUM IN2346 Introduction to Deep Learning Fall 2024, by Daniel Cremers](https://cvg.cit.tum.de/teaching/ws2024/i2dl) ([Summer 2023](https://www.youtube.com/playlist?list=PLQ8Y4kIIbzy_pGm2QAwF625E6nmcRu2sU))\n  - [UT Austin - Advances in Deep Learning](https://ut.philkr.net/advances_in_deeplearning/)\n  \n- #### **Reinforcement Learning**\n  - [CS234: Reinforcement Learning - Spring 2024 - Stanford University](https://www.youtube.com/playlist?list=PLoROMvodv4rN4wG6Nk6sNpTEbuOSosZdX) ([Winter 2019](https://www.youtube.com/playlist?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u))\n  - [CSE 542: Reinforcement Learning - Spring 2024 - University of Washington](https://courses.cs.washington.edu/courses/cse542/24sp/schedule/)\n  - [CSE 579: Reinforcement Learning - Autumn 2024 - University of Washington](https://courses.cs.washington.edu/courses/cse579/24au/schedule/)\n  - [CSC 2547: Introduction to Reinforcement Learning - Spring 2021 - University of Toronto](https://amfarahmand.github.io/IntroRL/) ([YouTube](https://www.youtube.com/playlist?list=PLCveiXxL2xNbiDq51a8iJwPRq2aO0ykrq))\n  - [Introduction to reinforcement learning - UCL](https://www.youtube.com/playlist?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ)\n  - [Reinforcement Learning - IIT Madras](https://www.youtube.com/playlist?list=PLyqSpQzTE6M_FwzHFAyf4LSkz_IjMyjD9) ([TA - Manav Mishra](https://www.youtube.com/playlist?list=PLpKrAXMumEsjAR1Ybb0qbTKGYd9RY0vxa), [TA - Prabhleen Kukreja](https://www.youtube.com/playlist?list=PLTt_oMmEiDJhBPjy_aedcoeIY-IsaDBOn), [TA - Sandarbh Yadav ](https://www.youtube.com/playlist?list=PLoo_WPzXEM1ShdfOxv-adi3JdhRX4rA7F), [TA - Avik Kar](https://www.youtube.com/playlist?list=PLz2x4RAIbeXkTJFEipkD_ds3z0qx8_5D7))\n  - [Special topics in ML (Reinforcement Learning) IIT madras](https://www.youtube.com/playlist?list=PLZ2ps__7DhBbDiVplM2I9q2XNso1Qfj62)\n  - [CS885 Reinforcement Learning - Spring 2018 - University of Waterloo](https://www.youtube.com/playlist?list=PLdAoL1zKcqTXFJniO3Tqqn6xMBBL07EDc)\n  - [CS 285 - Deep Reinforcement Learning- UC Berkeley](https://www.youtube.com/playlist?list=PLkFD6_40KJIwhWJpGazJ9VSj9CFMkb79A)\n  - [CS 294 112 - Reinforcement Learning](https://www.youtube.com/playlist?list=PLkFD6_40KJIxJMR-j5A1mkxK26gh_qg37)\n  - [NUS CS 6101 - Deep Reinforcement Learning](https://www.youtube.com/playlist?list=PLllwxvcS7ca5wOmRLKm6ri-OaC0INYehv)\n  - [ECE 8851: Reinforcement Learning](https://www.youtube.com/playlist?list=PL_Nk3YvgORJs1tCLQnlnSRsOJArj_cP9u)\n  - [CS294-112, Deep Reinforcement Learning Sp17](http://rll.berkeley.edu/deeprlcourse/) ([YouTube](https://www.youtube.com/playlist?list=PLkFD6_40KJIwTmSbCv9OVJB3YaO4sFwkX))\n  - [UCL Course 2015 on Reinforcement Learning by David Silver from DeepMind](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html) ([YouTube](https://www.youtube.com/watch?v=2pWv7GOvuf0))\n  - [Deep RL Bootcamp - Berkeley Aug 2017](https://sites.google.com/view/deep-rl-bootcamp/lectures)\n  - [Reinforcement Learning - IIT Madras](https://www.youtube.com/playlist?list=PLyqSpQzTE6M_FwzHFAyf4LSkz_IjMyjD9)\n  - [Reinforcement Learning Course at KTH (FDD3359 - 2022)](https://www.youtube.com/playlist?list=PL21JFJEtbq0JLNo53UIkbIwkc2njCVUUR)\n  - [Reinforcement Learning Course at ASU, Spring 2022](https://www.youtube.com/playlist?list=PLmH30BG15SIoXhxLldoio0BhsIY84YMDj)\n  - [CS 4789/5789: Introduction to Reinforcement Learning - Cornell](https://www.youtube.com/playlist?list=PLQVNhPb8ajtCjWSKUvKU8cX5lueYP9s3X)\n  - [S20/IE613 - Online (Machine) Learning/ Bandit Algorithms](https://www.youtube.com/playlist?list=PLDREIwGwrHBdiBm1q0cVJLZn4Cn6Hig2s)\n  - [Reinforcement Learning - Fall 2021 chandar-lab](https://www.youtube.com/playlist?list=PLImtCgowF_ES_JdF_UcM60EXTcGZg67Ua)\n  - [CMU 10 703 Deep Reinforcement Learning & Control fall 2022, by Katerina Fragkiadaki](https://scs.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx#folderID=%22ee5794a2-cb54-4edc-836b-aefc01023243%22)\n  - [ECE524 Foundations of Reinforcement Learning at Princeton University, Spring 2024](https://www.youtube.com/playlist?list=PLYXvCE1En13epbogBmgafC_Yyyk9oQogl)\n  - [REINFORCEMENT LEARNING AND OPTIMAL CONTROL - Dimitri P. Bertsekas, ASU](https://web.mit.edu/dimitrib/www/RLbook.html)\n  - [CMU 16 745 Optimal Control and Reinforcement Learning spring by Zac Manchester](https://www.youtube.com/@roboticexplorationlab3724/playlists)\n  - [CMU 16 899 Adaptive Control and Reinforcement Learning fall 2020, by  Changliu Liu](https://www.youtube.com/playlist?list=PLZL5VXraKdz-0zByoPNzNTqSirR4veU8z)\n  - [Jadavpur University, 2025: Introduction to Reinforcement Learning](https://www.youtube.com/playlist?list=PLcNLn_ApooUzGJW60RcD2HRrL7sm0HG-w)\n  - [EE675 (2024) Introduction to Reinforcement Learning Course | IIT Kanpur](https://www.youtube.com/playlist?list=PLZAmMLcSnKRICBNyjraAhQdtdJFFgyRL5)\n  - [Reinforcement Learning Course by Fr\u00e9d\u00e9ric Godin - Concordia University](https://www.youtube.com/playlist?list=PLFskzTP727yc5eXXm09Xst7bIk5fi7mD6)\n  - [CS 285: Deep RL, 2023](https://www.youtube.com/playlist?list=PL_iWQOsE6TfVYGEGiAOMaOzzv41Jfm_Ps)\n  - [Mathematical Foundations of Reinforcement Learning - WINDY Lab](https://www.youtube.com/playlist?list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8)\n  - [Reinforcement Learning (HMC CS 181V)\u2014Spring, 2020 - Neil Rhodes](https://www.youtube.com/playlist?list=PL2Yggtk_pK69evEzfwQHm9ASOCbXPlXPS)\n  - [Reinforcement Learning Course: Lectures (Summer 2023) by Paderborn University](https://www.youtube.com/playlist?list=PL4GzQQuIDBGv-IFxRSgydCR7OrOM_xKqN)\n  - [CS292F (Spring 2021) Statistical Foundation of Reinforcement Learning - UCSD](https://cseweb.ucsd.edu/~yuxiangw/classes/RLCourse-2021Spring/)\n  - [Algorithmic Foundations of Interactive Learning - CMU](https://interactive-learning-algos.github.io/)\n  \n- #### **Advanced Machine Learning**\n  - [Advanced Machine Learning, 2021-2022, Sem I - by Prof. Madhavan Mukund, CMI](https://www.cmi.ac.in/~madhavan/courses/aml2021)\n  - [18.409 Algorithmic Aspects of Machine Learning Spring 2015 - MIT](https://www.youtube.com/playlist?list=PLB3sDpSRdrOvI1hYXNsa6Lety7K8FhPpx)\n  - [CS 330 - Deep Multi-Task and Meta Learning - Fall 2019 - Stanford University](https://cs330.stanford.edu/) ([Youtube](https://www.youtube.com/playlist?list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5))\n  - [Stanford CS330: Deep Multi-Task and Meta Learning I Autumn 2022](https://www.youtube.com/playlist?list=PLoROMvodv4rNjRoawgt72BBNwL2V7doGI)\n  - [ES 661 (2023): Probabilistic Machine Learning - IIT Gandhinagar](https://www.youtube.com/playlist?list=PLftoLyLEwECBEJyfRBJoSBd0UaTjEcs3I)\n  - [Information Retrieval in High Dimensional Data](https://www.youtube.com/playlist?list=PLaE1lKCe0jH3ePp9wCU1ygTquVOXY-UYv)\n  - [Trustworthy Machine Learning - Winter Semester 2023-2024, University of T\u00fcbingen](https://scalabletrustworthyai.github.io/courses/tml_winter_2324/)\n  - [Trustworthy Machine Learning - Winter Semester 2024-2025, University of T\u00fcbingen](https://scalabletrustworthyai.github.io/courses/tml_winter_2425/)\n  - [ETH Z\u00fcrich Advanced Machine Learning fall 2019, by Joachim M. Buhmann](https://video.ethz.ch/lectures/d-infk/2019/autumn/252-0535-00L.html)\n  - [CS 159 Advanced Topics in Machine Learning, Spring 2021 - Caltech](https://1five9.github.io/)\n  - [CS 229br Advanced Topics in the theory of machine learning, Spring 2021 - Harvard](https://boazbk.github.io/mltheoryseminar/cs229br.html)\n  \n  \n- #### **Natural Language Processing**\n  - [CS 224N -Natural Language Processing with Deep Learning - Stanford University](http://web.stanford.edu/class/cs224n/) ([Lectures -  Winter 2019](https://youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z)) ([Lectures -  Winter 2021](https://youtube.com/playlist?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ)) ([Lectures - Spring 2024](https://www.youtube.com/playlist?list=PLoROMvodv4rOaMFbaqxPDoLWjDaRAdP9D))\n  - [CS 224N - Natural Language Processing, Stanford University](https://web.stanford.edu/~jurafsky/NLPCourseraSlides.html) ([Lecture videos](https://academictorrents.com/details/d2c8f8f1651740520b7dfab23438d89bc8c0c0ab))\n  - [Stanford XCS224U: Natural Language Understanding I Spring 2023](https://www.youtube.com/playlist?list=PLoROMvodv4rOwvldxftJTmoR3kRcWkJBp)\n  - [CS388: Natural Language Processing - UT Austin](https://www.cs.utexas.edu/~gdurrett/courses/online-course/materials.html)\n  - [CS 124 - From Languages to Information - Stanford University](https://www.youtube.com/channel/UC_48v322owNVtORXuMeRmpA/playlists?view=50&sort=dd&shelf_id=2)\n  - [CS 6340/5340 - Natural Language Processing - University of Utah - Spring 2024](https://utah-intro-nlp.github.io/) ([Youtube](https://www.youtube.com/playlist?list=PLbuogVdPnkCrPZ4Vc-GRnk730SLhC1L43))\n  - [CSE 447/517 - Natural Language Processing - University of Washington - Winter 2024](https://safe-fernleaf-26d.notion.site/Winter-24-CSE-447-517-Natural-Language-Processing-4142333a001143d2be5ecff1a535c4ab)\n  - [Neural Networks: Zero to Hero - Andrej Karpathy](https://karpathy.ai/zero-to-hero.html)\n  - [fast.ai Code-First Intro to Natural Language Processing](https://www.youtube.com/playlist?list=PLtmWHNX-gukKocXQOkQjuVxglSDYWsSh9) ([Github](https://github.com/fastai/course-nlp))\n  - [MOOC - Natural Language Processing - Coursera, University of Michigan](https://www.youtube.com/playlist?list=PLLssT5z_DsK8BdawOVCCaTCO99Ya58ryR)\n  - [Natural Language Processing at UT Austin (Greg Durrett)](https://www.youtube.com/playlist?list=PLofp2YXfp7Tbk88uH4jejfXPd2OpWuSLq)\n  - [CS224U: Natural Language Understanding - Spring 2019 - Stanford University](https://www.youtube.com/playlist?list=PLoROMvodv4rObpMCir6rNNUlFAn56Js20)\n  - [Deep Learning for Natural Language Processing, 2017 - Oxford University](https://github.com/oxford-cs-deepnlp-2017/lectures)\n  - [Natural Language Processing - IIT Bombay](https://nptel.ac.in/courses/106101007/)\n  - [CMU Advanced NLP Fall 2024](https://phontron.com/class/anlp-fall2024/schedule/) ([Lectures - Fall 2024](https://www.youtube.com/playlist?list=PL8PYTP1V4I8D4BeyjwWczukWq9d8PNyZp)) ([Lectures - Fall 2021](https://www.youtube.com/playlist?list=PL8PYTP1V4I8AYSXn_GKVgwXVluCT9chJ6))\n  - [CMU Neural Nets for NLP 2021](https://www.youtube.com/playlist?list=PL8PYTP1V4I8AkaHEJ7lOOrlex-pcxS-XV)\n  - [Natural Language Processing - Michael Collins - Columbia University](https://www.youtube.com/playlist?list=PLA212ij5XG8OTDRl8IWFiJgHR9Ve2k9pv)\n  - [CMU CS11-711 - Advanced Natural Language Processing](https://cmu-l3.github.io/anlp-spring2025/) ([Lectures - Spring 2025](https://www.youtube.com/playlist?list=PLqC25OT8ZpD3WxQ0FwWMGPS_BcWdcKyZy))\n  - [CMU CS11-737 - Multilingual Natural Language Processing](https://www.youtube.com/playlist?list=PL8PYTP1V4I8CHhppU6n1Q9-04m96D9gt5)\n  - [UMass CS685: Advanced Natural Language Processing (Spring 2022)](https://www.youtube.com/playlist?list=PLWnsVgP6CzadI4-FT2Po4wsEK7MHCIQ-d)\n  - [Natural Language Processing (CMSC 470)](https://www.youtube.com/playlist?list=PLwrUPjGidcJ4UkSoi7_rmn-1kcedLqgdL)\n  - [Stanford CS25 - Transformers United 2023](https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM)\n  - [Natural Language Processing (IN2361) - TUM](https://live.rbg.tum.de/?year=2019&term=W&slug=nlp&view=3)\n  - [CS 886: Recent Advances on Foundation Models Winter 2024 - University of Waterloo](https://cs.uwaterloo.ca/~wenhuche/teaching/cs886/)\n  - [UC Berkeley CS 194/294-196 Large Language Model Agents Fall 2024, by Dawn Song & Xinyun Chen](https://rdi.berkeley.edu/llm-agents/f24) ([YouTube playlist](https://www.youtube.com/playlist?list=PLS01nW3RtgopsNLeM936V4TNSsvvVglLc))\n  - [UC Berkeley CS 194/294-267 Understanding Large Language Models Foundations and Safety spring 2024, by Dawn Song & Dan Hendrycks](https://www.youtube.com/playlist?list=PLJ66BAXN6D8H_gRQJGjmbnS5qCWoxJNfe)\n  - [Introduction to Large Language Models (LLMs), IIT Delhi](https://www.youtube.com/playlist?list=PLqGkIjcOyrGnjyBHl4GE2S9kX47X96FH-)\n  - [Natural Language Processing (Spring 2024) - University of Utah](https://www.youtube.com/playlist?list=PLbuogVdPnkCrPZ4Vc-GRnk730SLhC1L43)\n  - [Multilingual NLP 2020 - CMU](https://www.youtube.com/playlist?list=PL8PYTP1V4I8CHhppU6n1Q9-04m96D9gt5)\n  - [Introduction to large language models - IIT Madras](https://www.youtube.com/playlist?list=PLZ2ps__7DhBbaMNZoyW2Hizl8DG6ikkjo)\n  - [Speech Technology - IIT Madras](https://www.youtube.com/playlist?list=PLZ2ps__7DhBaI_g3_V-CFrgFIf-0Yksiv)\n  \n- #### **Generative AI**\n  - [Stanford CS236: Deep Generative Models I 2023 I Stefano Ermon](https://www.youtube.com/playlist?list=PLoROMvodv4rPOWA-omMM6STXaWW4FvJT8)\n  - [CS 6785 - Deep Generative Models - Cornell Tech, Spring 2023)](https://www.youtube.com/playlist?list=PL2UML_KCiC0UPzjW9BjO-IW6dqliu9O4B)\n  - [MIT 6.S184 Flow Matching and Diffusion Models, 2025](https://diffusion.csail.mit.edu)\n  \n- #### **Computer Vision**\n  - [CS 231n - Convolutional Neural Networks for Visual Recognition, Stanford University](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)\n  - [CS 198-126: Modern Computer Vision Fall 2022 (UC Berkeley)](https://www.youtube.com/playlist?list=PLzWRmD0Vi2KVsrCqA4VnztE4t71KnTnP5)\n  - [Machine Learning for Robotics and Computer Vision, WS 2013/2014 - TU M\u00fcnchen](https://vision.in.tum.de/teaching/ws2013/ml_ws13) ([YouTube](https://www.youtube.com/playlist?list=PLTBdjV_4f-EIiongKlS9OKrBEp8QR47Wl))\n  - [COGSCI 1 - Intro to Cognitive Science Summer 2022 - UC Berkeley](https://www.youtube.com/playlist?list=PLaMjLYzDGxvz1oT5gpFiY6rJZnlJ-1Xu-)\n  - [Informatics 1 - Cognitive Science 2015/16- University of Edinburgh](http://groups.inf.ed.ac.uk/vision/VIDEO/2015/inf1cs.htm)\n  - [Informatics 2A - Processing Formal and Natural Languages 2016-17 - University of Edinburgh](http://www.inf.ed.ac.uk/teaching/courses/inf2a/schedule.html)\n  - [NOC:Deep Learning For Visual Computing - IIT Kharagpur](https://nptel.ac.in/courses/108/105/108105103/)\n  - [Extreme Classification ](https://www.youtube.com/playlist?list=PLXtAHOcKKDTk43wjXud9GQS-l-QA5DQxH)\n  - [EECS 498/598 - Deep Learning for Computer Vision - University of Michigan - Fall 2019](https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2019/) ([Youtube](https://www.youtube.com/playlist?list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r))\n  - [Computer Vision - FAU Spring 2021](https://www.fau.tv/course/id/2306) ([Spring 2018](https://www.fau.tv/course/id/734))\n  - [CAP5415 Computer Vision - UCF Fall 2023](https://www.youtube.com/playlist?list=PLd3hlSJsX_InWyCQtwqQ7y6KnwhxNCgRf)\n  - [CAP6412 Advanced Computer Vision - UCF Spring 2024](https://www.crcv.ucf.edu/courses/cap6412-spring-2024/schedule/) ([Youtube](https://www.youtube.com/playlist?list=PLd3hlSJsX_IlSr0ua4v8WQezazAMXEJE4))\n  - [Advanced Deep Learning for Computer vision (ADL4CV) (IN2364) - TU Munich](https://dvl.in.tum.de/teaching/adl4cv-ss20/) ([Youtube](https://www.youtube.com/playlist?list=PLog3nOPCjKBnjhuHMIXu4ISE4Z4f2jm39))\n  - [Computer Vision III: Detection, Segmentation and Tracking (CV3DST) (IN2375) - TU Munich](https://www.youtube.com/playlist?list=PLog3nOPCjKBkamdw8F6Hw_4YbRiDRb2rb)\n  \n- #### **Time Series Analysis**\n  - [02417 Time Series Analysis](https://www.youtube.com/playlist?list=PLtiTxpFJ4k6TZ0g496fVcQpt_-XJRNkbi)\n  - [Applied Time Series Analysis](https://www.youtube.com/playlist?list=PLl0FT6O_WWDBm-4W-eoK34omYmEMseQDX)\n  \n- #### **Optimization**\n  - [Optimisation for Machine Learning: Theory and Implementation (Hindi) - IIT](https://www.youtube.com/playlist?list=PLyqSpQzTE6M-pmLzCoMu_ANU6atEFyyJl)\n  - [Rochester DSCC 435 Optimization for Machine Learning fall 2023, by Jiaming Liang](https://www.youtube.com/playlist?list=PLuJY91x7h5orCtyh6mChurVQ2ZZef9qPF)\n  - [Princeton ELE539/COS512 Optimization for Machine Learning spring 2021, by Chi Jin](https://sites.google.com/view/cjin/teaching/ece539cos512-2021-ver)\n  - [UT Dallas CS 7301 Advanced Topics in Optimization for Machine Learning spring 2021, by Rishabh Iyer](https://github.com/rishabhk108/AdvancedOptML) ([YouTube](https://www.youtube.com/playlist?list=PLGod0_zT9w92_evaYrf3-rE67AmgPJoUU))\n  - [Convex Analysis, Summer 2021 - TU Braunschweig](https://www.tu-braunschweig.de/index.php?eID=dumpFile&t=f&f=128341&token=3c40ee32c6c029df85f7e552522f4a87470e3401) ([YouTube](https://www.youtube.com/playlist?list=PLPomPKAI5ZlJBThiwc3bya7qngzw9-0QD))\n  - [EE364a: Convex Optimization I - Stanford University](https://www.youtube.com/playlist?list=PLoROMvodv4rMJqxxviPa4AmDClvcbHi6h)\n  - [10-725 Convex Optimization, Spring 2015 - CMU](http://www.stat.cmu.edu/~ryantibs/convexopt-S15/)\n  - [10-725 Convex Optimization: Fall 2016 - CMU](http://www.stat.cmu.edu/~ryantibs/convexopt/)\n  - [10-725 Optimization Fall 2012 - CMU](http://www.cs.cmu.edu/~ggordon/10725-F12/schedule.html)\n  - [10-801 Advanced Optimization and Randomized Methods - CMU](http://www.cs.cmu.edu/~suvrit/teach/aopt.html) ([YouTube](https://www.youtube.com/playlist?list=PLjTcdlvIS6cjdA8WVXNIk56X_SjICxt0d))\n  - [AM 207 - Stochastic Methods for Data Analysis, Inference and Optimization, Harvard University](http://am207.github.io/2016/index.html)\n  - [MIT 6.S098 Applied Convex Optimization IAP 2022, by Alexandre Amice, Benoit Legat](https://alexandreamice.github.io/teaching/convex_optimization/) ([YouTube](https://www.youtube.com/playlist?list=PL5SG6ajT9NZKxdvM1jQOLXmeKO7MfyLxR))\n  - [University of Twente Discrete Optimization, by Marc Uetz](https://marcuetz.personalweb.utwente.nl/do/) ([Fall 2020](https://www.youtube.com/playlist?list=PLIygTcviGPKBUY9e1MCruRr5-I24LNm4g))\n  - [UC Davis MAT 168 Optimization winter 2024, by Matthias K\u00f6ppe](https://video.ucdavis.edu/channel/MAT+168+Optimization+%28Matthias+K%C3%B6ppe%3B+Winter+2024%29/329241352)\n  - [Purdue University CHE 597 Computational Optimization spring 2025, by Can Li](https://canli1.github.io/courses)\n  - [UCSD CS292F Convex Optimization Spring 2020, by Yu-Xiang Wang](https://cseweb.ucsd.edu/~yuxiangw/classes/CS292F-2020Spring/) ([Youtube](https://www.youtube.com/playlist?list=PLTN4aNO9NiB5VxYILKPBXoy9g1tUqmnBx))\n  - [UIUC ECE 490 Introduction to Optimization fall 2020, by Venugopal V. Veeravalli](https://courses.grainger.illinois.edu/ece490/fa2020/) ([YouTube](https://www.youtube.com/playlist?list=PLIygTcviGPKC5wSXtE6s2qdSYutRH1AUU))\n  - [University of Wisconsin-Madison CS/ECE/ISyE 524 Introduction to Optimization spring 2017-18, by Laurent Lessard](https://laurentlessard.com/teaching/524-intro-to-optimization/)\n  - [University of Wisconsin-Madison ISyE/Math/CS/Stat 525 Linear Optimization fall 2021, by Alberto Del Pia](https://www.youtube.com/playlist?list=PLeO_PhASIA0Ot69TqANAnNxoykHGOQp2Y)\n  - [University of Wisconsin-Madison ISyE/Math/CS 728 Integer Optimization (second part of the course) spring 2020](https://www.youtube.com/playlist?list=PLeO_PhASIA0NlDNF9y-SsgVEYcvAMj2CY)\n  - [Columbia IEOR E4007 Optimization Models and Methods 2005, by Garud Iyengar](https://www.youtube.com/playlist?list=PLIygTcviGPKCNQ2xHRwrLOxkEWv9OfGiF)\n \n- #### **Unsupervised Learning**\n  - [CS294 Deep Unsupervised Learning Spring 2024](https://sites.google.com/view/berkeley-cs294-158-sp24/home)\n  - [Deep Unsupervised Learning -- Berkeley Spring 2020](https://www.youtube.com/playlist?list=PLwRJQ4m4UJjPiJP3691u-qWwPGVKzSlNP)\n  - [CS294-158 Deep Unsupervised Learning SP19](https://www.youtube.com/channel/UCf4SX8kAZM_oGcZjMREsU9w/videos)\n  - [UC San Diego COGS 118A Supervised Machine Learning fall 2020, by Jason Fleischer](https://www.youtube.com/playlist?list=PLaaNbhBDEsoF_ad5N2mlOsvB-RwSTYUjQ)\n  - [UC San Diego COGS 118B Unsupervised Machine Learning winter 2024, by Jason Fleischer](https://www.youtube.com/playlist?list=PLaaNbhBDEsoEU9RRbwCqJWFAZH2LjNwBN)\n  - [UIUC STAT 437 Unsupervised Learning spring 2024, by Tori Ellison](https://www.youtube.com/playlist?list=PLIygTcviGPKB133Vh7zxsxFoblyfS4P5Y)\n  - [Johns Hopkins Unsupervised Learning spring 2017, by Rene Vidal](https://www.youtube.com/playlist?list=PLaBAmmD3yH4Nta9Y6g9hOV4dcnpTzeW4q)\n  - [Unsupervised Learning (STAT 841), Winter 2017](https://www.youtube.com/playlist?list=PLehuLRPyt1HzQoXEhtNuYTmd0aNQvtyAK)\n  \n- #### **Misc Machine Learning Topics**\n  - [Quantum Machine Learning | 2021 Qiskit Global Summer School](https://www.youtube.com/playlist?list=PLOFEBzvs-VvqJwybFxkTiDzhf5E11p8BI)\n  - [CS 6955 - Clustering, Spring 2015, University of Utah](https://www.youtube.com/playlist?list=PLbuogVdPnkCpRvi-qSMCdOwyn4UYoPxTI)\n  - [Info 290 - Analyzing Big Data with Twitter, UC Berkeley school of information](http://blogs.ischool.berkeley.edu/i290-abdt-s12/) ([YouTube](https://www.youtube.com/playlist?list=PLE8C1256A28C1487F))\n  - [CS224W Machine Learning with Graphs | Spring 2021 | Stanford University](https://www.youtube.com/playlist?list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn)\n  - [9.520 - Statistical Learning Theory and Applications, Fall 2015 - MIT](https://www.youtube.com/playlist?list=PLyGKBDfnk-iDj3FBd0Avr_dLbrU8VG73O)\n  - [Statistical Learning Theory, Spring 2019 - ETH Z\u00fcrich](https://video.ethz.ch/lectures/d-infk/2019/spring/252-0526-00L.html)\n  - [Course on the Statistical Learning Theory, University of S\u00e3o Paulo, ICMC](https://www.youtube.com/playlist?list=PLKWX1jIoUZaVpVhMfevAE7iYNcDHPEJI_)\n  - [Reinforcement Learning - UCL](https://www.youtube.com/playlist?list=PLacBNHqv7n9gp9cBMrA6oDbzz_8JqhSKo)\n  - [Regularization Methods for Machine Learning 2016](http://academictorrents.com/details/493251615310f9b6ae1f483126292378137074cd) ([YouTube](https://www.youtube.com/playlist?list=PLbF0BXX_6CPJ20Gf_KbLFnPWjFTvvRwCO))\n  - [Statistical Inference in Big Data - University of Toronto](http://fields2015bigdata2inference.weebly.com/materials.html)\n  - [Reinforcement Learning - IIT Madras](https://nptel.ac.in/courses/106106143/)\n  - [Statistical Rethinking Winter 2015 - Richard McElreath](https://www.youtube.com/playlist?list=PLDcUM9US4XdMdZOhJWJJD4mDBMnbTWw_z)\n  - [Foundations of Machine Learning - Blmmoberg Edu](https://bloomberg.github.io/foml/#home)\n  - [Introduction to reinforcement learning - UCL](https://www.youtube.com/playlist?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ)\n  - [Statistical Foundation of Reinforcement Learning - UCSD, by Yu-Xiang Wang, Spring 2021](https://cseweb.ucsd.edu/~yuxiangw/classes/RLCourse-2021Spring/) ([Youtube](https://www.youtube.com/playlist?list=PLTN4aNO9NiB5X8HkOBTkIfZ8VFSLH1W8W))\n  - [Web Information Retrieval (Proff. L. Becchetti - A. Vitaletti)](https://www.youtube.com/playlist?list=PLAQopGWlIcya-9yzQ8c8UtPOuCv0mFZkr)\n  - [Big Data Systems (WT 2019/20) - Prof. Dr. Tilmann Rabl - HPI](https://www.tele-task.de/series/1286/)\n  - [Distributed Data Analytics (WT 2017/18) - Dr. Thorsten Papenbrock - HPI](https://www.tele-task.de/series/1179/)\n  - [Introduction to Data-Centric AI - MIT](https://dcai.csail.mit.edu/)\n  - [Parallel Computing and Scientific Machine Learning](https://www.youtube.com/playlist?list=PLCAl7tjCwWyGjdzOOnlbGnVNZk0kB8VSa)\n  - [Machine Learning System Design - System Design Fight Club](https://www.youtube.com/playlist?list=PLlvnxKilk3aKx0oFua-HTtFf-d_inQ8Qn)\n  - [UT Austin ECE 381V Bandits and Online Learning fall 2021, by Sanjay Shakkottai](https://docs.google.com/document/d/1r6jXNd1DD9o8v4q4XqxShRXWYdhtLMjEXWDzuv0T6LU/edit)\n  - [UCSD MATH 273B Information Geometry and its Applications winter 2022, by Melvin Leok](https://www.youtube.com/playlist?list=PLHZhjPByiV3L94AeJ9FcK1yrnRDOt3Vit)\n  - [Cornell ECE 5545 Machine Learning Hardware and Systems Spring 2022, by Mohamed Abdelfattah](https://www.youtube.com/playlist?list=PL0mFAhrXqy9CuopJhAB8GVu_Oy7J0ery6)\n  - [High Dimensional Analysis: Random Matrices and Machine Learning by Roland Speicher](https://rolandspeicher.com/lectures/course-on-high-dimensional-analysis-random-matrices-and-machine-learning-summer-term-2023/)([Youtube](https://www.youtube.com/playlist?list=PLY11JnnnTUCabY4nc0hKptrd5qEWtLoo2))\n  - [ACP SUMMER SCHOOL 2023 on Machine Learning for Constraint Programming](https://www.youtube.com/playlist?list=PLcByDTr7vRTYJ2s6DL-3bzjGwtQif33y3)\n  - [EE512A - Advanced Inference in Graphical Models, Fall Quarter, 2014](https://people.ece.uw.edu/bilmes/classes/ee512/ee512_fall_2014/)\n  - [University of Wisconsin-Madison CS/ECE 561 - Probability and Information Theory in Machine Learning fall 2020, by Matthew Malley](https://mediaspace.wisc.edu/channel/CS_ECE%2B561%2B-%2BProbability%2Band%2BInfo%2BTheory%2Bin%2BMachine%2BLearning/191748913)\n  - [University of Maryland CMSC828U Algorithms in Machine Learning: Guarantees and Analyses fall 2020, by Furong Huang](https://www.cs.umd.edu/class/fall2020/cmsc828u//schedule/) ([YouTube playlist](https://www.youtube.com/playlist?list=PLM0SC1uXFSPo5TO_UN7fia1luj73yuHEC))\n  - [Statistical Physics of Machine Learning](https://www.youtube.com/playlist?list=PL04QVxpjcnjgzMr9ehyZUSkwu0Wr0cF_N)\n  - [11-755 - Machine Learning for Signal Processing, CMU](https://cmu-mlsp.github.io/mlspcourse/schedule/) ([YouTube-2024](https://www.youtube.com/playlist?list=PLWhigAi1mTpO3Rw-1D2b2AowYeIU5eRzD), [YouTube-2023](https://www.youtube.com/playlist?list=PLWhigAi1mTpPMV3hECmkEv3s8Lc8H_VUd))\n  - [Machine Learning for 3D Data, Fall 2023, KAIST](https://mhsung.github.io/kaist-cs479-fall-2023/)\n  - [Machine Learning for Physicists, Spring 2019, FAU](https://www.fau.tv/course/id/778) ([Spring 2017](https://www.fau.tv/course/id/574))\n  - [CSCE 585 - Machine Learning Systems, University of South Carolina](https://pooyanjamshidi.github.io/mls/lectures/) ([YouTube-2020](https://www.youtube.com/playlist?list=PLtkQf9LiEmLFS56WTpRJ3PZwwCYyi2Cdu))\n  - [CS-E4740 - Federated Learning, Spring 2023, Aalto University](https://www.youtube.com/playlist?list=PLrbn2dGrLJK8c6hCQXBVFoYsPXG-g_75c)\n\n\n------------------------------\n\n\n### Computer Networks\n\n- [CS 144 Introduction to Computer Networking - Stanford University, Fall 2013](http://www.scs.stanford.edu/10au-cs144/) ([Lecture videos](https://www.youtube.com/playlist?list=PL6RdenZrxrw9inR-IJv-erlOKRHjymxMN))\n- [Computer Networking: A Top-Down Approach](https://www.youtube.com/playlist?list=PLm556dMNleHc1MWN5BX9B2XkwkNE2Djiu)\n- [Computer Communication Networks, Rensselaer Polytechnic Institute - Fall 2001](https://www.ecse.rpi.edu/Homepages/koushik/shivkuma-teaching/video_index.html) ([Videos](https://www.ecse.rpi.edu/Homepages/koushik/shivkuma-teaching/video_index.html#ccn_video)) ([Slides](https://www.ecse.rpi.edu/Homepages/koushik/shivkuma-teaching/video_index.html#ccn_foils))\n- [Audio/Video Recordings and Podcasts of Professor Raj Jain's Lectures - Washington University in St. Louis](http://www.cse.wustl.edu/~jain/videos.htm) ([YouTube](https://www.youtube.com/user/ProfRajJain/playlists))\n- [Computer Networks, Tanenbaum, Wetherall Computer Networks 5e - Video Lectures](http://media.pearsoncmg.com/ph/streaming/esm/tanenbaum5e_videonotes/tanenbaum_videoNotes.html)\n- [CSEP 561 - PMP Network Systems, Fall 2013 - University of Washington](https://courses.cs.washington.edu/courses/csep561/13au/) ([Videos](https://courses.cs.washington.edu/courses/csep561/13au/video/))\n- [CSEP 561 \u2013 Network Systems, Autumn 2008 - University of Washington](https://courses.cs.washington.edu/courses/csep561/08au/) ([Videos](https://courses.cs.washington.edu/courses/csep561/08au/lectures/))\n- [ECE/CS 438 - Communication Networks, Fall 2020 - UIUC](https://courses.physics.illinois.edu/cs438/fa2020/)\n- [Computer Networks - IIT Kharagpur](https://nptel.ac.in/courses/106105081/)\n- [Introduction to Data Communications 2013, Steven Gordon - Thammasat University, Thailand](https://www.youtube.com/playlist?list=PLvifRcqOOwF8u4iC7hFTMVC_WD6SEpnkx)\n- [Introduction to Complex Networks - RIT](https://www.youtube.com/playlist?list=PLE9AAD550EA21F3DC)\n- [Structural Analysis and Visualization of Networks](http://www.leonidzhukov.net/hse/2015/networks/)\n- [Columbia ELEN E4703 Wireless Communications spring 2006, by Angel Lozano](https://www.youtube.com/playlist?list=PLIygTcviGPKAaY0VGGjQ2whITq23AOjaR)\n- [Columbia COMS W4119 Computer Networks fall 2004, by Vishal Misra](https://www.youtube.com/playlist?list=PLIygTcviGPKCh6XgTHofe6zPK_LQ_nVW3)\n- [Columbia ELEN E4710 An Introduction to Network Engineering fall 2004, by Dan Rubenstein](https://www.cs.columbia.edu/~danr/courses/4710/Fall04/) ([Videos](https://www.youtube.com/playlist?list=PLIygTcviGPKClQelPKe2f05aReRMncyR7))\n- [Data Communication - IIT Kharagpur](https://nptel.ac.in/courses/106105082/)\n- [Error Correcting Codes - IISC Bangalore](https://nptel.ac.in/courses/117108044/)\n- [Information Theory and Coding - IIT Bombay](https://nptel.ac.in/courses/117101053/)\n- [Complex Network : Theory and Application - IIT Kharagpur](https://nptel.ac.in/courses/106105154/)\n- [Advanced 3G and 4G Wireless Mobile Communications - IIT Kanpur](https://nptel.ac.in/courses/117104099/)\n- [Broadband Networks: Concepts and Technology - IIT Bombay](https://nptel.ac.in/courses/117101050/)\n- [Coding Theory - IIT Madras](https://nptel.ac.in/courses/117106031/)\n- [Digital Communication - IIT Bombay](https://nptel.ac.in/courses/117101051/)\n- [Digital Voice & Picture Communication - IIT Kharagpur](https://nptel.ac.in/courses/117105081/)\n- [Wireless Ad Hoc and Sensor Networks - IIT Kharagpur](https://nptel.ac.in/courses/106105160/)\n- [Internetworking with TCP/IP by Prof. Dr. Christoph Meinel - HPI](https://www.youtube.com/playlist?list=PLoOmvuyo5UAfY5VrkObHTckZHwPsS1VCA)\n- [CS798: Mathematical Foundations of Computer Networking - University of Waterloo](https://www.youtube.com/playlist?list=PLFB088DB91845CA34)\n- [CS 168 Introduction to the Internet: Architecture and Protocols, Fall 2022 - UC Berkeley](https://fa22.cs168.io/) ([YouTube - Fall 2022](https://www.youtube.com/playlist?list=PLIygTcviGPKD4RbcpiXKeyXrIImw-mGd-)) ([Spring 2025](https://sp25.cs168.io/))\n- [Advanced Topics in Communication Networks, Fall 2022 - ETH Z\u00fcrich](https://video.ethz.ch/lectures/d-itet/2022/autumn/227-0575-00L.html)\n- [CS/ECE 438 Communication Networks [F23] - UIUC](https://rrc-uiuc.notion.site/Communication-Networks-F23-d37ef65a3d1b4b7ba3d49d929c03d546)\n\n------------------------------\n\n### Math for Computer Scientist\n\n- [Maths courses all topics covered - Khan Academy](https://www.khanacademy.org/math/)\n- **Calculus**\n  - [18.01 Single Variable Calculus, Fall 2006 - MIT OCW](https://ocw.mit.edu/courses/mathematics/18-01-single-variable-calculus-fall-2006/)\n  - [18.02 Multivariable Calculus, Fall 2007 - MIT OCW](https://ocw.mit.edu/courses/mathematics/18-02-multivariable-calculus-fall-2007/)\n  - [18.03 Differential Equations, Spring 2010 - MIT OCW](https://ocw.mit.edu/courses/mathematics/18-03-differential-equations-spring-2010/)\n  - [Highlights of Calculus - Gilbert Strang, MIT OCW](https://ocw.mit.edu/resources/res-18-005-highlights-of-calculus-spring-2010/)\n  - [MAT123 Introduction to Calculus (Fall 2015) - Stony Brook](https://www.math.stonybrook.edu/~scott/mat123.fall15/)\n  - [Vector Calculus for Engineers - HKUST](https://www.youtube.com/playlist?list=PLkZjai-2JcxnYmkg6fpzz4WFumGVl7MOa)\n- **Discrete Math**\n  - [6.042J - Mathematics for Computer Science, MIT OCW](https://ocw.mit.edu/courses/6-042j-mathematics-for-computer-science-fall-2010/video_galleries/video-lectures/) \n  - [Computer Science 70, 001 - Spring 2015](https://www.youtube.com/playlist?list=-XXv-cvA_iD8wQm8U0gG_Z1uHjImKXFy)\n  - [CSE 547 Discrete Mathematics, Prof Skiena, University of Stony Brook](http://www3.cs.stonybrook.edu/~algorith/math-video/)\n  - [Discrete Structures (Summer 2011) - Rutgers, The State University of New Jersey](https://itunes.apple.com/us/course/discrete-structures-summer/id698728837)\n  - [Discrete Mathematics and Mathematical Reasoning 2015/16 - University of Edinburgh](https://www.inf.ed.ac.uk/teaching/courses/dmmr/)\n  - [Discrete Mathematical Structures - IIT Madras](https://nptel.ac.in/courses/106106094/)\n  - [Discrete Structures - Pepperdine University](https://itunes.apple.com/us/course/discrete-structures/id546468789)\n  - [CMU 21 228 Discrete Mathematics spring 2021, by Po-Shen Loh](https://www.youtube.com/playlist?list=PLgTkKBA6LRqYuuQ-LboerRblBoD_q_eUM)\n  - [COMP2804: Discrete Structures II](https://cglab.ca/~morin/teaching/2804/)\n- **Probability & Statistics**\n  - [Statistics - CrashCourse](https://www.youtube.com/playlist?list=PL8dPuuaLjXtNM_Y-bUAhblSAdWRnmBUcr)\n  - [6.041 Probabilistic Systems Analysis and Applied Probability - MIT OCW](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-041sc-probabilistic-systems-analysis-and-applied-probability-fall-2013/)\n  - [Stanford CS109 Introduction to Probability for Computer Scientists I 2022 I Chris Piech](https://www.youtube.com/playlist?list=PLoROMvodv4rOpr_A7B9SriE_iZmkanvUg)\n  - [MIT RES.6-012 Introduction to Probability, Spring 2018 - MIT](https://www.youtube.com/playlist?list=PLUl4u3cNGP60hI9ATjSFgLZpbNJ7myAg6)\n  - [Statistics 110 - Probability - Harvard University](https://www.youtube.com/playlist?list=PL2SOU6wwxB0uwwH80KTQ6ht66KWxbzTIo)\n  - [STAT 2.1x: Descriptive Statistics | UC Berkeley](https://www.youtube.com/playlist?list=PL_Ig1a5kxu56TfFnGlRlH2YpOBWGiYsQD)\n  - [STAT 2.2x: Probability | UC Berkeley](https://www.youtube.com/playlist?list=PL_Ig1a5kxu57qPZnHm-ie-D7vs9g7U-Cl)\n  - [MOOC - Statistics: Making Sense of Data, Coursera](http://academictorrents.com/details/a0cbaf3e03e0893085b6fbdc97cb6220896dddf2)\n  - [MOOC - Statistics One - Coursera](https://www.youtube.com/playlist?list=PLycnP7USbo1V3jlyjAzWUB201cLxPq4NP)\n  - [Probability and Random Processes - IIT Kharagpur](https://nptel.ac.in/courses/117105085/)\n  - [MOOC - Statistical Inference - Coursera](https://www.youtube.com/playlist?list=PLgIPpm6tJZoSvrYM54BUqJJ4CWrYeGO40)\n  - [131B - Introduction to Probability and Statistics, UCI](https://www.youtube.com/playlist?list=PLqOZ6FD_RQ7k-j-86QUC2_0nEu0QOP-Wy)\n  - [STATS 250 - Introduction to Statistics and Data Analysis, UMichigan](https://www.youtube.com/playlist?list=PL432AB57AF9F43D4F)\n  - [Sets, Counting and Probability - Harvard](http://matterhorn.dce.harvard.edu/engage/ui/index.html#/1999/01/82347)\n  - [Opinionated Lessons in Statistics](http://www.opinionatedlessons.org/) ([Youtube](https://www.youtube.com/playlist?list=PLUAHeOPjkJseXJKbuk9-hlOfZU9Wd6pS0))\n  - [Statistics - Brandon Foltz](https://www.youtube.com/user/BCFoltz/playlists)\n  - [Statistical Rethinking: A Bayesian Course Using R and Stan](https://github.com/rmcelreath/statrethinking_winter2019) ([Lectures](https://www.youtube.com/playlist?list=PLDcUM9US4XdPz-KxHM4XHt7uUVGWWVSus)) ([Book](http://www.stat.columbia.edu/~gelman/book/))\n  - [02402 Introduction to Statistics E12 - Technical University of Denmark](https://www.youtube.com/playlist?list=PLMn2aW3wpAtPC8tZHQy6nwWsFG7P6sPqw) ([F17](https://www.youtube.com/playlist?list=PLgowegO9Se58_BnUNnaARajEE_bX-GJEz))\n  - [Engineering Probability (ECSE-2500) - RPI](https://www.youtube.com/playlist?list=PLuh62Q4Sv7BU1dN2G6ncyiMbML7OXh_Jx)\n  - [Purdue ECE302 Introduction to Probability for Data Science](https://www.youtube.com/playlist?list=PL4FSfq6xtSvyqsbgrYDeyV0WWXlXqwy2M)\n  - [Undergraduate Probability with Professor Roman Vershynin](https://www.math.uci.edu/~rvershyn/teaching/ugp/ugp.html)\n  - [High-Dimensional Probability](https://www.math.uci.edu/~rvershyn/teaching/hdp/hdp.html)\n  - [Mathematical Statistics - 2024](https://www.youtube.com/playlist?list=PLLyj1Zd4UWrPZH-fknPLak0tlUpUISBZR) ([YouTube-2020](https://www.youtube.com/playlist?list=PLLyj1Zd4UWrOk5-wIki_oOxHJnNj0_437))\n  - [Bayesian Data Analysis](https://www.youtube.com/playlist?list=PLBqnAso5Dy7O0IVoVn2b-WtetXQk5CDk6)\n  - [Bayesian Machine Learning and Information Processing](https://biaslab.github.io/teaching/bmlip/) ([YouTube-2021/22](https://www.youtube.com/playlist?list=PLkDDgjnfkmCELsBXiCKCVIHKHSjl-rpkJ)) ([YouTube-2020/21](https://www.youtube.com/playlist?list=PLkDDgjnfkmCH4zq9_ul-KaTsk-1j1EVEo))\n  - [Markov Processes - Spring 2023](https://www.youtube.com/playlist?list=PLLyj1Zd4UWrP3rME2XvFvE4Q5vI3H_7_Z)\n  - [Measure Theoretic Probability](https://www.youtube.com/playlist?list=PLLyj1Zd4UWrO6VtBSiQLsNlo9QBm30nxC)\n  - [Causal Inference Course - Brady Neal](https://www.youtube.com/playlist?list=PLoazKTcS0RzZ1SUgeOgc6SWt51gfT80N0)\n  - [Causal Inference -- Online Lectures (M.Sc/PhD Level)](https://www.youtube.com/playlist?list=PLyvUJLHD8IsJCB7ALqwjRG1BjL5JxE__H)\n  - [Machine Learning & Causal Inference: A Short Course](https://www.youtube.com/playlist?list=PLxq_lXOUlvQAoWZEqhRqHNezS30lI49G-)\n  - [Causal Inference Jonas Peters](https://www.youtube.com/playlist?list=PLzERW_Obpmv-_EXTV1zTmlv-Ab5Tfbp8X)\n  - [UIUC ECE 534 Random Processes fall 2020 - Ilan Shomorony](https://www.youtube.com/playlist?list=PL682UO4IMem-1IrJp1jTK_w1lh0qOwLx3)\n  - [ISyE 320 Simulation and Probabilistic Modeling spring 2022, by Qiaomin Xie - University of Wisconsin-Madison](https://www.youtube.com/playlist?list=PL682UO4IMem-7c512A5mwIn_J90z8Awgd)\n  - [Cambridge Principles of Statistics 2020, by Alberto J. Coca](https://www.youtube.com/playlist?list=PLn1JSlh3WT_YBwXFUfRAb1ZOXnPYDgVNa)\n  - [UC Berkeley STAT 150 Stochastic Processes spring 2021, by Brett Kolesnik](https://www.youtube.com/playlist?list=PL682UO4IMem_H6exlt8NMfBSc0137_wlo)\n  - [UIUC Math 564 Applied Stochastic Processes fall 2016, by Kay Kirkpatrick](https://www.youtube.com/playlist?list=PL682UO4IMem-5C_U0ml6O2kM1s1Yo-D9D)\n  - [CS/ECE 561 - Probability and Info Theory in Machine Learning](https://mediaspace.wisc.edu/channel/CS_ECE%2B561%2B-%2BProbability%2Band%2BInfo%2BTheory%2Bin%2BMachine%2BLearning/191748913)\n  - [UCLA Stats 10 Introduction to Statistical Reasoning summer 2022, by Miles Chen](https://www.youtube.com/playlist?list=PLIygTcviGPKCk7fc69zL_cOogOPQBzXhn)\n  - [UCLA Stats 101C Statistical Models and Data Mining summer 2022, by Miles Chen](https://www.youtube.com/playlist?list=PLIygTcviGPKAYdlamMMGkNGJuKFVYvYER)\n  - [UCLA Stats 102A Introduction to Computational Statistics with R winter 2024, by Miles Chen](https://www.youtube.com/playlist?list=PLIygTcviGPKCiE3kiMI7ofuQ3wmXQUcmx)\n  - [UCLA Stats 102B Computation and Optimization for Statistics spring 2024, by Miles Chen](https://www.youtube.com/playlist?list=PLIygTcviGPKD4XftRgjRlTITljOx792YN)\n  - [UCLA Stats 102C Introduction to Monte Carlo Methods fall 2023, by Miles Chen](https://www.youtube.com/playlist?list=PLIygTcviGPKDv0fZ7RxMGPuaa1Yqx_bzh)\n  - [UCLA Stats 200B Theoretical Statistics winter 2024, by Arash Amini](https://www.youtube.com/playlist?list=PLN_qg0-2-0SwfzKyD47bMBmwJnKeezZCW) ([Winter 2023](https://www.youtube.com/playlist?list=PLN_qg0-2-0Sw03Ffmuq8prIVSoTI3yVyR))\n  - [UCLA Stats 200C High-dimensional Statistics spring 2022, by Arash Amini](https://www.youtube.com/playlist?list=PLN_qg0-2-0SzyrvojbW4UZQjVG1CnBFMd) ([Spring 2021](https://www.youtube.com/playlist?list=PLN_qg0-2-0Sy-nbvOCLgt6uIQsOnmG-iV))\n  - [UCLA Stats 203 Large Sample Theory fall 2021, by Jingyi Jessica Li](https://www.youtube.com/playlist?list=PLAYxx7zX5F1P5GG-9U8eJPL_MIsl1_8Zh) ([Fall 2020](https://www.youtube.com/playlist?list=PLAYxx7zX5F1NKukTVwMADi1D5dbufWJkz))\n  - [UCSD Math 280 Probability Theory and Stochastic Processes, by Todd Kemp](https://mathweb.ucsd.edu/~tkemp/ProbabilityTube/) ([YouTube](https://www.youtube.com/@toddkemp-probability/playlists))\n  - [METU EE 531 Probability and Stochastic Processes, by Elif Uysal](https://ocw.metu.edu.tr/course/view.php?id=323)\n  - [6.262 Discrete Stochastic Processes - MIT OCW](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-262-discrete-stochastic-processes-spring-2011/)\n  - [18.650 Statistics for Applications - MIT OCW](https://ocw.mit.edu/courses/mathematics/18-650-statistics-for-applications-fall-2016/lecture-videos/)\n  - [STAT240 - Robust Statistics - UC Berkeley](https://www.stat.berkeley.edu/~jsteinhardt/stat240/index.html)\n- **Linear Algebra**\n  - [Mathematical Foundations of Machine Learning (Fall 2021) - University of Chicago - Rebecca Willett](https://willett.psd.uchicago.edu/teaching/mathematical-foundations-of-machine-learning-fall-2021/)\n  - [18.06 - Linear Algebra, Prof. Gilbert Strang, MIT OCW](https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/)\n  - [18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning - MIT OCW](https://ocw.mit.edu/courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/video_galleries/video-lectures/)\n  - [University of Wisconsin-Madison ECE/CS/ME 532 Matrix Methods in Machine Learning fall 2017, by Laurent Lessard](https://laurentlessard.com/teaching/532-matrix-methods/)\n  - [Linear Algebra (Princeton University)](https://www.youtube.com/playlist?list=PLGqzsq0erqU7w7ZrTZ-pWWk4-AOkiGEGp)\n  - [MOOC: Coding the Matrix: Linear Algebra through Computer Science Applications - Coursera](http://academictorrents.com/details/54cd86f3038dfd446b037891406ba4e0b1200d5a)\n  - [CS 053 - Coding the Matrix - Brown University](http://cs.brown.edu/courses/cs053/current/lectures.htm) ([Fall 14 videos](https://cs.brown.edu/video/channels/coding-matrix-fall-2014/))\n  - [Linear Algebra Review - CMU](http://www.cs.cmu.edu/~zkolter/course/linalg/outline.html)\n  - [A first course in Linear Algebra - N J Wildberger - UNSW](https://www.youtube.com/playlist?list=PL44B6B54CBF6A72DF)\n  - [INTRODUCTION TO MATRIX ALGEBRA](http://ma.mathforcollege.com/youtube/index.html)\n  - [Computational Linear Algebra - fast.ai](https://www.youtube.com/playlist?list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY) ([Github](https://github.com/fastai/numerical-linear-algebra))\n  - [ENGR108: Introduction to Applied Linear Algebra\u2014Vectors, Matrices, and Least Squares - Stanford University](https://www.youtube.com/playlist?list=PLoROMvodv4rMz-WbFQtNUsUElIh2cPmN9)\n  - [MIT 18.S096 Matrix Calculus For Machine Learning And Beyond](https://www.youtube.com/playlist?list=PLUl4u3cNGP62EaLLH92E_VCN4izBKK6OE)\n  - [Cornell MATH 2940 Linear Algebra for Engineers spring 2009, by Andy Ruina](https://vod.video.cornell.edu/channel/MATH%2B2940%2B-%2BLinear%2BAlgebra%2Bfor%2BEngineers%2BSpring%2B2009/114208531)\n- [10-600 Math Background for ML - CMU](https://www.youtube.com/playlist?list=PL7y-1rk2cCsA339crwXMWUaBRuLBvPBCg)\n- [MIT 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning](https://ocw.mit.edu/courses/mathematics/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/video-lectures/)\n- [Direct Methods for Sparse Linear Systems - Prof Tim Davis - UFL](https://www.youtube.com/playlist?list=PL5EvFKC69QIyRLFuxWRnH6hIw6e1-bBXB)\n- [36-705 - Intermediate Statistics - Larry Wasserman, CMU](http://www.stat.cmu.edu/~larry/=stat705/) ([YouTube](https://www.youtube.com/playlist?list=PLcW8xNfZoh7eI7KSWneVWq-7wr8ffRtHF))\n- [Combinatorics - IISC Bangalore](https://nptel.ac.in/courses/106108051/)\n- [Advanced Engineering Mathematics - Notre Dame](https://www.youtube.com/playlist?list=PLd-PuDzW85Ae4pzlylMLzq_a-RHPx8ryA)\n- [Statistical Computing for Scientists and Engineers - Notre Dame](https://www.youtube.com/playlist?list=PLd-PuDzW85AeltIRcjDY7Z4q49NEAuMcA)\n- [Statistical Computing, Fall 2017 - Notre Dame](https://www.youtube.com/playlist?list=PLd-PuDzW85AcSgNGnT5TUHt85SrCljT3V)\n- [Statistics 243 Introduction to Statistical Computing, Fall 2015 - UC Berkeley](https://www.youtube.com/playlist?list=PLIygTcviGPKCJjCMep25BtbRwehuNTd-l) ([Notes-2015](https://github.com/berkeley-stat243/stat243-fall-2015)) ([YouTube-2014](https://www.youtube.com/playlist?list=PLIygTcviGPKDwAtmOYy_dY_tPuE_8Gc68)) ([Notes-2014](https://github.com/berkeley-stat243/stat243-fall-2014)) ([YouTube-2013](https://www.youtube.com/playlist?list=PLIygTcviGPKBj_b9jy_ZkQM3v4r9iLocO))\n- [Mathematics for Machine Learning, Lectures by Ulrike von Luxburg - T\u00fcbingen Machine Learning](https://www.youtube.com/playlist?list=PL05umP7R6ij1a6KdEy8PVE9zoCv6SlHRS)\n- [Essential Mathematics for Machine Learning- July 2018 - IIT Roorkee - YouTube Lectures](https://www.youtube.com/playlist?list=PLLy_2iUCG87D1CXFxE-SxCFZUiJzQ3IvE)\n- [Numerics of Machine Learning (Winter 2022/23) - T\u00fcbingen Machine Learning](https://www.youtube.com/playlist?list=PL05umP7R6ij2lwDdj7IkuHoP9vHlEcH0s)\n- [Nonlinear Dynamics and Chaos - Steven Strogatz, Cornell University](https://www.youtube.com/playlist?list=PLbN57C5Zdl6j_qJA-pARJnKsmROzPnO9V)\n- [Nonlinear Dynamics & Chaos - Virginia Tech](https://www.youtube.com/playlist?list=PLUeHTafWecAUqSh3Gy0NNr7H3OsXoC-aK)\n- [An introduction to Optimization on smooth manifolds (with book) - EPFL](https://www.nicolasboumal.net/book/)\n- [Math Modelling](https://www.youtube.com/playlist?list=PLXsDp0z6VWFT5ZM86xh8i1AMFYxnrefLk)\n- [Large-Scale Convex Optimization: Algorithms & Analyses via Monotone Operators by Ernest Ryu and Wotao Yin](https://www.youtube.com/@large-scaleconvexoptimizat2973/videos)\n- [An Overview of Variational Analysis  2021 by Tyrrell Rockafellar](https://www.youtube.com/playlist?list=PLIismQEEd55Fnzllb-HEYK6k8dMmz2ctj)\n- [UW AMATH 584 Applied Linear Algebra & Numerical Analysis by Nathan Kutz](https://faculty.washington.edu/kutz/am584/am584.html)\n- [UW AMATH 584 Applied Linear Algebra & Introductory Numerical Analysis fall 2005, by Loyce Adams](https://www.youtube.com/playlist?list=PL682UO4IMem-NKDk9uLzRaTOnHPpr4knp)\n- [Stanford CME 206 Introduction to Numerical Methods for Engineering spring 2005, by Charbel Farhat](https://www.youtube.com/playlist?list=PL682UO4IMem97Nk-jJCqUiYZmwpCxMuYF)\n- [Stanford CME 200 Linear Algebra with Application to Engineering Computations autumn 2004, by Margot Gerritsen](https://www.youtube.com/playlist?list=PL682UO4IMem-ZiiCDz7C6bVRjxJ5P35zk)\n- [Stanford CME 302 Numerical Linear Algebra autumn 2007, by Gene Golub](https://www.youtube.com/playlist?list=PL682UO4IMem-OlrG8LXfWQJ2kV_4mpAau)\n- [TUe Numerical Linear Algebra 2021, by Martijn Anthonissen](https://www.youtube.com/playlist?list=PLRb3xghOQGNKbUt8zIRpwrQ-SZ6aIZNvt)\n- [Numerical Linear Algebra fall 2018, by Jaegul Choo](https://www.youtube.com/playlist?list=PLep-kTP3NkcMdmrw07VsKFt87FT584Cpd)\n- [MIT 6.S955 Applied Numerical Algorithms fall 2023, by Justin Solomon](https://www.youtube.com/playlist?list=PLQ3UicqQtfNv_Io_NT1b0Nzr9YDqpK3Lb)\n- [UC Berkeley Math 54 Linear Algebra & Differential Equations spring 2022, by Alexander Paulin](https://math.berkeley.edu/~apaulin/54%20(Spring%202022).html) ([Summer 2021, by Peter Koroteev](https://math.berkeley.edu/~pkoroteev/math54.html)) ([Summer 2020, by Luvreet Sangha](https://www.youtube.com/playlist?list=PLShth7hrtLHO2U1XkrI6ZgMyuPHDxRcob)) ([Spring 2018, by Alexander Paulin](https://math.berkeley.edu/~apaulin/54_001(Spring2018).html))\n- [UC Berkeley Math 55 Discrete Mathematics fall 2021, by Nikhil Srivastava](https://www.youtube.com/playlist?list=PLaVBOvvdB5ctaLM6AmkUaODhd4JhyP_zC)\n- [UC Berkeley Math 56 Linear Algebra fall 2023, by Alexander Paulin](https://math.berkeley.edu/~apaulin/56%20(Fall%202023).html)\n- [Fundamental Mathematics for Robotics spring 2020, by Ken Tomiyama](https://www.youtube.com/@citqualityeducation803/videos)\n- [Short Course on Casual Inference, by Sanjay Shakkottai](https://www.youtube.com/playlist?list=PLcip-Gs_jEK_l2pNG8V_0UDK9jyPtLyuq)\n- [UCLA STAT 100C Linear Models spring 2023, by Arash Amini](https://www.youtube.com/playlist?list=PLN_qg0-2-0SzrzpEoojAa4anJdaKa49GM)\n- [MSU Math for Computing](https://www.youtube.com/playlist?list=PLZ-krWGO-UEyLqtyA2pACX_tXXBTLWkI1)\n- [Mathematics of Data Science - ETH Zurich](https://www.youtube.com/playlist?list=PLiud-28tsatIKUitdoH3EEUZL-9i516IL)\n- [Mathematical Data Science 1 - Spring 2021 - FAU](https://www.fau.tv/course/id/2297) ([Spring 2020](https://www.fau.tv/course/id/1234))\n- [Engineering Mathematics (UW ME564 and ME565) - Steve Brunton](https://www.youtube.com/playlist?list=PLMrJAkhIeNNR2W2sPWsYxfrxcASrUt_9j)\n- [Beginning Scientific Computing - Steve Brunton](https://www.youtube.com/playlist?list=PLMrJAkhIeNNRTVrHYDfjNyqzZ6Q6rsTyf)\n- [Jadavpur University: Foundation_Math_forML_Autumn23](https://www.youtube.com/playlist?list=PLcNLn_ApooUyoctc147F-49oHnfvuj3Yt)\n- [FAU: Inverse Problems Autumn21](https://www.fau.tv/course/id/2705)\n\n\n------------------------------\n\n### Web Programming and Internet Technologies\n- [CS50's Web Programming with Python and JavaScript](https://www.edx.org/course/cs50s-web-programming-with-python-and-javascript)\n- [Web Design Decal - HTML/CSS/JavaScript Course, University of California, Berkeley](http://wdd.io/)\n- [CS 75 Building Dynamic Websites - Harvard University](http://cs75.tv/2012/summer/)\n- [Internet Technology - IIT Kharagpur](https://nptel.ac.in/courses/106105084/)\n- [Introduction to Modern Application Development - IIT Madras](https://nptel.ac.in/courses/106106156/)\n- [CSE 199 - How the Internet Works, Fall 2016 - University of Buffalo](https://www.youtube.com/playlist?list=PLk97mPCd8nvbxGGfkYkBXrSEvpTc1xTF8)\n- [Open Sourced Elective: Database and Rails - Intro to Ruby on Rails, University of Texas](https://www.schneems.com/ut-rails) ([Lectures - Youtube](https://www.youtube.com/playlist?list=PL7A85FD7803A8CB1F))\n- [CSE154 - Web Programming, Spring 2020 - University of Washington](https://courses.cs.washington.edu/courses/cse154/20sp/) ([Videos](https://www.youtube.com/playlist?list=PLrE1feouzSWqcgK0Lvt3r_8AwN6UndaOD))\n- [CSEP545 - Transaction Processing for E-Commerce, Winter 2012 - University of Washington](https://courses.cs.washington.edu/courses/csep545/12wi/) ([Videos](https://courses.cs.washington.edu/courses/csep545/12wi/video/))\n- [CT 310 Web Development - Colorado State University](https://www.cs.colostate.edu/~ct310/yr2016sp/home_progress.php)\n- [Internet Technologies and Applications 2012, Steven Gordon - Thammasat University, Thailand](https://www.youtube.com/playlist?list=PLvifRcqOOwF9cfLMTE-42fiBsWvBsOEkS)\n- [CSCI 3110 Advanced Topics in Web Development, Fall 2011 - ETSU iTunes](https://itunes.apple.com/WebObjects/MZStore.woa/wa/viewPodcast?id=454017618)\n- [CSCI 5710 e-Commerce Implementation, Fall 2015 - ETSU iTunes](https://itunes.apple.com/us/itunes-u/e-commerce-implementation/id1020427670)\n- [MOOC - Web Development - Udacity](https://www.youtube.com/playlist?list=PLAwxTw4SYaPlLXUhUNt1wINWrrH9axjcI)\n- [Web Technologies Prof. Dr. Christoph Meinel - HPI](https://open.hpi.de/courses/webtech2015/items/4oqxq6euKfhXgHOMwFBxbn)\n\n------------------------------\n\n### Theoretical CS and Programming Languages\n\n- [MIT 18.404J Theory of Computation - Fall 2020 - Lecture Slides](https://ocw.mit.edu/courses/18-404j-theory-of-computation-fall-2020/pages/lecture-notes)\n- [MIT 18.404J Theory of Computation - Fall 2020 - Video Lectures](https://www.youtube.com/playlist?list=PLUl4u3cNGP60_JNv2MmK3wkOt9syvfQWY)\n- [MOOC - Compilers - Stanford University](https://archive.org/details/academictorrents_e31e54905c7b2669c81fe164de2859be4697013a)\n- [CS 6120: Advanced Compilers: The Self-Guided Online Course - Cornell University](https://www.cs.cornell.edu/courses/cs6120/2020fa/self-guided/)\n- [CS 164 Hack your language, UC Berkeley](https://sites.google.com/a/bodik.org/cs164/home) ([Lectures - Youtube](https://www.youtube.com/playlist?list=PLuhjguFxSeVLvKvWwTUIpVwXdLtZPX1ZS))\n- [Theory of computation - Shai Simonson](http://www.aduni.org/courses/theory/index.php?view=cw)\n- [CS 173 Programming Languages, Brown University](http://cs.brown.edu/courses/cs173/2012/Videos/) ([Book](http://cs.brown.edu/courses/cs173/2012/book/))\n- [CS Theory Toolkit at CMU 2020](https://www.youtube.com/playlist?list=PLm3J0oaFux3ZYpFLwwrlv_EHH9wtH6pnX)\n- [CS 421 - Programming Languages and Compilers, UIUC](https://courses.engr.illinois.edu/cs421/fa2014/)\n- [CSC 253 - CPython internals: A ten-hour codewalk through the Python interpreter source code, University of Rochester](https://www.youtube.com/playlist?list=PLzV58Zm8FuBL6OAv1Yu6AwXZrnsFbbR0S)\n- [CSE341 - Programming Languages, Dan Grossman, Spring 2013 - University of Washington](https://courses.cs.washington.edu/courses/cse341/13sp/)\n- [CSEP 501 - Compiler Construction, University of Washington](https://courses.cs.washington.edu/courses/csep501/09au/lectures/video.html) ([Lectures - Youtube](https://www.youtube.com/playlist?list=PLTPQEx-31JXhfAWGnGzwbfhB2zUB7Jd4C))\n- [CSEP 505 Programming Languages, Winter 2015 - University of Washington](https://courses.cs.washington.edu/courses/csep505/15wi/video/)\n- [DMFP - Discrete Mathematics and Functional Programming, Wheaton College](http://cs.wheaton.edu/~tvandrun/dmfp/)\n- [CS 374 - Algorithms & Models of Computation (Fall 2014), UIUC](https://courses.engr.illinois.edu/cs498374/fa2014/lectures.html) ([Lecture videos](https://www.youtube.com/playlist?list=PL0v718LJg-7_4Zwx3CE7kZ398mhlB2TqF473))\n- [6.045 Automata, Computability, and Complexity, MIT](https://stellar.mit.edu/S/course/6/sp15/6.045/materials.html) ([Lecture Videos](http://stellar.mit.edu/S/course/6/sp15/6.045/special/videos/index.html))\n- [MOOC - Automata - Jeffrey Ullman - Coursera](https://www.youtube.com/playlist?list=PL82C4B8475CAC3F95)\n- [CS581 Theory of Computation - Portland State University](http://web.cecs.pdx.edu/~harry/videos/) ([Lectures - Youtube](https://www.youtube.com/playlist?list=PLbtzT1TYeoMjNOGEiaRmm_vMIwUAidnQz))\n- [Theory of Computation - Fall 2011 UC Davis](https://www.youtube.com/playlist?list=PLslgisHe5tBM8UTCt1f66oMkpmjCblzkt)\n- [TDA555 Introduction to Functional Programming - Chalmers University of Technology](http://www.cse.chalmers.se/edu/course/TDA555/index.html) ([Lectures - YouTube](https://www.youtube.com/playlist?list=PLIQ9jYeUxhgqEnjey91yRTITaXqZQy3Ta))\n- [Ryan O'Donnell Theoretical Computer Science Talks](https://www.youtube.com/playlist?list=PLm3J0oaFux3bLpmu56cDd1PUTp-aJVaTo)\n- [Philip Wadler Haskell lecture recordings](https://www.youtube.com/playlist?list=PLtRG9GLtNcHBv4cuh2w1cz5VsgY6adoc3)\n- [Functional Programming (2021) - University of Nottingham](http://www.cs.nott.ac.uk/~pszgmh/pgp.html)\n- [Functional Programming - University of Edinburgh - 2016-17](http://www.inf.ed.ac.uk/teaching/courses/inf1/fp/)\n- [MOOC - Functional Programming Principles in Scala by Martin Odersky](https://www.youtube.com/user/afigfigueira/playlists?sort=dd&view=50&shelf_id=9)\n- [CS294 - Program Synthesis for Everyone](https://people.eecs.berkeley.edu/~bodik/cs294fa12)\n- [MOOC - Principles of Reactive Programming, Scala - Coursera](https://www.youtube.com/playlist?list=PLMhMDErmC1TdBMxd3KnRfYiBV2ELvLyxN)\n- [Category Theory for Programmers, 2014 - Bartosz Milewski](https://bartoszmilewski.com/2014/10/28/category-theory-for-programmers-the-preface/) ([YouTube](https://www.youtube.com/playlist?list=PLbgaMIhjbmEnaH_LTkxLI7FMa2HsnawM_))\n- Oregon Programming Languages Summer School (Proof theory, type theory, category theory, verification)\n  - [2012 Lectures](https://www.cs.uoregon.edu/research/summerschool/summer12/curriculum.html)\n  - [2013 Lectures](https://www.cs.uoregon.edu/research/summerschool/summer13/curriculum.html)\n  - [2014 Lectures](https://www.cs.uoregon.edu/research/summerschool/summer14/curriculum.html)\n  - [2015 Lectures](https://www.cs.uoregon.edu/research/summerschool/summer15/curriculum.html)\n  - [2016 Lectures](https://www.cs.uoregon.edu/research/summerschool/summer16/curriculum.php)\n  - [ Latest YT playlists](https://www.youtube.com/@OPLSS/playlists)\n- [Inf1 - Computation and Logic 2015 - University of Edinburgh](http://www.inf.ed.ac.uk/teaching/courses/inf1/cl/)\n- [INFORMATICS 1 - FUNCTIONAL PROGRAMMING - University of Edinburgh](http://www.inf.ed.ac.uk/teaching/courses/inf1/fp/) ([Videos](http://groups.inf.ed.ac.uk/vision/VIDEO/2015/inf1fp.htm))\n- [Compiler Design - IISC Bangalore](https://nptel.ac.in/courses/106108052/)\n- [Compiler Design - IIT Kanpur](https://nptel.ac.in/courses/106104123/)\n- [Principles of Programming Languages - IIT Delhi](https://nptel.ac.in/courses/106102067/)\n- [Principles of Compiler Design - IISC Bangalore](https://nptel.ac.in/courses/106108113/)\n- [Functional Programming in Haskell - IIT Madras](https://nptel.ac.in/courses/106106137/)\n- [Theory of Computation - IIT Kanpur](https://nptel.ac.in/courses/106104148/)\n- [Theory of Automata, Formal Languages and Computation - IIT Madras](https://nptel.ac.in/courses/106106049/)\n- [Theory of Computation - IIT Kanpur](https://nptel.ac.in/courses/106104028/)\n- [Logic for CS - IIT Delhi](https://nptel.ac.in/courses/106102013/)\n- [Principles of Compiler Design - Swarthmore College](https://www.cs.swarthmore.edu/~jpolitz/cs75/s16/index.html)\n- [Undergrad Complexity Theory at CMU](https://www.youtube.com/playlist?list=PLm3J0oaFux3YL5vLXpzOyJiLtqLp6dCW2)\n- [Graduate Complexity Theory at CMU](https://www.youtube.com/playlist?list=PLm3J0oaFux3b8Gg1DdaJOzYNsaXYLAOKH)\n- [Great Ideas in Theoretical Computer Science at CMU](https://www.youtube.com/playlist?list=PLm3J0oaFux3aafQm568blS9blxtA_EWQv) \n  - [Another link](https://www.cs251.com/)\n- [Analysis of Boolean Functions at CMU](https://www.youtube.com/playlist?list=PLm3J0oaFux3YypJNaF6sRAf2zC1QzMuTA)\n- [Theoretical Computer Science (Bridging Course)(Tutorial) - SS 2015](http://ais.informatik.uni-freiburg.de/teaching/ss15/bridging/)\n- [Languages & Translators - UCLouvain LINFO2132](https://norswap.com/compilers/)\n- [Compiler Design by Sorav Bansal](https://www.youtube.com/playlist?list=PLf3ZkSCyj1tf3rPAkOKY5hUzDrDoekAc7)\n- [OCaml Programming: Correct + Efficient + Beautiful](https://www.youtube.com/playlist?list=PLre5AT9JnKShBOPeuiD9b-I4XROIJhkIU)\n- [Columbia IEOR E4004 Introduction to Operations Research: Deterministic Models summer 2005, by Jay Sethuraman](https://www.youtube.com/playlist?list=PLIygTcviGPKCVGq8cQ9MCBEm5wVXGsDpw)\n- [Columbia IEOR E4106 Introduction to Operations Research: Stochastic Models spring 2005, by Ward Whitt](https://www.youtube.com/playlist?list=PLIygTcviGPKDJMIOWQcnSSzKDFOG_dF4r)\n- [Columbia ELEN E6711 Stochastic Models in Information Systems fall 2005, by Yuliy Barsyhnikov](https://www.youtube.com/playlist?list=PL682UO4IMem9dvI-8vjxlPkM5onW9WoIp)\n- [Columbia ELEN E6717 Information Theory fall 2003, by Vittorio Castelli](https://www.youtube.com/playlist?list=PL682UO4IMem_IUZIFzlJi8vCTtS-Xi8eY)\n- [University of Washington EE514A/EE515A - Information Theory I/II fall 2013, by Jeff Bilmes](https://www.youtube.com/playlist?list=PLFze15KrfxbGsPDYxeLyeYorlHSBsTPv0)\n- [CMU 15 150 Principles of Functional Programming summer 2023, by Brandon Wu](https://www.youtube.com/playlist?list=PLsydD1kw8jng2t2G8USQNLz0faYZetPnH)\n- [CMU 21 738 Extremal Combinatorics spring 2020, by Po-Shen Loh](https://www.youtube.com/playlist?list=PLgTkKBA6LRqaGKITvQS1QuIBoEbOVwFTm)\n- [JHU Domain-Specific Languages (DSL) Class (Summer 2018)](https://www.youtube.com/playlist?list=PLW-6wqFEcgTqHMXV_8jI43QLkCv8VgqLk)\n- [Spring 2019 - Probabilistically Checkable and Interactive Proof Systems (Alessandro Chiesa)](https://www.youtube.com/playlist?list=PLkFD6_40KJIyWWtxCPBHwGsrutjvwM5_U)\n- [Algebraic Coding Theory - Stanford University](https://www.youtube.com/playlist?list=PLkvhuSoxwjI_UudECvFYArvG0cLbFlzSr)\n- [CS60094 Computational Number Theory](https://www.youtube.com/playlist?list=PLG63srgJBCAiOJ0hGikoiTq0CsUg_OGKu)\n\n------------------------------\n\n### Embedded Systems\n\n- [EE319K Embedded Systems - UT Austin](http://users.ece.utexas.edu/~valvano/Volume1/E-Book/VideoLinks.htm)\n- [EE445L Embedded Systems Design Lab, Fall 2015, UTexas](https://www.youtube.com/playlist?list=PLyg2vmIzGxXGBxFu8nvX3KBadSdsNAvbA)\n- [CS149 Introduction to Embedded Systems - Spring 2011 - UCBerkeley](https://youtube.com/playlist?list=PLu0nzW8Es1x0RIvuWdw1Diez2Clk8xAX5)\n- [CSE/ECE 474 Introduction to Embedded Systems - University of Washington](https://courses.cs.washington.edu/courses/cse474/23sp/) ([Lectures - YouTube-Spring 21](https://www.youtube.com/playlist?list=PLyB1_dMg8sy8W2EMtSUymI3KpAhXBQOlT))\n- [ECE 4760 Designing with Microcontrollers Fall 2016, Cornell University](http://people.ece.cornell.edu/land/courses/ece4760/) ([Lectures - Youtube](https://www.youtube.com/playlist?list=PLKcjQ_UFkrd4I5WOIxHEYZ7iY04lj15Ez))\n- [ECE 5760 - Advanced Microcontroller Design and system-on-chip, Spring 2016 - Cornell University](http://people.ece.cornell.edu/land/courses/ece5760/)\n- [Internet of Things by Prof. Dr.-Ing. Dietmar P. F. M\u00f6ller](https://video.tu-clausthal.de/vorlesung/408.html)\n- [CSE 351 - The Hardware/Software Interface, Spring 16 - University of Washington](https://courses.cs.washington.edu/courses/cse351/16sp/videos.html) ([Coursera](http://academictorrents.com/details/f1384286c8581bffba11e378fdb37608e649d82a))\n- [ECE 5030 - Electronic Bioinstrumentation, Spring 2014 - Cornell University](http://people.ece.cornell.edu/land/courses/ece5030/)\n- [ECE/CS 5780/6780 - Embedded Systems Design, Spring 14 - University of Utah](https://www.youtube.com/playlist?list=PLQefpK95HyFmao3zi-WDOMkeSev-Je5dE)\n- [EECS 373 - Introduction to Embedded System Design - University of Michigan](https://www.eecs.umich.edu/courses/eecs373/lectures.html) ([Lectures - YouTube-Fall 24](https://www.youtube.com/playlist?list=PLDe8F9fr1j_QULxa7xCozweLKYI4cXun4)) ([Lectures - YouTube-Fall 23](https://www.youtube.com/playlist?list=PLDe8F9fr1j_R2ljCeIq2FOLHaeZ6tBV_o))\n- [Embedded Systems Class - Version 1 - 2011 - UNCC](https://www.youtube.com/playlist?list=PLE4462C1C306E2EB2)\n- [Embedded Systems using the Renesas RX63N Processor - Version 3 - UNCC](https://www.youtube.com/playlist?list=PLPIqCiMhcdO5gxLJWt_hY5CPMzqg75IU5)\n- [Software Engineering for Embedded Systems (WS 2011/12) - HPI University of Potsdam](https://www.tele-task.de/series/864/)\n- [Embedded Software Testing - IIT Madras](https://nptel.ac.in/courses/117106112/)\n- [Embedded Systems - IIT Delhi](https://nptel.ac.in/courses/108102045/)\n- [Embedded Systems Design - IIT Kharagpur](https://nptel.ac.in/courses/106105159/)\n- [ARM Based Development - IIT Madras](https://nptel.ac.in/courses/117106111/)\n- [Software Engineering for Self Adaptive Systems - iTunes - HPI University of Potsdam](https://itunes.apple.com/us/itunes-u/software-engineering-for-self/id993578475)\n- [EE260 Embedded Systems by Robert Paz](https://www.youtube.com/playlist?list=PLnvE9iJk1wvib_pdUPdQGYZrkrmg9mf__)\n- [IoT Summer School](https://www.youtube.com/playlist?list=PLHih6DnKQaoYQ5PIT3Tp-UrqUguDYWYQu)\n- [ECSE 421 - Embedded Systems - McGill](http://www.cim.mcgill.ca/~jer/courses/es/)\n- [NOC:Advanced IOT Applications - IISc Bangalore](https://nptel.ac.in/courses/108/108/108108123/)\n- [NOC:Design for internet of things - IISc Bangalore](https://nptel.ac.in/courses/108/108/108108098/)\n\n------------------------------\n\n### Real time system evaluation\n\n- [Performance evaluation of Computer systems - IIT Madras](https://nptel.ac.in/courses/106/106/106106048/)\n- [Real Time systems - IIT Karaghpur](https://nptel.ac.in/courses/106/105/106105036/)\n- [EE 380 Colloquium on Computer Systems - Stanford University](https://www.youtube.com/playlist?list=PLoROMvodv4rMWw6rRoeSpkiseTHzWj6vu)\n- [System storages - IISc Bangalore](https://nptel.ac.in/courses/106/108/106108058/)\n- [High Performance Computing - IISC Bangalore](https://www.youtube.com/playlist?list=PL2F82ECDF8BB71B0C)\n- [2023 High Performance Computing Course Prof Dr - Ing Morris Riedel](https://www.youtube.com/playlist?list=PLmJwSK7qduwUBwrFn3SY8vi4AYa2rVTWH) ([2022](https://www.youtube.com/playlist?list=PLmJwSK7qduwWyqcSEB45HOyxq--z8njix))\n- [High Performance Computing | Udacity](https://www.youtube.com/playlist?list=PLAwxTw4SYaPk8NaXIiFQXWK6VPnrtMRXC)\n- [UCLA Stats 205 Hierarchical Linear Models spring 2024, by Jingyi Jessica Li](https://www.youtube.com/playlist?list=PLAYxx7zX5F1O2HbRr4gORnscbM9EszYbK)\n- [UF EML 6934 Optimal Control spring 2012, by Anil V. Rao](https://www.youtube.com/playlist?list=PLIygTcviGPKBCJ9JyHfBi4ftc9FDCtaBc)\n- [Real-World Algorithms for IoT and Data Science - UIUC](https://rrc-uiuc.notion.site/Real-World-Algorithms-for-IoT-and-Data-Science-74d8f612f74a4c1689760dafa31ef93d)\n\n------------------------------\n\n### Computer Organization and Architecture\n\n- **Computer Organization**\n  - [How Computers Work - Aduni](http://aduni.org/courses/hcw/index.php?view=cw)\n  - [CS 61C - Machine Structures, UC Berkeley Spring 2015](http://www.infocobuild.com/education/audio-video-courses/computer-science/cs61c-spring2015-berkeley.html) \n  - [6.004 - Computation Structures Spring 2013, MIT](https://www.youtube.com/playlist?list=PLrRW1w6CGAcXbMtDFj205vALOGmiRc82-)\n  - [CS/ECE 3810 Computer Organization, Fall 2015, , University of Utah](http://www.cs.utah.edu/~rajeev/cs3810/) ([YouTube](https://www.youtube.com/playlist?list=PLm7BxCUdWqZzjZ-jRe73KUfj2GsSS2FPy))\n  - [Digital Computer Organization - IIT Kharagpur](https://nptel.ac.in/courses/117105078/)\n  - [Computer Organization - IIT Madras](https://nptel.ac.in/courses/106106092/)\n  - [CS-224 - Computer Organization, 2009-2010 Spring, Bilkent University](http://video.bilkent.edu.tr/course_videos.php?courseid=16) ([YouTube playlist](https://www.youtube.com/playlist?list=PLhwVAYxlh5dvB1MkZrcRZy6x_a2yORNAu))\n  - [INFORMATICS 2C - INTRODUCTION TO COMPUTER SYSTEMS (AUTUMN 2016) - University of Edinburgh](http://groups.inf.ed.ac.uk/vision/VIDEO/2014/inf2c-cs.htm)\n- **Computer Architecture**\n  - [18-447 - Introduction to Computer Architecture, CMU](http://www.ece.cmu.edu/~ece447/s15/doku.php?id=schedule) ([Lectures - YouTube - Fall 15](https://youtube.com/playlist?list=PL5PHm2jkkXmi5CxxI7b3JCL1TWybTDtKq))\n  - [CSEP 548 - Computer Architecture Autumn 2012 - University of Washington](https://courses.cs.washington.edu/courses/csep548/12au/video/index.html)\n  - [CS/ECE 6810 Computer Architecture, Spring 2016, University of Utah](http://www.cs.utah.edu/~rajeev/cs6810/) ([YouTube](https://www.youtube.com/playlist?list=PL8EC1756A7B1764F6))\n  - [MOOC - Computer Architecture, David Wentzlaff - Princeton University/Coursera](http://academictorrents.com/details/53bae6d22f3b6e692673f9335e0a0198c1618426)\n  - [Computer Architecture - ETH Z\u00fcrich - Fall 2019](https://safari.ethz.ch/architecture/fall2019/doku.php?id=schedule)\n  - [Digital Circuits and Computer Architecture - ETH Zurich - Spring 2017](https://www.youtube.com/playlist?list=PL5Q2soXY2Zi-IXWTT7xoNYpst5-zdZQ6y)\n  - [Computer Architecture - IIT Delhi](https://nptel.ac.in/courses/106102062/)\n  - [Computer Architecture - IIT Kanpur](https://nptel.ac.in/courses/106104122/)\n  - [Computer Architecture - IIT Madras](https://nptel.ac.in/courses/106106134/)\n  - [High Performance Computer Architecture - IIT Kharagpur](https://nptel.ac.in/courses/106105033/)\n  - [BE5B35APO - Computer Architectures, Spring 2022, CTU - FEE](https://cw.fel.cvut.cz/b212/courses/b35apo/en/lectures/start) ([YouTube - Spring 2022](https://www.youtube.com/playlist?list=PLQL6z4JeTTQnTrML7HgagbjdpCtvdyu0M)) ([RISC-V simulator - QtRvSim](https://comparch.edu.cvut.cz/))\n  - [CS773: Computer Architecture for Performance and Security - IIT Bombay](https://www.youtube.com/playlist?list=PLw6vmiIQrilQ-KhbMAMeh9_eddYjHRjb6)\n  - [COL718 - Architecture of High Performance Computers - IIT Delhi](https://www.cse.iitd.ac.in/~srsarangi/courses/2020/col_718_2020/index.html)\n  - [CS 695: Virtualization and Cloud Computing - IIT Bombay -Spring 2021](https://www.cse.iitb.ac.in/~cs695/)\n- **Parallel Computer Architecture**\n  - [15-418 - Parallel Computer Architecture and Programming, CMU](http://15418.courses.cs.cmu.edu/spring2015/) ([Lecture Videos](https://scs.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx#folderID=%22a5862643-2416-49ef-b46b-13465d1b6df0%22))\n  - [CS 267 Applications of Parallel Computers, Spring 18 - UC Berkeley](https://sites.google.com/lbl.gov/cs267-spr2018/) ([YouTube-Spring 18](https://www.youtube.com/playlist?list=PLkFD6_40KJIx1CL7aIN9BwFL_sttEzfQ7)) ([Notes-Spring 16](https://people.eecs.berkeley.edu/~demmel/cs267_Spr16/)) ([YouTube-Spring 16](https://www.youtube.com/playlist?list=PLkFD6_40KJIzSfxr35ZT59-LZcibZmfp2))\n  - [MOOC - Heterogeneous Parallel Programming - Coursera](http://academictorrents.com/details/8903d0871c652b96c7b29db738cea76902d65888)\n  - [ECE 498AL - Programming Massively Parallel Processors](https://nanohub.org/resources/7225)\n  - [Parallel Computing - IIT Delhi](https://nptel.ac.in/courses/106102114/)\n  - [Parallel Architectures 2012/13- University of Edinburgh](http://groups.inf.ed.ac.uk/vision/VIDEO/2012/pa.htm)\n- **Digital Systems Design**\n  - [ELEC2141 Digital Circuit Design, UNSW](https://www.youtube.com/playlist?list=PLB52B8F4E464CEEF7)\n  - [Digital Systems Design - IIT Kharagpur](https://nptel.ac.in/courses/117105080/)\n  - [Digital Design Course - 2015 - UNCC](https://www.youtube.com/playlist?list=PLPIqCiMhcdO7bBmieyG5u41x2Ogcn67Bs)\n- [CS1 - Higher Computing - Richard Buckland UNSW](https://www.youtube.com/playlist?list=PL6B940F08B9773B9F)\n- [MOOC - From NAND to Tetris - Building a Modern Computer From First Principles](https://www.nand2tetris.org/) ([YouTube](https://www.youtube.com/playlist?list=PLNMIACtpT9BfztU0P92qlw8Gd4vxvvfT1))\n- [System Validation, TU Delft](https://ocw.tudelft.nl/courses/system-validation/)\n- [High Performance Computing - IISC Bangalore](https://nptel.ac.in/courses/106108055/)\n- [Introduction to ARM - Open SecurityTraining](https://www.youtube.com/playlist?list=PLUFkSN0XLZ-n91t_AX5zO007Giz1INwPd)\n- [Intro x86 (32 bit) - Open SecurityTraining](https://www.youtube.com/playlist?list=PL038BE01D3BAEFDB0)\n- [Intermediate x86 (32 bit) - Open SecurityTraining](https://www.youtube.com/playlist?list=PL8F8D45D6C1FFD177)\n- [Design of Digital Circuits - ETH Z\u00fcrich - Spring 2019](https://www.youtube.com/playlist?list=PL5Q2soXY2Zi8J58xLKBNFQFHRO3GrXxA9)\n- [Onur Mutlu @ TU Wien 2019 - Memory Systems](https://www.youtube.com/playlist?list=PL5Q2soXY2Zi_gntM55VoMlKlw7YrXOhbl)\n- [Memory Systems Course - Technion, Summer 2018](https://www.youtube.com/playlist?list=PL5Q2soXY2Zi-IymxXpH_9vlZCOeA7Yfn9)\n- [UC Berkeley EECS16A Designing Information Devices and Systems I summer 2020, by Grace Kuo, Panos Zarkos, Urmita Sikder](https://www.youtube.com/playlist?list=PL682UO4IMem9_svw6nCsGOaDZaYSjpgcB)\n- [UC Berkeley EECS 16B Designing Information Devices and Systems II fall 2020, by Seth Sanders, Miki Lustig](https://www.youtube.com/playlist?list=PL682UO4IMem_QO6og7qXyvjMUHXNI7Hqs)\n- [EE 503 - Statistical Signal Processing and Modeling - Fall 2020 - METU](https://ocw.metu.edu.tr/course/view.php?id=350) ([YouTube](https://www.youtube.com/playlist?list=PLhAAV-jkgAdx8wIshnGR6GZeJSXXC6cxl))\n- [ELEN E4810 - DIGITAL SIGNAL PROCESSING - Fall 2013 - Columbia](https://www.ee.columbia.edu/~dpwe/e4810/outline.html)\n- [ELEN E4896 - MUSIC SIGNAL PROCESSING - Spring 2016 - Columbia](https://www.ee.columbia.edu/~dpwe/e4896/outline.html)\n- [Columbia ELEN E6820 Speech and Audio Processing spring 2006, by Dan Ellis](https://www.youtube.com/playlist?list=PL682UO4IMem8IpOKncIUSEFsYWoJyS5ks)\n- [CMU 11 751 / 18 781 Speech Recognition and Understanding fall 2023, by Shinji Watanabe](https://www.youtube.com/playlist?list=PLfVqr2l0FG-tW8d5ZSz-_tCgQed_F1ndb) ([Fall 2022](https://www.youtube.com/playlist?list=PLfVqr2l0FG-u7chWKPQMDoT0o-I2ejxeK))\n- [CMU 11 492 Speech Processing fall 2021, by Alan W. Black](https://www.youtube.com/playlist?list=PL682UO4IMem9InUp3CZHL4g7IwzfKXe3c)\n\n------------------------------\n\n### Security\n\n- [Internet Security (WT 2018/19) - HPI University of Potsdam](https://www.tele-task.de/series/1227/)\n- [6.1600 Foundations of Computer Security - MIT Fall 2023](https://61600.csail.mit.edu/2023/)\n- [6.858 Computer Systems Security - MIT OCW](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-858-computer-systems-security-fall-2014/video_galleries/video-lectures/)\n- [CS 253 Web Security - Stanford University](https://www.youtube.com/playlist?list=PL1y1iaEtjSYiiSGVlL1cHsXN_kvJOOhu-)\n- [CS 161: Computer Security, UC Berkeley](https://fa23.cs161.org/) ([Videos](https://www.youtube.com/playlist?list=PLfBkt1-_BHX_VxOWlmstguFxojSN742vz)) ([Spring 2025](https://sp25.cs161.org/))\n- [6.875 - Cryptography - Fall 2021 - MIT](https://mit6875.github.io/fall2021.html) ([Spring 2018](https://www.youtube.com/playlist?list=PL6ogFv-ieghe8MOIcpD6UDtdK-UMHG8oH))\n- [CSEP590A - Practical Aspects of Modern Cryptography, Winter 2011 - University of Washington](https://courses.cs.washington.edu/courses/csep590a/11wi/) ([Videos](https://courses.cs.washington.edu/courses/csep590a/11wi/video/))\n- [CS461/ECE422 - Computer Security - University of Illinois at Urbana-Champaign](https://courses.engr.illinois.edu/cs461/sp2016/) ([Videos](https://recordings.engineering.illinois.edu:8443/ess/portal/section/8a560718-345a-417a-b665-6bd375a71ee2))\n- [Introduction to Cryptography, Christof Paar - Ruhr University Bochum, Germany](https://www.youtube.com/playlist?list=PLwJWuZfL_Ga2KJrTf9hOIgAQWkSpn9RNm)\n- [ECS235B Foundations of Computer and Information Security - UC Davis](https://itunes.apple.com/us/itunes-u/computer-science-foundations/id389259109)\n- [CIS 4930/ CIS 5930 - Offensive Computer Security, Florida State University](http://www.cs.fsu.edu/~redwood/OffensiveComputerSecurity/lectures.html)\n- [Introduction to Information Security I - IIT Madras](https://nptel.ac.in/courses/106106129/)\n- [Information Security - II - IIT Madras](https://nptel.ac.in/courses/106106141/)\n- [Introduction to Cryptology - IIT Roorkee](https://nptel.ac.in/courses/106107155/)\n- [Cryptography and Network Security - IIT Kharagpur](https://nptel.ac.in/courses/106105031/)\n- [18-636 Browser Security, Stanford](https://courseware.stanford.edu/pg/courses/334553/18636-spring-2013)\n- [Internet Security - Weaknesses and Targets (WT 2015/16)](https://www.tele-task.de/archive/series/overview/1084/) ([WT 2012/13](https://www.tele-task.de/archive/series/overview/921/) ([YouTube](https://www.youtube.com/playlist?list=PLVWVhkyKd-7taP50fB9PeZ2W_EPTOLD8o)))\n- [IT Security, Steven Gordon - Thammasat University, Thailand](https://www.youtube.com/playlist?list=PLvifRcqOOwF9-XSGfm-3uA9DfF7plasCF)\n- [Security and Cryptography, Steven Gordon - Thammasat University, Thailand](https://www.youtube.com/playlist?list=PLvifRcqOOwF-z2sfMb3w0uQzd7PfaLFlU)\n- [MOOC - Cryptography - Coursera](https://www.youtube.com/playlist?list=PL58C6Q25sEEHXvACYxiav_lC2DqSlC7Og)\n- [MOOC - Intro to Information Security - Udacity](https://www.youtube.com/playlist?list=PLAwxTw4SYaPkG-z00NybuIyDqT4sRh3ak)\n- [ICS 444 - Computer & Network Security](https://www.youtube.com/playlist?list=PLciCszvvRCTV09wIXJwiPmYF7thHiq4En)\n- [Privacy and Security in Online Social Networks - IIT Madras](https://nptel.ac.in/courses/106106146/)\n- [Malware Dynamic Analysis - Open SecurityTraining](http://opensecuritytraining.info/MalwareDynamicAnalysis.html) ([YouTube](https://www.youtube.com/playlist?list=PLUFkSN0XLZ-kqYbGpY4Gt_VATd4ytQg-Z))\n- [CSN09112 - Network Security and Cryptography - Bill Buchanan - Edinburgh Napier](https://asecuritysite.com/csn09112)\n- [CSN10107 - Security Testing and Network Forensics - Bill Buchanan - Edinburgh Napier](https://asecuritysite.com/csn10107)\n- [CSN11123 - Advanced Cloud and Network Forensics - Bill Buchanan - Edinburgh Napier](https://asecuritysite.com/csn11123)\n- [CSN11117 - e-Security - Bill Buchanan - Edinburgh Napier](https://asecuritysite.com/csn11117)\n- [CSN08704 - Telecommunications - Bill Buchanan - Edinburgh Napier](https://asecuritysite.com/csn08704)\n- [CSN11128 - Incident Response and Malware Analysis - Bill Buchanan - Edinburgh Napier](https://asecuritysite.com/CSN11128)\n- [Internet Security for Beginners by Dr. Christoph Meinel - HPI](https://www.youtube.com/playlist?list=PLoOmvuyo5UAdsi-IacgZJQF1MRw0Ee-HH)\n- [Offensive Security and Reverse Engineering, Chaplain University by Ali Hadi](https://www.youtube.com/playlist?list=PLCS2zI95IiNybAAQ0HL88YzwRpLXje5y6)\n- [Computer Systems Security, Fall 2020, Vinod Ganapathy, IISc Bangalore](https://www.csa.iisc.ac.in/~vg/teaching/SecurityLectures/)\n- [UC Berkeley CS 161 Computer Security, Summer 2021, by Nicholas Ngai and Peyrin Kao](https://www.youtube.com/playlist?list=PLIygTcviGPKAPoe_0mUsXYaxZfsFyQyFE)\n- [UCSD CS291A Differential Privacy Fall 2021, by Yu-Xiang Wang](https://cseweb.ucsd.edu/~yuxiangw/classes/DPCourse-2021Fall/) ([Youtube](https://www.youtube.com/playlist?list=PLTN4aNO9NiB7UiBbSFznHRSkdieBc0-sk))\n- [Zero Knowledge Proofs MOOC, UC Berkeley RDI Center on Decentralization & AI](https://www.youtube.com/playlist?list=PLS01nW3Rtgor_yJmQsGBZAg5XM4TSGpPs)\n- [Multimedia Security, Fall 2017, FAU](https://www.fau.tv/course/id/786)\n\n------------------------------\n\n### Computer Graphics\n\n- [ECS 175 - Computer Graphics, Fall 2009 - UC Davis](https://itunes.apple.com/us/itunes-u/computer-graphics-fall-2009/id457893733?mt=10)\n- [6.837 - Computer Graphics - Spring 2017 - MIT](https://www.youtube.com/playlist?list=PLkHIj5SCfn3_PCotoqTetMpJc_jkpkgLt)\n- [6.838 - Shape Analysis - Spring 2017- MIT](https://www.youtube.com/playlist?list=PLkHIj5SCfn3-FeWqD3xeOZWP2kQYY654o)\n- [Introduction to Computer Graphics - IIT Delhi](https://nptel.ac.in/courses/106102065/)\n- [Computer Graphics - IIT Madras](https://nptel.ac.in/courses/106106090/)\n- [Computer Graphics 2012, Wolfgang Huerst, Utrecht University](https://www.youtube.com/playlist?list=PLDFA8FCF0017504DE)\n- [CS 5630/6630 - Visualization, Fall 2016, University of Utah](http://dataviscourse.net/2016/index.html) ([Lectures - Youtube](https://www.youtube.com/playlist?list=PLbuogVdPnkCpQY3miQpTJtnHgCLze2lr0))\n- [Advanced Visualization UC Davis](https://www.youtube.com/playlist?list=PLslgisHe5tBNnySlj9TlL-n-4jNEK-xgi)\n- [Computer Graphics Fall 2011, Barbara Hecker](https://www.youtube.com/playlist?list=PL9C949E9F19381E61)\n- [Ray Tracing for Global Illumination, UCDavis](https://www.youtube.com/playlist?list=PLslgisHe5tBPckSYyKoU3jEA4bqiFmNBJ)\n- [Rendering / Ray Tracing Course, SS 2015 - TU Wien](https://www.youtube.com/playlist?list=PLujxSBD-JXgnGmsn7gEyN28P1DnRZG7qi)\nintroduction/id389259246))\n- [Computational Geometry - IIT Delhi](https://nptel.ac.in/courses/106102011/)\n- [CS 468 - Differential Geometry for Computer Science - Stanford University](http://graphics.stanford.edu/courses/cs468-13-spring/schedule.html) ([Lecture videos](https://www.youtube.com/playlist?list=PL_deCdukpyu1rdH85XsEEREbpoqEauiJl))\n- [CMU 15-462/662: Computer Graphics](http://15462.courses.cs.cmu.edu/fall2020/)\n- [UC Berkeley CS184/284A Computer Graphics and Imaging Spring 2022, by Ren Ng](https://cs184.eecs.berkeley.edu/sp22) ([YouTube playlist](https://www.youtube.com/playlist?list=PLIygTcviGPKABEUa7EJzO8tgYRz120EpP))\n- [CS 15-458/858: Discrete Differential Geometry - Carnegie Mellon University - Spring 2021](https://brickisland.net/DDGSpring2021/)\n- [IN2124: Basic Mathematical Tools for Imaging and Visualization - TUM - Winter 2021](https://live.rbg.tum.de/?year=2021&term=W&slug=GMMfIuV&view=3)\n\n------------------------------\n\n### Image Processing and Computer Vision\n\n- [Digital Image Processing - IIT Kharagpur](https://nptel.ac.in/courses/117105079/)\n- [CS 543 - Computer Vision \u2013 Spring 2017](https://courses.engr.illinois.edu/cs543/sp2017/) ([Recordings](https://echo360.org/section/283b0471-3d9f-4efb-9c51-bc00e778735e/home))\n- [CAP 5415 - Computer Vision - University of Central Florida](https://www.crcv.ucf.edu/courses/cap5415-fall-2012/)([Video Lectures](https://www.youtube.com/playlist?list=PLd3hlSJsX_ImKP68wfKZJVIPTd8Ie5u-9))\n- [EE637 - Digital Image Processing I - Purdue University](https://engineering.purdue.edu/~bouman/ece637/) ([Videos - Sp 2011](https://www.youtube.com/user/ModelBasedImaging),[Videos - Sp 2007](https://engineering.purdue.edu/~bouman/ece637/lectures/lectures07/))\n- [Computer Vision I: Variational Methods - TU M\u00fcnchen](https://vision.in.tum.de/teaching/ws2015/vmcv2015) ([YouTube](https://www.youtube.com/playlist?list=PLTBdjV_4f-EJ7A2iIH5L5ztqqrWYjP2RI))\n- [Computer Vision II: Multiple View Geometry (IN2228), SS 2016 - TU M\u00fcnchen](https://vision.in.tum.de/teaching/ss2016/mvg2016) ([YouTube](https://www.youtube.com/playlist?list=PLTBdjV_4f-EJn6udZ34tht9EVIW7lbeo4))\n- [EENG 512/CSCI 512 - Computer Vision - Colorado School of Mines](https://www.youtube.com/playlist?list=PL4B3F8D4A5CAD8DA3)\n- [Computer Vision for Visual Effects - RPI](https://www.ecse.rpi.edu/~rjradke/cvfxcourse.html) ([YouTube](https://www.youtube.com/playlist?list=PLuh62Q4Sv7BUJlKlt84HFqSWfW36MDd5a))\n- [Introduction to Image Processing - RPI](https://www.ecse.rpi.edu/~rjradke/improccourse.html) ([YouTube](https://www.youtube.com/playlist?list=PLuh62Q4Sv7BUf60vkjePfcOQc8sHxmnDX))\n- [CAP 6412 - Advanced Computer Vision - University of Central Florida](https://www.crcv.ucf.edu/cap6412-spring-2013/)([Video lectures](https://www.crcv.ucf.edu/cap6412-spring-2013/)) ([Spring 2018](https://www.youtube.com/playlist?list=PLd3hlSJsX_ImoNaeX5vFrxogGXTSmS993))\n- [Digital Signal Processing - RPI](https://www.ecse.rpi.edu/~rjradke/dspcourse.html)\n- [UC Berkeley EE 123 Digital Signal Processing fall 2003, by Avideh Zakhor](https://www.youtube.com/playlist?list=PLIygTcviGPKAVZtXnfoe689IBTJ2lNMmi)\n- [UC Berkeley EE 225B Digital Image Processing spring 2006, by Avideh Zakhor](https://www.youtube.com/playlist?list=PLIygTcviGPKAa5Vt6T6Noo8I0VaopzXuH)\n- [Advanced Vision 2014 - University of Edinburgh](http://homepages.inf.ed.ac.uk/rbf/AVINVERTED/main_av.htm)\n- [Photogrammetry Course - 2015/16 - University of Bonn, Germany](https://www.youtube.com/playlist?list=PLgnQpQtFTOGRsi5vzy9PiQpNWHjq-bKN1)\n- [MOOC - Introduction to Computer Vision - Udacity](https://www.youtube.com/playlist?list=PLAwxTw4SYaPnbDacyrK_kB_RUkuxQBlCm)\n- [ECSE-4540 - Intro to Digital Image Processing - Spring 2015 - RPI](https://www.youtube.com/playlist?list=PLuh62Q4Sv7BUf60vkjePfcOQc8sHxmnDX)\n- [Machine Learning for Computer Vision - Winter 2017-2018 - UniHeidelberg](https://www.youtube.com/playlist?list=PLuRaSnb3n4kSQFyt8VBldsQ9pO9Xtu8rY)\n- [High-Level Vision - CBCSL OSU](https://www.youtube.com/playlist?list=PLcXJymqaE9POZaT6UFAUUvrQiVQLfzCLN)\n- [Advanced Computer Vision - CBCSL OSU](https://www.youtube.com/playlist?list=PLcXJymqaE9POnU3bVmCVMmtSXzCpcj28T)\n- [Introduction to Image Processing & Computer Vision - CBCSL OSU](https://www.youtube.com/playlist?list=PLcXJymqaE9PMexHWGgXJVINpr6ajy5vuz)\n- [Machine Learning for Computer Vision - TU Munich](https://www.youtube.com/playlist?list=PLTBdjV_4f-EIiongKlS9OKrBEp8QR47Wl)\n- [Biometrics - IIT Kanpur](https://nptel.ac.in/courses/106104119/)\n- [Quantitative Big Imaging 2019 ETH Zurich](https://www.youtube.com/playlist?list=PLTWuXgjdOrnmXVVQG5DRkVeOIGOcTmCIw)\n- [Multiple View Geometry in Computer Vision](https://www.youtube.com/playlist?list=PLyH-5mHPFffFvCCZcbdWXAb_cTy4ZG3Dj)\n- [Modern C++ Course For CV (2020) - University of Bonn](https://www.ipb.uni-bonn.de/teaching/cpp-2020/lectures)\n- [Photogrammetry 1 Course \u2013 2020 - University of Bonn](https://www.ipb.uni-bonn.de/photo1-2020/)\n- [Photogrammetry II Course 2020/21 - University of Bonn](https://www.ipb.uni-bonn.de/photo2-2020/)\n- [3D Computer Vision - National University of Singapore](https://www.youtube.com/playlist?list=PLxg0CGqViygP47ERvqHw_v7FVnUovJeaz)\n- [Diagnostic Medical Image Processing - Fall 2014 - FAU](https://www.fau.tv/course/id/736) ([Fall 2011](https://fau.tv/course/id/123)) ([Fall 2010](https://www.fau.tv/course/id/52)) ([Fall 2009](https://www.fau.tv/course/id/13))\n- [Interventional Medical Image Processing - Spring 2016 - FAU](https://www.fau.tv/course/id/465) ([Spring 2015](https://www.fau.tv/course/id/354)) ([Spring 2012](https://www.fau.tv/course/id/151)) ([Spring 2011](https://www.fau.tv/course/id/105)) ([Spring 2009](https://www.fau.tv/course/id/3))\n------------------------------\n\n### Computational Physics\n\n- [Statistics and Machine Learning for Astronomy](https://www.youtube.com/playlist?list=PLo4wAAMJnA1wDQ2ZmTJCaBYdrXqBWUwT5)\n- [Astronomical data analysis using Python 2021 - NRC IUCAA](https://www.youtube.com/playlist?list=PL3jLiVc5sr3P7Uov0VFsEfwPOEG1rF-FO)\n- [SPARC Workshop on Machine Learning in Solar Physics and Space Weather - CESSI IISER Kolkata](https://www.youtube.com/playlist?list=PLtxxbMktGS8pjURPBXJTAkClnXVE_ZNni)\n- [Data-Driven Methods and Machine Learning in Atmospheric Sciences - IISC](https://www.youtube.com/playlist?list=PLnUDCXHuQXBaGrYSbDMWi2inp7GSe3__8)\n- [Computational Astrophysics - AstroTwinCoLo, 2015](https://www.youtube.com/playlist?list=PLPdkBLbDPtqoHDcjUweIJqe6GKnOz0-Qw)\n- [Astroinformatics 2019 Conference - Caltech](https://m.youtube.com/playlist?list=PL8_xPU5epJdcv2L4MzpzNd6gPyq6glmjc)\n- [Space Science with Python - Astroniz](https://www.youtube.com/playlist?list=PLNvIBWkEdZ2iCc8G9dvx6MQvBruJG-TE8)\n- [Computational Physics Course in Python, Rutgers 2021](https://www.youtube.com/playlist?list=PLXmUYdQdC9IGv61Y1lhGBH0NsDQRdwcJE)\n- [Landau Computational Physics Course](https://www.youtube.com/playlist?list=PLnWQ_pnPVzmJnp794rQXIcwJIjwy7Nb2U)\n- [Statistical Methods and Machine Learning in High Energy Physics](https://www.youtube.com/playlist?list=PL04QVxpjcnjjKDki5FHlKQ8839TGHvj8y)\n- [Physics Informed Machine Learning by Steve Brunton](https://www.youtube.com/playlist?list=PLMrJAkhIeNNQ0BaKuBKY43k4xMo6NSbBa)\n- [Physics-informed machine learning meets engineering seminar series](https://www.youtube.com/playlist?list=PLuD_SqLtxSdW5aasHsFTNw_MrZrtu5tZ3)\n- [Physics Informed Machine Learning Workshop](https://www.youtube.com/playlist?list=PLWL3MaEZQ5I2LYm5BAdLYso5wGe9WMNby)\n- [Jake VanderPlas: Machine learning in Astronomy python tutorial](https://www.youtube.com/playlist?list=PLzWVyeIO6Cmt1cWrzwF2yqW-9j5TgyQaD)\n------------------------------\n\n### Computational Biology\n\n- [ECS 124 - Foundations of Algorithms for Bioinformatics - Dan Gusfield, UC Davis](http://web.cs.ucdavis.edu/~gusfield/cs124videos/videolist.html) ([YouTube](https://www.youtube.com/playlist?list=PL_w_qWAQZtAbh8bHfzXYpdnVKCGCDmmoL))\n- [CSE549 - Computational Biology - Steven Skiena - 2010 SBU](https://www.youtube.com/playlist?list=PLA48145CC64FE7990)\n- [7.32 Systems Biology, Fall 2014 - MIT OCW](https://ocw.mit.edu/courses/physics/8-591j-systems-biology-fall-2014/)\n- [6.802J/ 6.874J Foundations of Computational and Systems Biology - MIT OCW](https://ocw.mit.edu/courses/7-91j-foundations-of-computational-and-systems-biology-spring-2014/video_galleries/video-lectures/)\n- [6.S897 Machine Learning For Healthcare](https://ocw.mit.edu/courses/6-s897-machine-learning-for-healthcare-spring-2019/video_galleries/lecture-videos/)\n- [6.047/6.878 Machine Learning for Genomics Fall 2020 - MIT](https://www.youtube.com/playlist?list=PLypiXJdtIca6dEYlNoZJwBaz__CdsaoKJ)\n- [6.874 MIT Deep Learning in Life Sciences - Spring 2021 - MIT](https://www.youtube.com/playlist?list=PLypiXJdtIca5sxV7aE3-PS9fYX3vUdIOX)\n- [6.047/6.878 Public Lectures on Computational Biology: Genomes, Networks, Evolution - MIT](http://compbio.mit.edu/lectures.html)\n- [Bio 84 - Your Genes and Your Health, Stanford University](https://cmgm.stanford.edu/bio84/)\n- [BioMedical Informatics 231 Computational Molecular Biology, Stanford University](https://cmgm.stanford.edu/biochem218/)\n- [BioMedical Informatics 258 Genomics, Bioinformatics & Medicine, Stanford University](http://biochem158.stanford.edu/)\n- [03-251: Introduction to Computational Molecular Biology - Carnegie Mellon University](https://www.youtube.com/playlist?list=PLUKmtlUTHfBPoI70nVz3C-82N4nznc3Iz)\n- [03-712: Biological Modeling and Simulation - Carnegie Mellon University](https://www.youtube.com/playlist?list=PLUKmtlUTHfBOgpZFmTvsTwkUh2K1RAse-)\n- [MOOC - Bioinformatics Algorithms: An Active Learning Approach - UC San Diego/Coursera](http://bioinformaticsalgorithms.com/videos.htm)\n- [Neural Networks and Biological Modeling - Lecturer: Prof. Wulfram Gerstner - EPFL](http://www.klewel.com/conferences/epfl-neural-networks/)\n- [Video Lectures of Wulfram Gerstner: Computational Neuroscience - EPFL](http://lcn.epfl.ch/~gerstner/VideoLecturesGerstner.html)\n- [An Introduction To Systems Biology](http://www.weizmann.ac.il/mcb/UriAlon/introduction-systems-biology-design-principles-biological-circuits)\n- [Introduction to Bioinformatics, METUOpenCourseWare](https://www.youtube.com/playlist?list=PLuiPz6iU5SQ-PAjlyz4b3EIIQ6dpZ2DE_)\n- [MOOC - Algorithms for DNA Sequencing, Coursera](https://www.youtube.com/playlist?list=PL2mpR0RYFQsBiCWVJSvVAO3OJ2t7DzoHA)\n- [Frontiers of Biomedical Engineering with W. Mark Saltzman - Yale](https://www.youtube.com/playlist?list=PL27E877E8206F196B)\n- [NOC:Computational Systems Biology - IIT Madras](https://nptel.ac.in/courses/102/106/102106068/)\n- [NOC:BioInformatics:Algorithms and Applications - IIT Madras](https://nptel.ac.in/courses/102/106/102106065/)\n- [Data Science and AI for Neuroscience Summer School - Caltech Neuroscience](https://www.youtube.com/playlist?list=PLlPxFwLgTtnfDtq_AO3dd62s33RHtU4bp)\n- [Theoretical and Computational Neuroscience Summer School - 2024 - CNeuro](https://www.youtube.com/playlist?list=PLrAPN1qtMsr9XM_2MqXx2brXBpNit3whQ)\n- [Neuroscience 299: Computing with High-Dimensional Vectors - Fall 2021 - UC Berkeley](https://redwood.berkeley.edu/courses/computing-with-high-dimensional-vectors/)\n- [BIO410/510 Bioinformatics - California State University, Monterey Bay](https://www.youtube.com/playlist?list=PL17NIL2mko8mOPN9W0e4LOjJ2Dkome7ZH)\n- [BIO412: Comparative Genomics - California State University, Monterey Bay](https://www.youtube.com/playlist?list=PL17NIL2mko8nJ1RvfIwtaFnukoWICqS5B)\n- [CENG 465 - Introduction to Bioinformatics (Spring 2020-2021)](https://www.youtube.com/playlist?list=PL0X39D1PSBWPfCPbHb8GC7hoev8SG4Aye)\n- [UCLA Stats M254 Statistical Methods in Computational Biology spring 2024, by Jingyi Jessica Li](https://www.youtube.com/playlist?list=PLAYxx7zX5F1PieIIeSFc7asuKWRYm6nGy)\n- [Cell and Molecular Biology for Engineers  ETH Zurich](https://video.ethz.ch/lectures/d-itet/2011/autumn/227-0945-00L/8e6b12f7-d8cc-4f78-83b1-352876a266d0.html)\n- [Statistical Models in Computational Biology](https://video.ethz.ch/lectures/d-bsse/2018/spring/636-0702-00L/21c63925-cd10-4d56-bac1-b412ed6fe385.html)\n- [ETH Z\u00fcrich Statistical Models in Computational Biology spring 2018, by Niko Beerenwinkel](https://video.ethz.ch/lectures/d-bsse/2018/spring/636-0702-00L/21c63925-cd10-4d56-bac1-b412ed6fe385.html)\n- [UC Berkeley CS 198-96 Introduction to Neurotechnology fall 2020](https://github.com/neurotech-berkeley/neurotech-course)\n- [MLCB24 - Machine Learning in Computational Biology Fall 2024](https://www.youtube.com/playlist?list=PLypiXJdtIca4gtioEPLIExlAKvu64z7rc)\n- [Introduction to Neural Computation - MIT OCW](https://ocw.mit.edu/courses/brain-and-cognitive-sciences/9-40-introduction-to-neural-computation-spring-2018/)\n- [Data Science for Biologists - Steve Brunton](https://www.youtube.com/playlist?list=PLMrJAkhIeNNQz4BMoGSsN8cbt8pHlokhV)\n- [Big Data and Biological Networks IIT Madras](https://www.youtube.com/playlist?list=PLZ2ps__7DhBapdXdBZInSjonWNAUL6Ycm)\n\n\n------------------------------\n\n### Quantum Computing\n\n- [15-859BB: Quantum Computation and Quantum Information 2018 - CMU](https://www.cs.cmu.edu/~odonnell/quantum18/) ([Youtube](https://www.youtube.com/playlist?list=PLm3J0oaFux3YL5qLskC6xQ24JpMwOAeJz))\n- [Quantum Computation and Information at CMU](https://www.youtube.com/playlist?list=PLm3J0oaFux3YL5qLskC6xQ24JpMwOAeJz)\n- [Ph/CS 219A Quantum Computation - Prof Preskill - Caltech](https://www.youtube.com/playlist?list=PL0ojjrEqIyPy-1RRD8cTD_lF1hflo89Iu)\n- [Quantum Mechanics and Quantum Computation - Umesh Vazirani](https://www.youtube.com/playlist?list=PL74Rel4IAsETUwZS_Se_P-fSEyEVQwni7)\n- [Introduction to quantum computing course 2022 - NYU](https://www.youtube.com/playlist?list=PLo0Vs5tDeRLRIPcJ83SN91M-asGuaa1AD)\n- [Phys 1470 - Foundations of Quantum Computing and Quantum Information - U of Pittsburgh](https://www.youtube.com/playlist?list=PL9KDUYiMK3D5etNeur9HocwcAcfcDtArw)\n- [Introduction to Quantum Computing From a Layperson to a Programmer in 30 Steps (EE225 SJSU)](https://www.youtube.com/playlist?list=PLnK6MrIqGXsJfcBdppW3CKJ858zR8P4eP)\n- [Quantum Computing Hardware and Architecture (EE274 SJSU)](https://www.youtube.com/playlist?list=PLnK6MrIqGXsL1KShnocSdwNSiKnBodpie)\n- [Quantum Physics for Non-Physicists 2021 - ETH Zurich](https://www.youtube.com/playlist?list=PLmE1-ewBrbkiOFq_vMXAww4GDMxDAB3pI) ([2020](https://www.youtube.com/playlist?list=PLmE1-ewBrbkiKoYQU4FawveQfhWU_4MkX))\n- [Introduction to Quantum Computing and Quantum Hardware - Qiskit](https://www.youtube.com/playlist?list=PLOFEBzvs-VvrXTMy5Y2IqmSaUjfnhvBHR)\n- [Understanding Quantum Information and Computation - Qiskit](https://www.youtube.com/playlist?list=PLOFEBzvs-VvqKKMXX4vbi4EB1uaErFMSO)\n- [Lectures in Quantum Computation and Quantum Information (IIT Madras)](https://www.youtube.com/playlist?list=PLqLyTdPNhQZwfLoL4QMeI6scnyH1c__tE)\n- [Quantum Information and Computing by Prof. D.K. Ghosh](https://www.youtube.com/playlist?list=PLq-Gm0yRYwThGmlypvSFQ-kT2rPaXKAZ5)\n- [Quantum Computing by Prof. Debabrata Goswami](https://www.youtube.com/playlist?list=PLq-Gm0yRYwTj7Fs6jyzYa83HErSrpXgPQ)\n- [The Building Blocks of a Quantum Computer: Part 1 - TU Delft](https://ocw.tudelft.nl/courses/building-blocks-quantum-computer-part-1/)\n- [The Building Blocks of a Quantum Computer: Part 2 - TU Delft](https://ocw.tudelft.nl/courses/building-blocks-quantum-computer-part-2/)\n- [Quantum Cryptography - TU Delft](https://ocw.tudelft.nl/courses/quantum-cryptography/)\n- [Introduction to Quantum Information](https://zhenyucai.com/post/intro_to_qi/)\n- [Quantum Computing for Everyone -- Part 1](https://www.youtube.com/playlist?list=PLfOgkuiMs5qApXtgIMREPicgbYIGjbf8e) ([Part 2](https://www.youtube.com/playlist?list=PLfOgkuiMs5qB-0J07mphWZ19j9Gll33ZU))\n- [Quantum Computer Systems \u2013 UChicago](https://www.youtube.com/playlist?list=PLfOgkuiMs5qCa8BUrFMumyvPqeoOL-iu8)\n- [Quantum computing for the determined - Michael Nielsen](https://www.youtube.com/playlist?list=PL1826E60FD05B44E4)\n- [Quantum Computing](https://www.youtube.com/playlist?list=PLxP0p--aBHmIe--9rczWe4AZmw03e2bz0)\n- [Advanced Topics in Quantum Info Th.](https://www.youtube.com/playlist?list=PLmE1-ewBrbkicLl3pp14OmxRWSfdNYfmh)\n- [Theory of Quantum Communication - University of Waterloo - Fall 2020](https://www.math.uwaterloo.ca/~wcleung/co781-f2020.html)\n- [Intro to Quantum Computing - Nathan Wiebe](https://www.youtube.com/channel/UCYpyuTbBhQ2FvTmVLL6b3zA/videos)\n- [COMS 4281 - Introduction to Quantum Computing -Columbia University](http://www.henryyuen.net/classes/spring2021/)\n- [Introduction to Quantum Information Science](https://www.youtube.com/playlist?list=PLkespgaZN4gmu0nWNmfMflVRqw0VPkCGH) ([Online book](https://qubit.guide/using-the-e-book), [PDF](https://qubit.guide/qubit_guide.pdf))\n- [A practical introduction to Quantum Computing: from Qubits to Quantum Machine Learning: CERN](https://www.youtube.com/playlist?list=PLu0nzW8Es1x3gEeqUm_32QbhmGHHUXSVo)\n- [PSI 2018/2019 - Quantum Information Review (Gottesman)](https://pirsa.org/C19010)\n------------------------------\n\n### Robotics and Control\n\n- [ROB 101: Computational Linear Algebra - University of Michigan](https://github.com/michiganrobotics/rob101/tree/main/Fall%202021) ([Youtube - Fall 2021](https://www.youtube.com/playlist?list=PLdPQZLMHRjDJ5d_dE4FeOviv0gRe4UYsB))\n- [ROB 102: Introduction to AI and Programming - University of Michigan](https://robotics102.github.io/)\n- [ROB 311: How to Build Robots and Make Them Move - University of Michigan](https://www.youtube.com/playlist?list=PLdPQZLMHRjDIWDJDjiBKjFe7ETD_rssB9)\n- [ROB 320: Robot Operating Systems - University of Michigan](https://autorob.org/)\n- [ROB 501: Mathematics for Robotics - University of Michigan](https://github.com/michiganrobotics/rob501) ([Youtube](https://www.youtube.com/playlist?list=PLdPQZLMHRjDIzO99aE7yAtdOHSVHMXfYH))\n- [ROB 530 MOBILE ROBOTICS at U of Michigan - WINTER 2022 -- Instructor: Maani Ghaffari](https://www.youtube.com/playlist?list=PLdMorpQLjeXmbFaVku4JdjmQByHHqTd1F)\n- [Autorob Winter 2022 - University of Michigan](https://m.youtube.com/playlist?list=PLf_SmXJixhnWMMU6_xYW7iS08-7h9kENY)\n- [DeepRob Winter 2023 - University of Michigan](https://m.youtube.com/playlist?list=PLf_SmXJixhnXoMs0Qvxe500BrjfbIOwSg)\n- [CS 223A - Introduction to Robotics, Stanford University](https://see.stanford.edu/Course/CS223A)\n- [6.832 Underactuated Robotics - MIT OCW](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-832-underactuated-robotics-spring-2009/)\n- [CS287 Advanced Robotics at UC Berkeley Fall 2019 -- Instructor: Pieter Abbeel](https://www.youtube.com/playlist?list=PLwRJQ4m4UJjNBPJdt8WamRAt4XKc639wF)\n- [CS 287 - Advanced Robotics, Fall 2011, UC Berkeley](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa11/) ([Videos](http://rll.berkeley.edu/cs287/lecture_videos/))\n- [CMU 16-715 Robot Dynamics 2022 - CMU](https://www.youtube.com/playlist?list=PLZnJoM76RM6ItAfZIxJYNKdaR_BobleLY)\n- [CMU 16-745 Optimal Control 2024 - CMU](https://www.youtube.com/playlist?list=PLZnJoM76RM6Jv4f7E7RnzW4rijTUTPI4u) ([Lecture notebooks](https://github.com/Optimal-Control-16-745/lecture-notebooks)) ([YouTube-2023](https://www.youtube.com/playlist?list=PLZnJoM76RM6KugDT9sw5zhAmqKnGeoLRa)) ([YouTube-2022](https://www.youtube.com/playlist?list=PLZnJoM76RM6Iaf59ICcU9-DzztGZvK_52))\n- [CS235 - Applied Robot Design for Non-Robot-Designers - Stanford University](https://www.youtube.com/user/StanfordCS235/videos)\n- [Lecture: Visual Navigation for Flying Robots](https://vision.in.tum.de/teaching/ss2012/visnav2012) ([YouTube](https://www.youtube.com/playlist?list=PLTBdjV_4f-EKeki5ps2WHqJqyQvxls4ha))\n- [CS 205A: Mathematical Methods for Robotics, Vision, and Graphics (Fall 2013)](https://www.youtube.com/playlist?list=PLQ3UicqQtfNvQ_VzflHYKhAqZiTxOkSwi)\n- [Robotics 1, Prof. De Luca, Universit\u00e0 di Roma](http://www.dis.uniroma1.it/~deluca/rob1_en/material_rob1_en_2014-15.html) ([YouTube](https://www.youtube.com/playlist?list=PLAQopGWlIcyaqDBW1zSKx7lHfVcOmWSWt))\n- [Robotics 2, Prof. De Luca, Universit\u00e0 di Roma](http://www.diag.uniroma1.it/~deluca/rob2_en/material_rob2_en.html) ([YouTube](https://www.youtube.com/playlist?list=PLAQopGWlIcya6LnIF83QlJTqvpYmJXnDm))\n- [Robot Mechanics and Control, SNU](https://www.youtube.com/playlist?list=PLkjy3Accn-E7mlbuSF4aajcMMckG4wLvW)\n- [Introduction to Robotics Course - UNCC](https://www.youtube.com/playlist?list=PL4847E1D1C121292F)\n- [SLAM Lectures](https://www.youtube.com/playlist?list=PLpUPoM7Rgzi_7YWn14Va2FODh7LzADBSm)\n- [CSE 478 \u2013 Autonomous Robotics \u2013 Winter 2025 - University of Washington](https://courses.cs.washington.edu/courses/cse478/25wi/schedule/) ([Winter 2024](https://courses.cs.washington.edu/courses/cse478/24wi/schedule/))\n- [CSE 571 \u2013 AI-Robotics \u2013 Spring 2023 - University of Washington](https://courses.cs.washington.edu/courses/cse571/23sp/)\n- [EE 259 \u2013 Principles of Sensing for Autonomy \u2013 Spring 2023 - Stanford University](https://www.youtube.com/playlist?list=PLoROMvodv4rOhE007XQu707Dy52qXiZGV)\n- [ME 597 \u2013 Autonomous Mobile Robotics \u2013 Fall 2014](http://wavelab.uwaterloo.ca/index6ea9.html?page_id=267)\n- [ME 780 \u2013 Perception For Autonomous Driving \u2013 Spring 2017](http://wavelab.uwaterloo.ca/indexaef8.html?page_id=481)\n- [ME780 \u2013 Nonlinear State Estimation for Robotics and Computer Vision \u2013 Spring 2017](http://wavelab.uwaterloo.ca/indexe9a5.html?page_id=533)\n- [METR 4202/7202 -- Robotics & Automation - University of Queensland](http://robotics.itee.uq.edu.au/~metr4202/lectures.html)\n- [UIUC CS-588 Autonomous Vehicle System Engineering Outline](http://luthuli.cs.uiuc.edu/~daf/courses/MAAV-21/588-2021-records.html)\n- [Robotics - IIT Bombay](https://nptel.ac.in/courses/112101099/)\n- [Introduction to Machine Vision](https://www.youtube.com/playlist?list=PL1pxneANaikCO1-Z0XTaljLR3SE8tgRXY)\n- [6.834J Cognitive Robotics - MIT OCW](https://ocw.mit.edu/courses/aeronautics-and-astronautics/16-412j-cognitive-robotics-spring-2016/)\n- [Hello (Real) World with ROS \u2013 Robot Operating System - TU Delft](https://ocw.tudelft.nl/courses/hello-real-world-ros-robot-operating-system/)\n- [Programming for Robotics (ROS) - ETH Zurich](https://www.youtube.com/playlist?list=PLE-BQwvVGf8HOvwXPgtDfWoxd4Cc6ghiP)\n- [Mechatronic System Design - TU Delft](https://ocw.tudelft.nl/courses/mechatronic-system-design/)\n- [CS 206 Evolutionary Robotics Course Spring 2020](https://www.youtube.com/playlist?list=PLAuiGdPEdw0inlKisMbjDypCbvcb_GBN9)\n- [Foundations of Robotics - UTEC 2018-I](https://www.youtube.com/playlist?list=PLoWGuY2dW-Acmc8V5NYSAXBxADMm1rE4p)\n- [Robotics and Control: Theory and Practice IIT Roorkee](https://www.youtube.com/playlist?list=PLLy_2iUCG87AjAXKbNMiKJZ2T9vvGpMB0)\n- [Mechatronics](https://www.youtube.com/playlist?list=PLtuwVtW88fOeTFS_szBWif0Mcc0lfNWaz)\n- [ME142 - Mechatronics Spring 2020 - UC Merced](https://www.youtube.com/playlist?list=PL-euleXgwWUNQ80DGq6qopHBmHcQyEyNQ)\n- [Mobile Sensing and Robotics - Bonn University](https://www.youtube.com/playlist?list=PLgnQpQtFTOGQJXx-x0t23RmRbjp_yMb4v)\n- [MSR2 - Sensors and State Estimation Course (2020) - Bonn University](https://www.youtube.com/playlist?list=PLgnQpQtFTOGQh_J16IMwDlji18SWQ2PZ6)\n- [SLAM Course (2013) - Bonn University](https://www.youtube.com/playlist?list=PLgnQpQtFTOGQrZ4O5QzbIHgl3b1JHimN_)\n- [ENGR486 Robot Modeling and Control (2014W)](https://www.youtube.com/playlist?list=PLJzZfbLAMTelwaLxFXteeblbY2ytU2AxX)\n- [Robotics by Prof. D K Pratihar - IIT Kharagpur](https://www.youtube.com/playlist?list=PLbRMhDVUMngcdUbBySzyzcPiFTYWr4rV_)\n- [Introduction to Mobile Robotics - SS 2019 - Universit\u00e4t Freiburg](http://ais.informatik.uni-freiburg.de/teaching/ss19/robotics/)\n- [Robot Mapping - WS 2018/19 - Universit\u00e4t Freiburg](http://ais.informatik.uni-freiburg.de/teaching/ws18/mapping/)\n- [Mechanism and Robot Kinematics - IIT Kharagpur](https://nptel.ac.in/courses/112/105/112105236/)\n- [Self-Driving Cars - Cyrill Stachniss - Winter 2020/21 - University of Bonn)](https://www.youtube.com/playlist?list=PLgnQpQtFTOGQo2Z_ogbonywTg8jxCI9pD)\n- [Aerial Robotics - University of Pennsylvania (UPenn)](https://www.youtube.com/playlist?list=PLblGgzWkqSqM7IWsgjDetdzZDS1NbkTnd)\n- [Modern Robotics - Northwestern University](https://www.youtube.com/playlist?list=PLggLP4f-rq02vX0OQQ5vrCxbJrzamYDfx)\n- [MIT 6.4210/6.4212 - Robotic Manipulation - MIT](https://manipulation.csail.mit.edu/Fall2022/index.html) ([Youtube](https://www.youtube.com/playlist?list=PLkx8KyIQkMfUSDs2hvTWzaq-cxGl8Ha69))\n- [Industrial Robotics and Automation - IIT (ISM) Dhanbad](https://www.youtube.com/playlist?list=PLXDsvE7qtfNdt9oYEhJ_LMXDUGu6bH-L6)\n- [MEE5114 Advanced Control for Robotics from Southern University of Science and Technology](https://www.youtube.com/playlist?list=PLYkCanigA7S4x-ExlnFsQN9WrNZREWEZd)\n- [Self-Driving Cars \u2014 Andreas Geiger](https://www.youtube.com/playlist?list=PL05umP7R6ij321zzKXK6XCQXAaaYjQbzr)\n- [Signal Processing: An Introduction by Nathan Kutz](https://www.youtube.com/playlist?list=PL6Vi_EcJpt8E96_JTKoOKY3HYWVGjf6b4)\n- [UC Santa Barbara ME 269 Network Systems, Dynamics and Control fall 2021, by Francesco Bullo](https://www.youtube.com/playlist?list=PL7bpQ3f3TaeMsueY06FCmbNIEOOY-Ri2_)\n- [Cornell MAE 4710/5710 Applied Dynamics spring 2020, by Andy Ruina](https://cornell.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx#folderID=%229b14904e-5cd8-4ada-b0ff-ab4901537d09%22) ([Part 2](https://vod.video.cornell.edu/channel/MAE%2B4710_5710%2B%2BApplied%2BDynamis_%2BSpring%2B2020/248138383))\n- [Cornell MAE 4730/5730 Intermediate Dynamics fall 2020, by Andy Ruina](https://vod.video.cornell.edu/channel/MAE%2B4730_5730%2BIntermediate%2BDynamics%2BFall%2B2020%2B%2BRuina/247509122)\n- [CMU 16 299 Introduction to Feedback Control Systems spring 2022, by Chris Atkeson](http://www.cs.cmu.edu/~cga/controls-intro-22/)\n- [University of Wisconsin-Madison ECE 332 Feedback Control Systems fall 2021, by Steven Fredette](https://www.youtube.com/playlist?list=PLIygTcviGPKB7-l9W1KHtBFUMvbskh5iR)\n- [MAE 509 Linear Matrix Inequality Methods in Optimal and Robust Control, by Matthew M. Peet](https://www.youtube.com/playlist?list=PL5ebyVGQORm6n158o-I_liUZ7Q5Od43li)\n- [UIUC CS 588 Autonomous Vehicle System Engineering fall 2021, by David Forsyth](http://luthuli.cs.uiuc.edu/~daf/courses/MAAV-21/588-2021-records.html)\n- [UCCS ECE4590/ECE5590 Model Predictive Control, by M. Scott Trimboli](http://mocha-java.uccs.edu/ECE5590/index.html)\n- [EPFL ME 425 Model Predictive Control fall 2020, by Colin Jones](https://www.youtube.com/playlist?list=PLHmHXT53cpnkpbwLqlKae0iKexM8SXKDM)\n- [Robots That Learn - UC Berkeley CS 294-277](https://www.youtube.com/playlist?list=PLPaC96j0xdLcYLTSoSk9PO1Yg-1udJd-S)\n- [Data-Driven Dynamical Systems with Machine Learning - Steve Brunton](https://www.youtube.com/playlist?list=PLMrJAkhIeNNR6DzT17-MM1GHLkuYVjhyt)\n- [Data-Driven Control with Machine Learning - Steve Brunton](https://www.youtube.com/playlist?list=PLMrJAkhIeNNQkv98vuPjO2X2qJO_UPeWR)\n- [Wheeled Mobile Robots](https://www.youtube.com/playlist?list=PLyqSpQzTE6M9CXsZljkH_lCxRSiaXF566)\n- [Introduction to Mobile Robots and Robot Operating System (ROS), HSE 2021](https://www.youtube.com/playlist?list=PL2PmRem6srUn6jc7Q6ahjL8x2qJg150mZ)\n- [Surgical Robotics Lectures - Carleton University](https://www.youtube.com/playlist?list=PLY6RHB0yqJVasji1rwZAGYirD8zW1ipj-)\n- [Introduction to Mechatronic and Robotics - IIT Bombay](https://www.youtube.com/playlist?list=PL_uaeekrhGzJMZTb5etIdXasVO9-WVPVH)\n- [Robotics Fall 2020 - UIC](https://www.youtube.com/playlist?list=PLc7bpbeTIk741sSBpPzj5hl_L8Wlrn1u8)\n- [Introduction to Robotics @ Princeton](https://www.youtube.com/playlist?list=PLF8B1bJgOQK67xkgYz_Xtx0ShjcqfdXwE)\n- [Evolutionary robotics course. Spring 2025](https://www.youtube.com/@joshbongard3314/playlists)\n- [Robotics: Basics and Selected Advanced - IISC Bangalore](https://www.youtube.com/playlist?list=PLgMDNELGJ1CZT9pdEEkDylXFPLdcqxn0t)\n- [Nonlinear Control Design Jan 2024 - IIT Bombay](https://github.com/Developer-Y/cs-video-courses?tab=readme-ov-file)\n\n\n------------------------------\n\n### Computational Finance\n\n- [COMP510 - Computational Finance - Steven Skiena - 2007 HKUST](https://www.youtube.com/playlist?list=PL9E205B8FAAD530E1)\n- [Computational Finance Course - Prof Grzelak](https://www.youtube.com/playlist?list=PL6zzGYGhbWrPaI-op1UfNl0uDglxdkaOB)\n- [Financial Engineering Course: Interest Rates and xVA - Prof Grzelak](https://www.youtube.com/playlist?list=PL6zzGYGhbWrMpjEKDtnrHWyIj-oVLKCYD)\n- [MOOC - Mathematical Methods for Quantitative Finance, University of Washington/Coursera)](http://academictorrents.com/details/dfc1ddde962101f00ef9764b91181bd6bb5c9e93)\n- [18.S096 Topics in Mathematics with Applications in Finance, MIT OCW](https://ocw.mit.edu/courses/18-s096-topics-in-mathematics-with-applications-in-finance-fall-2013/video_galleries/video-lectures/)\n- [Computational Finance - Universit\u00e4t Leipzig](https://www.youtube.com/playlist?list=PL4i4aZbplv9J5jan6mDbDHtjfm7ghMWfj)\n- [Machine Learning for Trading | Udacity](https://www.youtube.com/playlist?list=PLAwxTw4SYaPnIRwl6rad_mYwEk4Gmj7Mx)\n- [ACT 460 / STA 2502 \u2013 Stochastic Methods for Actuarial Science - University of Toronto](http://www.utstat.utoronto.ca/sjaimung/courses/sta2502/main.htm)\n- [MMF1928H / STA 2503F \u2013\nPricing Theory I / Applied Probability for Mathematical Finance - University of Toronto](http://www.utstat.toronto.edu/sjaimung/courses/mmf1928/content2013.htm)\n- [STA 4505H \u2013 High Frequency & Algorithmic trading - University of Toronto](http://www.utstat.utoronto.ca/sjaimung/courses/sta4505/main-2014.htm)\n- [Mathematical Finance - IIT Guwahati](https://nptel.ac.in/courses/111/103/111103126/)\n- [Quantitative Finance - IIT Kanpur](https://nptel.ac.in/courses/110/104/110104066/)\n- [Financial Derivatives & Risk Management - IIT Roorkee](https://nptel.ac.in/courses/110/107/110107128/)\n- [Financial Mathematics - IIT Roorkee](https://nptel.ac.in/courses/112/107/112107260/)\n- [Harvard Economics 2355 Deep Learning for Economics spring 2023, by Melissa Dell](https://www.youtube.com/playlist?list=PLGTgQIsun7ueGTRzDlBWqgX6xBlMce-QC)\n- [UW ECON 484 Econometrics and Data Science spring 2020, by Gregory Duncan](https://www.youtube.com/playlist?list=PLrE1feouzSWobe0NxC_v3cw_OyZL5i-IV)\n- [MATH69122 Stochastic Control for Finance](https://appliedprobability.blog/category/math69122-stochastic-control-for-finance/page/2/)\n- [UC Davis MAT 133 Mathematical Finance Spring 2024, by Matthias K\u00f6ppe](https://video.ucdavis.edu/channel/MAT+133+%28Spring+2024%29/340365092) ([Spring 2021](https://video.ucdavis.edu/channel/channelid/231002963))\n\n------------------------------\n\n### Network Science\n\n- [Network Science, 2021 - HSE](https://www.youtube.com/playlist?list=PLriUvS7IljvkGesFRuYjqRz4lKgodJgh2)\n- [Network Science TU Graz](https://courses.isds.tugraz.at/dhelic/netsci/index.html)\n- [MATH/COMP 479 Network Science Macalester College](https://www.youtube.com/playlist?list=PLlWULwPzrppXKYVyRxFt0YsgIDXiBDPNR)\n- [ACM Winter School on Network Science _Dec 2023, Ahmedabad University](https://www.youtube.com/playlist?list=PLPeEbErKGwN2qqFieu1z7KZ1SXxrmyCKx)\n- [Network Science 2021/2022 (ENS Lyon)](https://www.youtube.com/playlist?list=PL6e66HEqjx1a1XHy4IIQHwPSRLU0bRoK1)\n\n------------------------------\n\n### Blockchain Development\n\n- **Blockchain and Cryptocurrencies**\n  - [Blockchain, Solidity, and Full Stack Web3 Development with JavaScript](https://youtu.be/gyMwXuJrbJQ) \n  - [Blockchain Fundamentals Decal 2018 - Berkeley DeCal](https://www.youtube.com/playlist?list=PLSONl1AVlZNU0QTGpbgEQXKHcmgYz-ddT)\n  - [Blockchain for Developers Decal - Spring 2018 - Berkeley DeCal](https://www.youtube.com/playlist?list=PLSONl1AVlZNUzp71_H1kb87PvIh8kIZU9)\n  - [Cryptocurrency Engineering and Design - Spring 2018 - MIT](https://ocw.mit.edu/courses/mas-s62-cryptocurrency-engineering-and-design-spring-2018/video_galleries/lecture-videos/)\n  - [15.S12 Blockchain and Money, Fall 2018 - MIT](https://www.youtube.com/playlist?list=PLUl4u3cNGP63UUkfL0onkxF6MYgVa04Fn)\n  - [Blockchain - Foundations and Use Cases](https://www.coursera.org/learn/blockchain-foundations-and-use-cases/home/welcome)\n- **Become Blockchain Developer**\n  - [Solidity for Beginners - Dapp University](https://www.youtube.com/playlist?list=PLS5SEs8ZftgUq-aMMYeKf8nPqHrNqa3Iu) \n  - [Master Solidity - Dapp University](https://www.youtube.com/playlist?list=PLS5SEs8ZftgVnWHv2_mkvJjn5HBOkde3g)\n  - [IPFS Inter Planetary File System  Dapp University](https://www.youtube.com/playlist?list=PLS5SEs8ZftgWggD3tKfgwsIPXuIhorXZk)\n  - [Solidity, Blockchain, and Smart Contract Course \u2013 Beginner to Expert Python Tutorial - FreeCodingCamp](https://www.youtube.com/watch?v=M576WGiDBdQ)\n  - [Web 3.0 - Build Realtime Decentralized applications](https://www.youtube.com/playlist?list=PLS5SEs8ZftgVV6ah1fo2IvlHk1kTCy6un)\n\n------------------------------\n\n### Misc\n\n- **HCI**\n  - [CS147 - Introduction to Human-Computer Interaction Design - Stanford](https://hci.stanford.edu/courses/cs147/2015/au/calendar.php)\n  - [CSEP 510 - Human Computer Interaction](https://courses.cs.washington.edu/courses/csep510/04wi/)\n  - [Programming for Designers - COMP1400-T2 (2010) - UNSW](https://www.youtube.com/playlist?list=PL9444191613E018CC)\n  - [08-763 Intro to HCI for Technology Executives - Fall 2015 - CMU](https://scs.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx#folderID=%228bb9f417-9f60-4e00-84f6-4ef8e7425ae1%22&maxResults=150)\n  - [05-600 HCI Pro Seminar - Fall 2015 - CMU](https://scs.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx#folderID=%22d885fd87-1ba1-47a3-9bd2-c4bde55f9749%22&maxResults=150)\n- **Game Development**\n \t- [CS50's Introduction to Game Development](https://www.edx.org/course/cs50s-introduction-to-game-development)\n\t- [MIT CMS.611J Creating Video Games, Fall 2014](https://ocw.mit.edu/courses/comparative-media-studies-writing/cms-611j-creating-video-games-fall-2014/lecture-videos/)\n  - [MOOC - Beginning Game Programming with C# - Coursera](http://academictorrents.com/details/0a7ba7e62821e488a0061751fdb81f4298733bea)\n  - [Gatech ECE4795 GPU Programming for Video Games, Summer 2021](https://www.youtube.com/playlist?list=PLOunECWxELQQwayE8e3WjKPJsTGKknJ8w)\n- **Geospatial**\n  - [Introduction to Spatial Data Science, Autumn 2016, University of Chicago](https://www.youtube.com/playlist?list=PLzREt6r1Nenkr2vtYgbP4hs44HO_s_qEO)\n  - [Spatial Regression Analysis, Spring 2017, University of Chicago](https://www.youtube.com/playlist?list=PLzREt6r1Nenkk7x197-CKPFZ0BuAOCRGT)\n  - [Spatial Data Science, Autumn 2017, University of Chicago](https://www.youtube.com/playlist?list=PLzREt6r1Nenlu-MBaxCRL2KZNk62n7o1g)\n  - [Introduction to Geographic Information Systems - IIT Roorkee](https://nptel.ac.in/courses/105107155/)\n- [MOOC - Matlab - Coursera](https://youtu.be/6iN56l7dEMY)\n- [Computing for Computer Scientists - University of Michigan](https://c4cs.github.io)\n- [Linux System Administration Decal, Spring 2025, UC Berkeley](https://decal.ocf.berkeley.edu/)\n- [Linux Implementation/Administration Practicum - Redhat by Tulio Llosa](https://itunes.apple.com/us/itunes-u/linux-implementation-administration/id430673915)\n- [Innovative Computing - Harvard University](https://www.youtube.com/playlist?list=PLE3E96113F544495A)\n- [Linux Programming & Scripting - IIT Madras](https://nptel.ac.in/courses/117106113/)\n- [Model Checking - IIT Madras](https://nptel.ac.in/courses/106106136/)\n- [Virtual Reality - IIT Madras](https://nptel.ac.in/courses/106106138/)\n- [Dependable Systems (SS 2014) - HPI University of Potsdam](https://www.tele-task.de/series/1005/)\n- [Business Process Compliance (WT 2013/14) - HPI University of Potsdam](https://www.tele-task.de/series/979/)\n- [Design Thinking for Digital Engineering (SS 2018) - Dr. Julia von Thienen - HPI](https://www.tele-task.de/series/1206/)\n- [CS224w \u2013 Social Network Analysis \u2013 Autumn 2017 - Stanford University](http://snap.stanford.edu/class/cs224w-videos-2017/)\n- [The Missing Semester of Your CS Education](https://missing.csail.mit.edu/)\n- [University of Crete, Computer Science video lectures (mostly Greek language lectures, very few 100% English-speaking courses). Very popular CS destination for European Erasmus students](https://opencourses.uoc.gr/courses/course/index.php?categoryid=28)\n- [Stanford EE274 I Data Compression: Theory and Applications I 2023](https://www.youtube.com/playlist?list=PLoROMvodv4rPj4uhbgUAaEKwNNak8xgkz)\n- [Probabilistic Methods - University of Waterloo](https://www.youtube.com/playlist?list=PL2BdWtDKMS6nRF72s3TOGyBqXwMVHYiLU)\n- [Free Probability Theory and Ramanujan Graphs - Spring 2024](https://www.youtube.com/playlist?list=PL-FpbJb6Ix_Npx7GSSxnD2BLrLtwkdwt4)\n- [Asymptotics and perturbation methods - Prof. Steven Strogatz](https://www.youtube.com/playlist?list=PL5EH0ZJ7V0jV7kMYvPcZ7F9oaf_YAlfbI)\n- [ETH Z\u00fcrich AI in the Sciences and Engineering](https://www.youtube.com/playlist?list=PLJkYEExhe7rYFkBIB2U5pf_RWzYnFLj7r)\n- [Introduction to GIS Programming (Fall 2024) - Open Geospatial Solutions](https://www.youtube.com/playlist?list=PLAxJ4-o7ZoPfb18kNe2luWX9xKg1233i9)\n- [Gatech Guitar Amplification and Effects, by Aaron Lanterman](https://www.youtube.com/playlist?list=PLOunECWxELQS7JV_KeeTJJpgGjOftoaAH)\n- [Gatech ECE3084 Signals and Systems summer 2020, by Aaron Lanterman](https://www.youtube.com/playlist?list=PLOunECWxELQRYwsuj4BL4Hu1nvj9dxRQ6)\n- [Gatech ECE4450 Analog Circuits for Music Synthesis spring 2021, by Aaron Lanterman](https://www.youtube.com/playlist?list=PLOunECWxELQS5bMdWo9VhmZtsCjhjYNcV)\n- [UC Berkeley EE 120 Signals and Systems spring 2019, by Murat Arcak](https://www.youtube.com/playlist?list=PLIygTcviGPKCFASN_5cR04KUST-G97bb-)\n- [Stanford EE 102 spring 1999, Introduction to Signals and Systems, by Stephen Boyd](https://www.youtube.com/playlist?list=PLpGHT1n4-mAvjbBio7D_qpIN4bZqOOOST)\n- [Stanford EE 376a winter 2011, Information Theory, by Thomas Cover](https://www.youtube.com/user/classxteam/playlists)\n- [MIT RES.6.007 Signals and Systems, 1987 - MIT](https://www.youtube.com/playlist?list=PL41692B571DD0AF9B)\n- [MIT 9.19 Computational Psycholinguistics, 2023 - MIT](https://rlevy.github.io/9.19-syllabus/syllabus.html) ([YouTube](https://www.youtube.com/playlist?list=PLlp18pgkl6U_Dfv59jF-NlAW6dwqYKZim))\n- [UCCS ECE4510/ECE5510 Feedback Control Systems, by Gregory Plett](http://mocha-java.uccs.edu/ECE4510/index.html)\n- [UCCS ECE4520/ECE5520 Multivariable Control Systems I, by Gregory Plett](http://mocha-java.uccs.edu/ECE5520/index.html)\n- [UCCS ECE4530/ECE5530 Multivariable Control Systems II, by Gregory Plett](http://mocha-java.uccs.edu/ECE5530/index.html)\n- [UCCS ECE4540/ECE5540 Digital Control Systems, by Gregory Plett](http://mocha-java.uccs.edu/ECE5540/index.html)\n- [UCCS ECE4550/ECE5550 Applied Kalman Filtering, by Gregory Plett](http://mocha-java.uccs.edu/ECE5550/index.html)\n- [UCCS ECE4560/ECE5560 System Identification, by Gregory Plett](http://mocha-java.uccs.edu/ECE5560/index.html)\n- [UCCS ECE4570/ECE5570 Optimization for Systems and Control, by M. Scott Trimboli](http://mocha-java.uccs.edu/ECE5570-NEW/index.html)\n- [UCCS ECE4580/ECE5580 Multivariable Control in the Frequency Domain, by M. Scott Trimboli](http://mocha-java.uccs.edu/ECE5580/index.html)\n- [UCCS ECE4710/ECE5710 Modeling, Simulation, and Identification of Battery Dynamics, by Gregory Plett](http://mocha-java.uccs.edu/ECE5710/index.html)\n- [UCCS ECE4720/ECE5720 Battery Management and Control, by Gregory Plett](http://mocha-java.uccs.edu/ECE5720/index.html)\n- [Purdue ME 597 Distributed Energy Resources Spring 2024, by Kevin J. Kircher](https://www.youtube.com/playlist?list=PLcZDMdEnS08l_z5LVDQrEcyFrBsHlKVXd)\n- [Stanford AA228V/CS238V Validation of Safety Critical Systems Winter 2025, by Sydney Michelle Katz](https://www.youtube.com/playlist?list=PLoROMvodv4rOq1LMLI8U7djzDb8--xpaC)\n\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 22887094,
    "name": "tesseract",
    "full_name": "tesseract-ocr/tesseract",
    "description": "Tesseract Open Source OCR Engine (main repository)",
    "html_url": "https://github.com/tesseract-ocr/tesseract",
    "clone_url": "https://github.com/tesseract-ocr/tesseract.git",
    "owner_login": "tesseract-ocr",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/8401422?v=4",
    "stargazers_count": 68643,
    "watchers_count": 68643,
    "forks_count": 10096,
    "open_issues_count": 448,
    "size": 53638,
    "language": "C++",
    "languages": {
      "C++": 7501597,
      "Java": 74136,
      "Makefile": 67075,
      "CMake": 64923,
      "NSIS": 51446,
      "C": 41152,
      "Shell": 29511,
      "M4": 3378,
      "Python": 2520
    },
    "topics": [
      "hacktoberfest",
      "lstm",
      "machine-learning",
      "ocr",
      "ocr-engine",
      "tesseract",
      "tesseract-ocr"
    ],
    "license_name": "Apache License 2.0",
    "created_at": "2014-08-12T18:04:59+00:00",
    "updated_at": "2025-08-06T02:05:56+00:00",
    "pushed_at": "2025-07-06T20:04:26+00:00",
    "contributors_count": 100,
    "readme_length": 8142,
    "readme_content": "# Tesseract OCR\n\n[![Coverity Scan Build Status](https://scan.coverity.com/projects/tesseract-ocr/badge.svg)](https://scan.coverity.com/projects/tesseract-ocr)\n[![CodeQL](https://github.com/tesseract-ocr/tesseract/workflows/CodeQL/badge.svg)](https://github.com/tesseract-ocr/tesseract/security/code-scanning)\n[![OSS-Fuzz](https://img.shields.io/badge/oss--fuzz-fuzzing-brightgreen)](https://issues.oss-fuzz.com/issues?q=is:open%20title:tesseract-ocr)\n\\\n[![GitHub license](https://img.shields.io/badge/license-Apache--2.0-blue.svg)](https://raw.githubusercontent.com/tesseract-ocr/tesseract/main/LICENSE)\n[![Downloads](https://img.shields.io/badge/download-all%20releases-brightgreen.svg)](https://github.com/tesseract-ocr/tesseract/releases/)\n\n## Table of Contents\n\n* [Tesseract OCR](#tesseract-ocr)\n  * [About](#about)\n  * [Brief history](#brief-history)\n  * [Installing Tesseract](#installing-tesseract)\n  * [Running Tesseract](#running-tesseract)\n  * [For developers](#for-developers)\n  * [Support](#support)\n  * [License](#license)\n  * [Dependencies](#dependencies)\n  * [Latest Version of README](#latest-version-of-readme)\n\n## About\n\nThis package contains an **OCR engine** - `libtesseract` and a **command line program** - `tesseract`.\n\nTesseract 4 adds a new neural net (LSTM) based [OCR engine](https://en.wikipedia.org/wiki/Optical_character_recognition) which is focused on line recognition, but also still supports the legacy Tesseract OCR engine of Tesseract 3 which works by recognizing character patterns. Compatibility with Tesseract 3 is enabled by using the Legacy OCR Engine mode (--oem 0).\nIt also needs [traineddata](https://tesseract-ocr.github.io/tessdoc/Data-Files.html) files which support the legacy engine, for example those from the [tessdata](https://github.com/tesseract-ocr/tessdata) repository.\n\nStefan Weil is the current lead developer. Ray Smith was the lead developer until 2018. The maintainer is Zdenko Podobny. For a list of contributors see [AUTHORS](https://github.com/tesseract-ocr/tesseract/blob/main/AUTHORS)\nand GitHub's log of [contributors](https://github.com/tesseract-ocr/tesseract/graphs/contributors).\n\nTesseract has **unicode (UTF-8) support**, and can **recognize [more than 100 languages](https://tesseract-ocr.github.io/tessdoc/Data-Files-in-different-versions.html)** \"out of the box\".\n\nTesseract supports **[various image formats](https://tesseract-ocr.github.io/tessdoc/InputFormats)** including PNG, JPEG and TIFF.\n\nTesseract supports **various output formats**: plain text, hOCR (HTML), PDF, invisible-text-only PDF, TSV, ALTO and PAGE.\n\nYou should note that in many cases, in order to get better OCR results, you'll need to **[improve the quality](https://tesseract-ocr.github.io/tessdoc/ImproveQuality.html) of the image** you are giving Tesseract.\n\nThis project **does not include a GUI application**. If you need one, please see the [3rdParty](https://tesseract-ocr.github.io/tessdoc/User-Projects-%E2%80%93-3rdParty.html) documentation.\n\nTesseract **can be trained to recognize other languages**.\nSee [Tesseract Training](https://tesseract-ocr.github.io/tessdoc/Training-Tesseract.html) for more information.\n\n## Brief history\n\nTesseract was originally developed at Hewlett-Packard Laboratories Bristol UK and at Hewlett-Packard Co, Greeley Colorado USA between 1985 and 1994, with some more changes made in 1996 to port to Windows, and some C++izing in 1998. In 2005 Tesseract was open sourced by HP. From 2006 until November 2018 it was developed by Google.\n\nMajor version 5 is the current stable version and started with release\n[5.0.0](https://github.com/tesseract-ocr/tesseract/releases/tag/5.0.0) on November 30, 2021. Newer minor versions and bugfix versions are available from\n[GitHub](https://github.com/tesseract-ocr/tesseract/releases/).\n\nLatest source code is available from [main branch on GitHub](https://github.com/tesseract-ocr/tesseract/tree/main).\nOpen issues can be found in [issue tracker](https://github.com/tesseract-ocr/tesseract/issues),\nand [planning documentation](https://tesseract-ocr.github.io/tessdoc/Planning.html).\n\nSee **[Release Notes](https://tesseract-ocr.github.io/tessdoc/ReleaseNotes.html)**\nand **[Change Log](https://github.com/tesseract-ocr/tesseract/blob/main/ChangeLog)** for more details of the releases.\n\n## Installing Tesseract\n\nYou can either [Install Tesseract via pre-built binary package](https://tesseract-ocr.github.io/tessdoc/Installation.html)\nor [build it from source](https://tesseract-ocr.github.io/tessdoc/Compiling.html).\n\nBefore building Tesseract from source, please check that your system has a compiler which is one of the [supported compilers](https://tesseract-ocr.github.io/tessdoc/supported-compilers.html).\n\n## Running Tesseract\n\nBasic **[command line usage](https://tesseract-ocr.github.io/tessdoc/Command-Line-Usage.html)**:\n\n    tesseract imagename outputbase [-l lang] [--oem ocrenginemode] [--psm pagesegmode] [configfiles...]\n\nFor more information about the various command line options use `tesseract --help` or `man tesseract`.\n\nExamples can be found in the [documentation](https://tesseract-ocr.github.io/tessdoc/Command-Line-Usage.html#simplest-invocation-to-ocr-an-image).\n\n## For developers\n\nDevelopers can use `libtesseract` [C](https://github.com/tesseract-ocr/tesseract/blob/main/include/tesseract/capi.h) or\n[C++](https://github.com/tesseract-ocr/tesseract/blob/main/include/tesseract/baseapi.h) API to build their own application. If you need bindings to `libtesseract` for other programming languages, please see the\n[wrapper](https://tesseract-ocr.github.io/tessdoc/AddOns.html#tesseract-wrappers) section in the AddOns documentation.\n\nDocumentation of Tesseract generated from source code by doxygen can be found on [tesseract-ocr.github.io](https://tesseract-ocr.github.io/).\n\n## Support\n\nBefore you submit an issue, please review **[the guidelines for this repository](https://github.com/tesseract-ocr/tesseract/blob/main/CONTRIBUTING.md)**.\n\nFor support, first read the [documentation](https://tesseract-ocr.github.io/tessdoc/),\nparticularly the [FAQ](https://tesseract-ocr.github.io/tessdoc/FAQ.html) to see if your problem is addressed there.\nIf not, search the [Tesseract user forum](https://groups.google.com/g/tesseract-ocr), the [Tesseract developer forum](https://groups.google.com/g/tesseract-dev) and [past issues](https://github.com/tesseract-ocr/tesseract/issues), and if you still can't find what you need, ask for support in the mailing-lists.\n\nMailing-lists:\n\n* [tesseract-ocr](https://groups.google.com/g/tesseract-ocr) - For tesseract users.\n* [tesseract-dev](https://groups.google.com/g/tesseract-dev) - For tesseract developers.\n\nPlease report an issue only for a **bug**, not for asking questions.\n\n## License\n\n    The code in this repository is licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n\n**NOTE**: This software depends on other packages that may be licensed under different open source licenses.\n\nTesseract uses [Leptonica library](http://leptonica.com/) which essentially\nuses a [BSD 2-clause license](http://leptonica.com/about-the-license.html).\n\n## Dependencies\n\nTesseract uses [Leptonica library](https://github.com/DanBloomberg/leptonica)\nfor opening input images (e.g. not documents like pdf).\nIt is suggested to use leptonica with built-in support for [zlib](https://zlib.net),\n[png](https://sourceforge.net/projects/libpng) and\n[tiff](http://www.simplesystems.org/libtiff) (for multipage tiff).\n\n## Latest Version of README\n\nFor the latest online version of the README.md see:\n\n<https://github.com/tesseract-ocr/tesseract/blob/main/README.md>\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 115478820,
    "name": "awesome-scalability",
    "full_name": "binhnguyennus/awesome-scalability",
    "description": "The Patterns of Scalable, Reliable, and Performant Large-Scale Systems",
    "html_url": "https://github.com/binhnguyennus/awesome-scalability",
    "clone_url": "https://github.com/binhnguyennus/awesome-scalability.git",
    "owner_login": "binhnguyennus",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/15001306?v=4",
    "stargazers_count": 63689,
    "watchers_count": 63689,
    "forks_count": 6498,
    "open_issues_count": 15,
    "size": 1557,
    "language": null,
    "languages": {},
    "topics": [
      "architecture",
      "awesome",
      "awesome-list",
      "backend",
      "big-data",
      "computer-science",
      "design-patterns",
      "devops",
      "distributed-systems",
      "interview",
      "interview-practice",
      "interview-questions",
      "lists",
      "machine-learning",
      "programming",
      "resources",
      "scalability",
      "system",
      "system-design",
      "web-development"
    ],
    "license_name": "MIT License",
    "created_at": "2017-12-27T03:46:40+00:00",
    "updated_at": "2025-08-06T01:50:12+00:00",
    "pushed_at": "2025-07-26T06:03:03+00:00",
    "contributors_count": 27,
    "readme_length": 124848,
    "readme_content": "[![Logo](/logo.png)](http://awesome-scalability.com/)\n\nAn updated and organized reading list for illustrating the patterns of scalable, reliable, and performant large-scale systems. Concepts are explained in the articles of prominent engineers and credible references. Case studies are taken from battle-tested systems that serve millions to billions of users.\n\n#### If your system goes slow\n> Understand your problems: scalability problem (fast for a single user but slow under heavy load) or performance problem (slow for a single user) by reviewing some [design principles](#principle) and checking how [scalability](#scalability) and [performance](#performance) problems are solved at tech companies. The section of [intelligence](#intelligence) are created for those who work with data and machine learning at big (data) and deep (learning) scale.\n\n#### If your system goes down\n> \"Even if you lose all one day, you can build all over again if you retain your calm!\" - Thuan Pham, former CTO of Uber. So, keep calm and mind the [availability](#availability) and [stability](#stability) matters! \n\n#### If you are having a system design interview\n> Look at some [interview notes](#interview) and [real-world architectures with completed diagrams](#architecture) to get a comprehensive view before designing your system on whiteboard. You can check some [talks](#talk) of engineers from tech giants to know how they build, scale, and optimize their systems. Good luck!\n\n#### If you are building your dream team\n> The goal of scaling team is not growing team size but increasing team output and value. You can find out how tech companies reach that goal in various aspects: hiring, management, organization, culture, and communication in the [organization](#organization) section.\n\n#### Community power\n\n> Contributions are greatly welcome! You may want to take a look at the [contribution guidelines](CONTRIBUTING.md). If you see a link here that is no longer maintained or is not a good fit, please submit a pull request!\n\n> Many long hours of hard work have gone into this project. If you find it helpful, please share on Facebook, [on Twitter](https://ctt.ec/V8B2p), [on Weibo](http://t.cn/RnjFLCB), or on your chat groups! Knowledge is power, knowledge shared is power multiplied. Thank you!\n\n## Content\n- [Principle](#principle)\n- [Scalability](#scalability)\n- [Availability](#availability)\n- [Stability](#stability)\n- [Performance](#performance)\n- [Intelligence](#intelligence)\n- [Architecture](#architecture)\n- [Interview](#interview)\n- [Organization](#organization)\n- [Talk](#talk)\n- [Book](#book)\n\n## Principle\n* [Lessons from Giant-Scale Services - Eric Brewer, UC Berkeley & Google](https://people.eecs.berkeley.edu/~brewer/papers/GiantScale-IEEE.pdf)\n* [Designs, Lessons and Advice from Building Large Distributed Systems - Jeff Dean, Google](https://www.cs.cornell.edu/projects/ladis2009/talks/dean-keynote-ladis2009.pdf)\n* [How to Design a Good API & Why it Matters - Joshua Bloch, CMU & Google](https://www.infoq.com/presentations/effective-api-design)\n* [On Efficiency, Reliability, Scaling - James Hamilton, VP at AWS](http://mvdirona.com/jrh/work/)\n* [Principles of Chaos Engineering](https://www.usenix.org/conference/srecon17americas/program/presentation/rosenthal)\n* [Finding the Order in Chaos](https://www.usenix.org/conference/srecon16/program/presentation/lueder)\n* [The Twelve-Factor App](https://12factor.net/)\n* [Clean Architecture](https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html)\n* [High Cohesion and Low Coupling](http://www.math-cs.gordon.edu/courses/cs211/lectures-2009/Cohesion,Coupling,MVC.pdf)\n* [Monoliths and Microservices](https://medium.com/@SkyscannerEng/monoliths-and-microservices-8c65708c3dbf)\n* [CAP Theorem and Trade-offs](http://robertgreiner.com/2014/08/cap-theorem-revisited/)\n* [CP Databases and AP Databases](https://blog.andyet.com/2014/10/01/right-database)\n* [Stateless vs Stateful Scalability](http://ithare.com/scaling-stateful-objects/)\t\n* [Scale Up vs Scale Out: Hidden Costs](https://blog.codinghorror.com/scaling-up-vs-scaling-out-hidden-costs/)\n* [ACID and BASE](https://neo4j.com/blog/acid-vs-base-consistency-models-explained/)\n* [Blocking/Non-Blocking and Sync/Async](https://blogs.msdn.microsoft.com/csliu/2009/08/27/io-concept-blockingnon-blocking-vs-syncasync/)\n* [Performance and Scalability of Databases](https://use-the-index-luke.com/sql/testing-scalability)\n* [Database Isolation Levels and Effects on Performance and Scalability](http://highscalability.com/blog/2011/2/10/database-isolation-levels-and-their-effects-on-performance-a.html)\n* [The Probability of Data Loss in Large Clusters](https://martin.kleppmann.com/2017/01/26/data-loss-in-large-clusters.html)\n* [Data Access for Highly-Scalable Solutions: Using SQL, NoSQL, and Polyglot Persistence](https://docs.microsoft.com/en-us/previous-versions/msp-n-p/dn271399(v=pandp.10))\n* [SQL vs NoSQL](https://www.upwork.com/hiring/data/sql-vs-nosql-databases-whats-the-difference/)\n* [SQL vs NoSQL - Lesson Learned at Salesforce](https://engineering.salesforce.com/sql-or-nosql-9eaf1d92545b)\n* [NoSQL Databases: Survey and Decision Guidance](https://medium.baqend.com/nosql-databases-a-survey-and-decision-guidance-ea7823a822d)\n* [How Sharding Works](https://medium.com/@jeeyoungk/how-sharding-works-b4dec46b3f6)\n* [Consistent Hashing](http://www.tom-e-white.com/2007/11/consistent-hashing.html)\n* [Consistent Hashing: Algorithmic Tradeoffs](https://medium.com/@dgryski/consistent-hashing-algorithmic-tradeoffs-ef6b8e2fcae8)\n* [Don\u2019t be tricked by the Hashing Trick](https://booking.ai/dont-be-tricked-by-the-hashing-trick-192a6aae3087)\n* [Uniform Consistent Hashing at Netflix](https://medium.com/netflix-techblog/distributing-content-to-open-connect-3e3e391d4dc9)\n* [Eventually Consistent - Werner Vogels, CTO at Amazon](https://www.allthingsdistributed.com/2008/12/eventually_consistent.html)\n* [Cache is King](https://www.stevesouders.com/blog/2012/10/11/cache-is-king/)\n* [Anti-Caching](https://www.the-paper-trail.org/post/2014-06-06-paper-notes-anti-caching/)\n* [Understand Latency](http://highscalability.com/latency-everywhere-and-it-costs-you-sales-how-crush-it)\n* [Latency Numbers Every Programmer Should Know](http://norvig.com/21-days.html#answers)\n* [The Calculus of Service Availability](https://queue.acm.org/detail.cfm?id=3096459&__s=dnkxuaws9pogqdnxmx8i)\n* [Architecture Issues When Scaling Web Applications: Bottlenecks, Database, CPU, IO](http://highscalability.com/blog/2014/5/12/4-architecture-issues-when-scaling-web-applications-bottlene.html)\t\n* [Common Bottlenecks](http://highscalability.com/blog/2012/5/16/big-list-of-20-common-bottlenecks.html)\n* [Life Beyond Distributed Transactions](https://queue.acm.org/detail.cfm?id=3025012)\n* [Relying on Software to Redirect Traffic Reliably at Various Layers](https://www.usenix.org/conference/srecon15/program/presentation/taveira)\n* [Breaking Things on Purpose](https://www.usenix.org/conference/srecon17americas/program/presentation/andrus)\n* [Avoid Over Engineering](https://medium.com/@rdsubhas/10-modern-software-engineering-mistakes-bc67fbef4fc8)\n* [Scalability Worst Practices](https://www.infoq.com/articles/scalability-worst-practices)\n* [Use Solid Technologies - Don\u2019t Re-invent the Wheel - Keep It Simple!](https://medium.com/@DataStax/instagram-engineerings-3-rules-to-a-scalable-cloud-application-architecture-c44afed31406)\n* [Simplicity by Distributing Complexity](https://engineering.zalando.com/posts/2018/01/simplicity-by-distributing-complexity.html)\n* [Why Over-Reusing is Bad](http://tech.transferwise.com/why-over-reusing-is-bad/)\n* [Performance is a Feature](https://blog.codinghorror.com/performance-is-a-feature/)\n* [Make Performance Part of Your Workflow](https://codeascraft.com/2014/12/11/make-performance-part-of-your-workflow/)\n* [The Benefits of Server Side Rendering over Client Side Rendering](https://medium.com/walmartlabs/the-benefits-of-server-side-rendering-over-client-side-rendering-5d07ff2cefe8)\n* [Automate and Abstract: Lessons at Facebook](https://architecht.io/lessons-from-facebook-on-engineering-for-scale-f5716f0afc7a)\n* [AWS Do's and Don'ts](https://8thlight.com/blog/sarah-sunday/2017/09/15/aws-dos-and-donts.html)\n* [(UI) Design Doesn\u2019t Scale - Stanley Wood, Design Director at Spotify](https://medium.com/@hellostanley/design-doesnt-scale-4d81e12cbc3e)\n* [Linux Performance](http://www.brendangregg.com/linuxperf.html)\n* [Building Fast and Resilient Web Applications - Ilya Grigorik](https://www.igvita.com/2016/05/20/building-fast-and-resilient-web-applications/)\n* [Accept Partial Failures, Minimize Service Loss](https://www.usenix.org/conference/srecon17asia/program/presentation/wang_daxin)\n* [Design for Resiliency](http://highscalability.com/blog/2012/12/31/designing-for-resiliency-will-be-so-2013.html)\n* [Design for Self-healing](https://docs.microsoft.com/en-us/azure/architecture/guide/design-principles/self-healing)\n* [Design for Scaling Out](https://docs.microsoft.com/en-us/azure/architecture/guide/design-principles/scale-out)\t\n* [Design for Evolution](https://docs.microsoft.com/en-us/azure/architecture/guide/design-principles/design-for-evolution)\n* [Learn from Mistakes](http://highscalability.com/blog/2013/8/26/reddit-lessons-learned-from-mistakes-made-scaling-to-1-billi.html)\n\n## Scalability\n* [Microservices and Orchestration](https://martinfowler.com/microservices/)\n\t* [Domain-Oriented Microservice Architecture at Uber](https://eng.uber.com/microservice-architecture/)\n\t* [Service Architecture (3 parts: Domain Gateways, Value-Added Services, BFF) at SoundCloud](https://developers.soundcloud.com/blog/service-architecture-3)\n\t* [Container (8 parts) at Riot Games](https://engineering.riotgames.com/news/thinking-inside-container)\n\t* [Containerization at Pinterest](https://medium.com/@Pinterest_Engineering/containerization-at-pinterest-92295347f2f3)\n\t* [Evolution of Container Usage at Netflix](https://medium.com/netflix-techblog/the-evolution-of-container-usage-at-netflix-3abfc096781b)\n\t* [Dockerizing MySQL at Uber](https://eng.uber.com/dockerizing-mysql/)\n\t* [Testing of Microservices at Spotify](https://labs.spotify.com/2018/01/11/testing-of-microservices/)\n\t* [Docker in Production at Treehouse](https://medium.com/treehouse-engineering/lessons-learned-running-docker-in-production-5dce99ece770)\n\t* [Microservice at SoundCloud](https://developers.soundcloud.com/blog/inside-a-soundcloud-microservice)\n\t* [Operate Kubernetes Reliably at Stripe](https://stripe.com/blog/operating-kubernetes)\n\t* [Cross-Cluster Traffic Mirroring with Istio at Trivago](https://tech.trivago.com/2020/06/10/cross-cluster-traffic-mirroring-with-istio/)\n\t* [Agrarian-Scale Kubernetes (3 parts) at New York Times](https://open.nytimes.com/agrarian-scale-kubernetes-part-3-ee459887ed7e)\n\t* [Nanoservices at BBC](https://medium.com/bbc-design-engineering/powering-bbc-online-with-nanoservices-727840ba015b)\n\t* [PowerfulSeal: Testing Tool for Kubernetes Clusters at Bloomberg](https://www.techatbloomberg.com/blog/powerfulseal-testing-tool-kubernetes-clusters/)\n\t* [Conductor: Microservices Orchestrator at Netflix](https://medium.com/netflix-techblog/netflix-conductor-a-microservices-orchestrator-2e8d4771bf40)\n\t* [Docker Containers that Power Over 100.000 Online Shops at Shopify](https://shopifyengineering.myshopify.com/blogs/engineering/docker-at-shopify-how-we-built-containers-that-power-over-100-000-online-shops)\n\t* [Microservice Architecture at Medium](https://medium.engineering/microservice-architecture-at-medium-9c33805eb74f)\n\t* [From bare-metal to Kubernetes at Betabrand](https://boxunix.com/post/bare_metal_to_kube/)\n\t* [Kubernetes at Tinder](https://medium.com/tinder-engineering/tinders-move-to-kubernetes-cda2a6372f44)\n\t* [Kubernetes at Quora](https://www.quora.com/q/quoraengineering/Adopting-Kubernetes-at-Quora)\t\n\t* [Kubernetes Platform at Pinterest](https://medium.com/pinterest-engineering/building-a-kubernetes-platform-at-pinterest-fb3d9571c948)\n\t* [Microservices at Nubank](https://medium.com/building-nubank/microservices-at-nubank-an-overview-2ebcb336c64d)\n\t* [Payment Transaction Management in Microservices at Mercari](https://engineering.mercari.com/en/blog/entry/20210831-2019-06-07-155849/)\n\t* [Service Mesh at Snap](https://eng.snap.com/monolith-to-multicloud-microservices-snap-service-mesh)\n\t* [GRIT: Protocol for Distributed Transactions across Microservices at eBay](https://tech.ebayinc.com/engineering/grit-a-protocol-for-distributed-transactions-across-microservices/)\n\t* [Rubix: Kubernetes at Palantir](https://medium.com/palantir/introducing-rubix-kubernetes-at-palantir-ab0ce16ea42e)\n\t* [CRISP: Critical Path Analysis for Microservice Architectures at Uber](https://eng.uber.com/crisp-critical-path-analysis-for-microservice-architectures/)\n* [Distributed Caching](https://www.wix.engineering/post/scaling-to-100m-to-cache-or-not-to-cache)\n\t* [EVCache: Distributed In-memory Caching at Netflix](https://medium.com/netflix-techblog/caching-for-a-global-netflix-7bcc457012f1)\n\t* [EVCache Cache Warmer Infrastructure at Netflix](https://medium.com/netflix-techblog/cache-warming-agility-for-a-stateful-service-2d3b1da82642)\n\t* [Memsniff: Robust Memcache Traffic Analyzer at Box](https://blog.box.com/blog/introducing-memsniff-robust-memcache-traffic-analyzer/)\n\t* [Caching with Consistent Hashing and Cache Smearing at Etsy](https://codeascraft.com/2017/11/30/how-etsy-caches/)\n\t* [Analysis of Photo Caching at Facebook](https://code.facebook.com/posts/220956754772273/an-analysis-of-facebook-photo-caching/)\n\t* [Cache Efficiency Exercise at Facebook](https://code.facebook.com/posts/964122680272229/web-performance-cache-efficiency-exercise/)\n\t* [tCache: Scalable Data-aware Java Caching at Trivago](http://tech.trivago.com/2015/10/15/tcache/)\n\t* [Pycache: In-process Caching at Quora](https://engineering.quora.com/Pycache-lightning-fast-in-process-caching)\t\n\t* [Reduce Memcached Memory Usage by 50% at Trivago](http://tech.trivago.com/2017/12/19/how-trivago-reduced-memcached-memory-usage-by-50/)\n\t* [Caching Internal Service Calls at Yelp](https://engineeringblog.yelp.com/2018/03/caching-internal-service-calls-at-yelp.html)\n\t* [Estimating the Cache Efficiency using Big Data at Allegro](https://allegro.tech/2017/01/estimating-the-cache-efficiency-using-big-data.html)\n\t* [Distributed Cache at Zalando](https://engineering.zalando.com/posts/2018/04/distributed-cache-akka-kubernetes.html)\n\t* [Distributed Cache for S3 at ClickHouse](https://clickhouse.com/blog/building-a-distributed-cache-for-s3)\n\t* [Application Data Caching from RAM to SSD at NetFlix](https://medium.com/netflix-techblog/evolution-of-application-data-caching-from-ram-to-ssd-a33d6fa7a690)\n\t* [Tradeoffs of Replicated Cache at Skyscanner](https://medium.com/@SkyscannerEng/the-tradeoffs-of-a-replicated-cache-b6680c722f58)\n\t* [Location Caching with Quadtrees at Yext](http://engblog.yext.com/post/geolocation-caching)\n\t* [Video Metadata Caching at Vimeo](https://medium.com/vimeo-engineering-blog/video-metadata-caching-at-vimeo-a54b25f0b304)\n\t* [Scaling Redis at Twitter](http://highscalability.com/blog/2014/9/8/how-twitter-uses-redis-to-scale-105tb-ram-39mm-qps-10000-ins.html)\n\t* [Scaling Job Queue with Redis at Slack](https://slack.engineering/scaling-slacks-job-queue-687222e9d100)\n\t* [Moving persistent data out of Redis at Github](https://githubengineering.com/moving-persistent-data-out-of-redis/)\n\t* [Storing Hundreds of Millions of Simple Key-Value Pairs in Redis at Instagram](https://engineering.instagram.com/storing-hundreds-of-millions-of-simple-key-value-pairs-in-redis-1091ae80f74c)\n\t* [Redis at Trivago](http://tech.trivago.com/2017/01/25/learn-redis-the-hard-way-in-production/)\n\t* [Optimizing Redis Storage at Deliveroo](https://deliveroo.engineering/2017/01/19/optimising-membership-queries.html)\n\t* [Memory Optimization in Redis at Wattpad](http://engineering.wattpad.com/post/23244724794/store-more-stuff-memory-optimization-in-redis)\n\t* [Redis Fleet at Heroku](https://blog.heroku.com/rolling-redis-fleet)\n\t* [Solving Remote Build Cache Misses (2 parts) at SoundCloud](https://developers.soundcloud.com/blog/gradle-remote-build-cache-misses-part-2)\n\t* [Ratings & Reviews (2 parts) at Flipkart](https://blog.flipkart.tech/ratings-reviews-flipkart-part-2-574ab08e75cf)\n\t* [Prefetch Caching of Items at eBay](https://tech.ebayinc.com/engineering/prefetch-caching-of-ebay-items/)\n\t* [Cross-Region Caching Library at Wix](https://www.wix.engineering/post/how-we-built-a-cross-region-caching-library)\n\t* [Improving Distributed Caching Performance and Efficiency at Pinterest](https://medium.com/pinterest-engineering/improving-distributed-caching-performance-and-efficiency-at-pinterest-92484b5fe39b)\n\t* [Standardize and Improve Microservices Caching at DoorDash](https://doordash.engineering/2023/10/19/how-doordash-standardized-and-improved-microservices-caching/)\n    * [HTTP Caching and CDN](https://developer.mozilla.org/en-US/docs/Web/HTTP/Caching)\n        * [Zynga Geo Proxy: Reducing Mobile Game Latency at Zynga](https://www.zynga.com/blogs/engineering/zynga-geo-proxy-reducing-mobile-game-latency)\n        * [Google AMP at Cond\u00e9 Nast](https://technology.condenast.com/story/the-why-and-how-of-google-amp-at-conde-nast)\n        * [A/B Tests on Hosting Infrastructure (CDNs) at Deliveroo](https://deliveroo.engineering/2016/09/19/ab-testing-cdns.html)\n        * [HAProxy with Kubernetes for User-facing Traffic at SoundCloud](https://developers.soundcloud.com/blog/how-soundcloud-uses-haproxy-with-kubernetes-for-user-facing-traffic)\n        * [Bandaid: Service Proxy at Dropbox](https://blogs.dropbox.com/tech/2018/03/meet-bandaid-the-dropbox-service-proxy/)\n\t\t* [Service Workers at Slack](https://slack.engineering/service-workers-at-slack-our-quest-for-faster-boot-times-and-offline-support-3492cf79c88)\n\t\t* [CDN Services at Spotify](https://labs.spotify.com/2020/02/24/how-spotify-aligned-cdn-services-for-a-lightning-fast-streaming-experience/)\n* [Distributed Locking](https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html)\n\t* [Chubby: Lock Service for Loosely Coupled Distributed Systems at Google](https://blog.acolyer.org/2015/02/13/the-chubby-lock-service-for-loosely-coupled-distributed-systems/)\n\t* [Distributed Locking at Uber](https://www.youtube.com/watch?v=MDuagr729aU)\n\t* [Distributed Locks using Redis at GoSquared](https://engineering.gosquared.com/distributed-locks-using-redis)\n\t* [ZooKeeper at Twitter](https://blog.twitter.com/engineering/en_us/topics/infrastructure/2018/zookeeper-at-twitter.html)\n\t* [Eliminating Duplicate Queries using Distributed Locking at Chartio](https://chartio.com/blog/eliminating-duplicate-queries-using-distributed-locking/)\n* [Distributed Tracking, Tracing, and Measuring](https://www.oreilly.com/ideas/understanding-the-value-of-distributed-tracing)\n\t* [Zipkin: Distributed Systems Tracing at Twitter](https://blog.twitter.com/engineering/en_us/a/2012/distributed-systems-tracing-with-zipkin.html)\n\t* [Improve Zipkin Traces using Kubernetes Pod Metadata at SoundCloud](https://developers.soundcloud.com/blog/using-kubernetes-pod-metadata-to-improve-zipkin-traces)\n\t* [Canopy: Scalable Distributed Tracing & Analysis at Facebook](https://www.infoq.com/presentations/canopy-scalable-tracing-analytics-facebook)\n\t* [Pintrace: Distributed Tracing at Pinterest](https://medium.com/@Pinterest_Engineering/distributed-tracing-at-pinterest-with-new-open-source-tools-a4f8a5562f6b)\n\t* [XCMetrics: All-in-One Tool for Tracking Xcode Build Metrics at Spotify](https://engineering.atspotify.com/2021/01/20/introducing-xcmetrics-our-all-in-one-tool-for-tracking-xcode-build-metrics/)\n\t* [Real-time Distributed Tracing at LinkedIn](https://engineering.linkedin.com/distributed-service-call-graph/real-time-distributed-tracing-website-performance-and-efficiency)\t\n\t* [Tracking Service Infrastructure at Scale at Shopify](https://www.usenix.org/conference/srecon17americas/program/presentation/arthorne)\t\n\t* [Distributed Tracing at HelloFresh](https://engineering.hellofresh.com/scaling-hellofresh-distributed-tracing-7b182928247d)\n\t* [Analyzing Distributed Trace Data at Pinterest](https://medium.com/@Pinterest_Engineering/analyzing-distributed-trace-data-6aae58919949)\n\t* [Distributed Tracing at Uber](https://eng.uber.com/distributed-tracing/)\n\t* [JVM Profiler: Tracing Distributed JVM Applications at Uber](https://eng.uber.com/jvm-profiler/)\n\t* [Data Checking at Dropbox](https://www.usenix.org/conference/srecon17asia/program/presentation/mah)\n\t* [Tracing Distributed Systems at Showmax](https://tech.showmax.com/2016/10/tracing-distributed-systems-at-showmax/)\n\t* [osquery Across the Enterprise at Palantir](https://medium.com/@palantir/osquery-across-the-enterprise-3c3c9d13ec55)\n\t* [StatsD at Etsy](https://codeascraft.com/2011/02/15/measure-anything-measure-everything/)\n* [Distributed Scheduling](https://www.csee.umbc.edu/courses/graduate/CMSC621/fall02/lectures/ch11.pdf)\n\t* [Distributed Task Scheduling (3 parts) at PagerDuty](https://www.pagerduty.com/eng/distributed-task-scheduling-3/)\n    * [Building Cron at Google](https://landing.google.com/sre/sre-book/chapters/distributed-periodic-scheduling/)\n    * [Distributed Cron Architecture at Quora](https://engineering.quora.com/Quoras-Distributed-Cron-Architecture)\n    * [Chronos: A Replacement for Cron at Airbnb](https://medium.com/airbnb-engineering/chronos-a-replacement-for-cron-f05d7d986a9d)\n    * [Scheduler at Nextdoor](https://engblog.nextdoor.com/we-don-t-run-cron-jobs-at-nextdoor-6f7f9cc62040)\n    * [Peloton: Unified Resource Scheduler for Diverse Cluster Workloads at Uber](https://eng.uber.com/peloton/)\n    * [Fenzo: OSS Scheduler for Apache Mesos Frameworks at Netflix](https://medium.com/netflix-techblog/fenzo-oss-scheduler-for-apache-mesos-frameworks-5c340e77e543)\n    * [Airflow - Workflow Orchestration](https://airflow.apache.org/)\n\t\t* [Airflow at Airbnb](https://medium.com/airbnb-engineering/airflow-a-workflow-management-platform-46318b977fd8)\n\t\t* [Airflow at Adyen](https://www.adyen.com/knowledge-hub/apache-airflow-at-adyen)\n\t\t* [Airflow at Pandora](https://engineering.pandora.com/apache-airflow-at-pandora-1d7a844d68ee)\n        * [Airflow at Robinhood](https://medium.com/robinhood-engineering/why-robinhood-uses-airflow-aed13a9a90c8)\n        * [Airflow at Lyft](https://eng.lyft.com/running-apache-airflow-at-lyft-6e53bb8fccff)\n        * [Airflow at Drivy](https://drivy.engineering/airflow-architecture/)\n\t\t* [Airflow at Grab](https://engineering.grab.com/experimentation-platform-data-pipeline)\n\t\t* [Airflow at Adobe](https://medium.com/adobetech/adobe-experience-platform-orchestration-service-with-apache-airflow-952203723c0b)\n        * [Auditing Airflow Job Runs at Walmart](https://medium.com/walmartlabs/auditing-airflow-batch-jobs-73b45100045)\n        * [MaaT: DAG-based Distributed Task Scheduler at Alibaba](https://hackernoon.com/meet-maat-alibabas-dag-based-distributed-task-scheduler-7c9cf0c83438)\n        * [boundary-layer: Declarative Airflow Workflows at Etsy](https://www.etsy.com/codeascraft/boundary-layer-declarative-airflow-workflows)\n* [Distributed Monitoring and Alerting](https://www.oreilly.com/ideas/monitoring-distributed-systems)\n\t* [Unicorn: Remediation System at eBay](https://www.ebayinc.com/stories/blogs/tech/unicorn-rheos-remediation-center/)\n\t* [M3: Metrics and Monitoring Platform at Uber](https://eng.uber.com/optimizing-m3/)\n\t* [Athena: Automated Build Health Management System at Dropbox](https://blogs.dropbox.com/tech/2019/05/athena-our-automated-build-health-management-system/)\n\t* [Vortex: Monitoring Server Applications at Dropbox](https://blogs.dropbox.com/tech/2019/11/monitoring-server-applications-with-vortex/)\t\n\t* [Nuage: Cloud Management Service at LinkedIn](https://engineering.linkedin.com/blog/2019/solving-manageability-challenges-with-nuage)\n\t* [Telltale: Application Monitoring at Netflix](https://netflixtechblog.com/telltale-netflix-application-monitoring-simplified-5c08bfa780ba)\n\t* [ThirdEye: Monitoring Platform at LinkedIn](https://engineering.linkedin.com/blog/2019/06/smart-alerts-in-thirdeye--linkedins-real-time-monitoring-platfor)\n\t* [Periskop: Exception Monitoring Service at SoundCloud](https://developers.soundcloud.com/blog/periskop-exception-monitoring-service)\n    * [Securitybot: Distributed Alerting Bot at Dropbox](https://blogs.dropbox.com/tech/2017/02/meet-securitybot-open-sourcing-automated-security-at-scale/)\t\n    * [Monitoring System at Alibaba](https://www.usenix.org/conference/srecon18asia/presentation/xinchi)\n    * [Real User Monitoring at Dailymotion](https://medium.com/dailymotion/real-user-monitoring-1948375f8be5)\n    * [Alerting Ecosystem at Uber](https://eng.uber.com/observability-at-scale/)\n\t* [Alerting Framework at Airbnb](https://medium.com/airbnb-engineering/alerting-framework-at-airbnb-35ba48df894f)\n\t* [Alerting on Service-Level Objectives (SLOs) at SoundCloud](https://developers.soundcloud.com/blog/alerting-on-slos)\n    * [Job-based Forecasting Workflow for Observability Anomaly Detection at Uber](https://eng.uber.com/observability-anomaly-detection/)\n\t* [Monitoring and Alert System using Graphite and Cabot at HackerEarth](http://engineering.hackerearth.com/2017/03/21/monitoring-and-alert-system-using-graphite-and-cabot/)\n    * [Observability (2 parts) at Twitter](https://blog.twitter.com/engineering/en_us/a/2016/observability-at-twitter-technical-overview-part-ii.html)\n    * [Distributed Security Alerting at Slack](https://slack.engineering/distributed-security-alerting-c89414c992d6)\n    * [Real-Time News Alerting at Bloomberg](https://www.infoq.com/presentations/news-alerting-bloomberg)\n\t* [Data Pipeline Monitoring System at LinkedIn](https://engineering.linkedin.com/blog/2019/an-inside-look-at-linkedins-data-pipeline-monitoring-system-)\n\t* [Monitoring and Observability at Picnic](https://blog.picnic.nl/monitoring-and-observability-at-picnic-684cefd845c4)\n* [Distributed Security](https://msdn.microsoft.com/en-us/library/cc767123.aspx)\n\t* [Approach to Security at Scale at Dropbox](https://blogs.dropbox.com/tech/2018/02/security-at-scale-the-dropbox-approach/)\n\t* [Aardvark and Repokid: AWS Least Privilege for Distributed, High-Velocity Development at Netflix](https://medium.com/netflix-techblog/introducing-aardvark-and-repokid-53b081bf3a7e)\t\n\t* [LISA: Distributed Firewall at LinkedIn](https://www.slideshare.net/MikeSvoboda/2017-lisa-linkedins-distributed-firewall-dfw)\n\t* [Secure Infrastructure To Store Bitcoin In The Cloud at Coinbase](https://engineering.coinbase.com/how-coinbase-builds-secure-infrastructure-to-store-bitcoin-in-the-cloud-30a6504e40ba)\n\t* [BinaryAlert: Real-time Serverless Malware Detection at Airbnb](https://medium.com/airbnb-engineering/binaryalert-real-time-serverless-malware-detection-ca44370c1b90)\n\t* [Scalable IAM Architecture to Secure Access to 100 AWS Accounts at Segment](https://segment.com/blog/secure-access-to-100-aws-accounts/)\n\t* [OAuth Audit Toolbox at Indeed](http://engineering.indeedblog.com/blog/2018/04/oaudit-toolbox/)\n\t* [Active Directory Password Blacklisting at Yelp](https://engineeringblog.yelp.com/2018/04/ad-password-blacklisting.html)\t\n\t* [Syscall Auditing at Scale at Slack](https://slack.engineering/syscall-auditing-at-scale-e6a3ca8ac1b8)\n\t* [Athenz: Fine-Grained, Role-Based Access Control at Yahoo](https://yahooeng.tumblr.com/post/160481899076/open-sourcing-athenz-fine-grained-role-based)\n\t* [WebAuthn Support for Secure Sign In at Dropbox](https://blogs.dropbox.com/tech/2018/05/introducing-webauthn-support-for-secure-dropbox-sign-in/)\n\t* [Security Development Lifecycle at Slack](https://slack.engineering/moving-fast-and-securing-things-540e6c5ae58a)\n\t* [Unprivileged Container Builds at Kinvolk](https://kinvolk.io/blog/2018/04/towards-unprivileged-container-builds/)\n\t* [Diffy: Differencing Engine for Digital Forensics in the Cloud at Netflix](https://medium.com/netflix-techblog/netflix-sirt-releases-diffy-a-differencing-engine-for-digital-forensics-in-the-cloud-37b71abd2698)\n\t* [Detecting Credential Compromise in AWS at Netflix](https://medium.com/netflix-techblog/netflix-cloud-security-detecting-credential-compromise-in-aws-9493d6fd373a)\n\t* [Scalable User Privacy at Spotify](https://labs.spotify.com/2018/09/18/scalable-user-privacy/)\n\t* [AVA: Audit Web Applications at Indeed](https://engineering.indeedblog.com/blog/2018/09/application-scanning/)\n\t* [TTL as a Service: Automatic Revocation of Stale Privileges at Yelp](https://engineeringblog.yelp.com/2018/11/ttl-as-a-service.html)\n\t* [Enterprise Key Management at Slack](https://slack.engineering/engineering-dive-into-slack-enterprise-key-management-1fce471b178c)\t\n\t* [Scalability and Authentication at Twitch](https://blog.twitch.tv/en/2019/03/15/how-twitch-addresses-scalability-and-authentication/)\n\t* [Edge Authentication and Token-Agnostic Identity Propagation at Netflix](https://netflixtechblog.com/edge-authentication-and-token-agnostic-identity-propagation-514e47e0b602)\n\t* [Hardening Kubernetes Infrastructure with Cilium at Palantir](https://blog.palantir.com/hardening-palantirs-kubernetes-infrastructure-with-cilium-1c40d4c7ef0)\n\t* [Improving Web Vulnerability Management through Automation at Lyft](https://eng.lyft.com/improving-web-vulnerability-management-through-automation-2631570d8415)\n\t* [Clock Skew when Syncing Password Payloads at Drobbox](https://dropbox.tech/application/dropbox-passwords-clock-skew-payload-sync-merge)\n* [Distributed Messaging, Queuing, and Event Streaming](https://arxiv.org/pdf/1704.00411.pdf)\n\t* [Cape: Event Stream Processing Framework at Dropbox](https://blogs.dropbox.com/tech/2017/05/introducing-cape/)\n\t* [Brooklin: Distributed Service for Near Real-Time Data Streaming at LinkedIn](https://engineering.linkedin.com/blog/2019/brooklin-open-source)\n\t* [Samza: Stream Processing System for Latency Insighs at LinkedIn](https://engineering.linkedin.com/blog/2018/04/samza-aeon--latency-insights-for-asynchronous-one-way-flows)\t\n\t* [Bullet: Forward-Looking Query Engine for Streaming Data at Yahoo](https://yahooeng.tumblr.com/post/161855616651/open-sourcing-bullet-yahoos-forward-looking)\n\t* [EventHorizon: Tool for Watching Events Streaming at Etsy](https://codeascraft.com/2018/05/29/the-eventhorizon-saga/)\n\t* [Qmessage: Distributed, Asynchronous Task Queue at Quora](https://engineering.quora.com/Qmessage-Handling-Billions-of-Tasks-Per-Day)\n\t* [Cherami: Message Queue System for Transporting Async Tasks at Uber](https://eng.uber.com/cherami/)\n\t* [Dynein: Distributed Delayed Job Queueing System at Airbnb](https://medium.com/airbnb-engineering/dynein-building-a-distributed-delayed-job-queueing-system-93ab10f05f99)\n\t* [Timestone: Queueing System for Non-Parallelizable Workloads at Netflix](https://netflixtechblog.com/timestone-netflixs-high-throughput-low-latency-priority-queueing-system-with-built-in-support-1abf249ba95f)\n\t* [Messaging Service at Riot Games](https://engineering.riotgames.com/news/riot-messaging-service)\n\t* [Messaging System Model at Dropbox](https://dropbox.tech/infrastructure/infrastructure-messaging-system-model-async-platform-evolution)\n\t* [Debugging Production with Event Logging at Zillow](https://www.zillow.com/engineering/debugging-production-event-logging/)\n\t* [Cross-platform In-app Messaging Orchestration Service at Netflix](https://medium.com/netflix-techblog/building-a-cross-platform-in-app-messaging-orchestration-service-86ba614f92d8)\n\t* [Video Gatekeeper at Netflix](https://medium.com/netflix-techblog/re-architecting-the-video-gatekeeper-f7b0ac2f6b00)\n\t* [Scaling Push Messaging for Millions of Devices at Netflix](https://www.infoq.com/presentations/neflix-push-messaging-scale)\n\t* [Delaying Asynchronous Message Processing with RabbitMQ at Indeed](http://engineering.indeedblog.com/blog/2017/06/delaying-messages/)\t\n\t* [Benchmarking Streaming Computation Engines at Yahoo](https://yahooeng.tumblr.com/post/135321837876/benchmarking-streaming-computation-engines-at)\n\t* [Improving Stream Data Quality With Protobuf Schema Validation at Deliveroo](https://deliveroo.engineering/2019/02/05/improving-stream-data-quality-with-protobuf-schema-validation.html)\n\t* [Scaling Email Infrastructure at Medium](https://medium.engineering/scaling-email-infrastructure-for-medium-digest-254223c883b8)\n\t* [Real-time Messaging at Slack](https://slack.engineering/real-time-messaging/)\n\t* [Event Stream Database at Nike](https://medium.com/nikeengineering/moving-faster-with-aws-by-creating-an-event-stream-database-dedec8ca3eeb)\n\t* [Event Tracking System at Udemy](https://medium.com/udemy-engineering/designing-the-new-event-tracking-system-at-udemy-a45e502216fd)\n    * [Event-Driven Messaging](https://martinfowler.com/articles/201701-event-driven.html)\n        * [Domain-Driven Design at Alibaba](https://medium.com/swlh/creating-coding-excellence-with-domain-driven-design-88f73d2232c3)\n        * [Domain-Driven Design at Weebly](https://medium.com/weebly-engineering/how-to-organize-your-monolith-before-breaking-it-into-services-69cbdb9248b0)\n        * [Domain-Driven Design at Moonpig](https://engineering.moonpig.com/development/modelling-for-domain-driven-design)\n        * [Scaling Event Sourcing for Netflix Downloads](https://www.infoq.com/presentations/netflix-scale-event-sourcing)\n        * [Scaling Event-Sourcing at Jet.com](https://medium.com/@eulerfx/scaling-event-sourcing-at-jet-9c873cac33b8)\n        * [Event Sourcing (2 parts) at eBay](https://www.ebayinc.com/stories/blogs/tech/event-sourcing-in-action-with-ebays-continuous-delivery-team/)\n\t\t* [Event Sourcing at FREE NOW](https://medium.com/inside-freenow/event-sourcing-an-evolutionary-perspective-31e7387aa6f1)\n\t\t* [Scalable content feed using Event Sourcing and CQRS patterns at Brainly](https://medium.com/engineering-brainly/scalable-content-feed-using-event-sourcing-and-cqrs-patterns-e09df98bf977)\n    * [Pub-Sub Messaging](https://aws.amazon.com/pub-sub-messaging/)\n\t\t* [Pulsar: Pub-Sub Messaging at Scale at Yahoo](https://yahooeng.tumblr.com/post/150078336821/open-sourcing-pulsar-pub-sub-messaging-at-scale)\n\t\t* [Wormhole: Pub-Sub System at Facebook](https://code.facebook.com/posts/188966771280871/wormhole-pub-sub-system-moving-data-through-space-and-time/)\n\t\t* [MemQ: Cloud Native Pub-Sub System at Pinterest](https://medium.com/pinterest-engineering/memq-an-efficient-scalable-cloud-native-pubsub-system-4402695dd4e7)\n\t\t* [Pub-Sub in Microservices at Netflix](https://medium.com/netflix-techblog/how-netflix-microservices-tackle-dataset-pub-sub-4a068adcc9a)\n\t* [Kafka - Message Broker](https://martin.kleppmann.com/papers/kafka-debull15.pdf)\t\n\t\t* [Kafka at LinkedIn](https://engineering.linkedin.com/kafka/running-kafka-scale)\n\t\t* [Kafka at Pinterest](https://medium.com/pinterest-engineering/how-pinterest-runs-kafka-at-scale-ff9c6f735be)\n\t\t* [Kafka at Trello](https://tech.trello.com/why-we-chose-kafka/)\t\n\t\t* [Kafka at Salesforce](https://engineering.salesforce.com/how-apache-kafka-inspired-our-platform-events-architecture-2f351fe4cf63)\n\t\t* [Kafka at The New York Times](https://open.nytimes.com/publishing-with-apache-kafka-at-the-new-york-times-7f0e3b7d2077)\n\t\t* [Kafka at Yelp](https://engineeringblog.yelp.com/2016/07/billions-of-messages-a-day-yelps-real-time-data-pipeline.html)\n\t\t* [Kafka at Criteo](https://medium.com/criteo-labs/upgrading-kafka-on-a-large-infra-3ee99f56e970)\n\t\t* [Kafka on Kubernetes at Shopify](https://shopifyengineering.myshopify.com/blogs/engineering/running-apache-kafka-on-kubernetes-at-shopify)\n\t\t* [Kafka on PaaSTA: Running Kafka on Kubernetes at Yelp (2 parts)](https://engineeringblog.yelp.com/2022/03/kafka-on-paasta-part-two.html)\n\t\t* [Migrating Kafka's Zookeeper with No Downtime at Yelp](https://engineeringblog.yelp.com/2019/01/migrating-kafkas-zookeeper-with-no-downtime.html)\n\t\t* [Reprocessing and Dead Letter Queues with Kafka at Uber](https://eng.uber.com/reliable-reprocessing/)\n\t\t* [Chaperone: Audit Kafka End-to-End at Uber](https://eng.uber.com/chaperone/)\n\t\t* [Finding Kafka throughput limit in infrastructure at Dropbox](https://blogs.dropbox.com/tech/2019/01/finding-kafkas-throughput-limit-in-dropbox-infrastructure/)\n\t\t* [Cost Orchestration at Walmart](https://medium.com/walmartlabs/cost-orchestration-at-walmart-f34918af67c4)\n\t\t* [InfluxDB and Kafka to Scale to Over 1 Million Metrics a Second at Hulu](https://medium.com/hulu-tech-blog/how-hulu-uses-influxdb-and-kafka-to-scale-to-over-1-million-metrics-a-second-1721476aaff5)\n\t\t* [Scaling Kafka to Support Data Growth at PayPal](https://medium.com/paypal-tech/scaling-kafka-to-support-paypals-data-growth-a0b4da420fab)\n\t* [Stream Data Deduplication](https://en.wikipedia.org/wiki/Data_deduplication)\n\t\t* [Exactly-once Semantics with Kafka](https://www.confluent.io/blog/exactly-once-semantics-are-possible-heres-how-apache-kafka-does-it/)\n\t\t* [Real-time Deduping at Tapjoy](http://eng.tapjoy.com/blog-list/real-time-deduping-at-scale)\n\t\t* [Deduplication at Segment](https://segment.com/blog/exactly-once-delivery/)\n\t\t* [Deduplication at Mail.Ru](https://medium.com/@andrewsumin/efficient-storage-how-we-went-down-from-50-pb-to-32-pb-99f9c61bf6b4)\n\t\t* [Petabyte Scale Data Deduplication at Mixpanel](https://medium.com/mixpaneleng/petabyte-scale-data-deduplication-mixpanel-engineering-e808c70c99f8)\n* [Distributed Logging](https://blog.codinghorror.com/the-problem-with-logging/)\n\t* [Logging at LinkedIn](https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying)\n\t* [Scalable and Reliable Log Ingestion at Pinterest](https://medium.com/@Pinterest_Engineering/scalable-and-reliable-data-ingestion-at-pinterest-b921c2ee8754)\n\t* [High-performance Replicated Log Service at Twitter](https://blog.twitter.com/engineering/en_us/topics/infrastructure/2015/building-distributedlog-twitter-s-high-performance-replicated-log-servic.html)\n\t* [Logging Service with Spark at CERN Accelerator](https://databricks.com/blog/2017/12/14/the-architecture-of-the-next-cern-accelerator-logging-service.html)\n\t* [Logging and Aggregation at Quora](https://engineering.quora.com/Logging-and-Aggregation-at-Quora)\n\t* [Collection and Analysis of Daemon Logs at Badoo](https://badoo.com/techblog/blog/2016/06/06/collection-and-analysis-of-daemon-logs-at-badoo/)\n\t* [Log Parsing with Static Code Analysis at Palantir](https://medium.com/palantir/using-static-code-analysis-to-improve-log-parsing-18f0d1843965)\t\t\n\t* [Centralized Application Logging at eBay](https://tech.ebayinc.com/engineering/low-latency-and-high-throughput-cal-ingress/)\n\t* [Enrich VPC Flow Logs at Hyper Scale to provide Network Insight at Netflix](https://netflixtechblog.com/hyper-scale-vpc-flow-logs-enrichment-to-provide-network-insight-e5f1db02910d)\t\n\t* [BookKeeper: Distributed Log Storage at Yahoo](https://yahooeng.tumblr.com/post/109908973316/bookkeeper-yahoos-distributed-log-storage-is)\n\t* [LogDevice: Distributed Data Store for Logs at Facebook](https://code.facebook.com/posts/357056558062811/logdevice-a-distributed-data-store-for-logs/)\n\t* [LogFeeder: Log Collection System at Yelp](https://engineeringblog.yelp.com/2018/03/introducing-logfeeder.html)\n\t* [DBLog: Generic Change-Data-Capture Framework at Netflix](https://medium.com/netflix-techblog/dblog-a-generic-change-data-capture-framework-69351fb9099b)\t\n* [Distributed Searching](http://nwds.cs.washington.edu/files/nwds/pdf/Distributed-WR.pdf)\n\t* [Search Architecture at Instagram](https://instagram-engineering.com/search-architecture-eeb34a936d3a)\n\t* [Search Architecture at eBay](http://www.cs.otago.ac.nz/homepages/andrew/papers/2017-8.pdf)\n\t* [Search Architecture at Box](https://medium.com/box-tech-blog/scaling-box-search-using-lumos-22d9e0cb4175)\n\t* [Search Discovery Indexing Platform at Coupang](https://medium.com/coupang-tech/the-evolution-of-search-discovery-indexing-platform-fa43e41305f9)\n\t* [Universal Search System at Pinterest](https://medium.com/pinterest-engineering/building-a-universal-search-system-for-pinterest-e4cb03a898d4)\n\t* [Improving Search Engine Efficiency by over 25% at eBay](https://www.ebayinc.com/stories/blogs/tech/making-e-commerce-search-faster/)\t\n\t* [Indexing and Querying Telemetry Logs with Lucene at Palantir](https://medium.com/palantir/indexing-and-querying-telemetry-logs-with-lucene-234c5ce3e5f3)\n\t* [Query Understanding at TripAdvisor](https://www.tripadvisor.com/engineering/query-understanding-at-tripadvisor/)\n\t* [Search Federation Architecture at LinkedIn (2018)](https://engineering.linkedin.com/blog/2018/03/search-federation-architecture-at-linkedin)\n\t* [Search at Slack](https://slack.engineering/search-at-slack-431f8c80619e)\n\t* [Search Engine at DoorDash](https://careersatdoordash.com/blog/introducing-doordashs-in-house-search-engine/)\n\t* [Stability and Scalability for Search at Twitter](https://blog.twitter.com/engineering/en_us/topics/infrastructure/2022/stability-and-scalability-for-search)\n\t* [Search Service at Twitter (2014)](https://blog.twitter.com/engineering/en_us/a/2014/building-a-complete-tweet-index.html)\n\t* [Autocomplete Search (2 parts) at Traveloka](https://medium.com/traveloka-engineering/high-quality-autocomplete-search-part-2-d5b15bb0dadf)\n\t* [Data-Driven Autocorrection System at Canva](https://product.canva.com/building-a-data-driven-autocorrection-system/)\n\t* [Adapting Search to Indian Phonetics at Flipkart](https://blog.flipkart.tech/adapting-search-to-indian-phonetics-cdbe65259686)\n\t* [Nautilus: Search Engine at Dropbox](https://blogs.dropbox.com/tech/2018/09/architecture-of-nautilus-the-new-dropbox-search-engine/)\n\t* [Galene: Search Architecture of LinkedIn](https://engineering.linkedin.com/search/did-you-mean-galene)\n\t* [Manas: High Performing Customized Search System at Pinterest](https://medium.com/@Pinterest_Engineering/manas-a-high-performing-customized-search-system-cf189f6ca40f)\n\t* [Sherlock: Near Real Time Search Indexing at Flipkart](https://blog.flipkart.tech/sherlock-near-real-time-search-indexing-95519783859d)\n\t* [Nebula: Storage Platform to Build Search Backends at Airbnb](https://medium.com/airbnb-engineering/nebula-as-a-storage-platform-to-build-airbnbs-search-backends-ecc577b05f06)\n\t* [ELK (Elasticsearch, Logstash, Kibana) Stack](https://logz.io/blog/15-tech-companies-chose-elk-stack/)\n\t\t* [Predictions in Real Time with ELK at Uber](https://eng.uber.com/elk/)\n\t\t* [Building a scalable ELK stack at Envato](https://webuild.envato.com/blog/building-a-scalable-elk-stack/)\n\t\t* [ELK at Robinhood](https://robinhood.engineering/taming-elk-4e1349f077c3)\n\t\t* [Scaling Elasticsearch Clusters at Uber](https://www.infoq.com/presentations/uber-elasticsearch-clusters?utm_source=presentations_about_Case_Study&utm_medium=link&utm_campaign=Case_Study)\n\t\t* [Elasticsearch Performance Tuning Practice at eBay](https://www.ebayinc.com/stories/blogs/tech/elasticsearch-performance-tuning-practice-at-ebay/)\n\t\t* [Improve Performance using Elasticsearch Plugins (2 parts) at Tinder](https://medium.com/tinder-engineering/how-we-improved-our-performance-using-elasticsearch-plugins-part-2-b051da2ee85b)\n\t\t* [Elasticsearch at Kickstarter](https://kickstarter.engineering/elasticsearch-at-kickstarter-db3c487887fc)\n\t\t* [Log Parsing with Logstash and Google Protocol Buffers at Trivago](https://tech.trivago.com/2016/01/19/logstash_protobuf_codec/)\n\t\t* [Fast Order Search using Data Pipeline and Elasticsearch at Yelp](https://engineeringblog.yelp.com/2018/06/fast-order-search.html)\n\t\t* [Moving Core Business Search to Elasticsearch at Yelp](https://engineeringblog.yelp.com/2017/06/moving-yelps-core-business-search-to-elasticsearch.html)\n\t\t* [Sharding out Elasticsearch at Vinted](http://engineering.vinted.com/2017/06/05/sharding-out-elasticsearch/)\n\t\t* [Self-Ranking Search with Elasticsearch at Wattpad](http://engineering.wattpad.com/post/146216619727/self-ranking-search-with-elasticsearch-at-wattpad)\n\t\t* [Vulcanizer: a library for operating Elasticsearch at Github](https://github.blog/2019-03-05-vulcanizer-a-library-for-operating-elasticsearch/)\t\n* [Distributed Storage](http://highscalability.com/blog/2011/11/1/finding-the-right-data-solution-for-your-application-in-the.html)\n\t* [In-memory Storage](https://medium.com/@denisanikin/what-an-in-memory-database-is-and-how-it-persists-data-efficiently-f43868cff4c1)\n\t\t* [MemSQL Architecture - The Fast (MVCC, InMem, LockFree, CodeGen) And Familiar (SQL)](http://highscalability.com/blog/2012/8/14/memsql-architecture-the-fast-mvcc-inmem-lockfree-codegen-and.html)\n\t\t* [Optimizing Memcached Efficiency at Quora](https://engineering.quora.com/Optimizing-Memcached-Efficiency)\n\t\t* [Real-Time Data Warehouse with MemSQL on Cisco UCS](https://blogs.cisco.com/datacenter/memsql)\n\t\t* [Moving to MemSQL at Tapjoy](http://eng.tapjoy.com/blog-list/moving-to-memsql)\n\t\t* [MemSQL and Kinesis for Real-time Insights at Disney](https://conferences.oreilly.com/strata/strata-ca/public/schedule/detail/68131)\n\t\t* [MemSQL to Query Hundreds of Billions of Rows in a Dashboard at Pandora](https://engineering.pandora.com/using-memsql-at-pandora-79a86cb09b57)\n\t* [Object Storage](http://www.datacenterknowledge.com/archives/2013/10/04/object-storage-the-future-of-scale-out)\n\t\t* [Scaling HDFS at Uber](https://eng.uber.com/scaling-hdfs/)\n\t\t* [Reasons for Choosing S3 over HDFS at Databricks](https://databricks.com/blog/2017/05/31/top-5-reasons-for-choosing-s3-over-hdfs.html)\n\t\t* [File System on Amazon S3 at Quantcast](https://www.quantcast.com/blog/quantcast-file-system-on-amazon-s3/)\n\t\t* [Image Recovery at Scale Using S3 Versioning at Trivago](https://tech.trivago.com/2018/09/03/efficient-image-recovery-at-scale-using-amazon-s3-versioning/)\n\t\t* [Cloud Object Store at Yahoo](https://yahooeng.tumblr.com/post/116391291701/yahoo-cloud-object-store-object-storage-at)\n\t\t* [Ambry: Distributed Immutable Object Store at LinkedIn](https://www.usenix.org/conference/srecon17americas/program/presentation/shenoy)\n\t\t* [Dynamometer: Scale Testing HDFS on Minimal Hardware with Maximum Fidelity at LinkedIn](https://engineering.linkedin.com/blog/2018/02/dynamometer--scale-testing-hdfs-on-minimal-hardware-with-maximum)\n\t\t* [Hammerspace: Persistent, Concurrent, Off-heap Storage at Airbnb](https://medium.com/airbnb-engineering/hammerspace-persistent-concurrent-off-heap-storage-3db39bb04472)\n\t\t* [MezzFS: Mounting Object Storage in Media Processing Platform at Netflix](https://medium.com/netflix-techblog/mezzfs-mounting-object-storage-in-netflixs-media-processing-platform-cda01c446ba)\t\n\t\t* [Magic Pocket: In-house Multi-exabyte Storage System at Dropbox](https://blogs.dropbox.com/tech/2016/05/inside-the-magic-pocket/)\n* [Relational Databases](https://www.mysql.com/products/cluster/scalability.html)\n\t* [MySQL at Uber](https://www.uber.com/en-SG/blog/mysql-at-uber/)\n\t* [MySQL at Pinterest](https://medium.com/@Pinterest_Engineering/learn-to-stop-using-shiny-new-things-and-love-mysql-3e1613c2ce14)\n\t* [PostgreSQL at Twitch](https://blog.twitch.tv/en/2016/10/11/how-twitch-uses-postgresql-c34aa9e56f58)\n\t* [Scaling MySQL-based Financial Reporting System at Airbnb](https://medium.com/airbnb-engineering/tracking-the-money-scaling-financial-reporting-at-airbnb-6d742b80f040)\n\t* [Scaling MySQL at Wix](https://www.wix.engineering/post/scaling-to-100m-mysql-is-a-better-nosql)\n\t* [Building and Deploying MySQL Raft at Meta](https://engineering.fb.com/2023/05/16/data-infrastructure/mysql-raft-meta/)\n\t* [MaxScale (MySQL) Database Proxy at Airbnb](https://medium.com/airbnb-engineering/unlocking-horizontal-scalability-in-our-web-serving-tier-d907449cdbcf)\n\t* [Switching from Postgres to MySQL at Uber](https://www.uber.com/en-NL/blog/postgres-to-mysql-migration/)\n\t* [Handling Growth with Postgres at Instagram](https://engineering.instagram.com/handling-growth-with-postgres-5-tips-from-instagram-d5d7e7ffdfcb)\n\t* [Scaling the Analytics Database (Postgres) at TransferWise](http://tech.transferwise.com/scaling-our-analytics-database/)\n\t* [Updating a 50 Terabyte PostgreSQL Database at Adyen](https://medium.com/adyen/updating-a-50-terabyte-postgresql-database-f64384b799e7)\n\t* [Scaling Database Access for 100s of Billions of Queries per Day at PayPal](https://medium.com/paypal-engineering/scaling-database-access-for-100s-of-billions-of-queries-per-day-paypal-introducing-hera-e192adacda54)\n\t* [Minimizing Read-Write MySQL Downtime at Yelp](https://engineeringblog.yelp.com/2020/11/minimizing-read-write-mysql-downtime.html)\n\t* [Migrating MySQL from 5.6 to 8.0 at Facebook](https://engineering.fb.com/2021/07/22/data-infrastructure/mysql/)\n\t* [Migration from HBase to MyRocks at Quora](https://quoraengineering.quora.com/Migration-from-HBase-to-MyRocks-at-Quora)\n\t* [Replication](https://docs.microsoft.com/en-us/sql/relational-databases/replication/types-of-replication)\n\t\t* [MySQL Parallel Replication (4 parts) at Booking.com](https://medium.com/booking-com-infrastructure/evaluating-mysql-parallel-replication-part-4-annex-under-the-hood-eb456cf8b2fb)\n\t\t* [Mitigating MySQL Replication Lag and Reducing Read Load at Github](https://githubengineering.com/mitigating-replication-lag-and-reducing-read-load-with-freno/)\n\t\t* [Read Consistency with Database Replicas at Shopify](https://shopify.engineering/read-consistency-database-replicas)\n\t\t* [Black-Box Auditing: Verifying End-to-End Replication Integrity between MySQL and Redshift at Yelp](https://engineeringblog.yelp.com/2018/04/black-box-auditing.html)\n\t\t* [Partitioning Main MySQL Database at Airbnb](https://medium.com/airbnb-engineering/how-we-partitioned-airbnb-s-main-database-in-two-weeks-55f7e006ff21)\n\t\t* [Herb: Multi-DC Replication Engine for Schemaless Datastore at Uber](https://eng.uber.com/herb-datacenter-replication/)\n\t* [Sharding](https://quabase.sei.cmu.edu/mediawiki/index.php/Shard_data_set_across_multiple_servers_(Range-based))\n\t\t* [Sharding MySQL at Pinterest](https://medium.com/@Pinterest_Engineering/sharding-pinterest-how-we-scaled-our-mysql-fleet-3f341e96ca6f)\n\t\t* [Sharding MySQL at Twilio](https://www.twilio.com/engineering/2014/06/26/how-we-replaced-our-data-pipeline-with-zero-downtime)\n\t\t* [Sharding MySQL at Square](https://medium.com/square-corner-blog/sharding-cash-10280fa3ef3b)\n\t\t* [Sharding MySQL at Quora](https://www.quora.com/q/quoraengineering/MySQL-sharding-at-Quora)\n\t\t* [Sharding Layer of Schemaless Datastore at Uber](https://eng.uber.com/schemaless-rewrite/)\n\t\t* [Sharding & IDs at Instagram](https://instagram-engineering.com/sharding-ids-at-instagram-1cf5a71e5a5c)\n\t\t* [Sharding Postgres at Notion](https://www.notion.so/blog/sharding-postgres-at-notion)\n\t\t* [Solr: Improving Performance for Batch Indexing at Box](https://blog.box.com/blog/solr-improving-performance-batch-indexing/)\t\n\t\t* [Geosharded Recommendations (3 parts) at Tinder](https://medium.com/tinder-engineering/geosharded-recommendations-part-3-consistency-2d2cb2f0594b)\n\t\t* [Scaling Services with Shard Manager at Facebook](https://engineering.fb.com/production-engineering/scaling-services-with-shard-manager/)\n\t* [Presto the Distributed SQL Query Engine](https://research.fb.com/wp-content/uploads/2019/03/Presto-SQL-on-Everything.pdf?)\n\t\t* [Presto at Pinterest](https://medium.com/@Pinterest_Engineering/presto-at-pinterest-a8bda7515e52)\n\t\t* [Presto Infrastructure at Lyft](https://eng.lyft.com/presto-infrastructure-at-lyft-b10adb9db01)\n\t\t* [Presto at Grab](https://engineering.grab.com/scaling-like-a-boss-with-presto)\n\t\t* [Engineering Data Analytics with Presto and Apache Parquet at Uber](https://eng.uber.com/presto/)\n\t\t* [Data Wrangling at Slack](https://slack.engineering/data-wrangling-at-slack-f2e0ff633b69)\n\t\t* [Presto in Big Data Platform on AWS at Netflix](https://medium.com/netflix-techblog/using-presto-in-our-big-data-platform-on-aws-938035909fd4)\n\t\t* [Presto Auto Scaling at Eventbrite](https://www.eventbrite.com/engineering/big-data-workloads-presto-auto-scaling/)\n\t\t* [Speed Up Presto with Alluxio Local Cache at Uber](https://www.uber.com/en-MY/blog/speed-up-presto-with-alluxio-local-cache/)\n* [NoSQL Databases](https://www.thoughtworks.com/insights/blog/nosql-databases-overview)\n\t* [Key-Value Databases](http://www.cs.ucsb.edu/~agrawal/fall2009/dynamo.pdf)\n\t\t* [DynamoDB at Nike](https://medium.com/nikeengineering/becoming-a-nimble-giant-how-dynamo-db-serves-nike-at-scale-4cc375dbb18e)\n\t\t* [DynamoDB at Segment](https://segment.com/blog/the-million-dollar-eng-problem/)\n\t\t* [DynamoDB at Mapbox](https://blog.mapbox.com/scaling-mapbox-infrastructure-with-dynamodb-streams-d53eabc5e972)\n\t\t* [Manhattan: Distributed Key-Value Database at Twitter](https://blog.twitter.com/engineering/en_us/a/2014/manhattan-our-real-time-multi-tenant-distributed-database-for-twitter-scale.html)\n\t\t* [Sherpa: Distributed NoSQL Key-Value Store at Yahoo](https://yahooeng.tumblr.com/post/120730204806/sherpa-scales-new-heights)\n\t\t* [HaloDB: Embedded Key-Value Storage Engine at Yahoo](https://yahooeng.tumblr.com/post/178262468576/introducing-halodb-a-fast-embedded-key-value)\n\t\t* [MPH: Fast and Compact Immutable Key-Value Stores at Indeed](http://engineering.indeedblog.com/blog/2018/02/indeed-mph/)\n\t\t* [Venice: Distributed Key-Value Database at Linkedin](https://engineering.linkedin.com/blog/2017/02/building-venice-with-apache-helix)\n\t* [Columnar Databases](https://aws.amazon.com/nosql/columnar/)\n\t\t* [Cassandra](http://www.cs.cornell.edu/projects/ladis2009/papers/lakshman-ladis2009.pdf)\n\t\t\t* [Cassandra at Instagram](https://www.slideshare.net/DataStax/cassandra-at-instagram-2016)\n\t\t\t* [Storing Images in Cassandra at Walmart](https://medium.com/walmartlabs/building-object-store-storing-images-in-cassandra-walmart-scale-a6b9c02af593)\n\t\t\t* [Storing Messages with Cassandra at Discord](https://blog.discordapp.com/how-discord-stores-billions-of-messages-7fa6ec7ee4c7)\n\t\t\t* [Scaling Cassandra Cluster at Walmart](https://medium.com/walmartlabs/avoid-pitfalls-in-scaling-your-cassandra-cluster-lessons-and-remedies-a71ca01f8c04)\n\t\t\t* [Scaling Ad Analytics with Cassandra at Yelp](https://engineeringblog.yelp.com/2016/08/how-we-scaled-our-ad-analytics-with-cassandra.html)\n\t\t\t* [Scaling to 100+ Million Reads/Writes using Spark and Cassandra at Dream11](https://medium.com/dream11-tech-blog/leaderboard-dream11-4efc6f93c23e)\t\t\n\t\t\t* [Moving Food Feed from Redis to Cassandra at Zomato](https://www.zomato.com/blog/how-we-moved-our-food-feed-from-redis-to-cassandra)\n\t\t\t* [Benchmarking Cassandra Scalability on AWS at Netflix](https://medium.com/netflix-techblog/benchmarking-cassandra-scalability-on-aws-over-a-million-writes-per-second-39f45f066c9e)\n\t\t\t* [Service Decomposition at Scale with Cassandra at Intuit QuickBooks](https://quickbooks-engineering.intuit.com/service-decomposition-at-scale-70405ac2f637)\n\t\t\t* [Cassandra for Keeping Counts In Sync at SoundCloud](https://developers.soundcloud.com/blog/keeping-counts-in-sync)\n\t\t\t* [Cassandra Driver Configuration for Improved Performance and Load Balancing at Glassdoor](https://medium.com/glassdoor-engineering/cassandra-driver-configuration-for-improved-performance-and-load-balancing-1b0106ce12bb)\n\t\t\t* [cstar: Cassandra Orchestration Tool at Spotify](https://labs.spotify.com/2018/09/04/introducing-cstar-the-spotify-cassandra-orchestration-tool-now-open-source/)\n\t\t* [HBase](https://hbase.apache.org/)\n\t\t\t* [HBase at Salesforce](https://engineering.salesforce.com/investing-in-big-data-apache-hbase-b9d98661a66b)\n\t\t\t* [HBase in Facebook Messages](https://www.facebook.com/notes/facebook-engineering/the-underlying-technology-of-messages/454991608919/)\n\t\t\t* [HBase in Imgur Notification](https://blog.imgur.com/2015/09/15/tech-tuesday-imgur-notifications-from-mysql-to-hbase/)\n\t\t\t* [Improving HBase Backup Efficiency at Pinterest](https://medium.com/@Pinterest_Engineering/improving-hbase-backup-efficiency-at-pinterest-86159da4b954)\n\t\t\t* [HBase at Xiaomi](https://www.slideshare.net/HBaseCon/hbase-practice-at-xiaomi)\n\t\t* [Redshift](https://www.allthingsdistributed.com/2018/11/amazon-redshift-performance-optimization.html)\n\t\t\t* [Redshift at GIPHY](https://engineering.giphy.com/scaling-redshift-without-scaling-costs/)\n\t\t\t* [Redshift at Hudl](https://www.hudl.com/bits/the-low-hanging-fruit-of-redshift-performance)\n\t\t\t* [Redshift at Drivy](https://drivy.engineering/redshift_tips_ticks_part_1/)\n\t* [Document Databases](https://msdn.microsoft.com/en-us/magazine/hh547103.aspx)\n\t\t* [eBay: Building Mission-Critical Multi-Data Center Applications with MongoDB](https://www.mongodb.com/blog/post/ebay-building-mission-critical-multi-data-center-applications-with-mongodb)\n\t\t* [MongoDB at Baidu: Multi-Tenant Cluster Storing 200+ Billion Documents across 160 Shards](https://www.mongodb.com/blog/post/mongodb-at-baidu-powering-100-apps-across-600-nodes-at-pb-scale)\n\t\t* [Migrating Mongo Data at Addepar](https://medium.com/build-addepar/migrating-mountains-of-mongo-data-63e530539952)\n\t\t* [The AWS and MongoDB Infrastructure of Parse (acquired by Facebook)](https://medium.baqend.com/parse-is-gone-a-few-secrets-about-their-infrastructure-91b3ab2fcf71)\n\t\t* [Migrating Mountains of Mongo Data at Addepar](https://medium.com/build-addepar/migrating-mountains-of-mongo-data-63e530539952)\n\t\t* [Couchbase Ecosystem at LinkedIn](https://engineering.linkedin.com/blog/2017/12/couchbase-ecosystem-at-linkedin)\n\t\t* [SimpleDB at Zendesk](https://medium.com/zendesk-engineering/resurrecting-amazon-simpledb-9404034ec506)\n\t\t* [Espresso: Distributed Document Store at LinkedIn](https://engineering.linkedin.com/espresso/introducing-espresso-linkedins-hot-new-distributed-document-store)\n\t* [Graph Databases](https://www.eecs.harvard.edu/margo/papers/systor13-bench/)\n\t\t* [FlockDB: Distributed Graph Database at Twitter](https://blog.twitter.com/engineering/en_us/a/2010/introducing-flockdb.html)\n\t\t* [TAO: Distributed Data Store for the Social Graph at Facebook](https://www.cs.cmu.edu/~pavlo/courses/fall2013/static/papers/11730-atc13-bronson.pdf)\n\t\t* [Akutan: Distributed Knowledge Graph Store at eBay](https://tech.ebayinc.com/engineering/akutan-a-distributed-knowledge-graph-store/)\n* [Time Series Databases](https://www.influxdata.com/time-series-database/)\n\t* [Beringei: High-performance Time Series Storage Engine at Facebook](https://code.facebook.com/posts/952820474848503/beringei-a-high-performance-time-series-storage-engine/)\n\t* [MetricsDB: TimeSeries Database for storing metrics at Twitter](https://blog.twitter.com/engineering/en_us/topics/infrastructure/2019/metricsdb.html)\t\n\t* [Atlas: In-memory Dimensional Time Series Database at Netflix](https://medium.com/netflix-techblog/introducing-atlas-netflixs-primary-telemetry-platform-bd31f4d8ed9a)\n\t* [Heroic: Time Series Database at Spotify](https://labs.spotify.com/2015/11/17/monitoring-at-spotify-introducing-heroic/)\n\t* [Roshi: Distributed Storage System for Time-Series Event at SoundCloud](https://developers.soundcloud.com/blog/roshi-a-crdt-system-for-timestamped-events)\n\t* [Goku: Time Series Database at Pinterest](https://medium.com/@Pinterest_Engineering/goku-building-a-scalable-and-high-performant-time-series-database-system-a8ff5758a181)\n\t* [Scaling Time Series Data Storage (2 parts) at Netflix](https://medium.com/netflix-techblog/scaling-time-series-data-storage-part-ii-d67939655586)\n\t* [Time Series Data Abstraction Layer at Netflix](https://netflixtechblog.com/introducing-netflix-timeseries-data-abstraction-layer-31552f6326f8)\n\t* [Druid - Real-time Analytics Database](https://druid.apache.org/)\n\t\t* [Druid at Airbnb](https://medium.com/airbnb-engineering/druid-airbnb-data-platform-601c312f2a4c)\n\t\t* [Druid at Walmart](https://medium.com/walmartlabs/event-stream-analytics-at-walmart-with-druid-dcf1a37ceda7)\n\t\t* [Druid at eBay](https://tech.ebayinc.com/engineering/monitoring-at-ebay-with-druid/)\n\t\t* [Druid at Netflix](https://netflixtechblog.com/how-netflix-uses-druid-for-real-time-insights-to-ensure-a-high-quality-experience-19e1e8568d06)\n* [Distributed Repositories, Dependencies, and Configurations Management](https://betterexplained.com/articles/intro-to-distributed-version-control-illustrated/)\n\t* [DGit: Distributed Git at Github](https://githubengineering.com/introducing-dgit/)\n\t* [Stemma: Distributed Git Server at Palantir](https://medium.com/@palantir/stemma-distributed-git-server-70afbca0fc29)\n\t* [Configuration Management for Distributed Systems at Flickr](https://code.flickr.net/2016/03/24/configuration-management-for-distributed-systems-using-github-and-cfg4j/)\n\t* [Git Repository at Microsoft](https://blogs.msdn.microsoft.com/bharry/2017/05/24/the-largest-git-repo-on-the-planet/)\n\t* [Solve Git Problem with Large Repositories at Microsoft](https://www.infoq.com/news/2017/02/GVFS)\t\n\t* [Single Repository at Google](https://cacm.acm.org/magazines/2016/7/204032-why-google-stores-billions-of-lines-of-code-in-a-single-repository/fulltext)\t\n\t* [Scaling Infrastructure and (Git) Workflow at Adyen](https://medium.com/adyen/from-0-100-billion-scaling-infrastructure-and-workflow-at-adyen-7b63b690dfb6)\t\n\t* [Dotfiles Distribution at Booking.com](https://medium.com/booking-com-infrastructure/dotfiles-distribution-dedb69c66a75)\n\t* [Secret Detector: Preventing Secrets in Source Code at Yelp](https://engineeringblog.yelp.com/2018/06/yelps-secret-detector.html)\n\t* [Managing Software Dependency at Scale at LinkedIn](https://engineering.linkedin.com/blog/2018/09/managing-software-dependency-at-scale)\n\t* [Merging Code in High-velocity Repositories at LinkedIn](https://engineering.linkedin.com/blog/2020/continuous-integration)\n\t* [Dynamic Configuration at Twitter](https://blog.twitter.com/engineering/en_us/topics/infrastructure/2018/dynamic-configuration-at-twitter.html)\n\t* [Dynamic Configuration at Mixpanel](https://medium.com/mixpaneleng/dynamic-configuration-at-mixpanel-94bfcf97d6b8)\n\t* [Dynamic Configuration at GoDaddy](https://sg.godaddy.com/engineering/2019/03/06/dynamic-configuration-for-nodejs/)\n\t* [Fleet Management (3 parts) at Spotify](https://engineering.atspotify.com/2023/5/fleet-management-at-spotify-part-3-fleet-wide-refactoring)\n* [Scaling Continuous Integration and Continuous Delivery](https://www.synopsys.com/blogs/software-security/agile-cicd-devops-glossary/)\n\t* [Continuous Integration Stack at Facebook](https://code.fb.com/web/rapid-release-at-massive-scale/)\n\t* [Continuous Integration with Distributed Repositories and Dependencies at Netflix](https://medium.com/netflix-techblog/towards-true-continuous-integration-distributed-repositories-and-dependencies-2a2e3108c051)\n\t* [Continuous Integration and Deployment with Bazel at Dropbox](https://blogs.dropbox.com/tech/2019/12/continuous-integration-and-deployment-with-bazel/)\n\t* [Adopting Bazel for Web at Airbnb](https://medium.com/airbnb-engineering/adopting-bazel-for-web-at-scale-a784b2dbe325)\n\t* [Continuous Deployments at BuzzFeed](https://tech.buzzfeed.com/continuous-deployments-at-buzzfeed-d171f76c1ac4)\n\t* [Screwdriver: Continuous Delivery Build System for Dynamic Infrastructure at Yahoo](https://yahooeng.tumblr.com/post/155765242061/open-sourcing-screwdriver-yahoos-continuous)\n\t* [CI/CD at Betterment](https://www.betterment.com/resources/ci-cd-shortening-the-feedback-loop/)\n\t* [CI/CD at Brainly](https://medium.com/engineering-brainly/ci-cd-at-scale-fdfb0f49e031)\n\t* [Scaling iOS CI with Anka at Shopify](https://engineering.shopify.com/blogs/engineering/scaling-ios-ci-with-anka)\n\t* [Scaling Jira Server at Yelp](https://engineeringblog.yelp.com/2019/04/Scaling-Jira-Server-Administration-For-The-Enterprise.html)\n\t* [Auto-scaling CI/CD cluster at Flexport](https://flexport.engineering/how-flexport-halved-testing-costs-with-an-auto-scaling-ci-cd-cluster-8304297222f)\n\n## Availability\n* [Resilience Engineering: Learning to Embrace Failure](https://queue.acm.org/detail.cfm?id=2371297)\t\n\t* [Resilience Engineering with Project Waterbear at LinkedIn](https://engineering.linkedin.com/blog/2017/11/resilience-engineering-at-linkedin-with-project-waterbear)\n\t* [Resiliency against Traffic Oversaturation at iHeartRadio](https://tech.iheart.com/resiliency-against-traffic-oversaturation-77c5ed92a5fb)\n\t* [Resiliency in Distributed Systems at GO-JEK](https://blog.gojekengineering.com/resiliency-in-distributed-systems-efd30f74baf4)\n\t* [Practical NoSQL Resilience Design Pattern for the Enterprise at eBay](https://www.ebayinc.com/stories/blogs/tech/practical-nosql-resilience-design-pattern-for-the-enterprise/)\n\t* [Ensuring Resilience to Disaster at Quora](https://engineering.quora.com/Ensuring-Quoras-Resilience-to-Disaster)\n\t* [Site Resiliency at Expedia](https://www.infoq.com/presentations/expedia-website-resiliency?utm_source=presentations_about_Case_Study&utm_medium=link&utm_campaign=Case_Study)\n\t* [Resiliency and Disaster Recovery with Kafka at eBay](https://tech.ebayinc.com/engineering/resiliency-and-disaster-recovery-with-kafka/)\n\t* [Disaster Recovery for Multi-Region Kafka at Uber](https://eng.uber.com/kafka/)\n* [Failover](http://cloudpatterns.org/mechanisms/failover_system)\n\t* [The Evolution of Global Traffic Routing and Failover](https://www.usenix.org/conference/srecon16/program/presentation/heady)\n\t* [Testing for Disaster Recovery Failover Testing](https://www.usenix.org/conference/srecon17asia/program/presentation/liu_zehua)\n\t* [Designing a Microservices Architecture for Failure](https://blog.risingstack.com/designing-microservices-architecture-for-failure/)\n\t* [ELB for Automatic Failover at GoSquared](https://engineering.gosquared.com/use-elb-automatic-failover)\n\t* [Eliminate the Database for Higher Availability at American Express](http://americanexpress.io/eliminate-the-database-for-higher-availability/)\n\t* [Failover with Redis Sentinel at Vinted](http://engineering.vinted.com/2015/09/03/failover-with-redis-sentinel/)\n\t* [High-availability SaaS Infrastructure at FreeAgent](http://engineering.freeagent.com/2017/02/06/ha-infrastructure-without-breaking-the-bank/)\n\t* [MySQL High Availability at GitHub](https://github.blog/2018-06-20-mysql-high-availability-at-github/)\n\t* [MySQL High Availability at Eventbrite](https://www.eventbrite.com/engineering/mysql-high-availability-at-eventbrite/)\n\t* [Business Continuity & Disaster Recovery at Walmart](https://medium.com/walmartlabs/business-continuity-disaster-recovery-in-the-microservices-world-ef2adca363df)\n* [Load Balancing](https://blog.vivekpanyam.com/scaling-a-web-service-load-balancing/)\n\t* [Introduction to Modern Network Load Balancing and Proxying](https://blog.envoyproxy.io/introduction-to-modern-network-load-balancing-and-proxying-a57f6ff80236)\n\t* [Top Five (Load Balancing) Scalability Patterns](https://www.f5.com/company/blog/top-five-scalability-patterns)\n\t* [Load Balancing infrastructure to support more than 1.3 billion users at Facebook](https://www.usenix.org/conference/srecon15europe/program/presentation/shuff)\n\t* [DHCPLB: DHCP Load Balancer at Facebook](https://code.facebook.com/posts/1734309626831603/dhcplb-an-open-source-load-balancer/)\n\t* [Katran: Scalable Network Load Balancer at Facebook](https://code.facebook.com/posts/1906146702752923/open-sourcing-katran-a-scalable-network-load-balancer/)\n\t* [Deterministic Aperture: A Distributed, Load Balancing Algorithm at Twitter](https://blog.twitter.com/engineering/en_us/topics/infrastructure/2019/daperture-load-balancer.html)\t\n\t* [Load Balancing with Eureka at Netflix](https://medium.com/netflix-techblog/netflix-shares-cloud-load-balancing-and-failover-tool-eureka-c10647ef95e5)\n\t* [Edge Load Balancing at Netflix](https://medium.com/netflix-techblog/netflix-edge-load-balancing-695308b5548c)\n\t* [Zuul 2: Cloud Gateway at Netflix](https://medium.com/netflix-techblog/open-sourcing-zuul-2-82ea476cb2b3)\n\t* [Load Balancing at Yelp](https://engineeringblog.yelp.com/2017/05/taking-zero-downtime-load-balancing-even-further.html)\n\t* [Load Balancing at Github](https://githubengineering.com/introducing-glb/)\n\t* [Consistent Hashing to Improve Load Balancing at Vimeo](https://medium.com/vimeo-engineering-blog/improving-load-balancing-with-a-new-consistent-hashing-algorithm-9f1bd75709ed)\n\t* [UDP Load Balancing at 500 pixel](https://developers.500px.com/udp-load-balancing-with-keepalived-167382d7ad08)\n\t* [QALM: QoS Load Management Framework at Uber](https://eng.uber.com/qalm/)\t\n\t* [Traffic Steering using Rum DNS at LinkedIn](https://www.usenix.org/conference/srecon17europe/program/presentation/rastogi)\n\t* [Traffic Infrastructure (Edge Network) at Dropbox](https://blogs.dropbox.com/tech/2018/10/dropbox-traffic-infrastructure-edge-network/)\n\t* [Intelligent DNS based load balancing at Dropbox](https://blogs.dropbox.com/tech/2020/01/intelligent-dns-based-load-balancing-at-dropbox/)\n\t* [Monitor DNS systems at Stripe](https://stripe.com/en-sg/blog/secret-life-of-dns)\n\t* [Multi-DNS Architecture (3 parts) at Monday](https://medium.com/monday-engineering/how-and-why-we-migrated-our-dns-from-cloudflare-to-a-multi-dns-architecture-part-3-584a470f4062)\n\t* [Dynamic Anycast DNS Infrastructure at Hulu](https://medium.com/hulu-tech-blog/building-hulus-dynamic-anycast-dns-infrastructure-985a7a11fd30)\n* [Rate Limiting](https://www.keycdn.com/support/rate-limiting/)\n\t* [Rate Limiting for Scaling to Millions of Domains at Cloudflare](https://blog.cloudflare.com/counting-things-a-lot-of-different-things/)\n\t* [Cloud Bouncer: Distributed Rate Limiting at Yahoo](https://yahooeng.tumblr.com/post/111288877956/cloud-bouncer-distributed-rate-limiting-at-yahoo)\n\t* [Scaling API with Rate Limiters at Stripe](https://stripe.com/blog/rate-limiters)\n\t* [Distributed Rate Limiting at Allegro](https://allegro.tech/2017/04/hermes-max-rate.html)\n\t* [Ratequeue: Core Queueing-And-Rate-Limiting System at Twilio](https://www.twilio.com/blog/2017/11/chaos-engineering-ratequeue-ha.html)\n\t* [Quotas Service at Grab](https://engineering.grab.com/quotas-service)\n\t* [Rate Limiting at Figma](https://medium.com/figma-design/an-alternative-approach-to-rate-limiting-f8a06cf7c94c)\t\n* [Autoscaling](https://medium.com/@BotmetricHQ/top-11-hard-won-lessons-learned-about-aws-auto-scaling-5bfe56da755f)\n\t* [Autoscaling Pinterest](https://medium.com/@Pinterest_Engineering/auto-scaling-pinterest-df1d2beb4d64)\n\t* [Autoscaling Based on Request Queuing at Square](https://medium.com/square-corner-blog/autoscaling-based-on-request-queuing-c4c0f57f860f)\n\t* [Autoscaling Jenkins at Trivago](http://tech.trivago.com/2017/02/17/your-definite-guide-for-autoscaling-jenkins/)\n\t* [Autoscaling Pub-Sub Consumers at Spotify](https://labs.spotify.com/2017/11/20/autoscaling-pub-sub-consumers/)\n\t* [Autoscaling Bigtable Clusters based on CPU Load at Spotify](https://labs.spotify.com/2018/12/18/bigtable-autoscaler-saving-money-and-time-using-managed-storage/)\n\t* [Autoscaling AWS Step Functions Activities at Yelp](https://engineeringblog.yelp.com/2019/06/autoscaling-aws-step-functions-activities.html)\n\t* [Scryer: Predictive Auto Scaling Engine at Netflix](https://medium.com/netflix-techblog/scryer-netflixs-predictive-auto-scaling-engine-a3f8fc922270)\t\n\t* [Bouncer: Simple AWS Auto Scaling Rollovers at Palantir](https://medium.com/palantir/bouncer-simple-aws-auto-scaling-rollovers-c5af601d65d4)\n\t* [Clusterman: Autoscaling Mesos Clusters at Yelp](https://engineeringblog.yelp.com/2019/02/autoscaling-mesos-clusters-with-clusterman.html)\n* [Availability in Globally Distributed Storage Systems at Google](http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/36737.pdf)\t\n* [NodeJS High Availability at Yahoo](https://yahooeng.tumblr.com/post/68823943185/nodejs-high-availability)\n* [Operations (11 parts) at LinkedIn](https://www.linkedin.com/pulse/introduction-every-day-monday-operations-benjamin-purgason)\n* [Monitoring Powers High Availability for LinkedIn Feed](https://www.usenix.org/conference/srecon17americas/program/presentation/barot)\n* [Supporting Global Events at Facebook](https://code.facebook.com/posts/166966743929963/how-production-engineers-support-global-events-on-facebook/)\n* [High Availability at BlaBlaCar](https://medium.com/blablacar-tech/the-expendables-backends-high-availability-at-blablacar-8cea3b95b26b)\n* [High Availability at Netflix](https://medium.com/@NetflixTechBlog/tips-for-high-availability-be0472f2599c)\n* [High Availability Cloud Infrastructure at Twilio](https://www.twilio.com/engineering/2011/12/12/scaling-high-availablity-infrastructure-in-cloud)\n* [Automating Datacenter Operations at Dropbox](https://blogs.dropbox.com/tech/2019/01/automating-datacenter-operations-at-dropbox/)\n* [Globalizing Player Accounts at Riot Games](https://technology.riotgames.com/news/globalizing-player-accounts)\n\n## Stability\n* [Circuit Breaker](https://martinfowler.com/bliki/CircuitBreaker.html)\n\t* [Circuit Breaking in Distributed Systems](https://www.infoq.com/presentations/circuit-breaking-distributed-systems)\n\t* [Circuit Breaker for Scaling Containers](https://f5.com/about-us/blog/articles/the-art-of-scaling-containers-circuit-breakers-28919)\n\t* [Lessons in Resilience at SoundCloud](https://developers.soundcloud.com/blog/lessons-in-resilience-at-SoundCloud)\n\t* [Protector: Circuit Breaker for Time Series Databases at Trivago](http://tech.trivago.com/2016/02/23/protector/)\n\t* [Improved Production Stability with Circuit Breakers at Heroku](https://blog.heroku.com/improved-production-stability-with-circuit-breakers)\n\t* [Circuit Breaker at Zendesk](https://medium.com/zendesk-engineering/the-joys-of-circuit-breaking-ee6584acd687)\n\t* [Circuit Breaker at Traveloka](https://medium.com/traveloka-engineering/circuit-breakers-dont-let-your-dependencies-bring-you-down-5ba1c5cf1eec)\n\t* [Circuit Breaker at Shopify](https://shopify.engineering/circuit-breaker-misconfigured)\n* [Timeouts](https://www.javaworld.com/article/2824163/application-performance/stability-patterns-applied-in-a-restful-architecture.html)\n\t* [Fault Tolerance (Timeouts and Retries, Thread Separation, Semaphores, Circuit Breakers) at Netflix](https://medium.com/netflix-techblog/fault-tolerance-in-a-high-volume-distributed-system-91ab4faae74a)\n\t* [Enforce Timeout: A Reliability Methodology at DoorDash](https://doordash.engineering/2018/12/21/enforce-timeout-a-doordash-reliability-methodology/)\n\t* [Troubleshooting a Connection Timeout Issue with tcp_tw_recycle Enabled at eBay](https://www.ebayinc.com/stories/blogs/tech/a-vip-connection-timeout-issue-caused-by-snat-and-tcp-tw-recycle/)\n* [Crash-safe Replication for MySQL at Booking.com](https://medium.com/booking-com-infrastructure/better-crash-safe-replication-for-mysql-a336a69b317f)\n* [Bulkheads: Partition and Tolerate Failure in One Part](https://skife.org/architecture/fault-tolerance/2009/12/31/bulkheads.html)\n* [Steady State: Always Put Logs on Separate Disk](https://docs.microsoft.com/en-us/sql/relational-databases/policy-based-management/place-data-and-log-files-on-separate-drives)\n* [Throttling: Maintain a Steady Pace](http://www.sosp.org/2001/papers/welsh.pdf)\n* [Multi-Clustering: Improving Resiliency and Stability of a Large-scale Monolithic API Service at LinkedIn](https://engineering.linkedin.com/blog/2017/11/improving-resiliency-and-stability-of-a-large-scale-api)\n* [Determinism (4 parts) in League of Legends Server](https://engineering.riotgames.com/news/determinism-league-legends-fixing-divergences)\n\n## Performance\n* [Performance Optimization on OS, Storage, Database, Network](https://stackify.com/application-performance-metrics/)\n\t* [Improving Performance with Background Data Prefetching at Instagram](https://engineering.instagram.com/improving-performance-with-background-data-prefetching-b191acb39898)\n\t* [Fixing Linux filesystem performance regressions at LinkedIn](https://engineering.linkedin.com/blog/2020/fixing-linux-filesystem-performance-regressions)\n\t* [Compression Techniques to Solve Network I/O Bottlenecks at eBay](https://www.ebayinc.com/stories/blogs/tech/how-ebays-shopping-cart-used-compression-techniques-to-solve-network-io-bottlenecks/)\n\t* [Optimizing Web Servers for High Throughput and Low Latency at Dropbox](https://blogs.dropbox.com/tech/2017/09/optimizing-web-servers-for-high-throughput-and-low-latency/)\n\t* [Linux Performance Analysis in 60.000 Milliseconds at Netflix](https://medium.com/netflix-techblog/linux-performance-analysis-in-60-000-milliseconds-accc10403c55)\n\t* [Live Downsizing Google Cloud Persistent Disks (PD-SSD) at Mixpanel](https://engineering.mixpanel.com/2018/07/31/live-downsizing-google-cloud-pds-for-fun-and-profit/)\n\t* [Decreasing RAM Usage by 40% Using jemalloc with Python & Celery at Zapier](https://zapier.com/engineering/celery-python-jemalloc/)\n\t* [Reducing Memory Footprint at Slack](https://slack.engineering/reducing-slacks-memory-footprint-4480fec7e8eb)\n\t* [Continuous Load Testing at Slack](https://slack.engineering/continuous-load-testing/)\n\t* [Performance Improvements at Pinterest](https://medium.com/@Pinterest_Engineering/driving-user-growth-with-performance-improvements-cfc50dafadd7)\n\t* [Server Side Rendering at Wix](https://www.youtube.com/watch?v=f9xI2jR71Ms)\n\t* [30x Performance Improvements on MySQLStreamer at Yelp](https://engineeringblog.yelp.com/2018/02/making-30x-performance-improvements-on-yelps-mysqlstreamer.html)\n\t* [Optimizing APIs at Netflix](https://medium.com/netflix-techblog/optimizing-the-netflix-api-5c9ac715cf19)\n\t* [Performance Monitoring with Riemann and Clojure at Walmart](https://medium.com/walmartlabs/performance-monitoring-with-riemann-and-clojure-eafc07fcd375)\n\t* [Performance Tracking Dashboard for Live Games at Zynga](https://www.zynga.com/blogs/engineering/live-games-have-evolving-performance)\n\t* [Optimizing CAL Report Hadoop MapReduce Jobs at eBay](https://www.ebayinc.com/stories/blogs/tech/optimization-of-cal-report-hadoop-mapreduce-job/)\n\t* [Performance Tuning on Quartz Scheduler at eBay](https://www.ebayinc.com/stories/blogs/tech/performance-tuning-on-quartz-scheduler/)\n\t* [Profiling C++ (Part 1: Optimization, Part 2: Measurement and Analysis) at Riot Games](https://engineering.riotgames.com/news/profiling-optimisation)\n\t* [Profiling React Server-Side Rendering at HomeAway](https://medium.com/homeaway-tech-blog/profiling-react-server-side-rendering-to-free-the-node-js-event-loop-7f0fe455a901)\n\t* [Hardware-Assisted Video Transcoding at Dailymotion](https://medium.com/dailymotion-engineering/hardware-assisted-video-transcoding-at-dailymotion-66cd2db448ae)\n\t* [Cross Shard Transactions at 10 Million RPS at Dropbox](https://blogs.dropbox.com/tech/2018/11/cross-shard-transactions-at-10-million-requests-per-second/)\n\t* [API Profiling at Pinterest](https://medium.com/@Pinterest_Engineering/api-profiling-at-pinterest-6fa9333b4961)\n\t* [Pagelets Parallelize Server-side Processing at Yelp](https://engineeringblog.yelp.com/2017/07/generating-web-pages-in-parallel-with-pagelets.html)\n\t* [Improving key expiration in Redis at Twitter](https://blog.twitter.com/engineering/en_us/topics/infrastructure/2019/improving-key-expiration-in-redis.html)\n\t* [Ad Delivery Network Performance Optimization with Flame Graphs at MindGeek](https://medium.com/mindgeek-engineering-blog/ad-delivery-network-performance-optimization-with-flame-graphs-bc550cf59cf7)\n\t* [Predictive CPU isolation of containers at Netflix](https://medium.com/netflix-techblog/predictive-cpu-isolation-of-containers-at-netflix-91f014d856c7)\n\t* [Improving HDFS I/O Utilization for Efficiency at Uber](https://eng.uber.com/improving-hdfs-i-o-utilization-for-efficiency/)\n\t* [Cloud Jewels: Estimating kWh in the Cloud at Etsy](https://codeascraft.com/2020/04/23/cloud-jewels-estimating-kwh-in-the-cloud/)\n\t* [Unthrottled: Fixing CPU Limits in the Cloud (2 parts) at Indeed](https://engineering.indeedblog.com/blog/2019/12/unthrottled-fixing-cpu-limits-in-the-cloud/)\n* [Performance Optimization by Tuning Garbage Collection](https://confluence.atlassian.com/enterprise/garbage-collection-gc-tuning-guide-461504616.html)\n\t* [Garbage Collection in Java Applications at LinkedIn](https://engineering.linkedin.com/garbage-collection/garbage-collection-optimization-high-throughput-and-low-latency-java-applications)\n\t* [Garbage Collection in High-Throughput, Low-Latency Machine Learning Services at Adobe](https://medium.com/adobetech/engineering-high-throughput-low-latency-machine-learning-services-7d45edac0271)\n\t* [Garbage Collection in Redux Applications at SoundCloud](https://developers.soundcloud.com/blog/garbage-collection-in-redux-applications)\n\t* [Garbage Collection in Go Application at Twitch](https://blog.twitch.tv/go-memory-ballast-how-i-learnt-to-stop-worrying-and-love-the-heap-26c2462549a2)\n\t* [Analyzing V8 Garbage Collection Logs at Alibaba](https://www.linux.com/blog/can-nodejs-scale-ask-team-alibaba)\n\t* [Python Garbage Collection for Dropping 50% Memory Growth Per Request at Instagram](https://instagram-engineering.com/copy-on-write-friendly-python-garbage-collection-ad6ed5233ddf)\n\t* [Performance Impact of Removing Out of Band Garbage Collector (OOBGC) at Github](https://githubengineering.com/removing-oobgc/)\n\t* [Debugging Java Memory Leaks at Allegro](https://allegro.tech/2018/05/a-comedy-of-errors-debugging-java-memory-leaks.html)\n\t* [Optimizing JVM at Alibaba](https://www.youtube.com/watch?v=X4tmr3nhZRg)\n\t* [Tuning JVM Memory for Large-scale Services at Uber](https://eng.uber.com/jvm-tuning-garbage-collection/)\n\t* [Solr Performance Tuning at Walmart](https://medium.com/walmartglobaltech/solr-performance-tuning-beb7d0d0f8d9)\n\t* [Memory Tuning a High Throughput Microservice at Flipkart](https://blog.flipkart.tech/memory-tuning-a-high-throughput-microservice-ed57b3e60997)\n* [Performance Optimization on Image, Video, Page Load](https://developers.google.com/web/fundamentals/performance/why-performance-matters/)\n\t* [Optimizing 360 Photos at Scale at Facebook](https://code.facebook.com/posts/129055711052260/optimizing-360-photos-at-scale/)\n\t* [Reducing Image File Size in the Photos Infrastructure at Etsy](https://codeascraft.com/2017/05/30/reducing-image-file-size-at-etsy/)\n\t* [Improving GIF Performance at Pinterest](https://medium.com/@Pinterest_Engineering/improving-gif-performance-on-pinterest-8dad74bf92f1)\n\t* [Optimizing Video Playback Performance at Pinterest](https://medium.com/@Pinterest_Engineering/optimizing-video-playback-performance-caf55ce310d1)\n\t* [Optimizing Video Stream for Low Bandwidth with Dynamic Optimizer at Netflix](https://medium.com/netflix-techblog/optimized-shot-based-encodes-now-streaming-4b9464204830)\n\t* [Adaptive Video Streaming at YouTube](https://youtube-eng.googleblog.com/2018/04/making-high-quality-video-efficient.html)\n    * [Reducing Video Loading Time at Dailymotion](https://medium.com/dailymotion/reducing-video-loading-time-fa9c997a2294)\n\t* [Improving Homepage Performance at Zillow](https://www.zillow.com/engineering/improving-homepage-performance/)\n\t* [The Process of Optimizing for Client Performance at Expedia](https://medium.com/expedia-engineering/go-fast-or-go-home-the-process-of-optimizing-for-client-performance-57bb497402e)\n\t* [Web Performance at BBC](https://medium.com/bbc-design-engineering/bbc-world-service-web-performance-26b08f7abfcc)\n* [Performance Optimization by Brotli Compression](https://blogs.akamai.com/2016/02/understanding-brotlis-potential.html)\n\t* [Boosting Site Speed Using Brotli Compression at LinkedIn](https://engineering.linkedin.com/blog/2017/05/boosting-site-speed-using-brotli-compression)\t\n\t* [Brotli at Booking.com](https://medium.com/booking-com-development/bookings-journey-with-brotli-978b249d34f3)\n\t* [Brotli at Treebo](https://tech.treebo.com/a-tale-of-brotli-compression-bcb071d9780a)\n\t* [Deploying Brotli for Static Content at Dropbox](https://dropbox.tech/infrastructure/deploying-brotli-for-static-content)\n\t* [Progressive Enhancement with Brotli at Yelp](https://engineeringblog.yelp.com/2017/07/progressive-enhancement-with-brotli.html)\n\t* [Speeding Up Redis with Compression at DoorDash](https://doordash.engineering/2019/01/02/speeding-up-redis-with-compression/)\n* [Performance Optimization on Languages and Frameworks](https://www.techempower.com/benchmarks/)\n\t* [Python at Netflix](https://netflixtechblog.com/python-at-netflix-bba45dae649e)\n\t* [Python at scale (3 parts) at Instagram](https://instagram-engineering.com/python-at-scale-strict-modules-c0bb9245c834)\n\t* [OCaml best practices (2 parts) at Issuu](https://engineering.issuu.com/2018/12/10/our-current-ocaml-best-practices-part-2)\n\t* [PHP at Slack](https://slack.engineering/taking-php-seriously-cf7a60065329)\n\t* [Go at Trivago](https://tech.trivago.com/2020/03/02/why-we-chose-go/)\n\t* [TypeScript at Etsy](https://codeascraft.com/2021/11/08/etsys-journey-to-typescript/)\n\t* [Kotlin for taming state at Etsy](https://www.etsy.com/sg-en/codeascraft/sealed-classes-opened-my-mind)\n\t* [Kotlin at DoorDash](https://doordash.engineering/2022/03/22/how-to-leverage-functional-programming-in-kotlin-to-write-better-cleaner-code/)\n\t* [BPF and Go at Bumble](https://medium.com/bumble-tech/bpf-and-go-modern-forms-of-introspection-in-linux-6b9802682223)\n\t* [Ruby on Rails at GitLab](https://medium.com/gitlab-magazine/why-we-use-ruby-on-rails-to-build-gitlab-601dce4a7a38)\n\t* [Rust in production at Figma](https://medium.com/figma-design/rust-in-production-at-figma-e10a0ec31929)\n\t* [Choosing a Language Stack at WeWork](https://engineering.wework.com/choosing-a-language-stack-cac3726928f6)\n\t* [Switching from Go to Rust at Discord](https://blog.discord.com/why-discord-is-switching-from-go-to-rust-a190bbca2b1f)\n\t* [ASP.NET Core Performance Optimization at Agoda](https://medium.com/agoda-engineering/happy-asp-net-core-performance-optimization-4e21a383d299)\n\t* [Data Race Patterns in Go at Uber](https://eng.uber.com/data-race-patterns-in-go/)\n\t* [Java 21 Virtual Threads at Netflix](https://netflixtechblog.com/java-21-virtual-threads-dude-wheres-my-lock-3052540e231d)\t\n    \n## Intelligence\n* [Big Data](https://insights.sei.cmu.edu/sei_blog/2017/05/reference-architectures-for-big-data-systems.html)\t\n\t* [Data Platform at Uber](https://eng.uber.com/uber-big-data-platform/)\n\t* [Data Platform at BMW](https://www.unibw.de/code/events-u/jt-2018-workshops/ws3_bigdata_vortrag_widmann.pdf)\n\t* [Data Platform at Netflix](https://www.youtube.com/watch?v=CSDIThSwA7s)\n\t* [Data Platform at Flipkart](https://blog.flipkart.tech/overview-of-flipkart-data-platform-20c6d3e9a196)\n\t* [Data Platform at Coupang](https://medium.com/coupang-tech/evolving-the-coupang-data-platform-308e305a9c45)\n\t* [Data Platform at DoorDash](https://doordash.engineering/2020/09/25/how-doordash-is-scaling-its-data-platform/)\n\t* [Data Platform at Khan Academy](http://engineering.khanacademy.org/posts/khanalytics.htm)\n\t* [Data Infrastructure at Airbnb](https://medium.com/airbnb-engineering/data-infrastructure-at-airbnb-8adfb34f169c)\n\t* [Data Infrastructure at LinkedIn](https://www.infoq.com/presentations/big-data-infrastructure-linkedin)\n\t* [Data Infrastructure at GO-JEK](https://blog.gojekengineering.com/data-infrastructure-at-go-jek-cd4dc8cbd929)\n\t* [Data Ingestion Infrastructure at Pinterest](https://medium.com/@Pinterest_Engineering/scalable-and-reliable-data-ingestion-at-pinterest-b921c2ee8754)\n\t* [Data Analytics Architecture at Pinterest](https://medium.com/@Pinterest_Engineering/behind-the-pins-building-analytics-f7b508cdacab)\n\t* [Data Orchestration Service at Spotify](https://engineering.atspotify.com/2022/03/why-we-switched-our-data-orchestration-service/)\n\t* [Big Data Processing (2 parts) at Spotify](https://labs.spotify.com/2017/10/23/big-data-processing-at-spotify-the-road-to-scio-part-2/)\n\t* [Big Data Processing at Uber](https://cdn.oreillystatic.com/en/assets/1/event/160/Big%20data%20processing%20with%20Hadoop%20and%20Spark%2C%20the%20Uber%20way%20Presentation.pdf)\n\t* [Analytics Pipeline at Lyft](https://cdn.oreillystatic.com/en/assets/1/event/269/Lyft_s%20analytics%20pipeline_%20From%20Redshift%20to%20Apache%20Hive%20and%20Presto%20Presentation.pdf)\n\t* [Analytics Pipeline at Grammarly](https://tech.grammarly.com/blog/building-a-versatile-analytics-pipeline-on-top-of-apache-spark)\n\t* [Analytics Pipeline at Teads](https://medium.com/teads-engineering/give-meaning-to-100-billion-analytics-events-a-day-d6ba09aa8f44)\n\t* [ML Data Pipelines for Real-Time Fraud Prevention at PayPal](https://www.infoq.com/presentations/paypal-ml-fraud-prevention-2018)\n\t* [Big Data Analytics and ML Techniques at LinkedIn](https://cdn.oreillystatic.com/en/assets/1/event/269/Big%20data%20analytics%20and%20machine%20learning%20techniques%20to%20drive%20and%20grow%20business%20Presentation%201.pdf)\n\t* [Self-Serve Reporting Platform on Hadoop at LinkedIn](https://cdn.oreillystatic.com/en/assets/1/event/137/Building%20a%20self-serve%20real-time%20reporting%20platform%20at%20LinkedIn%20Presentation%201.pdf)\n\t* [Privacy-Preserving Analytics and Reporting at LinkedIn](https://engineering.linkedin.com/blog/2019/04/privacy-preserving-analytics-and-reporting-at-linkedin)\n\t* [Analytics Platform for Tracking Item Availability at Walmart](https://medium.com/walmartlabs/how-we-build-a-robust-analytics-platform-using-spark-kafka-and-cassandra-lambda-architecture-70c2d1bc8981)\n\t* [Real-Time Analytics for Mobile App Crashes using Apache Pinot at Uber](https://www.uber.com/en-SG/blog/real-time-analytics-for-mobile-app-crashes/)\n\t* [HALO: Hardware Analytics and Lifecycle Optimization at Facebook](https://code.fb.com/data-center-engineering/hardware-analytics-and-lifecycle-optimization-halo-at-facebook/)\n\t* [RBEA: Real-time Analytics Platform at King](https://techblog.king.com/rbea-scalable-real-time-analytics-king/)\n\t* [AresDB: GPU-Powered Real-time Analytics Engine at Uber](https://eng.uber.com/aresdb/)\n\t* [AthenaX: Streaming Analytics Platform at Uber](https://eng.uber.com/athenax/)\n\t* [Jupiter: Config Driven Adtech Batch Ingestion Platform at Uber](https://www.uber.com/en-SG/blog/jupiter-batch-ingestion-platform/)\n\t* [Delta: Data Synchronization and Enrichment Platform at Netflix](https://medium.com/netflix-techblog/delta-a-data-synchronization-and-enrichment-platform-e82c36a79aee)\n\t* [Keystone: Real-time Stream Processing Platform at Netflix](https://medium.com/netflix-techblog/keystone-real-time-stream-processing-platform-a3ee651812a)\n\t* [Databook: Turning Big Data into Knowledge with Metadata at Uber](https://eng.uber.com/databook/)\n\t* [Amundsen: Data Discovery & Metadata Engine at Lyft](https://eng.lyft.com/amundsen-lyfts-data-discovery-metadata-engine-62d27254fbb9)\n\t* [Maze: Funnel Visualization Platform at Uber](https://eng.uber.com/maze/)\n\t* [Metacat: Making Big Data Discoverable and Meaningful at Netflix](https://medium.com/netflix-techblog/metacat-making-big-data-discoverable-and-meaningful-at-netflix-56fb36a53520)\n\t* [SpinalTap: Change Data Capture System at Airbnb](https://medium.com/airbnb-engineering/capturing-data-evolution-in-a-service-oriented-architecture-72f7c643ee6f)\n\t* [Accelerator: Fast Data Processing Framework at eBay](https://www.ebayinc.com/stories/blogs/tech/announcing-the-accelerator-processing-1-000-000-000-lines-per-second-on-a-single-computer/)\n\t* [Omid: Transaction Processing Platform at Yahoo](https://yahooeng.tumblr.com/post/180867271141/a-new-chapter-for-omid)\n\t* [TensorFlowOnSpark: Distributed Deep Learning on Big Data Clusters at Yahoo](https://yahooeng.tumblr.com/post/157196488076/open-sourcing-tensorflowonspark-distributed-deep)\n\t* [CaffeOnSpark: Distributed Deep Learning on Big Data Clusters at Yahoo](https://yahooeng.tumblr.com/post/139916828451/caffeonspark-open-sourced-for-distributed-deep)\n\t* [Spark on Scala: Analytics Reference Architecture at Adobe](https://medium.com/adobetech/spark-on-scala-adobe-analytics-reference-architecture-7457f5614b4c)\n\t* [Experimentation Platform (2 parts) at Spotify](https://engineering.atspotify.com/2020/11/02/spotifys-new-experimentation-platform-part-2/)\n\t* [Experimentation Platform at Airbnb](https://medium.com/airbnb-engineering/https-medium-com-jonathan-parks-scaling-erf-23fd17c91166)\n\t* [Smart Product Platform at Zalando](https://engineering.zalando.com/posts/2017/10/zalando-smart-product-platform.html)\n\t* [Log Analysis Platform at LINE](https://www.slideshare.net/wyukawa/strata2017-sg)\n\t* [Data Visualisation Platform at Myntra](https://medium.com/myntra-engineering/universal-dashboarding-platform-udp-data-visualisation-platform-at-myntra-5f2522fcf72d)\n\t* [Building and Scaling Data Lineage at Netflix](https://medium.com/netflix-techblog/building-and-scaling-data-lineage-at-netflix-to-improve-data-infrastructure-reliability-and-1a52526a7977)\n\t* [Building a scalable data management system for computer vision tasks at Pinterest](https://medium.com/@Pinterest_Engineering/building-a-scalable-data-management-system-for-computer-vision-tasks-a6dee8f1c580)\n\t* [Structured Data at Etsy](https://codeascraft.com/2019/07/31/an-introduction-to-structured-data-at-etsy/)\n\t* [Scaling a Mature Data Pipeline - Managing Overhead at Airbnb](https://medium.com/airbnb-engineering/scaling-a-mature-data-pipeline-managing-overhead-f34835cbc866)\n\t* [Spark Partitioning Strategies at Airbnb](https://medium.com/airbnb-engineering/on-spark-hive-and-small-files-an-in-depth-look-at-spark-partitioning-strategies-a9a364f908)\n\t* [Scaling the Hadoop Distributed File System at LinkedIn](https://engineering.linkedin.com/blog/2021/the-exabyte-club--linkedin-s-journey-of-scaling-the-hadoop-distr)\n\t* [Scaling Hadoop YARN cluster beyond 10,000 nodes at LinkedIn](https://engineering.linkedin.com/blog/2021/scaling-linkedin-s-hadoop-yarn-cluster-beyond-10-000-nodes)\n\t* [Scaling Big Data Access Controls at Pinterest](https://medium.com/pinterest-engineering/securely-scaling-big-data-access-controls-at-pinterest-bbc3406a1695)\n* [Distributed Machine Learning](https://www.csie.ntu.edu.tw/~cjlin/talks/bigdata-bilbao.pdf)\n\t* [Machine Learning Platform at Yelp](https://engineeringblog.yelp.com/2020/07/ML-platform-overview.html)\n\t* [Machine Learning Platform at Etsy](https://codeascraft.com/2021/12/21/redesigning-etsys-machine-learning-platform/)\n\t* [Machine Learning Platform at Zalando](https://engineering.zalando.com/posts/2022/04/zalando-machine-learning-platform.html)\n\t* [Scaling AI/ML Infrastructure at Uber](https://www.uber.com/en-SG/blog/scaling-ai-ml-infrastructure-at-uber/)\n\t* [Recommendation System at Lyft](https://eng.lyft.com/the-recommendation-system-at-lyft-67bc9dcc1793)\n\t* [Reinforcement Learning Platform at Lyft](https://eng.lyft.com/lyfts-reinforcement-learning-platform-670f77ff46ec)\n\t* [Platform for Serving Recommendations at Etsy](https://www.etsy.com/sg-en/codeascraft/building-a-platform-for-serving-recommendations-at-etsy)\n\t* [Infrastructure to Run User Forecasts at Spotify](https://engineering.atspotify.com/2022/06/how-we-built-infrastructure-to-run-user-forecasts-at-spotify/)\n\t* [Aroma: Using ML for Code Recommendation at Facebook](https://code.fb.com/developer-tools/aroma/)\n\t* [Flyte: Cloud Native Machine Learning and Data Processing Platform at Lyft](https://eng.lyft.com/introducing-flyte-cloud-native-machine-learning-and-data-processing-platform-fb2bb3046a59)\n\t* [LyftLearn: ML Model Training Infrastructure built on Kubernetes at Lyft](https://eng.lyft.com/lyftlearn-ml-model-training-infrastructure-built-on-kubernetes-aef8218842bb)\n\t* [Horovod: Open Source Distributed Deep Learning Framework for TensorFlow at Uber](https://eng.uber.com/horovod/)\n\t* [Genie: Gen AI On-Call Copilot at Uber](https://www.uber.com/blog/genie-ubers-gen-ai-on-call-copilot/)\n\t* [COTA: Improving Customer Care with NLP & Machine Learning at Uber](https://eng.uber.com/cota/)\n\t* [Manifold: Model-Agnostic Visual Debugging Tool for Machine Learning at Uber](https://eng.uber.com/manifold/)\t\n\t* [Repo-Topix: Topic Extraction Framework at Github](https://githubengineering.com/topics/)\n\t* [Concourse: Generating Personalized Content Notifications in Near-Real-Time at LinkedIn](https://engineering.linkedin.com/blog/2018/05/concourse--generating-personalized-content-notifications-in-near)\n\t* [Altus Care: Applying a Chatbot to Platform Engineering at eBay](https://www.ebayinc.com/stories/blogs/tech/altus-care-apply-chatbot-to-ebay-platform-engineering/)\n\t* [PyKrylov: Accelerating Machine Learning Research at eBay](https://tech.ebayinc.com/engineering/pykrylov-accelerating-machine-learning-research-at-ebay/)\n\t* [Box Graph: Spontaneous Social Network at Box](https://blog.box.com/blog/box-graph-how-we-built-spontaneous-social-network/)\n\t* [PricingNet: Pricing Modelling with Neural Networks at Skyscanner](https://hackernoon.com/pricingnet-modelling-the-global-airline-industry-with-neural-networks-833844d20ea6)\n\t* [PinText: Multitask Text Embedding System at Pinterest](https://medium.com/pinterest-engineering/pintext-a-multitask-text-embedding-system-in-pinterest-b80ece364555)\n\t* [SearchSage: Learning Search Query Representations at Pinterest](https://medium.com/pinterest-engineering/searchsage-learning-search-query-representations-at-pinterest-654f2bb887fc)\n\t* [Cannes: ML saves $1.7M a year on document previews at Dropbox](https://dropbox.tech/machine-learning/cannes--how-ml-saves-us--1-7m-a-year-on-document-previews)\n\t* [Scaling Gradient Boosted Trees for Click-Through-Rate Prediction at Yelp](https://engineeringblog.yelp.com/2018/01/building-a-distributed-ml-pipeline-part1.html)\t\n\t* [Learning with Privacy at Scale at Apple](https://machinelearning.apple.com/2017/12/06/learning-with-privacy-at-scale.html)\n\t* [Deep Learning for Image Classification Experiment at Mercari](https://medium.com/mercari-engineering/mercaris-image-classification-experiment-using-deep-learning-9b4e994a18ec)\n\t* [Deep Learning for Frame Detection in Product Images at Allegro](https://allegro.tech/2016/12/deep-learning-for-frame-detection.html)\n\t* [Content-based Video Relevance Prediction at Hulu](https://medium.com/hulu-tech-blog/content-based-video-relevance-prediction-b2c448e14752)\n\t* [Moderating Inappropriate Video Content at Yelp](https://engineeringblog.yelp.com/2024/03/moderating-inappropriate-video-content-at-yelp.html)\n\t* [Improving Photo Selection With Deep Learning at TripAdvisor](http://engineering.tripadvisor.com/improving-tripadvisor-photo-selection-deep-learning/)\n\t* [Personalized Recommendations for Experiences Using Deep Learning at TripAdvisor](https://www.tripadvisor.com/engineering/personalized-recommendations-for-experiences-using-deep-learning/)\n\t* [Personalised Recommender Systems at BBC](https://medium.com/bbc-design-engineering/developing-personalised-recommender-systems-at-the-bbc-e26c5e0c4216)\n\t* [Machine Learning (2 parts) at Cond\u00e9 Nast](https://technology.condenast.com/story/handbag-brand-and-color-detection)\n\t* [Natural Language Processing and Content Analysis (2 parts) at Cond\u00e9 Nast](https://technology.condenast.com/story/natural-language-processing-and-content-analysis-at-conde-nast-part-2-system-architecture)\n\t* [Mapping the World of Music Using Machine Learning (2 parts) at iHeartRadio](https://tech.iheart.com/mapping-the-world-of-music-using-machine-learning-part-2-aa50b6a0304c)\n\t* [Machine Learning to Improve Streaming Quality at Netflix](https://medium.com/netflix-techblog/using-machine-learning-to-improve-streaming-quality-at-netflix-9651263ef09f)\n\t* [Machine Learning to Match Drivers & Riders at GO-JEK](https://blog.gojekengineering.com/how-we-use-machine-learning-to-match-drivers-riders-b06d617b9e5)\n\t* [Improving Video Thumbnails with Deep Neural Nets at YouTube](https://youtube-eng.googleblog.com/2015/10/improving-youtube-video-thumbnails-with_8.html)\n\t* [Quantile Regression for Delivering On Time at Instacart](https://tech.instacart.com/how-instacart-delivers-on-time-using-quantile-regression-2383e2e03edb)\n\t* [Cross-Lingual End-to-End Product Search with Deep Learning at Zalando](https://engineering.zalando.com/posts/2018/02/search-deep-neural-network.html)\n\t* [Machine Learning at Jane Street](https://blog.janestreet.com/real-world-machine-learning-part-1/)\n\t* [Machine Learning for Ranking Answers End-to-End at Quora](https://engineering.quora.com/A-Machine-Learning-Approach-to-Ranking-Answers-on-Quora)\n\t* [Clustering Similar Stories Using LDA at Flipboard](http://engineering.flipboard.com/2017/02/storyclustering)\n\t* [Similarity Search at Flickr](https://code.flickr.net/2017/03/07/introducing-similarity-search-at-flickr/)\n\t* [Large-Scale Machine Learning Pipeline for Job Recommendations at Indeed](http://engineering.indeedblog.com/blog/2016/04/building-a-large-scale-machine-learning-pipeline-for-job-recommendations/)\n\t* [Deep Learning from Prototype to Production at Taboola](http://engineering.taboola.com/deep-learning-from-prototype-to-production/)\n\t* [Atom Smashing using Machine Learning at CERN](https://cdn.oreillystatic.com/en/assets/1/event/144/Atom%20smashing%20using%20machine%20learning%20at%20CERN%20Presentation.pdf)\n\t* [Mapping Tags at Medium](https://medium.engineering/mapping-mediums-tags-1b9a78d77cf0)\n\t* [Clustering with the Dirichlet Process Mixture Model in Scala at Monsanto](http://engineering.monsanto.com/2015/11/23/chinese-restaurant-process/)\n\t* [Map Pins with DBSCAN & Random Forests at Foursquare](https://engineering.foursquare.com/you-are-probably-here-better-map-pins-with-dbscan-random-forests-9d51e8c1964d)\n\t* [Forecasting at Uber](https://eng.uber.com/forecasting-introduction/)\n\t* [Financial Forecasting at Uber](https://eng.uber.com/transforming-financial-forecasting-machine-learning/)\n\t* [Productionizing ML with Workflows at Twitter](https://blog.twitter.com/engineering/en_us/topics/insights/2018/ml-workflows.html)\n\t* [GUI Testing Powered by Deep Learning at eBay](https://www.ebayinc.com/stories/blogs/tech/gui-testing-powered-by-deep-learning/)\n\t* [Scaling Machine Learning to Recommend Driving Routes at Pivotal](http://engineering.pivotal.io/post/scaling-machine-learning-to-recommend-driving-routes/)\n\t* [Real-Time Predictions at DoorDash](https://www.infoq.com/presentations/doordash-real-time-predictions)\n\t* [Machine Intelligence at Dropbox](https://blogs.dropbox.com/tech/2018/09/machine-intelligence-at-dropbox-an-update-from-our-dbxi-team/)\n\t* [Machine Learning for Indexing Text from Billions of Images at Dropbox](https://blogs.dropbox.com/tech/2018/10/using-machine-learning-to-index-text-from-billions-of-images/)\n\t* [Modeling User Journeys via Semantic Embeddings at Etsy](https://codeascraft.com/2018/07/12/modeling-user-journey-via-semantic-embeddings/)\n\t* [Automated Fake Account Detection at LinkedIn](https://engineering.linkedin.com/blog/2018/09/automated-fake-account-detection-at-linkedin)\n\t* [Building Knowledge Graph at Airbnb](https://medium.com/airbnb-engineering/contextualizing-airbnb-by-building-knowledge-graph-b7077e268d5a)\n\t* [Core Modeling at Instagram](https://instagram-engineering.com/core-modeling-at-instagram-a51e0158aa48)\n\t* [Neural Architecture Search (NAS) for Prohibited Item Detection at Mercari](https://tech.mercari.com/entry/2019/04/26/163000)\n\t* [Computer Vision at Airbnb](https://medium.com/airbnb-engineering/amenity-detection-and-beyond-new-frontiers-of-computer-vision-at-airbnb-144a4441b72e)\n\t* [3D Home Backend Algorithms at Zillow](https://www.zillow.com/engineering/behind-zillow-3d-home-backend-algorithms/)\n\t* [Long-term Forecasts at Lyft](https://eng.lyft.com/making-long-term-forecasts-at-lyft-fac475b3ba52)\n\t* [Discovering Popular Dishes with Deep Learning at Yelp](https://engineeringblog.yelp.com/2019/10/discovering-popular-dishes-with-deep-learning.html)\n\t* [SplitNet Architecture for Ad Candidate Ranking at Twitter](https://blog.twitter.com/engineering/en_us/topics/infrastructure/2019/splitnet-architecture-for-ad-candidate-ranking.html)\n\t* [Jobs Filter at Indeed](https://engineering.indeedblog.com/blog/2019/09/jobs-filter/)\n\t* [Architecting Restaurant Wait Time Predictions at Yelp](https://engineeringblog.yelp.com/2019/12/architecting-wait-time-estimations.html)\n\t* [Music Personalization at Spotify](https://labs.spotify.com/2016/08/07/commodity-music-ml-services/)\n\t* [Deep Learning for Domain Name Valuation at GoDaddy](https://sg.godaddy.com/engineering/2019/07/26/domain-name-valuation/)\n\t* [Similarity Clustering to Catch Fraud Rings at Stripe](https://stripe.com/blog/similarity-clustering)\n\t* [Personalized Search at Etsy](https://codeascraft.com/2020/10/29/bringing-personalized-search-to-etsy/)\n\t* [ML Feature Serving Infrastructure at Lyft](https://eng.lyft.com/ml-feature-serving-infrastructure-at-lyft-d30bf2d3c32a)\n\t* [Context-Specific Bidding System at Etsy](https://codeascraft.com/2021/03/23/how-we-built-a-context-specific-bidding-system-for-etsy-ads/)\n\t* [Moderating Promotional Spam and Inappropriate Content in Photos at Scale at Yelp](https://engineeringblog.yelp.com/2021/05/moderating-promotional-spam-and-inappropriate-content-in-photos-at-scale-at-yelp.html)\n\t* [Optimizing Payments with Machine Learning at Dropbox](https://dropbox.tech/machine-learning/optimizing-payments-with-machine-learning)\n\t* [Scaling Media Machine Learning at Netflix](https://netflixtechblog.com/scaling-media-machine-learning-at-netflix-f19b400243)\n\t* [Similarity Engine at eBay](https://tech.ebayinc.com/engineering/ebays-blazingly-fast-billion-scale-vector-similarity-engine/)\n\t* [Machine Learning in Content Moderation at Etsy](https://www.etsy.com/codeascraft/machine-learning-in-content-moderation-at-etsy)\n\n## Architecture\n* [Tech Stack at Medium](https://medium.engineering/the-stack-that-helped-medium-drive-2-6-millennia-of-reading-time-e56801f7c492)\n* [Tech Stack at Shopify](https://engineering.shopify.com/blogs/engineering/e-commerce-at-scale-inside-shopifys-tech-stack)\n* [Building Services (4 parts) at Airbnb](https://medium.com/airbnb-engineering/building-services-at-airbnb-part-4-23c95e428064)\n* [Architecture of Evernote](https://evernote.com/blog/a-digest-of-evernotes-architecture/)\n* [Architecture of Chat Service (3 parts) at Riot Games](https://engineering.riotgames.com/news/chat-service-architecture-persistence)\n* [Architecture of League of Legends Client Update](https://technology.riotgames.com/news/architecture-league-client-update)\n* [Architecture of Ad Platform at Twitter](https://blog.twitter.com/engineering/en_us/topics/infrastructure/2020/building-twitters-ad-platform-architecture-for-the-future.html)\n* [Architecture of API Gateway at Uber](https://eng.uber.com/architecture-api-gateway/)\n* [Architecture of API Gateway at Tinder](https://medium.com/tinder/how-we-built-the-tinder-api-gateway-831c6ca5ceca)\n* [Basic Architecture of Slack](https://slack.engineering/how-slack-built-shared-channels-8d42c895b19f)\n* [Lightweight Distributed Architecture to Handle Thousands of Library Releases at eBay](https://tech.ebayinc.com/engineering/a-lightweight-distributed-architecture-to-handle-thousands-of-library-releases-at-ebay/)\n* [Back-end at LinkedIn](https://engineering.linkedin.com/architecture/brief-history-scaling-linkedin)\n* [Back-end at Flickr](https://yahooeng.tumblr.com/post/157200523046/introducing-tripod-flickrs-backend-refactored)\n* [Infrastructure (3 parts) at Zendesk](https://medium.com/zendesk-engineering/the-history-of-infrastructure-at-zendesk-part-3-foundation-team-forming-and-evolving-9859e40f5390)\n* [Cloud Infrastructure at Grubhub](https://bytes.grubhub.com/cloud-infrastructure-at-grubhub-94db998a898a)\n* [Real-time Presence Platform at LinkedIn](https://engineering.linkedin.com/blog/2018/01/now-you-see-me--now-you-dont--linkedins-real-time-presence-platf)\n* [Settings Platform at LinkedIn](https://engineering.linkedin.com/blog/2019/05/building-member-trust-through-a-centralized-and-scalable-setting)\n* [Nearline System for Scale and Performance (2 parts) at Glassdoor](https://medium.com/glassdoor-engineering/building-a-nearline-system-for-scale-and-performance-part-ii-9e01bf51b23d)\n* [Real-time User Action Counting System for Ads at Pinterest](https://medium.com/@Pinterest_Engineering/building-a-real-time-user-action-counting-system-for-ads-88a60d9c9a)\n* [API Platform at Riot Games](https://engineering.riotgames.com/news/riot-games-api-deep-dive)\n* [Games Platform at The New York Times](https://open.nytimes.com/play-by-play-moving-the-nyt-games-platform-to-gcp-with-zero-downtime-cf425898d569)\n* [Kabootar: Communication Platform at Swiggy](https://bytes.swiggy.com/kabootar-swiggys-communication-platform-e5a43cc25629)\n* [Simone: Distributed Simulation Service at Netflix](https://medium.com/netflix-techblog/https-medium-com-netflix-techblog-simone-a-distributed-simulation-service-b2c85131ca1b)\n* [Seagull: Distributed System that Helps Running > 20 Million Tests Per Day at Yelp](https://engineeringblog.yelp.com/2017/04/how-yelp-runs-millions-of-tests-every-day.html)\n* [PriceAggregator: Intelligent System for Hotel Price Fetching (3 parts) at Agoda](https://medium.com/agoda-engineering/priceaggregator-an-intelligent-system-for-hotel-price-fetching-part-3-52acfc705081)\n* [Phoenix: Testing Platform (3 parts) at Tinder](https://medium.com/tinder-engineering/phoenix-tinders-testing-platform-part-iii-520728b9537)\n* [Hexagonal Architecture at Netflix](https://netflixtechblog.com/ready-for-changes-with-hexagonal-architecture-b315ec967749)\n* [Architecture of Sticker Services at LINE](https://www.slideshare.net/linecorp/architecture-sustaining-line-sticker-services)\n* [Stack Overflow Enterprise at Palantir](https://medium.com/@palantir/terraforming-stack-overflow-enterprise-in-aws-47ee431e6be7)\n* [Architecture of Following Feed, Interest Feed, and Picked For You at Pinterest](https://medium.com/@Pinterest_Engineering/building-a-dynamic-and-responsive-pinterest-7d410e99f0a9)\n* [API Specification Workflow at WeWork](https://engineering.wework.com/our-api-specification-workflow-9337448d6ee6)\n* [Media Database at Netflix](https://medium.com/netflix-techblog/implementing-the-netflix-media-database-53b5a840b42a)\n* [Member Transaction History Architecture at Walmart](https://medium.com/walmartlabs/member-transaction-history-architecture-8b6e34b87c21)\n* [Sync Engine (2 parts) at Dropbox](https://dropbox.tech/infrastructure/-testing-our-new-sync-engine)\n* [Ads Pacing Service at Twitter](https://blog.twitter.com/engineering/en_us/topics/infrastructure/2021/how-we-built-twitter-s-highly-reliable-ads-pacing-service)\n* [Rapid Event Notification System at Netflix](https://netflixtechblog.com/rapid-event-notification-system-at-netflix-6deb1d2b57d1)\n* [Architectures of Finance, Banking, and Payment Systems](https://www.redhat.com/architect/portfolio/detail/12-integrating-a-modern-payments-architecture)\n\t* [Bank Backend at Monzo](https://monzo.com/blog/2016/09/19/building-a-modern-bank-backend/)\n\t* [Trading Platform for Scale at Wealthsimple](https://medium.com/@Wealthsimple/engineering-at-wealthsimple-reinventing-our-trading-platform-for-scale-17e332241b6c)\n\t* [Core Banking System at Margo Bank](https://medium.com/margobank/choosing-an-architecture-85750e1e5a03)\n\t* [Architecture of Nubank](https://www.infoq.com/presentations/nubank-architecture)\n\t* [Tech Stack at TransferWise](http://tech.transferwise.com/the-transferwise-stack-heartbeat-of-our-little-revolution/)\n\t* [Tech Stack at Addepar](https://medium.com/build-addepar/our-tech-stack-a4f55dab4b0d)\n\t* [Avoiding Double Payments in a Distributed Payments System at Airbnb](https://medium.com/airbnb-engineering/avoiding-double-payments-in-a-distributed-payments-system-2981f6b070bb)\n\t* [Scaling Payments (3 parts) at Etsy](https://www.etsy.com/sg-en/codeascraft/scaling-etsy-payments-with-vitess-part-3--reducing-cutover-risk)\n\t* [Handles Millions of Digital Transactions Safely Everyday at Paytm](https://paytm.com/blog/engineering/how-paytm-handles-millions-of-digital-transactions-safely-everyday/)\n\t* [Billing and Payment Platform at Grammarly](https://www.grammarly.com/blog/engineering/billing-and-payments-platform/)\n\n## Interview\n* [Designing Large-Scale Systems](https://www.somethingsimilar.com/2013/01/14/notes-on-distributed-systems-for-young-bloods/)\n\t* [My Scaling Hero - Jeff Atwood (a dose of Endorphins before your interview, JK)](https://blog.codinghorror.com/my-scaling-hero/)\n\t* [Software Engineering Advice from Building Large-Scale Distributed Systems - Jeff Dean](https://static.googleusercontent.com/media/research.google.com/en//people/jeff/stanford-295-talk.pdf)\n\t* [Introduction to Architecting Systems for Scale](https://lethain.com/introduction-to-architecting-systems-for-scale/)\n\t* [Anatomy of a System Design Interview](https://hackernoon.com/anatomy-of-a-system-design-interview-4cb57d75a53f)\n\t* [8 Things You Need to Know Before a System Design Interview](http://blog.gainlo.co/index.php/2015/10/22/8-things-you-need-to-know-before-system-design-interviews/)\n\t* [Top 10 System Design Interview Questions ](https://hackernoon.com/top-10-system-design-interview-questions-for-software-engineers-8561290f0444)\n\t* [Top 10 Common Large-Scale Software Architectural Patterns in a Nutshell](https://towardsdatascience.com/10-common-software-architectural-patterns-in-a-nutshell-a0b47a1e9013)\n\t* [Cloud Big Data Design Patterns - Lynn Langit](https://lynnlangit.com/2017/03/14/beyond-relational/)\t\n\t* [How NOT to design Netflix in your 45-minute System Design Interview?](https://hackernoon.com/how-not-to-design-netflix-in-your-45-minute-system-design-interview-64953391a054)\n\t* [API Best Practices: Webhooks, Deprecation, and Design](https://zapier.com/engineering/api-best-practices/)\n* [Explaining Low-Level Systems (OS, Network/Protocol, Database, Storage)](https://www.cse.wustl.edu/~jain/cse567-06/ftp/os_monitors/index.html)\n\t* [The Precise Meaning of I/O Wait Time in Linux](http://veithen.github.io/2013/11/18/iowait-linux.html)\n\t* [Paxos Made Live \u2013 An Engineering Perspective](https://research.google.com/archive/paxos_made_live.html)\n\t* [How to do Distributed Locking](https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html)\n\t* [SQL Transaction Isolation Levels Explained](http://elliot.land/post/sql-transaction-isolation-levels-explained)\n* [\"What Happens When... and How\" Questions](https://www.glassdoor.com/Interview/What-happens-when-you-type-www-google-com-in-your-browser-QTN_56396.htm)\n\t* [Netflix: What Happens When You Press Play?](http://highscalability.com/blog/2017/12/11/netflix-what-happens-when-you-press-play.html)\n\t* [Monzo: How Peer-To-Peer Payments Work](https://monzo.com/blog/2018/04/05/how-monzo-to-monzo-payments-work/)\n\t* [Transit and Peering: How Your Requests Reach GitHub](https://githubengineering.com/transit-and-peering-how-your-requests-reach-github/)\n\t* [How Spotify Streams Music](https://labs.spotify.com/2018/08/31/smoother-streaming-with-bbr/)\n\n## Organization\n* [Engineering Levels at SoundCloud](https://developers.soundcloud.com/blog/engineering-levels)\n* [Engineering Roles at Palantir](https://medium.com/palantir/dev-versus-delta-demystifying-engineering-roles-at-palantir-ad44c2a6e87)\n* [Engineering Career Framework at Dropbox](https://dropbox.tech/culture/our-updated-engineering-career-framework)\n* [Scaling Engineering Teams at Twitter](https://www.youtube.com/watch?v=-PXi_7Ld5kU)\n* [Scaling Decision-Making Across Teams at LinkedIn](https://engineering.linkedin.com/blog/2018/03/scaling-decision-making-across-teams-within-linkedin-engineering)\n* [Scaling Data Science Team at GOJEK](https://blog.gojekengineering.com/the-dynamics-of-scaling-an-organisation-cb96dbe8aecd)\n* [Scaling Agile at Zalando](https://engineering.zalando.com/posts/2018/05/scaling-agile-zalando.html)\n* [Scaling Agile at bol.com](https://hackernoon.com/how-we-run-bol-com-with-60-autonomous-teams-fe7a98c0759)\n* [Lessons Learned from Scaling a Product Team at Intercom](https://blog.intercom.com/how-we-build-software/)\n* [Hiring, Managing, and Scaling Engineering Teams at Typeform](https://medium.com/@eleonorazucconi/toby-oliver-cto-typeform-on-hiring-managing-and-scaling-engineering-teams-86bef9e5a708)\t\n* [Scaling the Datagram Team at Instagram](https://instagram-engineering.com/scaling-the-datagram-team-fc67bcf9b721)\n* [Scaling the Design Team at Flexport](https://medium.com/flexport-design/designing-a-design-team-a9a066bc48a5)\n* [Team Model for Scaling a Design System at Salesforce](https://medium.com/salesforce-ux/the-salesforce-team-model-for-scaling-a-design-system-d89c2a2d404b)\n* [Building Analytics Team (4 parts) at Wish](https://medium.com/wish-engineering/scaling-the-analytics-team-at-wish-part-4-recruiting-2a9823b9f5a)\n* [From 2 Founders to 1000 Employees at Transferwise](https://medium.com/transferwise-ideas/from-2-founders-to-1000-employees-how-a-small-scale-startup-grew-into-a-global-community-9f26371a551b)\n* [Lessons Learned Growing a UX Team from 10 to 170 at Adobe](https://medium.com/thinking-design/lessons-learned-growing-a-ux-team-from-10-to-170-f7b47be02262)\n* [Five Lessons from Scaling at Pinterest](https://medium.com/@sarahtavel/five-lessons-from-scaling-pinterest-6a699a889b08)\n* [Approach Engineering at Vinted](http://engineering.vinted.com/2018/09/04/how-we-approach-engineering-at-vinted/)\n* [Using Metrics to Improve the Development Process (and Coach People) at Indeed](https://engineering.indeedblog.com/blog/2018/10/using-metrics-to-improve-the-development-process-and-coach-people/)\n* [Mistakes to Avoid while Creating an Internal Product at Skyscanner](https://medium.com/@SkyscannerEng/9-mistakes-to-avoid-while-creating-an-internal-product-63d579b00b1a)\n* [RACI (Responsible, Accountable, Consulted, Informed) at Etsy](https://codeascraft.com/2018/01/04/selecting-a-cloud-provider/)\n* [Four Pillars of Leading People (Empathy, Inspiration, Trust, Honesty) at Zalando](https://engineering.zalando.com/posts/2018/10/four-pillars-leadership.html)\n* [Pair Programming at Shopify](https://engineering.shopify.com/blogs/engineering/pair-programming-explained)\n* [Distributed Responsibility at Asana](https://blog.asana.com/2017/12/distributed-responsibility-engineering-manager/)\n* [Rotating Engineers at Zalando](https://engineering.zalando.com/posts/2019/03/rotating-engineers-at-zalando.html)\n* [Experiment Idea Review at Pinterest](https://medium.com/pinterest-engineering/how-pinterest-supercharged-its-growth-team-with-experiment-idea-review-fd6571a02fb8)\n* [Tech Migrations at Spotify](https://engineering.atspotify.com/2020/06/25/tech-migrations-the-spotify-way/)\n* [Improving Code Ownership at Yelp](https://engineeringblog.yelp.com/2021/01/whose-code-is-it-anyway.html)\n* [Agile Code Base at eBay](https://tech.ebayinc.com/engineering/how-creating-an-agile-code-base-helped-ebay-pivot-for-apple-silicon/)\n* [Agile Data Engineering at Miro](https://medium.com/miro-engineering/agile-data-engineering-at-miro-ec2dcc8a3fcb)\n* [Automated Incident Management through Slack at Airbnb](https://medium.com/airbnb-engineering/incident-management-ae863dc5d47f)\n* [Refactor Organization at BBC](https://medium.com/bbc-product-technology/refactor-organisation-80e4e171d922)\n* [Code Review](https://ai.google/research/pubs/pub47025)\n\t* [Code Review at Palantir](https://medium.com/@palantir/code-review-best-practices-19e02780015f)\n\t* [Code Review at LINE](https://engineering.linecorp.com/en/blog/effective-code-review/)\n\t* [Code Reviews at Medium](https://medium.engineering/code-reviews-at-medium-bed2c0dce13a)\n\t* [Code Review at LinkedIn](https://engineering.linkedin.com/blog/2018/06/scaling-collective-code-ownership-with-code-reviews)\n\t* [Code Review at Disney](https://medium.com/disney-streaming/the-secret-to-better-code-reviews-c14c7884b9ac)\n\t* [Code Review at Netlify](https://www.netlify.com/blog/2020/03/05/feedback-ladders-how-we-encode-code-reviews-at-netlify/)\n\n## Talk\n* [Distributed Systems in One Lesson - Tim Berglund, Senior Director of Developer Experience at Confluent](https://www.youtube.com/watch?v=Y6Ev8GIlbxc)\n* [Building Real Time Infrastructure at Facebook - Jeff Barber and Shie Erlich, Software Engineer at Facebook](https://www.usenix.org/conference/srecon17americas/program/presentation/erlich)\n* [Building Reliable Social Infrastructure for Google - Marc Alvidrez, Senior Manager at Google](https://www.usenix.org/conference/srecon16/program/presentation/alvidrez)\n* [Building a Distributed Build System at Google Scale - Aysylu Greenberg, SDE at Google](https://www.youtube.com/watch?v=K8YuavUy6Qc)\n* [Site Reliability Engineering at Dropbox - Tammy Butow, Site Reliability Engineering Manager at Dropbox](https://www.youtube.com/watch?v=ggizCjUCCqE)\n* [How Google Does Planet-Scale for Planet-Scale Infra - Melissa Binde, SRE Director for Google Cloud Platform](https://www.youtube.com/watch?v=H4vMcD7zKM0)\n* [Netflix Guide to Microservices - Josh Evans, Director of Operations Engineering at Netflix](https://www.youtube.com/watch?v=CZ3wIuvmHeM&t=2837s)\n* [Achieving Rapid Response Times in Large Online Services - Jeff Dean, Google Senior Fellow](https://www.youtube.com/watch?v=1-3Ahy7Fxsc)\n* [Architecture to Handle 80K RPS Celebrity Sales at Shopify - Simon Eskildsen, Engineering Lead at Shopify](https://www.youtube.com/watch?v=N8NWDHgWA28)\n* [Lessons of Scale at Facebook - Bobby Johnson, Director of Engineering at Facebook](https://www.youtube.com/watch?v=QCHiNEw73AU)\n* [Performance Optimization for the Greater China Region at Salesforce - Jeff Cheng, Enterprise Architect at Salesforce](https://www.salesforce.com/video/1757880/)\n* [How GIPHY Delivers a GIF to 300 Millions Users - Alex Hoang and Nima Khoshini, Services Engineers at GIPHY](https://vimeo.com/252367076)\n* [High Performance Packet Processing Platform at Alibaba - Haiyong Wang, Senior Director at Alibaba](https://www.youtube.com/watch?v=wzsxJqeVIhY&list=PLMu8-hpCxIVENuAue7bd0eCAglLGY_8AW&index=7)\n* [Solving Large-scale Data Center and Cloud Interconnection Problems -  Ihab Tarazi, CTO at Equinix](https://atscaleconference.com/videos/solving-large-scale-data-center-and-cloud-interconnection-problems/)\n* [Scaling Dropbox - Kevin Modzelewski, Back-end Engineer at Dropbox](https://www.youtube.com/watch?v=PE4gwstWhmc)\n* [Scaling Reliability at Dropbox - Sat Kriya Khalsa, SRE at Dropbox](https://www.youtube.com/watch?v=IhGWOaD5BYQ)\n* [Scaling with Performance at Facebook - Bill Jia, VP of Infrastructure at Facebook](https://atscaleconference.com/videos/performance-scale-2018-opening-remarks/)\n* [Scaling Live Videos to a Billion Users at Facebook - Sachin Kulkarni, Director of Engineering at Facebook](https://www.youtube.com/watch?v=IO4teCbHvZw)\n* [Scaling Infrastructure at Instagram - Lisa Guo, Instagram Engineering](https://www.youtube.com/watch?v=hnpzNAPiC0E)\n* [Scaling Infrastructure at Twitter - Yao Yue, Staff Software Engineer at Twitter](https://www.youtube.com/watch?v=6OvrFkLSoZ0)\n* [Scaling Infrastructure at Etsy - Bethany Macri, Engineering Manager at Etsy](https://www.youtube.com/watch?v=LfqyhM1LeIU)\n* [Scaling Real-time Infrastructure at Alibaba for Global Shopping Holiday - Xiaowei Jiang, Senior Director at Alibaba](https://atscaleconference.com/videos/scaling-alibabas-real-time-infrastructure-for-global-shopping-holiday/)\n* [Scaling Data Infrastructure at Spotify - Matti (Lepist\u00f6) Pehrs, Spotify](https://www.youtube.com/watch?v=cdsfRXr9pJU)\n* [Scaling Pinterest - Marty Weiner, Pinterest\u2019s founding engineer](https://www.youtube.com/watch?v=jQNCuD_hxdQ&list=RDhnpzNAPiC0E&index=11)\n* [Scaling Slack - Bing Wei, Software Engineer (Infrastructure) at Slack](https://www.infoq.com/presentations/slack-scalability)\n* [Scaling Backend at Youtube - Sugu Sougoumarane, SDE at Youtube](https://www.youtube.com/watch?v=5yDO-tmIoXY&feature=youtu.be)\n* [Scaling Backend at Uber - Matt Ranney, Chief Systems Architect at Uber](https://www.youtube.com/watch?v=nuiLcWE8sPA)\n* [Scaling Global CDN at Netflix - Dave Temkin, Director of Global Networks at Netflix](https://www.youtube.com/watch?v=tbqcsHg-Q_o)\n* [Scaling Load Balancing Infra to Support 1.3 Billion Users at Facebook - Patrick Shuff, Production Engineer at Facebook](https://www.youtube.com/watch?v=bxhYNfFeVF4)\n* [Scaling (a NSFW site) to 200 Million Views A Day And Beyond - Eric Pickup, Lead Platform Developer at MindGeek](https://www.youtube.com/watch?v=RlkCdM_f3p4)\n* [Scaling Counting Infrastructure at Quora - Chun-Ho Hung and Nikhil Gar, SEs at Quora](https://www.infoq.com/presentations/quora-analytics)\n* [Scaling Git at Microsoft - Saeed Noursalehi, Principal Program Manager at Microsoft](https://www.youtube.com/watch?v=g_MPGU_m01s)\n* [Scaling Multitenant Architecture Across Multiple Data Centres at Shopify - Weingarten, Engineering Lead at Shopify](https://www.youtube.com/watch?v=F-f0-k46WVk)\n\n## A Piece of Cake\nRoses are red. Violets are blue. [Binh](https://nguyenquocbinh.org/) likes sweet. [Treat Binh a tiramisu?](https://paypal.me/binhnguyennus) :cake:\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 33015583,
    "name": "keras",
    "full_name": "keras-team/keras",
    "description": "Deep Learning for humans",
    "html_url": "https://github.com/keras-team/keras",
    "clone_url": "https://github.com/keras-team/keras.git",
    "owner_login": "keras-team",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/34455048?v=4",
    "stargazers_count": 63281,
    "watchers_count": 63281,
    "forks_count": 19604,
    "open_issues_count": 298,
    "size": 47349,
    "language": "Python",
    "languages": {
      "Python": 9098012,
      "Shell": 3974
    },
    "topics": [
      "data-science",
      "deep-learning",
      "jax",
      "machine-learning",
      "neural-networks",
      "python",
      "pytorch",
      "tensorflow"
    ],
    "license_name": "Apache License 2.0",
    "created_at": "2015-03-28T00:35:42+00:00",
    "updated_at": "2025-08-06T00:47:41+00:00",
    "pushed_at": "2025-08-05T22:50:52+00:00",
    "contributors_count": 100,
    "readme_length": 4985,
    "readme_content": "# Keras 3: Deep Learning for Humans\n\nKeras 3 is a multi-backend deep learning framework, with support for JAX, TensorFlow, PyTorch, and OpenVINO (for inference-only).\nEffortlessly build and train models for computer vision, natural language processing, audio processing,\ntimeseries forecasting, recommender systems, etc.\n\n- **Accelerated model development**: Ship deep learning solutions faster thanks to the high-level UX of Keras\nand the availability of easy-to-debug runtimes like PyTorch or JAX eager execution.\n- **State-of-the-art performance**: By picking the backend that is the fastest for your model architecture (often JAX!),\nleverage speedups ranging from 20% to 350% compared to other frameworks. [Benchmark here](https://keras.io/getting_started/benchmarks/).\n- **Datacenter-scale training**: Scale confidently from your laptop to large clusters of GPUs or TPUs.\n\nJoin nearly three million developers, from burgeoning startups to global enterprises, in harnessing the power of Keras 3.\n\n\n## Installation\n\n### Install with pip\n\nKeras 3 is available on PyPI as `keras`. Note that Keras 2 remains available as the `tf-keras` package.\n\n1. Install `keras`:\n\n```\npip install keras --upgrade\n```\n\n2. Install backend package(s).\n\nTo use `keras`, you should also install the backend of choice: `tensorflow`, `jax`, or `torch`.\nNote that `tensorflow` is required for using certain Keras 3 features: certain preprocessing layers\nas well as `tf.data` pipelines.\n\n### Local installation\n\n#### Minimal installation\n\nKeras 3 is compatible with Linux and macOS systems. For Windows users, we recommend using WSL2 to run Keras.\nTo install a local development version:\n\n1. Install dependencies:\n\n```\npip install -r requirements.txt\n```\n\n2. Run installation command from the root directory.\n\n```\npython pip_build.py --install\n```\n\n3. Run API generation script when creating PRs that update `keras_export` public APIs:\n\n```\n./shell/api_gen.sh\n```\n\n#### Adding GPU support\n\nThe `requirements.txt` file will install a CPU-only version of TensorFlow, JAX, and PyTorch. For GPU support, we also\nprovide a separate `requirements-{backend}-cuda.txt` for TensorFlow, JAX, and PyTorch. These install all CUDA\ndependencies via `pip` and expect a NVIDIA driver to be pre-installed. We recommend a clean Python environment for each\nbackend to avoid CUDA version mismatches. As an example, here is how to create a JAX GPU environment with `conda`:\n\n```shell\nconda create -y -n keras-jax python=3.10\nconda activate keras-jax\npip install -r requirements-jax-cuda.txt\npython pip_build.py --install\n```\n\n## Configuring your backend\n\nYou can export the environment variable `KERAS_BACKEND` or you can edit your local config file at `~/.keras/keras.json`\nto configure your backend. Available backend options are: `\"tensorflow\"`, `\"jax\"`, `\"torch\"`, `\"openvino\"`. Example:\n\n```\nexport KERAS_BACKEND=\"jax\"\n```\n\nIn Colab, you can do:\n\n```python\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\n\nimport keras\n```\n\n**Note:** The backend must be configured before importing `keras`, and the backend cannot be changed after\nthe package has been imported.\n\n**Note:** The OpenVINO backend is an inference-only backend, meaning it is designed only for running model\npredictions using `model.predict()` method.\n\n## Backwards compatibility\n\nKeras 3 is intended to work as a drop-in replacement for `tf.keras` (when using the TensorFlow backend). Just take your\nexisting `tf.keras` code, make sure that your calls to `model.save()` are using the up-to-date `.keras` format, and you're\ndone.\n\nIf your `tf.keras` model does not include custom components, you can start running it on top of JAX or PyTorch immediately.\n\nIf it does include custom components (e.g. custom layers or a custom `train_step()`), it is usually possible to convert it\nto a backend-agnostic implementation in just a few minutes.\n\nIn addition, Keras models can consume datasets in any format, regardless of the backend you're using:\nyou can train your models with your existing `tf.data.Dataset` pipelines or PyTorch `DataLoaders`.\n\n## Why use Keras 3?\n\n- Run your high-level Keras workflows on top of any framework -- benefiting at will from the advantages of each framework,\ne.g. the scalability and performance of JAX or the production ecosystem options of TensorFlow.\n- Write custom components (e.g. layers, models, metrics) that you can use in low-level workflows in any framework.\n    - You can take a Keras model and train it in a training loop written from scratch in native TF, JAX, or PyTorch.\n    - You can take a Keras model and use it as part of a PyTorch-native `Module` or as part of a JAX-native model function.\n- Make your ML code future-proof by avoiding framework lock-in.\n- As a PyTorch user: get access to power and usability of Keras, at last!\n- As a JAX user: get access to a fully-featured, battle-tested, well-documented modeling and training library.\n\n\nRead more in the [Keras 3 release announcement](https://keras.io/keras_3/).\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 843222,
    "name": "scikit-learn",
    "full_name": "scikit-learn/scikit-learn",
    "description": "scikit-learn: machine learning in Python",
    "html_url": "https://github.com/scikit-learn/scikit-learn",
    "clone_url": "https://github.com/scikit-learn/scikit-learn.git",
    "owner_login": "scikit-learn",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/365630?v=4",
    "stargazers_count": 62909,
    "watchers_count": 62909,
    "forks_count": 26111,
    "open_issues_count": 2143,
    "size": 174328,
    "language": "Python",
    "languages": {
      "Python": 12617536,
      "Cython": 728806,
      "C++": 147428,
      "Shell": 46980,
      "C": 41895,
      "Meson": 32297,
      "CSS": 13133,
      "JavaScript": 1730,
      "Makefile": 1034
    },
    "topics": [
      "data-analysis",
      "data-science",
      "machine-learning",
      "python",
      "statistics"
    ],
    "license_name": "BSD 3-Clause \"New\" or \"Revised\" License",
    "created_at": "2010-08-17T09:43:38+00:00",
    "updated_at": "2025-08-06T00:47:07+00:00",
    "pushed_at": "2025-08-05T12:23:19+00:00",
    "contributors_count": 100,
    "readme_length": 7440,
    "readme_content": ".. -*- mode: rst -*-\n\n|Azure| |Codecov| |CircleCI| |Nightly wheels| |Ruff| |PythonVersion| |PyPi| |DOI| |Benchmark|\n\n.. |Azure| image:: https://dev.azure.com/scikit-learn/scikit-learn/_apis/build/status/scikit-learn.scikit-learn?branchName=main\n   :target: https://dev.azure.com/scikit-learn/scikit-learn/_build/latest?definitionId=1&branchName=main\n\n.. |CircleCI| image:: https://circleci.com/gh/scikit-learn/scikit-learn/tree/main.svg?style=shield\n   :target: https://circleci.com/gh/scikit-learn/scikit-learn\n\n.. |Codecov| image:: https://codecov.io/gh/scikit-learn/scikit-learn/branch/main/graph/badge.svg?token=Pk8G9gg3y9\n   :target: https://codecov.io/gh/scikit-learn/scikit-learn\n\n.. |Nightly wheels| image:: https://github.com/scikit-learn/scikit-learn/actions/workflows/wheels.yml/badge.svg?event=schedule\n   :target: https://github.com/scikit-learn/scikit-learn/actions?query=workflow%3A%22Wheel+builder%22+event%3Aschedule\n\n.. |Ruff| image:: https://img.shields.io/badge/code%20style-ruff-000000.svg\n   :target: https://github.com/astral-sh/ruff\n\n.. |PythonVersion| image:: https://img.shields.io/pypi/pyversions/scikit-learn.svg\n   :target: https://pypi.org/project/scikit-learn/\n\n.. |PyPi| image:: https://img.shields.io/pypi/v/scikit-learn\n   :target: https://pypi.org/project/scikit-learn\n\n.. |DOI| image:: https://zenodo.org/badge/21369/scikit-learn/scikit-learn.svg\n   :target: https://zenodo.org/badge/latestdoi/21369/scikit-learn/scikit-learn\n\n.. |Benchmark| image:: https://img.shields.io/badge/Benchmarked%20by-asv-blue\n   :target: https://scikit-learn.org/scikit-learn-benchmarks\n\n.. |PythonMinVersion| replace:: 3.10\n.. |NumPyMinVersion| replace:: 1.22.0\n.. |SciPyMinVersion| replace:: 1.8.0\n.. |JoblibMinVersion| replace:: 1.2.0\n.. |ThreadpoolctlMinVersion| replace:: 3.1.0\n.. |MatplotlibMinVersion| replace:: 3.5.0\n.. |Scikit-ImageMinVersion| replace:: 0.19.0\n.. |PandasMinVersion| replace:: 1.4.0\n.. |SeabornMinVersion| replace:: 0.9.0\n.. |PytestMinVersion| replace:: 7.1.2\n.. |PlotlyMinVersion| replace:: 5.14.0\n\n.. image:: https://raw.githubusercontent.com/scikit-learn/scikit-learn/main/doc/logos/scikit-learn-logo.png\n  :target: https://scikit-learn.org/\n\n**scikit-learn** is a Python module for machine learning built on top of\nSciPy and is distributed under the 3-Clause BSD license.\n\nThe project was started in 2007 by David Cournapeau as a Google Summer\nof Code project, and since then many volunteers have contributed. See\nthe `About us <https://scikit-learn.org/dev/about.html#authors>`__ page\nfor a list of core contributors.\n\nIt is currently maintained by a team of volunteers.\n\nWebsite: https://scikit-learn.org\n\nInstallation\n------------\n\nDependencies\n~~~~~~~~~~~~\n\nscikit-learn requires:\n\n- Python (>= |PythonMinVersion|)\n- NumPy (>= |NumPyMinVersion|)\n- SciPy (>= |SciPyMinVersion|)\n- joblib (>= |JoblibMinVersion|)\n- threadpoolctl (>= |ThreadpoolctlMinVersion|)\n\n=======\n\nScikit-learn plotting capabilities (i.e., functions start with ``plot_`` and\nclasses end with ``Display``) require Matplotlib (>= |MatplotlibMinVersion|).\nFor running the examples Matplotlib >= |MatplotlibMinVersion| is required.\nA few examples require scikit-image >= |Scikit-ImageMinVersion|, a few examples\nrequire pandas >= |PandasMinVersion|, some examples require seaborn >=\n|SeabornMinVersion| and plotly >= |PlotlyMinVersion|.\n\nUser installation\n~~~~~~~~~~~~~~~~~\n\nIf you already have a working installation of NumPy and SciPy,\nthe easiest way to install scikit-learn is using ``pip``::\n\n    pip install -U scikit-learn\n\nor ``conda``::\n\n    conda install -c conda-forge scikit-learn\n\nThe documentation includes more detailed `installation instructions <https://scikit-learn.org/stable/install.html>`_.\n\n\nChangelog\n---------\n\nSee the `changelog <https://scikit-learn.org/dev/whats_new.html>`__\nfor a history of notable changes to scikit-learn.\n\nDevelopment\n-----------\n\nWe welcome new contributors of all experience levels. The scikit-learn\ncommunity goals are to be helpful, welcoming, and effective. The\n`Development Guide <https://scikit-learn.org/stable/developers/index.html>`_\nhas detailed information about contributing code, documentation, tests, and\nmore. We've included some basic information in this README.\n\nImportant links\n~~~~~~~~~~~~~~~\n\n- Official source code repo: https://github.com/scikit-learn/scikit-learn\n- Download releases: https://pypi.org/project/scikit-learn/\n- Issue tracker: https://github.com/scikit-learn/scikit-learn/issues\n\nSource code\n~~~~~~~~~~~\n\nYou can check the latest sources with the command::\n\n    git clone https://github.com/scikit-learn/scikit-learn.git\n\nContributing\n~~~~~~~~~~~~\n\nTo learn more about making a contribution to scikit-learn, please see our\n`Contributing guide\n<https://scikit-learn.org/dev/developers/contributing.html>`_.\n\nTesting\n~~~~~~~\n\nAfter installation, you can launch the test suite from outside the source\ndirectory (you will need to have ``pytest`` >= |PyTestMinVersion| installed)::\n\n    pytest sklearn\n\nSee the web page https://scikit-learn.org/dev/developers/contributing.html#testing-and-improving-test-coverage\nfor more information.\n\n    Random number generation can be controlled during testing by setting\n    the ``SKLEARN_SEED`` environment variable.\n\nSubmitting a Pull Request\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nBefore opening a Pull Request, have a look at the\nfull Contributing page to make sure your code complies\nwith our guidelines: https://scikit-learn.org/stable/developers/index.html\n\nProject History\n---------------\n\nThe project was started in 2007 by David Cournapeau as a Google Summer\nof Code project, and since then many volunteers have contributed. See\nthe `About us <https://scikit-learn.org/dev/about.html#authors>`__ page\nfor a list of core contributors.\n\nThe project is currently maintained by a team of volunteers.\n\n**Note**: `scikit-learn` was previously referred to as `scikits.learn`.\n\nHelp and Support\n----------------\n\nDocumentation\n~~~~~~~~~~~~~\n\n- HTML documentation (stable release): https://scikit-learn.org\n- HTML documentation (development version): https://scikit-learn.org/dev/\n- FAQ: https://scikit-learn.org/stable/faq.html\n\nCommunication\n~~~~~~~~~~~~~\n\nMain Channels\n^^^^^^^^^^^^^\n\n- **Website**: https://scikit-learn.org\n- **Blog**: https://blog.scikit-learn.org\n- **Mailing list**: https://mail.python.org/mailman/listinfo/scikit-learn\n\nDeveloper & Support\n^^^^^^^^^^^^^^^^^^^^^^\n\n- **GitHub Discussions**: https://github.com/scikit-learn/scikit-learn/discussions\n- **Stack Overflow**: https://stackoverflow.com/questions/tagged/scikit-learn\n- **Discord**: https://discord.gg/h9qyrK8Jc8\n\nSocial Media Platforms\n^^^^^^^^^^^^^^^^^^^^^^\n\n- **LinkedIn**: https://www.linkedin.com/company/scikit-learn\n- **YouTube**: https://www.youtube.com/channel/UCJosFjYm0ZYVUARxuOZqnnw/playlists\n- **Facebook**: https://www.facebook.com/scikitlearnofficial/\n- **Instagram**: https://www.instagram.com/scikitlearnofficial/\n- **TikTok**: https://www.tiktok.com/@scikit.learn\n- **Bluesky**: https://bsky.app/profile/scikit-learn.org\n- **Mastodon**: https://mastodon.social/@sklearn@fosstodon.org\n\nResources\n^^^^^^^^^\n\n- **Calendar**: https://blog.scikit-learn.org/calendar/\n- **Logos & Branding**: https://github.com/scikit-learn/scikit-learn/tree/main/doc/logos\n\nCitation\n~~~~~~~~\n\nIf you use scikit-learn in a scientific publication, we would appreciate citations: https://scikit-learn.org/stable/about.html#citing-scikit-learn\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 669879380,
    "name": "LLMs-from-scratch",
    "full_name": "rasbt/LLMs-from-scratch",
    "description": "Implement a ChatGPT-like LLM in PyTorch from scratch, step by step",
    "html_url": "https://github.com/rasbt/LLMs-from-scratch",
    "clone_url": "https://github.com/rasbt/LLMs-from-scratch.git",
    "owner_login": "rasbt",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/5618407?v=4",
    "stargazers_count": 62368,
    "watchers_count": 62368,
    "forks_count": 8734,
    "open_issues_count": 5,
    "size": 12979,
    "language": "Jupyter Notebook",
    "languages": {
      "Jupyter Notebook": 2358078,
      "Python": 723300,
      "Dockerfile": 532
    },
    "topics": [
      "ai",
      "artificial-intelligence",
      "chatgpt",
      "deep-learning",
      "from-scratch",
      "gpt",
      "language-model",
      "large-language-models",
      "llm",
      "machine-learning",
      "python",
      "pytorch",
      "transformer"
    ],
    "license_name": "Other",
    "created_at": "2023-07-23T18:15:57+00:00",
    "updated_at": "2025-08-06T01:57:20+00:00",
    "pushed_at": "2025-08-05T18:42:24+00:00",
    "contributors_count": 44,
    "readme_length": 15694,
    "readme_content": "# Build a Large Language Model (From Scratch)\n\nThis repository contains the code for developing, pretraining, and finetuning a GPT-like LLM and is the official code repository for the book [Build a Large Language Model (From Scratch)](https://amzn.to/4fqvn0D).\n\n<br>\n<br>\n\n<a href=\"https://amzn.to/4fqvn0D\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover.jpg?123\" width=\"250px\"></a>\n\n<br>\n\nIn [*Build a Large Language Model (From Scratch)*](http://mng.bz/orYv), you'll learn and understand how large language models (LLMs) work from the inside out by coding them from the ground up, step by step. In this book, I'll guide you through creating your own LLM, explaining each stage with clear text, diagrams, and examples.\n\nThe method described in this book for training and developing your own small-but-functional model for educational purposes mirrors the approach used in creating large-scale foundational models such as those behind ChatGPT. In addition, this book includes code for loading the weights of larger pretrained models for finetuning.\n\n- Link to the official [source code repository](https://github.com/rasbt/LLMs-from-scratch)\n- [Link to the book at Manning (the publisher's website)](http://mng.bz/orYv)\n- [Link to the book page on Amazon.com](https://www.amazon.com/gp/product/1633437167)\n- ISBN 9781633437166\n\n<a href=\"http://mng.bz/orYv#reviews\"><img src=\"https://sebastianraschka.com//images/LLMs-from-scratch-images/other/reviews.png\" width=\"220px\"></a>\n\n\n<br>\n<br>\n\nTo download a copy of this repository, click on the [Download ZIP](https://github.com/rasbt/LLMs-from-scratch/archive/refs/heads/main.zip) button or execute the following command in your terminal:\n\n```bash\ngit clone --depth 1 https://github.com/rasbt/LLMs-from-scratch.git\n```\n\n<br>\n\n(If you downloaded the code bundle from the Manning website, please consider visiting the official code repository on GitHub at [https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch) for the latest updates.)\n\n<br>\n<br>\n\n\n# Table of Contents\n\nPlease note that this `README.md` file is a Markdown (`.md`) file. If you have downloaded this code bundle from the Manning website and are viewing it on your local computer, I recommend using a Markdown editor or previewer for proper viewing. If you haven't installed a Markdown editor yet, [Ghostwriter](https://ghostwriter.kde.org) is a good free option.\n\nYou can alternatively view this and other files on GitHub at [https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch) in your browser, which renders Markdown automatically.\n\n<br>\n<br>\n\n\n> **Tip:**\n> If you're seeking guidance on installing Python and Python packages and setting up your code environment, I suggest reading the [README.md](setup/README.md) file located in the [setup](setup) directory.\n\n<br>\n<br>\n\n[![Code tests Linux](https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-linux-uv.yml/badge.svg)](https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-linux-uv.yml)\n[![Code tests Windows](https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-windows-uv-pip.yml/badge.svg)](https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-windows-uv-pip.yml)\n[![Code tests macOS](https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-macos-uv.yml/badge.svg)](https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-macos-uv.yml)\n\n\n\n\n<br>\n\n| Chapter Title                                              | Main Code (for Quick Access)                                                                                                    | All Code + Supplementary      |\n|------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------|-------------------------------|\n| [Setup recommendations](setup)                             | -                                                                                                                               | -                             |\n| Ch 1: Understanding Large Language Models                  | No code                                                                                                                         | -                             |\n| Ch 2: Working with Text Data                               | - [ch02.ipynb](ch02/01_main-chapter-code/ch02.ipynb)<br/>- [dataloader.ipynb](ch02/01_main-chapter-code/dataloader.ipynb) (summary)<br/>- [exercise-solutions.ipynb](ch02/01_main-chapter-code/exercise-solutions.ipynb)               | [./ch02](./ch02)            |\n| Ch 3: Coding Attention Mechanisms                          | - [ch03.ipynb](ch03/01_main-chapter-code/ch03.ipynb)<br/>- [multihead-attention.ipynb](ch03/01_main-chapter-code/multihead-attention.ipynb) (summary) <br/>- [exercise-solutions.ipynb](ch03/01_main-chapter-code/exercise-solutions.ipynb)| [./ch03](./ch03)             |\n| Ch 4: Implementing a GPT Model from Scratch                | - [ch04.ipynb](ch04/01_main-chapter-code/ch04.ipynb)<br/>- [gpt.py](ch04/01_main-chapter-code/gpt.py) (summary)<br/>- [exercise-solutions.ipynb](ch04/01_main-chapter-code/exercise-solutions.ipynb) | [./ch04](./ch04)           |\n| Ch 5: Pretraining on Unlabeled Data                        | - [ch05.ipynb](ch05/01_main-chapter-code/ch05.ipynb)<br/>- [gpt_train.py](ch05/01_main-chapter-code/gpt_train.py) (summary) <br/>- [gpt_generate.py](ch05/01_main-chapter-code/gpt_generate.py) (summary) <br/>- [exercise-solutions.ipynb](ch05/01_main-chapter-code/exercise-solutions.ipynb) | [./ch05](./ch05)              |\n| Ch 6: Finetuning for Text Classification                   | - [ch06.ipynb](ch06/01_main-chapter-code/ch06.ipynb)  <br/>- [gpt_class_finetune.py](ch06/01_main-chapter-code/gpt_class_finetune.py)  <br/>- [exercise-solutions.ipynb](ch06/01_main-chapter-code/exercise-solutions.ipynb) | [./ch06](./ch06)              |\n| Ch 7: Finetuning to Follow Instructions                    | - [ch07.ipynb](ch07/01_main-chapter-code/ch07.ipynb)<br/>- [gpt_instruction_finetuning.py](ch07/01_main-chapter-code/gpt_instruction_finetuning.py) (summary)<br/>- [ollama_evaluate.py](ch07/01_main-chapter-code/ollama_evaluate.py) (summary)<br/>- [exercise-solutions.ipynb](ch07/01_main-chapter-code/exercise-solutions.ipynb) | [./ch07](./ch07)  |\n| Appendix A: Introduction to PyTorch                        | - [code-part1.ipynb](appendix-A/01_main-chapter-code/code-part1.ipynb)<br/>- [code-part2.ipynb](appendix-A/01_main-chapter-code/code-part2.ipynb)<br/>- [DDP-script.py](appendix-A/01_main-chapter-code/DDP-script.py)<br/>- [exercise-solutions.ipynb](appendix-A/01_main-chapter-code/exercise-solutions.ipynb) | [./appendix-A](./appendix-A) |\n| Appendix B: References and Further Reading                 | No code                                                                                                                         | -                             |\n| Appendix C: Exercise Solutions                             | No code                                                                                                                         | -                             |\n| Appendix D: Adding Bells and Whistles to the Training Loop | - [appendix-D.ipynb](appendix-D/01_main-chapter-code/appendix-D.ipynb)                                                          | [./appendix-D](./appendix-D)  |\n| Appendix E: Parameter-efficient Finetuning with LoRA       | - [appendix-E.ipynb](appendix-E/01_main-chapter-code/appendix-E.ipynb)                                                          | [./appendix-E](./appendix-E) |\n\n<br>\n&nbsp;\n\nThe mental model below summarizes the contents covered in this book.\n\n<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/mental-model.jpg\" width=\"650px\">\n\n\n<br>\n&nbsp;\n\n## Prerequisites\n\nThe most important prerequisite is a strong foundation in Python programming.\nWith this knowledge, you will be well prepared to explore the fascinating world of LLMs\nand understand the concepts and code examples presented in this book.\n\nIf you have some experience with deep neural networks, you may find certain concepts more familiar, as LLMs are built upon these architectures.\n\nThis book uses PyTorch to implement the code from scratch without using any external LLM libraries. While proficiency in PyTorch is not a prerequisite, familiarity with PyTorch basics is certainly useful. If you are new to PyTorch, Appendix A provides a concise introduction to PyTorch. Alternatively, you may find my book, [PyTorch in One Hour: From Tensors to Training Neural Networks on Multiple GPUs](https://sebastianraschka.com/teaching/pytorch-1h/), helpful for learning about the essentials.\n\n\n\n<br>\n&nbsp;\n\n## Hardware Requirements\n\nThe code in the main chapters of this book is designed to run on conventional laptops within a reasonable timeframe and does not require specialized hardware. This approach ensures that a wide audience can engage with the material. Additionally, the code automatically utilizes GPUs if they are available. (Please see the [setup](https://github.com/rasbt/LLMs-from-scratch/blob/main/setup/README.md) doc for additional recommendations.)\n\n\n&nbsp;\n## Video Course\n\n[A 17-hour and 15-minute companion video course](https://www.manning.com/livevideo/master-and-build-large-language-models) where I code through each chapter of the book. The course is organized into chapters and sections that mirror the book's structure so that it can be used as a standalone alternative to the book or complementary code-along resource.\n\n<a href=\"https://www.manning.com/livevideo/master-and-build-large-language-models\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/video-screenshot.webp?123\" width=\"350px\"></a>\n\n\n\n&nbsp;\n## Exercises\n\nEach chapter of the book includes several exercises. The solutions are summarized in Appendix C, and the corresponding code notebooks are available in the main chapter folders of this repository (for example,  [./ch02/01_main-chapter-code/exercise-solutions.ipynb](./ch02/01_main-chapter-code/exercise-solutions.ipynb).\n\nIn addition to the code exercises, you can download a free 170-page PDF titled  [Test Yourself On Build a Large Language Model (From Scratch)](https://www.manning.com/books/test-yourself-on-build-a-large-language-model-from-scratch) from the Manning website. It contains approximately 30 quiz questions and solutions per chapter to help you test your understanding.\n\n<a href=\"https://www.manning.com/books/test-yourself-on-build-a-large-language-model-from-scratch\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/test-yourself-cover.jpg?123\" width=\"150px\"></a>\n\n\n\n&nbsp;\n## Bonus Material\n\nSeveral folders contain optional materials as a bonus for interested readers:\n\n- **Setup**\n  - [Python Setup Tips](setup/01_optional-python-setup-preferences)\n  - [Installing Python Packages and Libraries Used In This Book](setup/02_installing-python-libraries)\n  - [Docker Environment Setup Guide](setup/03_optional-docker-environment)\n- **Chapter 2: Working with text data**\n  - [Byte Pair Encoding (BPE) Tokenizer From Scratch](ch02/05_bpe-from-scratch/bpe-from-scratch.ipynb)\n  - [Comparing Various Byte Pair Encoding (BPE) Implementations](ch02/02_bonus_bytepair-encoder)\n  - [Understanding the Difference Between Embedding Layers and Linear Layers](ch02/03_bonus_embedding-vs-matmul)\n  - [Dataloader Intuition with Simple Numbers](ch02/04_bonus_dataloader-intuition)\n- **Chapter 3: Coding attention mechanisms**\n  - [Comparing Efficient Multi-Head Attention Implementations](ch03/02_bonus_efficient-multihead-attention/mha-implementations.ipynb)\n  - [Understanding PyTorch Buffers](ch03/03_understanding-buffers/understanding-buffers.ipynb)\n- **Chapter 4: Implementing a GPT model from scratch**\n  - [FLOPS Analysis](ch04/02_performance-analysis/flops-analysis.ipynb)\n  - [KV Cache](ch04/03_kv-cache)\n- **Chapter 5: Pretraining on unlabeled data:**\n  - [Alternative Weight Loading Methods](ch05/02_alternative_weight_loading/)\n  - [Pretraining GPT on the Project Gutenberg Dataset](ch05/03_bonus_pretraining_on_gutenberg)\n  - [Adding Bells and Whistles to the Training Loop](ch05/04_learning_rate_schedulers)\n  - [Optimizing Hyperparameters for Pretraining](ch05/05_bonus_hparam_tuning)\n  - [Building a User Interface to Interact With the Pretrained LLM](ch05/06_user_interface)\n  - [Converting GPT to Llama](ch05/07_gpt_to_llama)\n  - [Llama 3.2 From Scratch](ch05/07_gpt_to_llama/standalone-llama32.ipynb)\n  - [Qwen3 Dense and Mixture-of-Experts (MoE) From Scratch](ch05/11_qwen3/)\n  - [Memory-efficient Model Weight Loading](ch05/08_memory_efficient_weight_loading/memory-efficient-state-dict.ipynb)\n  - [Extending the Tiktoken BPE Tokenizer with New Tokens](ch05/09_extending-tokenizers/extend-tiktoken.ipynb)\n  - [PyTorch Performance Tips for Faster LLM Training](ch05/10_llm-training-speed)\n- **Chapter 6: Finetuning for classification**\n  - [Additional experiments finetuning different layers and using larger models](ch06/02_bonus_additional-experiments)\n  - [Finetuning different models on 50k IMDB movie review dataset](ch06/03_bonus_imdb-classification)\n  - [Building a User Interface to Interact With the GPT-based Spam Classifier](ch06/04_user_interface)\n- **Chapter 7: Finetuning to follow instructions**\n  - [Dataset Utilities for Finding Near Duplicates and Creating Passive Voice Entries](ch07/02_dataset-utilities)\n  - [Evaluating Instruction Responses Using the OpenAI API and Ollama](ch07/03_model-evaluation)\n  - [Generating a Dataset for Instruction Finetuning](ch07/05_dataset-generation/llama3-ollama.ipynb)\n  - [Improving a Dataset for Instruction Finetuning](ch07/05_dataset-generation/reflection-gpt4.ipynb)\n  - [Generating a Preference Dataset with Llama 3.1 70B and Ollama](ch07/04_preference-tuning-with-dpo/create-preference-data-ollama.ipynb)\n  - [Direct Preference Optimization (DPO) for LLM Alignment](ch07/04_preference-tuning-with-dpo/dpo-from-scratch.ipynb)\n  - [Building a User Interface to Interact With the Instruction Finetuned GPT Model](ch07/06_user_interface)\n\n<br>\n&nbsp;\n\n## Questions, Feedback, and Contributing to This Repository\n\n\nI welcome all sorts of feedback, best shared via the [Manning Forum](https://livebook.manning.com/forum?product=raschka&page=1) or [GitHub Discussions](https://github.com/rasbt/LLMs-from-scratch/discussions). Likewise, if you have any questions or just want to bounce ideas off others, please don't hesitate to post these in the forum as well.\n\nPlease note that since this repository contains the code corresponding to a print book, I currently cannot accept contributions that would extend the contents of the main chapter code, as it would introduce deviations from the physical book. Keeping it consistent helps ensure a smooth experience for everyone.\n\n\n&nbsp;\n## Citation\n\nIf you find this book or code useful for your research, please consider citing it.\n\nChicago-style citation:\n\n> Raschka, Sebastian. *Build A Large Language Model (From Scratch)*. Manning, 2024. ISBN: 978-1633437166.\n\nBibTeX entry:\n\n```\n@book{build-llms-from-scratch-book,\n  author       = {Sebastian Raschka},\n  title        = {Build A Large Language Model (From Scratch)},\n  publisher    = {Manning},\n  year         = {2024},\n  isbn         = {978-1633437166},\n  url          = {https://www.manning.com/books/build-a-large-language-model-from-scratch},\n  github       = {https://github.com/rasbt/LLMs-from-scratch}\n}\n```\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 290091948,
    "name": "annotated_deep_learning_paper_implementations",
    "full_name": "labmlai/annotated_deep_learning_paper_implementations",
    "description": "\ud83e\uddd1\u200d\ud83c\udfeb 60+ Implementations/tutorials of deep learning papers with side-by-side notes \ud83d\udcdd; including transformers (original, xl, switch, feedback, vit, ...), optimizers (adam, adabelief, sophia, ...), gans(cyclegan, stylegan2, ...), \ud83c\udfae reinforcement learning (ppo, dqn), capsnet, distillation, ... \ud83e\udde0",
    "html_url": "https://github.com/labmlai/annotated_deep_learning_paper_implementations",
    "clone_url": "https://github.com/labmlai/annotated_deep_learning_paper_implementations.git",
    "owner_login": "labmlai",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/64068543?v=4",
    "stargazers_count": 62311,
    "watchers_count": 62311,
    "forks_count": 6312,
    "open_issues_count": 26,
    "size": 154225,
    "language": "Python",
    "languages": {
      "Python": 1350554,
      "Jupyter Notebook": 158659,
      "Makefile": 1984
    },
    "topics": [
      "attention",
      "deep-learning",
      "deep-learning-tutorial",
      "gan",
      "literate-programming",
      "lora",
      "machine-learning",
      "neural-networks",
      "optimizers",
      "pytorch",
      "reinforcement-learning",
      "transformer",
      "transformers"
    ],
    "license_name": "MIT License",
    "created_at": "2020-08-25T02:29:34+00:00",
    "updated_at": "2025-08-06T02:05:15+00:00",
    "pushed_at": "2025-08-01T10:20:35+00:00",
    "contributors_count": 37,
    "readme_length": 7598,
    "readme_content": "[![Twitter](https://img.shields.io/twitter/follow/labmlai?style=social)](https://twitter.com/labmlai)\n\n# [labml.ai Deep Learning Paper Implementations](https://nn.labml.ai/index.html)\n\nThis is a collection of simple PyTorch implementations of\nneural networks and related algorithms.\nThese implementations are documented with explanations,\n\n[The website](https://nn.labml.ai/index.html)\nrenders these as side-by-side formatted notes.\nWe believe these would help you understand these algorithms better.\n\n![Screenshot](https://nn.labml.ai/dqn-light.png)\n\nWe are actively maintaining this repo and adding new \nimplementations almost weekly.\n[![Twitter](https://img.shields.io/twitter/follow/labmlai?style=social)](https://twitter.com/labmlai) for updates.\n\n## Paper Implementations\n\n#### \u2728 [Transformers](https://nn.labml.ai/transformers/index.html)\n\n* [Multi-headed attention](https://nn.labml.ai/transformers/mha.html)\n* [Transformer building blocks](https://nn.labml.ai/transformers/models.html) \n* [Transformer XL](https://nn.labml.ai/transformers/xl/index.html)\n    * [Relative multi-headed attention](https://nn.labml.ai/transformers/xl/relative_mha.html)\n* [Rotary Positional Embeddings](https://nn.labml.ai/transformers/rope/index.html)\n* [Attention with Linear Biases (ALiBi)](https://nn.labml.ai/transformers/alibi/index.html)\n* [RETRO](https://nn.labml.ai/transformers/retro/index.html)\n* [Compressive Transformer](https://nn.labml.ai/transformers/compressive/index.html)\n* [GPT Architecture](https://nn.labml.ai/transformers/gpt/index.html)\n* [GLU Variants](https://nn.labml.ai/transformers/glu_variants/simple.html)\n* [kNN-LM: Generalization through Memorization](https://nn.labml.ai/transformers/knn)\n* [Feedback Transformer](https://nn.labml.ai/transformers/feedback/index.html)\n* [Switch Transformer](https://nn.labml.ai/transformers/switch/index.html)\n* [Fast Weights Transformer](https://nn.labml.ai/transformers/fast_weights/index.html)\n* [FNet](https://nn.labml.ai/transformers/fnet/index.html)\n* [Attention Free Transformer](https://nn.labml.ai/transformers/aft/index.html)\n* [Masked Language Model](https://nn.labml.ai/transformers/mlm/index.html)\n* [MLP-Mixer: An all-MLP Architecture for Vision](https://nn.labml.ai/transformers/mlp_mixer/index.html)\n* [Pay Attention to MLPs (gMLP)](https://nn.labml.ai/transformers/gmlp/index.html)\n* [Vision Transformer (ViT)](https://nn.labml.ai/transformers/vit/index.html)\n* [Primer EZ](https://nn.labml.ai/transformers/primer_ez/index.html)\n* [Hourglass](https://nn.labml.ai/transformers/hour_glass/index.html)\n\n#### \u2728 [Low-Rank Adaptation (LoRA)](https://nn.labml.ai/lora/index.html)\n\n#### \u2728 [Eleuther GPT-NeoX](https://nn.labml.ai/neox/index.html)\n* [Generate on a 48GB GPU](https://nn.labml.ai/neox/samples/generate.html)\n* [Finetune on two 48GB GPUs](https://nn.labml.ai/neox/samples/finetune.html)\n* [LLM.int8()](https://nn.labml.ai/neox/utils/llm_int8.html)\n\n#### \u2728 [Diffusion models](https://nn.labml.ai/diffusion/index.html)\n\n* [Denoising Diffusion Probabilistic Models (DDPM)](https://nn.labml.ai/diffusion/ddpm/index.html)\n* [Denoising Diffusion Implicit Models (DDIM)](https://nn.labml.ai/diffusion/stable_diffusion/sampler/ddim.html)\n* [Latent Diffusion Models](https://nn.labml.ai/diffusion/stable_diffusion/latent_diffusion.html)\n* [Stable Diffusion](https://nn.labml.ai/diffusion/stable_diffusion/index.html)\n\n#### \u2728 [Generative Adversarial Networks](https://nn.labml.ai/gan/index.html)\n* [Original GAN](https://nn.labml.ai/gan/original/index.html)\n* [GAN with deep convolutional network](https://nn.labml.ai/gan/dcgan/index.html)\n* [Cycle GAN](https://nn.labml.ai/gan/cycle_gan/index.html)\n* [Wasserstein GAN](https://nn.labml.ai/gan/wasserstein/index.html)\n* [Wasserstein GAN with Gradient Penalty](https://nn.labml.ai/gan/wasserstein/gradient_penalty/index.html)\n* [StyleGAN 2](https://nn.labml.ai/gan/stylegan/index.html)\n\n#### \u2728 [Recurrent Highway Networks](https://nn.labml.ai/recurrent_highway_networks/index.html)\n\n#### \u2728 [LSTM](https://nn.labml.ai/lstm/index.html)\n\n#### \u2728 [HyperNetworks - HyperLSTM](https://nn.labml.ai/hypernetworks/hyper_lstm.html)\n\n#### \u2728 [ResNet](https://nn.labml.ai/resnet/index.html)\n\n#### \u2728 [ConvMixer](https://nn.labml.ai/conv_mixer/index.html)\n\n#### \u2728 [Capsule Networks](https://nn.labml.ai/capsule_networks/index.html)\n\n#### \u2728 [U-Net](https://nn.labml.ai/unet/index.html)\n\n#### \u2728 [Sketch RNN](https://nn.labml.ai/sketch_rnn/index.html)\n\n#### \u2728 Graph Neural Networks\n\n* [Graph Attention Networks (GAT)](https://nn.labml.ai/graphs/gat/index.html)\n* [Graph Attention Networks v2 (GATv2)](https://nn.labml.ai/graphs/gatv2/index.html)\n\n#### \u2728 [Counterfactual Regret Minimization (CFR)](https://nn.labml.ai/cfr/index.html)\n\nSolving games with incomplete information such as poker with CFR.\n\n* [Kuhn Poker](https://nn.labml.ai/cfr/kuhn/index.html)\n\n#### \u2728 [Reinforcement Learning](https://nn.labml.ai/rl/index.html)\n* [Proximal Policy Optimization](https://nn.labml.ai/rl/ppo/index.html) with\n [Generalized Advantage Estimation](https://nn.labml.ai/rl/ppo/gae.html)\n* [Deep Q Networks](https://nn.labml.ai/rl/dqn/index.html) with\n with [Dueling Network](https://nn.labml.ai/rl/dqn/model.html),\n [Prioritized Replay](https://nn.labml.ai/rl/dqn/replay_buffer.html)\n and Double Q Network.\n\n#### \u2728 [Optimizers](https://nn.labml.ai/optimizers/index.html)\n* [Adam](https://nn.labml.ai/optimizers/adam.html)\n* [AMSGrad](https://nn.labml.ai/optimizers/amsgrad.html)\n* [Adam Optimizer with warmup](https://nn.labml.ai/optimizers/adam_warmup.html)\n* [Noam Optimizer](https://nn.labml.ai/optimizers/noam.html)\n* [Rectified Adam Optimizer](https://nn.labml.ai/optimizers/radam.html)\n* [AdaBelief Optimizer](https://nn.labml.ai/optimizers/ada_belief.html)\n* [Sophia-G Optimizer](https://nn.labml.ai/optimizers/sophia.html)\n\n#### \u2728 [Normalization Layers](https://nn.labml.ai/normalization/index.html)\n* [Batch Normalization](https://nn.labml.ai/normalization/batch_norm/index.html)\n* [Layer Normalization](https://nn.labml.ai/normalization/layer_norm/index.html)\n* [Instance Normalization](https://nn.labml.ai/normalization/instance_norm/index.html)\n* [Group Normalization](https://nn.labml.ai/normalization/group_norm/index.html)\n* [Weight Standardization](https://nn.labml.ai/normalization/weight_standardization/index.html)\n* [Batch-Channel Normalization](https://nn.labml.ai/normalization/batch_channel_norm/index.html)\n* [DeepNorm](https://nn.labml.ai/normalization/deep_norm/index.html)\n\n#### \u2728 [Distillation](https://nn.labml.ai/distillation/index.html)\n\n#### \u2728 [Adaptive Computation](https://nn.labml.ai/adaptive_computation/index.html)\n\n* [PonderNet](https://nn.labml.ai/adaptive_computation/ponder_net/index.html)\n\n#### \u2728 [Uncertainty](https://nn.labml.ai/uncertainty/index.html)\n\n* [Evidential Deep Learning to Quantify Classification Uncertainty](https://nn.labml.ai/uncertainty/evidence/index.html)\n\n#### \u2728 [Activations](https://nn.labml.ai/activations/index.html)\n\n* [Fuzzy Tiling Activations](https://nn.labml.ai/activations/fta/index.html)\n\n#### \u2728 [Langauge Model Sampling Techniques](https://nn.labml.ai/sampling/index.html)\n* [Greedy Sampling](https://nn.labml.ai/sampling/greedy.html)\n* [Temperature Sampling](https://nn.labml.ai/sampling/temperature.html)\n* [Top-k Sampling](https://nn.labml.ai/sampling/top_k.html)\n* [Nucleus Sampling](https://nn.labml.ai/sampling/nucleus.html)\n\n#### \u2728 [Scalable Training/Inference](https://nn.labml.ai/scaling/index.html)\n* [Zero3 memory optimizations](https://nn.labml.ai/scaling/zero3/index.html)\n\n### Installation\n\n```bash\npip install labml-nn\n```\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 655099582,
    "name": "llm-course",
    "full_name": "mlabonne/llm-course",
    "description": "Course to get into Large Language Models (LLMs) with roadmaps and Colab notebooks.",
    "html_url": "https://github.com/mlabonne/llm-course",
    "clone_url": "https://github.com/mlabonne/llm-course.git",
    "owner_login": "mlabonne",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/81252890?v=4",
    "stargazers_count": 58612,
    "watchers_count": 58612,
    "forks_count": 6340,
    "open_issues_count": 64,
    "size": 7742,
    "language": null,
    "languages": {},
    "topics": [
      "course",
      "large-language-models",
      "llm",
      "machine-learning",
      "roadmap"
    ],
    "license_name": "Apache License 2.0",
    "created_at": "2023-06-17T22:16:25+00:00",
    "updated_at": "2025-08-06T01:23:02+00:00",
    "pushed_at": "2025-06-04T16:09:23+00:00",
    "contributors_count": 2,
    "readme_length": 56195,
    "readme_content": "<div align=\"center\">\n<img src=\"img/banner.png\" alt=\"LLM Course\">\n  <p align=\"center\">\n    \ud835\udd4f <a href=\"https://twitter.com/maximelabonne\">Follow me on X</a> \u2022 \n    \ud83e\udd17 <a href=\"https://huggingface.co/mlabonne\">Hugging Face</a> \u2022 \n    \ud83d\udcbb <a href=\"https://mlabonne.github.io/blog\">Blog</a> \u2022 \n    \ud83d\udcd9 <a href=\"https://packt.link/a/9781836200079\">LLM Engineer's Handbook</a>\n  </p>\n</div>\n<br/>\n\n<a href=\"https://a.co/d/a2M67rE\"><img align=\"right\" width=\"25%\" src=\"https://i.imgur.com/7iNjEq2.png\" alt=\"LLM Engineer's Handbook Cover\"/></a>The LLM course is divided into three parts:\n\n1. \ud83e\udde9 **LLM Fundamentals** is optional and covers fundamental knowledge about mathematics, Python, and neural networks.\n2. \ud83e\uddd1\u200d\ud83d\udd2c **The LLM Scientist** focuses on building the best possible LLMs using the latest techniques.\n3. \ud83d\udc77 **The LLM Engineer** focuses on creating LLM-based applications and deploying them.\n\n> [!NOTE]\n> Based on this course, I wrote the [LLM Engineer's Handbook](https://packt.link/a/9781836200079) with Paul Iuzstin. It's a hands-on and detailed book that covers an end-to-end LLM application from design to deployment. The LLM course will always stay free but feel free to support my work by purchasing the book.\n\nFor an interactive version of this course, I created an LLM assistant that will answer questions and test your knowledge in a personalized way on [**HuggingChat**](https://hf.co/chat/assistant/66029d2e5f4a884f7aabc9d1) or [**ChatGPT**](https://chat.openai.com/g/g-yviLuLqvI-llm-course).\n\n## \ud83d\udcdd Notebooks\n\nA list of notebooks and articles I wrote about LLMs.\n\n### Tools\n\n| Notebook | Description | Notebook |\n|----------|-------------|----------|\n| \ud83e\uddd0 [LLM AutoEval](https://github.com/mlabonne/llm-autoeval) | Automatically evaluate your LLMs using RunPod | <a href=\"https://colab.research.google.com/drive/1Igs3WZuXAIv9X0vwqiE90QlEPys8e8Oa?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n| \ud83e\udd71 LazyMergekit | Easily merge models using MergeKit in one click. | <a href=\"https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n| \ud83e\udd8e LazyAxolotl | Fine-tune models in the cloud using Axolotl in one click. | <a href=\"https://colab.research.google.com/drive/1TsDKNo2riwVmU55gjuBgB1AXVtRRfRHW?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n| \u26a1 AutoQuant | Quantize LLMs in GGUF, GPTQ, EXL2, AWQ, and HQQ formats in one click. | <a href=\"https://colab.research.google.com/drive/1b6nqC7UZVt8bx4MksX7s656GXPM-eWw4?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n| \ud83c\udf33 Model Family Tree | Visualize the family tree of merged models. | <a href=\"https://colab.research.google.com/drive/1s2eQlolcI1VGgDhqWIANfkfKvcKrMyNr?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n| \ud83d\ude80 ZeroSpace | Automatically create a Gradio chat interface using a free ZeroGPU. | <a href=\"https://colab.research.google.com/drive/1LcVUW5wsJTO2NGmozjji5CkC--646LgC\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n| \u2702\ufe0f AutoAbliteration | Automatically abliteration models with custom datasets. | <a href=\"https://colab.research.google.com/drive/1RmLv-pCMBBsQGXQIM8yF-OdCNyoylUR1?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n| \ud83e\uddfc AutoDedup | Automatically deduplicate datasets using the Rensa library. | <a href=\"https://colab.research.google.com/drive/1o1nzwXWAa8kdkEJljbJFW1VuI-3VZLUn?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n\n### Fine-tuning\n\n| Notebook | Description | Article | Notebook |\n|---------------------------------------|-------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Fine-tune Llama 3.1 with Unsloth | Ultra-efficient supervised fine-tuning in Google Colab. | [Article](https://mlabonne.github.io/blog/posts/2024-07-29_Finetune_Llama31.html) | <a href=\"https://colab.research.google.com/drive/164cg_O7SV7G8kZr_JXqLd6VC7pd86-1Z?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n| Fine-tune Llama 3 with ORPO | Cheaper and faster fine-tuning in a single stage with ORPO. | [Article](https://mlabonne.github.io/blog/posts/2024-04-19_Fine_tune_Llama_3_with_ORPO.html) | <a href=\"https://colab.research.google.com/drive/1eHNWg9gnaXErdAa8_mcvjMupbSS6rDvi\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n| Fine-tune Mistral-7b with DPO | Boost the performance of supervised fine-tuned models with DPO. | [Article](https://mlabonne.github.io/blog/posts/Fine_tune_Mistral_7b_with_DPO.html) | <a href=\"https://colab.research.google.com/drive/15iFBr1xWgztXvhrj5I9fBv20c7CFOPBE?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n| Fine-tune Mistral-7b with QLoRA | Supervised fine-tune Mistral-7b in a free-tier Google Colab with TRL. |  | <a href=\"https://colab.research.google.com/drive/1o_w0KastmEJNVwT5GoqMCciH-18ca5WS?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n| Fine-tune CodeLlama using Axolotl | End-to-end guide to the state-of-the-art tool for fine-tuning. | [Article](https://mlabonne.github.io/blog/posts/A_Beginners_Guide_to_LLM_Finetuning.html) | <a href=\"https://colab.research.google.com/drive/1Xu0BrCB7IShwSWKVcfAfhehwjDrDMH5m?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n| Fine-tune Llama 2 with QLoRA | Step-by-step guide to supervised fine-tune Llama 2 in Google Colab. | [Article](https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html) | <a href=\"https://colab.research.google.com/drive/1PEQyJO1-f6j0S_XJ8DV50NkpzasXkrzd?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n\n### Quantization\n\n| Notebook | Description | Article | Notebook |\n|---------------------------------------|-------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Introduction to Quantization | Large language model optimization using 8-bit quantization. | [Article](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html) | <a href=\"https://colab.research.google.com/drive/1DPr4mUQ92Cc-xf4GgAaB6dFcFnWIvqYi?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n| 4-bit Quantization using GPTQ | Quantize your own open-source LLMs to run them on consumer hardware. | [Article](https://mlabonne.github.io/blog/4bit_quantization/) | <a href=\"https://colab.research.google.com/drive/1lSvVDaRgqQp_mWK_jC9gydz6_-y6Aq4A?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n| Quantization with GGUF and llama.cpp | Quantize Llama 2 models with llama.cpp and upload GGUF versions to the HF Hub. | [Article](https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html) | <a href=\"https://colab.research.google.com/drive/1pL8k7m04mgE5jo2NrjGi8atB0j_37aDD?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n| ExLlamaV2: The Fastest Library to Run\u00a0LLMs | Quantize and run EXL2\u00a0models and upload them to the HF Hub. | [Article](https://mlabonne.github.io/blog/posts/ExLlamaV2_The_Fastest_Library_to_Run%C2%A0LLMs.html) | <a href=\"https://colab.research.google.com/drive/1yrq4XBlxiA0fALtMoT2dwiACVc77PHou?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n\n### Other\n\n| Notebook | Description | Article | Notebook |\n|---------------------------------------|-------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Merge LLMs with MergeKit | Create your own models easily, no GPU required! | [Article](https://mlabonne.github.io/blog/posts/2024-01-08_Merge_LLMs_with_mergekit%20copy.html) | <a href=\"https://colab.research.google.com/drive/1_JS7JKJAQozD48-LhYdegcuuZ2ddgXfr?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n| Create MoEs with MergeKit | Combine multiple experts into a single frankenMoE | [Article](https://mlabonne.github.io/blog/posts/2024-03-28_Create_Mixture_of_Experts_with_MergeKit.html) | <a href=\"https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n| Uncensor any LLM with abliteration | Fine-tuning without retraining | [Article](https://mlabonne.github.io/blog/posts/2024-06-04_Uncensor_any_LLM_with_abliteration.html) | <a href=\"https://colab.research.google.com/drive/1VYm3hOcvCpbGiqKZb141gJwjdmmCcVpR?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n| Improve ChatGPT with Knowledge Graphs | Augment ChatGPT's answers with knowledge graphs. | [Article](https://mlabonne.github.io/blog/posts/Article_Improve_ChatGPT_with_Knowledge_Graphs.html) | <a href=\"https://colab.research.google.com/drive/1mwhOSw9Y9bgEaIFKT4CLi0n18pXRM4cj?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n| Decoding Strategies in Large Language Models | A guide to text generation from beam search to nucleus sampling | [Article](https://mlabonne.github.io/blog/posts/2022-06-07-Decoding_strategies.html) | <a href=\"https://colab.research.google.com/drive/19CJlOS5lI29g-B3dziNn93Enez1yiHk2?usp=sharing\"><img src=\"img/colab.svg\" alt=\"Open In Colab\"></a> |\n\n## \ud83e\udde9 LLM Fundamentals\n\nThis section introduces essential knowledge about mathematics, Python, and neural networks. You might not want to start here but refer to it as needed.\n\n<details>\n<summary>Toggle section (optional)</summary>\n  \n![](img/roadmap_fundamentals.png)\n\n### 1. Mathematics for Machine Learning\n\nBefore mastering machine learning, it is important to understand the fundamental mathematical concepts that power these algorithms.\n\n- **Linear Algebra**: This is crucial for understanding many algorithms, especially those used in deep learning. Key concepts include vectors, matrices, determinants, eigenvalues and eigenvectors, vector spaces, and linear transformations.\n- **Calculus**: Many machine learning algorithms involve the optimization of continuous functions, which requires an understanding of derivatives, integrals, limits, and series. Multivariable calculus and the concept of gradients are also important.\n- **Probability and Statistics**: These are crucial for understanding how models learn from data and make predictions. Key concepts include probability theory, random variables, probability distributions, expectations, variance, covariance, correlation, hypothesis testing, confidence intervals, maximum likelihood estimation, and Bayesian inference.\n\n\ud83d\udcda Resources:\n\n- [3Blue1Brown - The Essence of Linear Algebra](https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab): Series of videos that give a geometric intuition to these concepts.\n- [StatQuest with Josh Starmer - Statistics Fundamentals](https://www.youtube.com/watch?v=qBigTkBLU6g&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9): Offers simple and clear explanations for many statistical concepts.\n- [AP Statistics Intuition by Ms Aerin](https://automata88.medium.com/list/cacc224d5e7d): List of Medium articles that provide the intuition behind every probability distribution.\n- [Immersive Linear Algebra](https://immersivemath.com/ila/learnmore.html): Another visual interpretation of linear algebra.\n- [Khan Academy - Linear Algebra](https://www.khanacademy.org/math/linear-algebra): Great for beginners as it explains the concepts in a very intuitive way.\n- [Khan Academy - Calculus](https://www.khanacademy.org/math/calculus-1): An interactive course that covers all the basics of calculus.\n- [Khan Academy - Probability and Statistics](https://www.khanacademy.org/math/statistics-probability): Delivers the material in an easy-to-understand format.\n\n---\n\n### 2. Python for Machine Learning\n\nPython is a powerful and flexible programming language that's particularly good for machine learning, thanks to its readability, consistency, and robust ecosystem of data science libraries.\n\n- **Python Basics**: Python programming requires a good understanding of the basic syntax, data types, error handling, and object-oriented programming.\n- **Data Science Libraries**: It includes familiarity with NumPy for numerical operations, Pandas for data manipulation and analysis, Matplotlib and Seaborn for data visualization.\n- **Data Preprocessing**: This involves feature scaling and normalization, handling missing data, outlier detection, categorical data encoding, and splitting data into training, validation, and test sets.\n- **Machine Learning Libraries**: Proficiency with Scikit-learn, a library providing a wide selection of supervised and unsupervised learning algorithms, is vital. Understanding how to implement algorithms like linear regression, logistic regression, decision trees, random forests, k-nearest neighbors (K-NN), and K-means clustering is important. Dimensionality reduction techniques like PCA and t-SNE are also helpful for visualizing high-dimensional data.\n\n\ud83d\udcda Resources:\n\n- [Real Python](https://realpython.com/): A comprehensive resource with articles and tutorials for both beginner and advanced Python concepts.\n- [freeCodeCamp - Learn Python](https://www.youtube.com/watch?v=rfscVS0vtbw): Long video that provides a full introduction into all of the core concepts in Python.\n- [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/): Free digital book that is a great resource for learning pandas, NumPy, Matplotlib, and Seaborn.\n- [freeCodeCamp - Machine Learning for Everybody](https://youtu.be/i_LwzRVP7bg): Practical introduction to different machine learning algorithms for beginners.\n- [Udacity - Intro to Machine Learning](https://www.udacity.com/course/intro-to-machine-learning--ud120): Free course that covers PCA and several other machine learning concepts.\n\n---\n\n### 3. Neural Networks\n\nNeural networks are a fundamental part of many machine learning models, particularly in the realm of deep learning. To utilize them effectively, a comprehensive understanding of their design and mechanics is essential.\n\n- **Fundamentals**: This includes understanding the structure of a neural network, such as layers, weights, biases, and activation functions (sigmoid, tanh, ReLU, etc.)\n- **Training and Optimization**: Familiarize yourself with backpropagation and different types of loss functions, like Mean Squared Error (MSE) and Cross-Entropy. Understand various optimization algorithms like Gradient Descent, Stochastic Gradient Descent, RMSprop, and Adam.\n- **Overfitting**: Understand the concept of overfitting (where a model performs well on training data but poorly on unseen data) and learn various regularization techniques (dropout, L1/L2 regularization, early stopping, data augmentation) to prevent it.\n- **Implement a Multilayer Perceptron (MLP)**: Build an MLP, also known as a fully connected network, using PyTorch.\n\n\ud83d\udcda Resources:\n\n- [3Blue1Brown - But what is a Neural Network?](https://www.youtube.com/watch?v=aircAruvnKk): This video gives an intuitive explanation of neural networks and their inner workings.\n- [freeCodeCamp - Deep Learning Crash Course](https://www.youtube.com/watch?v=VyWAvY2CF9c): This video efficiently introduces all the most important concepts in deep learning.\n- [Fast.ai - Practical Deep Learning](https://course.fast.ai/): Free course designed for people with coding experience who want to learn about deep learning.\n- [Patrick Loeber - PyTorch Tutorials](https://www.youtube.com/playlist?list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4): Series of videos for complete beginners to learn about PyTorch.\n\n---\n\n### 4. Natural Language Processing (NLP)\n\nNLP is a fascinating branch of artificial intelligence that bridges the gap between human language and machine understanding. From simple text processing to understanding linguistic nuances, NLP plays a crucial role in many applications like translation, sentiment analysis, chatbots, and much more.\n\n- **Text Preprocessing**: Learn various text preprocessing steps like tokenization (splitting text into words or sentences), stemming (reducing words to their root form), lemmatization (similar to stemming but considers the context), stop word removal, etc.\n- **Feature Extraction Techniques**: Become familiar with techniques to convert text data into a format that can be understood by machine learning algorithms. Key methods include Bag-of-words (BoW), Term Frequency-Inverse Document Frequency (TF-IDF), and n-grams.\n- **Word Embeddings**: Word embeddings are a type of word representation that allows words with similar meanings to have similar representations. Key methods include Word2Vec, GloVe, and FastText.\n- **Recurrent Neural Networks (RNNs)**: Understand the working of RNNs, a type of neural network designed to work with sequence data. Explore LSTMs and GRUs, two RNN variants that are capable of learning long-term dependencies.\n\n\ud83d\udcda Resources:\n\n- [Lena Voita - Word Embeddings](https://lena-voita.github.io/nlp_course/word_embeddings.html): Beginner-friendly course about concepts related to word embeddings.\n- [RealPython - NLP with spaCy in Python](https://realpython.com/natural-language-processing-spacy-python/): Exhaustive guide about the spaCy library for NLP tasks in Python.\n- [Kaggle - NLP Guide](https://www.kaggle.com/learn-guide/natural-language-processing): A few notebooks and resources for a hands-on explanation of NLP in Python.\n- [Jay Alammar - The Illustration Word2Vec](https://jalammar.github.io/illustrated-word2vec/): A good reference to understand the famous Word2Vec architecture.\n- [Jake Tae - PyTorch RNN from Scratch](https://jaketae.github.io/study/pytorch-rnn/): Practical and simple implementation of RNN, LSTM, and GRU models in PyTorch.\n- [colah's blog - Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/): A more theoretical article about the LSTM network.\n</details>\n\n## \ud83e\uddd1\u200d\ud83d\udd2c The LLM Scientist\n\nThis section of the course focuses on learning how to build the best possible LLMs using the latest techniques.\n\n![](img/roadmap_scientist.png)\n\n### 1. The LLM architecture\n\nAn in-depth knowledge of the Transformer architecture is not required, but it's important to understand the main steps of modern LLMs: converting text into numbers through tokenization, processing these tokens through layers including attention mechanisms, and finally generating new text through various sampling strategies.\n\n- **Architectural Overview**: Understand the evolution from encoder-decoder Transformers to decoder-only architectures like GPT, which form the basis of modern LLMs. Focus on how these models process and generate text at a high level.\n- **Tokenization**: Learn the principles of tokenization - how text is converted into numerical representations that LLMs can process. Explore different tokenization strategies and their impact on model performance and output quality.\n- **Attention mechanisms**: Master the core concepts of attention mechanisms, particularly self-attention and its variants. Understand how these mechanisms enable LLMs to process long-range dependencies and maintain context throughout sequences.\n- **Sampling techniques**: Explore various text generation approaches and their tradeoffs. Compare deterministic methods like greedy search and beam search with probabilistic approaches like temperature sampling and nucleus sampling.\n\n\ud83d\udcda **References**:\n* [Visual intro to Transformers](https://www.youtube.com/watch?v=wjZofJX0v4M) by 3Blue1Brown: Visual introduction to Transformers for complete beginners.\n* [LLM Visualization](https://bbycroft.net/llm) by Brendan Bycroft: Interactive 3D visualization of LLM internals.\n* [nanoGPT](https://www.youtube.com/watch?v=kCc8FmEb1nY) by Andrej Karpathy: A 2h-long YouTube video to reimplement GPT from scratch (for programmers). He also made a video about [tokenization](https://www.youtube.com/watch?v=zduSFxRajkE).\n* [Attention? Attention!](https://lilianweng.github.io/posts/2018-06-24-attention/) by Lilian Weng: Historical overview to introduce the need for attention mechanisms.\n* [Decoding Strategies in LLMs](https://mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html) by Maxime Labonne: Provide code and a visual introduction to the different decoding strategies to generate text.\n\n---\n### 2. Pre-training models\n\nPre-training is a computationally intensive and expensive process. While it's not the focus of this course, it's important to have a solid understanding of how models are pre-trained, especially in terms of data and parameters. Pre-training can also be performed by hobbyists at a small scale with <1B models.\n\n* **Data preparation**: Pre-training requires massive datasets (e.g., [Llama 3.1](https://arxiv.org/abs/2307.09288) was trained on 15 trillion tokens) that need careful curation, cleaning, deduplication, and tokenization. Modern pre-training pipelines implement sophisticated filtering to remove low-quality or problematic content.\n* **Distributed training**: Combine different parallelization strategies: data parallel (batch distribution), pipeline parallel (layer distribution), and tensor parallel (operation splitting). These strategies require optimized network communication and memory management across GPU clusters.\n* **Training optimization**: Use adaptive learning rates with warm-up, gradient clipping, and normalization to prevent explosions, mixed-precision training for memory efficiency, and modern optimizers (AdamW, Lion) with tuned hyperparameters.\n* **Monitoring**: Track key metrics (loss, gradients, GPU stats) using dashboards, implement targeted logging for distributed training issues, and set up performance profiling to identify bottlenecks in computation and communication across devices.\n\n\ud83d\udcda **References**:\n* [FineWeb](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1) by Penedo et al.: Article to recreate a large-scale dataset for LLM pretraining (15T), including FineWeb-Edu, a high-quality subset.\n* [RedPajama v2](https://www.together.ai/blog/redpajama-data-v2) by Weber et al.: Another article and paper about a large-scale pre-training dataset with a lot of interesting quality filters.\n* [nanotron](https://github.com/huggingface/nanotron) by Hugging Face: Minimalistic LLM training codebase used to make [SmolLM2](https://github.com/huggingface/smollm).\n* [Parallel training](https://www.andrew.cmu.edu/course/11-667/lectures/W10L2%20Scaling%20Up%20Parallel%20Training.pdf) by Chenyan Xiong: Overview of optimization and parallelism techniques.\n* [Distributed training](https://arxiv.org/abs/2407.20018) by Duan et al.: A survey about efficient training of LLM on distributed architectures.\n* [OLMo 2](https://allenai.org/olmo) by AI2: Open-source language model with model, data, training, and evaluation code.\n* [LLM360](https://www.llm360.ai/) by LLM360: A framework for open-source LLMs with training and data preparation code, data, metrics, and models.\n\n---\n### 3. Post-training datasets\n\nPost-training datasets have a precise structure with instructions and answers (supervised fine-tuning) or instructions and chosen/rejected answers (preference alignment). Conversational structures are a lot rarer than the raw text used for pre-training, which is why we often need to process seed data and refine it to improve the accuracy, diversity, and complexity of the samples. More information and examples are available in my repo [\ud83d\udcbe LLM Datasets](https://github.com/mlabonne/llm-datasets).\n\n* **Storage & chat templates**: Because of the conversational structure, post-training datasets are stored in a specific format like ShareGPT or OpenAI/HF. Then, these formats are mapped to a chat template like ChatML or Alpaca to produce the final samples the model is trained on.\n* **Synthetic data generation**: Create instruction-response pairs based on seed data using frontier models like GPT-4o. This approach allows for flexible and scalable dataset creation with high-quality answers. Key considerations include designing diverse seed tasks and effective system prompts.\n* **Data enhancement**: Enhance existing samples using techniques like verified outputs (using unit tests or solvers), multiple answers with rejection sampling, [Auto-Evol](https://arxiv.org/abs/2406.00770), Chain-of-Thought, Branch-Solve-Merge, personas, etc.\n* **Quality filtering**: Traditional techniques involve rule-based filtering, removing duplicates or near-duplicates (with MinHash or embeddings), and n-gram decontamination. Reward models and judge LLMs complement this step with fine-grained and customizable quality control.\n\n\ud83d\udcda **References**:\n* [Synthetic Data Generator](https://huggingface.co/spaces/argilla/synthetic-data-generator) by Argilla: Beginner-friendly way of building datasets using natural language in a Hugging Face space.\n* [LLM Datasets](https://github.com/mlabonne/llm-datasets) by Maxime Labonne: Curated list of datasets and tools for post-training.\n* [NeMo-Curator](https://github.com/NVIDIA/NeMo-Curator) by Nvidia: Dataset preparation and curation framework for pre- and post-training data.\n* [Distilabel](https://distilabel.argilla.io/dev/sections/pipeline_samples/) by Argilla: Framework to generate synthetic data. It also includes interesting reproductions of papers like UltraFeedback.\n* [Semhash](https://github.com/MinishLab/semhash) by MinishLab: Minimalistic library for near-deduplication and decontamination with a distilled embedding model.\n* [Chat Template](https://huggingface.co/docs/transformers/main/en/chat_templating) by Hugging Face: Hugging Face's documentation about chat templates.\n\n---\n### 4. Supervised Fine-Tuning\n\nSFT turns base models into helpful assistants, capable of answering questions and following instructions. During this process, they learn how to structure answers and reactivate a subset of knowledge learned during pre-training. Instilling new knowledge is possible but superficial: it cannot be used to learn a completely new language. Always prioritize data quality over parameter optimization.\n\n- **Training techniques**: Full fine-tuning updates all model parameters but requires significant compute. Parameter-efficient fine-tuning techniques like LoRA and QLoRA reduce memory requirements by training a small number of adapter parameters while keeping base weights frozen. QLoRA combines 4-bit quantization with LoRA to reduce VRAM usage. These techniques are all implemented in the most popular fine-tuning frameworks: [TRL](https://huggingface.co/docs/trl/en/index), [Unsloth](https://docs.unsloth.ai/), and [Axolotl](https://axolotl.ai/).\n- **Training parameters**: Key parameters include learning rate with schedulers, batch size, gradient accumulation, number of epochs, optimizer (like 8-bit AdamW), weight decay for regularization, and warmup steps for training stability. LoRA also adds three parameters: rank (typically 16-128), alpha (1-2x rank), and target modules.\n- **Distributed training**: Scale training across multiple GPUs using DeepSpeed or FSDP. DeepSpeed provides three ZeRO optimization stages with increasing levels of memory efficiency through state partitioning. Both methods support gradient checkpointing for memory efficiency.\n- **Monitoring**: Track training metrics including loss curves, learning rate schedules, and gradient norms. Monitor for common issues like loss spikes, gradient explosions, or performance degradation.\n\n\ud83d\udcda **References**:\n* [Fine-tune Llama 3.1 Ultra-Efficiently with Unsloth](https://huggingface.co/blog/mlabonne/sft-llama3) by Maxime Labonne: Hands-on tutorial on how to fine-tune a Llama 3.1 model using Unsloth.\n* [Axolotl - Documentation](https://axolotl-ai-cloud.github.io/axolotl/) by Wing Lian: Lots of interesting information related to distributed training and dataset formats.\n* [Mastering LLMs](https://parlance-labs.com/education/) by Hamel Husain: Collection of educational resources about fine-tuning (but also RAG, evaluation, applications, and prompt engineering).\n* [LoRA insights](https://lightning.ai/pages/community/lora-insights/) by Sebastian Raschka: Practical insights about LoRA and how to select the best parameters.\n\n---\n### 5. Preference Alignment\n\nPreference alignment is a second stage in the post-training pipeline, focused on aligning generated answers with human preferences. This stage was designed to tune the tone of LLMs and reduce toxicity and hallucinations. However, it has become increasingly important to also boost their performance and improve usefulness. Unlike SFT, there are many preference alignment algorithms. Here, we'll focus on the three most important ones: DPO, GRPO, and PPO.\n\n- **Rejection sampling**: For each prompt, use the trained model to generate multiple responses, and score them to infer chosen/rejected answers. This creates on-policy data, where both responses come from the model being trained, improving alignment stability.\n- **[Direct Preference Optimization](https://arxiv.org/abs/2305.18290)** Directly optimizes the policy to maximize the likelihood of chosen responses over rejected ones. It doesn't require reward modeling, which makes it more computationally efficient than RL techniques but slightly worse in terms of quality. Great for creating chat models.\n- **Reward model**: Train a reward model with human feedback to predict metrics like human preferences. It can leverage frameworks like [TRL](https://huggingface.co/docs/trl/en/index), [verl](https://github.com/volcengine/verl), and [OpenRLHF](https://github.com/OpenRLHF/OpenRLHF) for scalable training.\n- **Reinforcement Learning**: RL techniques like [GRPO](https://arxiv.org/abs/2402.03300) and [PPO](https://arxiv.org/abs/1707.06347) iteratively update a policy to maximize rewards while staying close to the initial behavior. They can use a reward model or reward functions to score responses. They tend to be computationally expensive and require careful tuning of hyperparameters, including learning rate, batch size, and clip range. Ideal for creating reasoning models.\n\n\ud83d\udcda **References**:\n* [Illustrating RLHF](https://huggingface.co/blog/rlhf) by Hugging Face: Introduction to RLHF with reward model training and fine-tuning with reinforcement learning.\n* [LLM Training: RLHF and Its Alternatives](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives) by Sebastian Raschka: Overview of the RLHF process and alternatives like RLAIF.\n* [Preference Tuning LLMs](https://huggingface.co/blog/pref-tuning) by Hugging Face: Comparison of the DPO, IPO, and KTO algorithms to perform preference alignment.\n* [Fine-tune with DPO](https://mlabonne.github.io/blog/posts/Fine_tune_Mistral_7b_with_DPO.html) by Maxime Labonne: Tutorial to fine-tune a Mistral-7b model with DPO and reproduce [NeuralHermes-2.5](https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B).\n* [Fine-tune with GRPO](https://huggingface.co/learn/llm-course/en/chapter12/5) by Maxime Labonne: Practical exercise to fine-tune a small model with GRPO.\n* [DPO Wandb logs](https://wandb.ai/alexander-vishnevskiy/dpo/reports/TRL-Original-DPO--Vmlldzo1NjI4MTc4) by Alexander Vishnevskiy: It shows you the main DPO metrics to track and the trends you should expect.\n\n---\n### 6. Evaluation\n\nReliably evaluating LLMs is a complex but essential task guiding data generation and training. It provides invaluable feedback about areas of improvement, which can be leveraged to modify the data mixture, quality, and training parameters. However, it's always good to remember Goodhart's law: \"When a measure becomes a target, it ceases to be a good measure.\"\n\n- **Automated benchmarks**: Evaluate models on specific tasks using curated datasets and metrics, like MMLU. It works well for concrete tasks but struggles with abstract and creative capabilities. It is also prone to data contamination.\n- **Human evaluation**: It involves humans prompting models and grading responses. Methods range from vibe checks to systematic annotations with specific guidelines and large-scale community voting (arena). It is more suited for subjective tasks and less reliable for factual accuracy.\n- **Model-based evaluation**: Use judge and reward models to evaluate model outputs. It highly correlates with human preferences but suffers from bias toward their own outputs and inconsistent scoring.\n- **Feedback signal**: Analyze error patterns to identify specific weaknesses, such as limitations in following complex instructions, lack of specific knowledge, or susceptibility to adversarial prompts. This can be improved with better data generation and training parameters.\n\n\ud83d\udcda **References**:\n* [Evaluation guidebook](https://github.com/huggingface/evaluation-guidebook) by Cl\u00e9mentine Fourrier: Practical insights and theoretical knowledge about LLM evaluation.\n* [Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) by Hugging Face: Main leaderboard to compare LLMs in an open and reproducible way (automated benchmarks).\n* [Language Model Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness) by EleutherAI: A popular framework for evaluating LLMs using automated benchmarks.\n* [Lighteval](https://github.com/huggingface/lighteval) by Hugging Face: Alternative evaluation framework that also includes model-based evaluations.\n* [Chatbot Arena](https://lmarena.ai/) by LMSYS: Elo rating of general-purpose LLMs, based on comparisons made by humans (human evaluation).\n\n---\n### 7. Quantization\n\nQuantization is the process of converting the parameters and activations of a model using a lower precision. For example, weights stored using 16 bits can be converted into a 4-bit representation. This technique has become increasingly important to reduce the computational and memory costs associated with LLMs.\n\n* **Base techniques**: Learn the different levels of precision (FP32, FP16, INT8, etc.) and how to perform na\u00efve quantization with absmax and zero-point techniques.\n* **GGUF & llama.cpp**: Originally designed to run on CPUs, [llama.cpp](https://github.com/ggerganov/llama.cpp) and the GGUF format have become the most popular tools to run LLMs on consumer-grade hardware. It supports storing special tokens, vocabulary, and metadata in a single file. \n* **GPTQ & AWQ**: Techniques like [GPTQ](https://arxiv.org/abs/2210.17323)/[EXL2](https://github.com/turboderp/exllamav2) and [AWQ](https://arxiv.org/abs/2306.00978) introduce layer-by-layer calibration that retains performance at extremely low bitwidths. They reduce catastrophic outliers using dynamic scaling, selectively skipping or re-centering the heaviest parameters.\n* **SmoothQuant & ZeroQuant**: New quantization-friendly transformations (SmoothQuant) and compiler-based optimizations (ZeroQuant) help mitigate outliers before quantization. They also reduce hardware overhead by fusing certain ops and optimizing dataflow.\n\n\ud83d\udcda **References**:\n* [Introduction to quantization](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html) by Maxime Labonne: Overview of quantization, absmax and zero-point quantization, and LLM.int8() with code.\n* [Quantize Llama models with llama.cpp](https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html) by Maxime Labonne: Tutorial on how to quantize a Llama 2 model using llama.cpp and the GGUF format.\n* [4-bit LLM Quantization with GPTQ](https://mlabonne.github.io/blog/posts/4_bit_Quantization_with_GPTQ.html) by Maxime Labonne: Tutorial on how to quantize an LLM using the GPTQ algorithm with AutoGPTQ.\n* [Understanding Activation-Aware Weight Quantization](https://medium.com/friendliai/understanding-activation-aware-weight-quantization-awq-boosting-inference-serving-efficiency-in-10bb0faf63a8) by FriendliAI: Overview of the AWQ technique and its benefits.\n* [SmoothQuant on Llama 2 7B](https://github.com/mit-han-lab/smoothquant/blob/main/examples/smoothquant_llama_demo.ipynb) by MIT HAN Lab: Tutorial on how to use SmoothQuant with a Llama 2 model in 8-bit precision.\n* [DeepSpeed Model Compression](https://www.deepspeed.ai/tutorials/model-compression/) by DeepSpeed: Tutorial on how to use ZeroQuant and extreme compression (XTC) with DeepSpeed Compression.\n\n---\n### 8. New Trends\n\nHere are notable topics that didn't fit into other categories. Some are established (model merging, multimodal) techniques, but others are more experimental (interpretability, test-time compute scaling) and the focus of numerous research papers. \n\n* **Model merging**: Merging trained models has become a popular way of creating performant models without any fine-tuning. The popular [mergekit](https://github.com/cg123/mergekit) library implements the most popular merging methods, like SLERP, [DARE](https://arxiv.org/abs/2311.03099), and [TIES](https://arxiv.org/abs/2311.03099).\n* **Multimodal models**: These models (like [CLIP](https://openai.com/research/clip), [Stable Diffusion](https://stability.ai/stable-image), or [LLaVA](https://llava-vl.github.io/)) process multiple types of inputs (text, images, audio, etc.) with a unified embedding space, which unlocks powerful applications like text-to-image.\n* **Interpretability**: Mechanistic interpretability techniques like Sparse Autoencoders (SAEs) have made remarkable progress to provide insights about the inner workings of LLMs. This has also been applied with techniques such as abliteration, which allow you to modify the behavior of models without training.\n* **Test-time compute**: Reasoning models trained with RL techniques can be further improved by scaling the compute budget during test time. It can involve multiple calls, MCTS, or specialized models like a Process Reward Model (PRM). Iterative steps with precise scoring significantly improve performance for complex reasoning tasks.\n\n\ud83d\udcda **References**:\n* [Merge LLMs with mergekit](https://mlabonne.github.io/blog/posts/2024-01-08_Merge_LLMs_with_mergekit.html) by Maxime Labonne: Tutorial about model merging using mergekit.\n* [Smol Vision](https://github.com/merveenoyan/smol-vision) by Merve Noyan: Collection of notebooks and scripts dedicated to small multimodal models.\n* [Large Multimodal Models](https://huyenchip.com/2023/10/10/multimodal.html) by Chip Huyen: Overview of multimodal systems and the recent history of this field.\n* [Unsensor any LLM with abliteration](https://huggingface.co/blog/mlabonne/abliteration) by Maxime Labonne: Direct application of interpretability techniques to modify the style of a model.\n* [Intuitive Explanation of SAEs](https://adamkarvonen.github.io/machine_learning/2024/06/11/sae-intuitions.html) by Adam Karvonen: Article about how SAEs work and why they make sense for interpretability.\n* [Scaling test-time compute](https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute) by Beeching et al.: Tutorial and experiments to outperform Llama 3.1 70B on MATH-500 with a 3B model.\n\n## \ud83d\udc77 The LLM Engineer\n\nThis section of the course focuses on learning how to build LLM-powered applications that can be used in production, with a focus on augmenting models and deploying them.\n\n![](img/roadmap_engineer.png)\n\n### 1. Running LLMs\n\nRunning LLMs can be difficult due to high hardware requirements. Depending on your use case, you might want to simply consume a model through an API (like GPT-4) or run it locally. In any case, additional prompting and guidance techniques can improve and constrain the output for your applications.\n\n* **LLM APIs**: APIs are a convenient way to deploy LLMs. This space is divided between private LLMs ([OpenAI](https://platform.openai.com/), [Google](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview), [Anthropic](https://docs.anthropic.com/claude/reference/getting-started-with-the-api), etc.) and open-source LLMs ([OpenRouter](https://openrouter.ai/), [Hugging Face](https://huggingface.co/inference-api), [Together AI](https://www.together.ai/), etc.).\n* **Open-source LLMs**: The [Hugging Face Hub](https://huggingface.co/models) is a great place to find LLMs. You can directly run some of them in [Hugging Face Spaces](https://huggingface.co/spaces), or download and run them locally in apps like [LM Studio](https://lmstudio.ai/) or through the CLI with [llama.cpp](https://github.com/ggerganov/llama.cpp) or [ollama](https://ollama.ai/).\n* **Prompt engineering**: Common techniques include zero-shot prompting, few-shot prompting, chain of thought, and ReAct. They work better with bigger models, but can be adapted to smaller ones.\n* **Structuring outputs**: Many tasks require a structured output, like a strict template or a JSON format. Libraries like [Outlines](https://github.com/outlines-dev/outlines) can be used to guide the generation and respect a given structure. Some APIs also support structured output generation natively using JSON schemas.\n\n\ud83d\udcda **References**:\n* [Run an LLM locally with LM Studio](https://www.kdnuggets.com/run-an-llm-locally-with-lm-studio) by Nisha Arya: Short guide on how to use LM Studio.\n* [Prompt engineering guide](https://www.promptingguide.ai/) by DAIR.AI: Exhaustive list of prompt techniques with examples\n* [Outlines - Quickstart](https://dottxt-ai.github.io/outlines/latest/quickstart/): List of guided generation techniques enabled by Outlines. \n* [LMQL - Overview](https://lmql.ai/docs/language/overview.html): Introduction to the LMQL language.\n\n---\n### 2. Building a Vector Storage\n\nCreating a vector storage is the first step to building a Retrieval Augmented Generation (RAG) pipeline. Documents are loaded, split, and relevant chunks are used to produce vector representations (embeddings) that are stored for future use during inference.\n\n* **Ingesting documents**: Document loaders are convenient wrappers that can handle many formats: PDF, JSON, HTML, Markdown, etc. They can also directly retrieve data from some databases and APIs (GitHub, Reddit, Google Drive, etc.).\n* **Splitting documents**: Text splitters break down documents into smaller, semantically meaningful chunks. Instead of splitting text after *n* characters, it's often better to split by header or recursively, with some additional metadata.\n* **Embedding models**: Embedding models convert text into vector representations. Picking task-specific models significantly improves performance for semantic search and RAG.\n* **Vector databases**: Vector databases (like [Chroma](https://www.trychroma.com/), [Pinecone](https://www.pinecone.io/), [Milvus](https://milvus.io/), [FAISS](https://faiss.ai/), [Annoy](https://github.com/spotify/annoy), etc.) are designed to store embedding vectors. They enable efficient retrieval of data that is 'most similar' to a query based on vector similarity.\n\n\ud83d\udcda **References**:\n* [LangChain - Text splitters](https://python.langchain.com/docs/how_to/#text-splitters): List of different text splitters implemented in LangChain.\n* [Sentence Transformers library](https://www.sbert.net/): Popular library for embedding models.\n* [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard): Leaderboard for embedding models.\n* [The Top 7 Vector Databases](https://www.datacamp.com/blog/the-top-5-vector-databases) by Moez Ali: A comparison of the best and most popular vector databases.\n\n---\n### 3. Retrieval Augmented Generation\n\nWith RAG, LLMs retrieve contextual documents from a database to improve the accuracy of their answers. RAG is a popular way of augmenting the model's knowledge without any fine-tuning.\n\n* **Orchestrators**: Orchestrators like [LangChain](https://python.langchain.com/docs/get_started/introduction) and [LlamaIndex](https://docs.llamaindex.ai/en/stable/) are popular frameworks to connect your LLMs with tools and databases. The Model Context Protocol (MCP) introduces a new standard to pass data and context to models across providers.\n* **Retrievers**: Query rewriters and generative retrievers like CoRAG and HyDE enhance search by transforming user queries. Multi-vector and hybrid retrieval methods combine embeddings with keyword signals to improve recall and precision.\n* **Memory**: To remember previous instructions and answers, LLMs and chatbots like ChatGPT add this history to their context window. This buffer can be improved with summarization (e.g., using a smaller LLM), a vector store + RAG, etc.\n* **Evaluation**: We need to evaluate both the document retrieval (context precision and recall) and generation stages (faithfulness and answer relevancy). It can be simplified with tools [Ragas](https://github.com/explodinggradients/ragas/tree/main) and [DeepEval](https://github.com/confident-ai/deepeval) (assessing quality).\n\n\ud83d\udcda **References**:\n* [Llamaindex - High-level concepts](https://docs.llamaindex.ai/en/stable/getting_started/concepts.html): Main concepts to know when building RAG pipelines.\n* [Model Context Protocol](https://modelcontextprotocol.io/introduction): Introduction to MCP with motivate, architecture, and quick starts.\n* [Pinecone - Retrieval Augmentation](https://www.pinecone.io/learn/series/langchain/langchain-retrieval-augmentation/): Overview of the retrieval augmentation process. \n* [LangChain - Q&A with RAG](https://python.langchain.com/docs/tutorials/rag/): Step-by-step tutorial to build a typical RAG pipeline.\n* [LangChain - Memory types](https://python.langchain.com/docs/how_to/chatbots_memory/): List of different types of memories with relevant usage.\n* [RAG pipeline - Metrics](https://docs.ragas.io/en/stable/concepts/metrics/index.html): Overview of the main metrics used to evaluate RAG pipelines.\n\n---\n### 4. Advanced RAG\n\nReal-life applications can require complex pipelines, including SQL or graph databases, as well as automatically selecting relevant tools and APIs. These advanced techniques can improve a baseline solution and provide additional features.\n\n* **Query construction**: Structured data stored in traditional databases requires a specific query language like SQL, Cypher, metadata, etc. We can directly translate the user instruction into a query to access the data with query construction.\n* **Tools**: Agents augment LLMs by automatically selecting the most relevant tools to provide an answer. These tools can be as simple as using Google or Wikipedia, or more complex like a Python interpreter or Jira. \n* **Post-processing**: Final step that processes the inputs that are fed to the LLM. It enhances the relevance and diversity of documents retrieved with re-ranking, [RAG-fusion](https://github.com/Raudaschl/rag-fusion), and classification.\n* **Program LLMs**: Frameworks like [DSPy](https://github.com/stanfordnlp/dspy) allow you to optimize prompts and weights based on automated evaluations in a programmatic way.\n\n\ud83d\udcda **References**:\n* [LangChain - Query Construction](https://blog.langchain.dev/query-construction/): Blog post about different types of query construction.\n* [LangChain - SQL](https://python.langchain.com/docs/tutorials/sql_qa/): Tutorial on how to interact with SQL databases with LLMs, involving Text-to-SQL and an optional SQL agent.\n* [Pinecone - LLM agents](https://www.pinecone.io/learn/series/langchain/langchain-agents/): Introduction to agents and tools with different types.\n* [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) by Lilian Weng: A more theoretical article about LLM agents.\n* [LangChain - OpenAI's RAG](https://blog.langchain.dev/applying-openai-rag/): Overview of the RAG strategies employed by OpenAI, including post-processing.\n* [DSPy in 8 Steps](https://dspy-docs.vercel.app/docs/building-blocks/solving_your_task): General-purpose guide to DSPy introducing modules, signatures, and optimizers.\n\n---\n### 5. Agents\n\nAn LLM agent can autonomously perform tasks by taking actions based on reasoning about its environment, typically through the use of tools or functions to interact with external systems.\n\n* **Agent fundamentals**: Agents operate using thoughts (internal reasoning to decide what to do next), action (executing tasks, often by interacting with external tools), and observation (analyzing feedback or results to refine the next step).\n* **Agent frameworks**: Agent development can be streamlined using different frameworks like [LangGraph](https://www.langchain.com/langgraph) (design and visualization of workflows), [LlamaIndex](https://docs.llamaindex.ai/en/stable/use_cases/agents/) (data-augmented agents with RAG), or [smolagents](https://github.com/huggingface/smolagents) (beginner-friendly, lightweight option).\n* **Multi-agents**: More experimental frameworks include collaboration between different agents, such as [CrewAI](https://docs.crewai.com/introduction) (role-based team orchestration), [AutoGen](https://github.com/microsoft/autogen) (conversation-driven multi-agent systems), and [OpenAI Agents SDK](https://github.com/openai/openai-agents-python) (production-ready with strong OpenAI model integration).\n\n\ud83d\udcda **References**:\n* [Agents Course](https://huggingface.co/learn/agents-course/unit0/introduction): Popular course about AI agents made by Hugging Face.\n* [AI Agents Comparison](https://langfuse.com/blog/2025-03-19-ai-agent-comparison) by Jannik Maierh\u00f6fer: Comparison of features across different open-source AI agent frameworks.\n* [LangGraph](https://langchain-ai.github.io/langgraph/concepts/why-langgraph/): Overview of how to build AI agents with LangGraph.\n* [LlamaIndex Agents](https://docs.llamaindex.ai/en/stable/use_cases/agents/): Uses cases and resources to build agents with LlamaIndex.\n* [smolagents](https://huggingface.co/docs/smolagents/index): Documentation with a guided tour, how-to guides, and more conceptual articles.\n\n---\n### 6. Inference optimization\n\nText generation is a costly process that requires expensive hardware. In addition to quantization, various techniques have been proposed to maximize throughput and reduce inference costs.\n\n* **Flash Attention**: Optimization of the attention mechanism to transform its complexity from quadratic to linear, speeding up both training and inference.\n* **Key-value cache**: Understand the key-value cache and the improvements introduced in [Multi-Query Attention](https://arxiv.org/abs/1911.02150) (MQA) and [Grouped-Query Attention](https://arxiv.org/abs/2305.13245) (GQA).\n* **Speculative decoding**: Use a small model to produce drafts that are then reviewed by a larger model to speed up text generation.\n\n\ud83d\udcda **References**:\n* [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one) by Hugging Face: Explain how to optimize inference on GPUs.\n* [LLM Inference](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices) by Databricks: Best practices for how to optimize LLM inference in production.\n* [Optimizing LLMs for Speed and Memory](https://huggingface.co/docs/transformers/main/en/llm_tutorial_optimization) by Hugging Face: Explain three main techniques to optimize speed and memory, namely quantization, Flash Attention, and architectural innovations.\n* [Assisted Generation](https://huggingface.co/blog/assisted-generation) by Hugging Face: HF's version of speculative decoding, it's an interesting blog post about how it works with code to implement it.\n\n---\n### 7. Deploying LLMs\n\nDeploying LLMs at scale is an engineering feat that can require multiple clusters of GPUs. In other scenarios, demos and local apps can be achieved with a much lower complexity. \n\n* **Local deployment**: Privacy is an important advantage that open-source LLMs have over private ones. Local LLM servers ([LM Studio](https://lmstudio.ai/), [Ollama](https://ollama.ai/), [oobabooga](https://github.com/oobabooga/text-generation-webui), [kobold.cpp](https://github.com/LostRuins/koboldcpp), etc.) capitalize on this advantage to power local apps. \n* **Demo deployment**: Frameworks like [Gradio](https://www.gradio.app/) and [Streamlit](https://docs.streamlit.io/) are helpful to prototype applications and share demos. You can also easily host them online, for example using [Hugging Face Spaces](https://huggingface.co/spaces).\n* **Server deployment**: Deploy LLMs at scale requires cloud (see also [SkyPilot](https://skypilot.readthedocs.io/en/latest/)) or on-prem infrastructure and often leverage optimized text generation frameworks like [TGI](https://github.com/huggingface/text-generation-inference), [vLLM](https://github.com/vllm-project/vllm/tree/main), etc.\n* **Edge deployment**: In constrained environments, high-performance frameworks like [MLC LLM](https://github.com/mlc-ai/mlc-llm) and [mnn-llm](https://github.com/wangzhaode/mnn-llm/blob/master/README_en.md) can deploy LLM in web browsers, Android, and iOS.\n\n\ud83d\udcda **References**:\n* [Streamlit - Build a basic LLM app](https://docs.streamlit.io/knowledge-base/tutorials/build-conversational-apps): Tutorial to make a basic ChatGPT-like app using Streamlit.\n* [HF LLM Inference Container](https://huggingface.co/blog/sagemaker-huggingface-llm): Deploy LLMs on Amazon SageMaker using Hugging Face's inference container.\n* [Philschmid\u00a0blog](https://www.philschmid.de/) by Philipp Schmid: Collection of high-quality articles about LLM deployment using Amazon SageMaker.\n* [Optimizing latence](https://hamel.dev/notes/llm/inference/03_inference.html) by Hamel Husain: Comparison of TGI, vLLM, CTranslate2, and mlc in terms of throughput and latency.\n\n---\n### 8. Securing LLMs\n\nIn addition to traditional security problems associated with software, LLMs have unique weaknesses due to the way they are trained and prompted.\n\n* **Prompt hacking**: Different techniques related to prompt engineering, including prompt injection (additional instruction to hijack the model's answer), data/prompt leaking (retrieve its original data/prompt), and jailbreaking (craft prompts to bypass safety features).\n* **Backdoors**: Attack vectors can target the training data itself, by poisoning the training data (e.g., with false information) or creating backdoors (secret triggers to change the model's behavior during inference).\n* **Defensive measures**: The best way to protect your LLM applications is to test them against these vulnerabilities (e.g., using red teaming and checks like [garak](https://github.com/leondz/garak/)) and observe them in production (with a framework like [langfuse](https://github.com/langfuse/langfuse)).\n\n\ud83d\udcda **References**:\n* [OWASP LLM Top 10](https://owasp.org/www-project-top-10-for-large-language-model-applications/) by HEGO Wiki: List of the 10 most critical vulnerabilities seen in LLM applications.\n* [Prompt Injection Primer](https://github.com/jthack/PIPE) by Joseph Thacker: Short guide dedicated to prompt injection for engineers.\n* [LLM Security](https://llmsecurity.net/) by [@llm_sec](https://twitter.com/llm_sec): Extensive list of resources related to LLM security.\n* [Red teaming LLMs](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/red-teaming) by Microsoft: Guide on how to perform red teaming with LLMs.\n\n---\n## Acknowledgements\n\nThis roadmap was inspired by the excellent [DevOps Roadmap](https://github.com/milanm/DevOps-Roadmap) from Milan Milanovi\u0107 and Romano Roth.\n\nSpecial thanks to:\n\n* Thomas Thelen for motivating me to create a roadmap\n* Andr\u00e9 Frade for his input and review of the first draft\n* Dino Dunn for providing resources about LLM security\n* Magdalena Kuhn for improving the \"human evaluation\" part\n* Odoverdose for suggesting 3Blue1Brown's video about Transformers\n* Everyone who contributed to the educational references in this course :)\n\n*Disclaimer: I am not affiliated with any sources listed here.*\n\n---\n\n[![Star History Chart](https://api.star-history.com/svg?repos=mlabonne/llm-course&type=Date)](https://www.star-history.com/#mlabonne/llm-course&Date)\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 83844720,
    "name": "face_recognition",
    "full_name": "ageitgey/face_recognition",
    "description": "The world's simplest facial recognition api for Python and the command line",
    "html_url": "https://github.com/ageitgey/face_recognition",
    "clone_url": "https://github.com/ageitgey/face_recognition.git",
    "owner_login": "ageitgey",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/896692?v=4",
    "stargazers_count": 55197,
    "watchers_count": 55197,
    "forks_count": 13641,
    "open_issues_count": 819,
    "size": 103959,
    "language": "Python",
    "languages": {
      "Python": 35036,
      "Dockerfile": 6930,
      "Makefile": 2286
    },
    "topics": [
      "face-detection",
      "face-recognition",
      "machine-learning",
      "python"
    ],
    "license_name": "MIT License",
    "created_at": "2017-03-03T21:52:39+00:00",
    "updated_at": "2025-08-05T21:50:19+00:00",
    "pushed_at": "2024-08-21T06:22:36+00:00",
    "contributors_count": 49,
    "readme_length": 19402,
    "readme_content": "# Face Recognition\n\n_You can also read a translated version of this file [in Chinese \u7b80\u4f53\u4e2d\u6587\u7248](https://github.com/ageitgey/face_recognition/blob/master/README_Simplified_Chinese.md) or [in Korean \ud55c\uad6d\uc5b4](https://github.com/ageitgey/face_recognition/blob/master/README_Korean.md) or [in Japanese \u65e5\u672c\u8a9e](https://github.com/m-i-k-i/face_recognition/blob/master/README_Japanese.md)._\n\nRecognize and manipulate faces from Python or from the command line with\nthe world's simplest face recognition library.\n\nBuilt using [dlib](http://dlib.net/)'s state-of-the-art face recognition\nbuilt with deep learning. The model has an accuracy of 99.38% on the\n[Labeled Faces in the Wild](http://vis-www.cs.umass.edu/lfw/) benchmark.\n\nThis also provides a simple `face_recognition` command line tool that lets\nyou do face recognition on a folder of images from the command line!\n\n\n[![PyPI](https://img.shields.io/pypi/v/face_recognition.svg)](https://pypi.python.org/pypi/face_recognition)\n[![Build Status](https://github.com/ageitgey/face_recognition/workflows/CI/badge.svg?branch=master&event=push)](https://github.com/ageitgey/face_recognition/actions?query=workflow%3ACI)\n[![Documentation Status](https://readthedocs.org/projects/face-recognition/badge/?version=latest)](http://face-recognition.readthedocs.io/en/latest/?badge=latest)\n\n## Features\n\n#### Find faces in pictures\n\nFind all the faces that appear in a picture:\n\n![](https://cloud.githubusercontent.com/assets/896692/23625227/42c65360-025d-11e7-94ea-b12f28cb34b4.png)\n\n```python\nimport face_recognition\nimage = face_recognition.load_image_file(\"your_file.jpg\")\nface_locations = face_recognition.face_locations(image)\n```\n\n#### Find and manipulate facial features in pictures\n\nGet the locations and outlines of each person's eyes, nose, mouth and chin.\n\n![](https://cloud.githubusercontent.com/assets/896692/23625282/7f2d79dc-025d-11e7-8728-d8924596f8fa.png)\n\n```python\nimport face_recognition\nimage = face_recognition.load_image_file(\"your_file.jpg\")\nface_landmarks_list = face_recognition.face_landmarks(image)\n```\n\nFinding facial features is super useful for lots of important stuff. But you can also use it for really stupid stuff\nlike applying [digital make-up](https://github.com/ageitgey/face_recognition/blob/master/examples/digital_makeup.py) (think 'Meitu'):\n\n![](https://cloud.githubusercontent.com/assets/896692/23625283/80638760-025d-11e7-80a2-1d2779f7ccab.png)\n\n#### Identify faces in pictures\n\nRecognize who appears in each photo.\n\n![](https://cloud.githubusercontent.com/assets/896692/23625229/45e049b6-025d-11e7-89cc-8a71cf89e713.png)\n\n```python\nimport face_recognition\nknown_image = face_recognition.load_image_file(\"biden.jpg\")\nunknown_image = face_recognition.load_image_file(\"unknown.jpg\")\n\nbiden_encoding = face_recognition.face_encodings(known_image)[0]\nunknown_encoding = face_recognition.face_encodings(unknown_image)[0]\n\nresults = face_recognition.compare_faces([biden_encoding], unknown_encoding)\n```\n\nYou can even use this library with other Python libraries to do real-time face recognition:\n\n![](https://cloud.githubusercontent.com/assets/896692/24430398/36f0e3f0-13cb-11e7-8258-4d0c9ce1e419.gif)\n\nSee [this example](https://github.com/ageitgey/face_recognition/blob/master/examples/facerec_from_webcam_faster.py) for the code.\n\n## Online Demos\n\nUser-contributed shared Jupyter notebook demo (not officially supported): [![Deepnote](https://beta.deepnote.org/buttons/try-in-a-jupyter-notebook.svg)](https://beta.deepnote.org/launch?template=face_recognition)\n\n## Installation\n\n### Requirements\n\n  * Python 3.3+ or Python 2.7\n  * macOS or Linux (Windows not officially supported, but might work)\n\n### Installation Options:\n\n#### Installing on Mac or Linux\n\nFirst, make sure you have dlib already installed with Python bindings:\n\n  * [How to install dlib from source on macOS or Ubuntu](https://gist.github.com/ageitgey/629d75c1baac34dfa5ca2a1928a7aeaf)\n  \nThen, make sure you have cmake installed:  \n \n```brew install cmake```\n\nFinally, install this module from pypi using `pip3` (or `pip2` for Python 2):\n\n```bash\npip3 install face_recognition\n```\n\nAlternatively, you can try this library with [Docker](https://www.docker.com/), see [this section](#deployment).\n\nIf you are having trouble with installation, you can also try out a\n[pre-configured VM](https://medium.com/@ageitgey/try-deep-learning-in-python-now-with-a-fully-pre-configured-vm-1d97d4c3e9b).\n\n#### Installing on an Nvidia Jetson Nano board\n\n * [Jetson Nano installation instructions](https://medium.com/@ageitgey/build-a-hardware-based-face-recognition-system-for-150-with-the-nvidia-jetson-nano-and-python-a25cb8c891fd)\n   * Please follow the instructions in the article carefully. There is current a bug in the CUDA libraries on the Jetson Nano that will cause this library to fail silently if you don't follow the instructions in the article to comment out a line in dlib and recompile it.\n\n#### Installing on Raspberry Pi 2+\n\n  * [Raspberry Pi 2+ installation instructions](https://gist.github.com/ageitgey/1ac8dbe8572f3f533df6269dab35df65)\n\n#### Installing on FreeBSD\n\n```bash\npkg install graphics/py-face_recognition\n```\n\n#### Installing on Windows\n\nWhile Windows isn't officially supported, helpful users have posted instructions on how to install this library:\n\n  * [@masoudr's Windows 10 installation guide (dlib + face_recognition)](https://github.com/ageitgey/face_recognition/issues/175#issue-257710508)\n\n#### Installing a pre-configured Virtual Machine image\n\n  * [Download the pre-configured VM image](https://medium.com/@ageitgey/try-deep-learning-in-python-now-with-a-fully-pre-configured-vm-1d97d4c3e9b) (for VMware Player or VirtualBox).\n\n## Usage\n\n### Command-Line Interface\n\nWhen you install `face_recognition`, you get two simple command-line \nprograms:\n\n* `face_recognition` - Recognize faces in a photograph or folder full for \n   photographs.\n* `face_detection` - Find faces in a photograph or folder full for photographs.\n\n#### `face_recognition` command line tool\n\nThe `face_recognition` command lets you recognize faces in a photograph or \nfolder full  for photographs.\n\nFirst, you need to provide a folder with one picture of each person you\nalready know. There should be one image file for each person with the\nfiles named according to who is in the picture:\n\n![known](https://cloud.githubusercontent.com/assets/896692/23582466/8324810e-00df-11e7-82cf-41515eba704d.png)\n\nNext, you need a second folder with the files you want to identify:\n\n![unknown](https://cloud.githubusercontent.com/assets/896692/23582465/81f422f8-00df-11e7-8b0d-75364f641f58.png)\n\nThen in you simply run the command `face_recognition`, passing in\nthe folder of known people and the folder (or single image) with unknown\npeople and it tells you who is in each image:\n\n```bash\n$ face_recognition ./pictures_of_people_i_know/ ./unknown_pictures/\n\n/unknown_pictures/unknown.jpg,Barack Obama\n/face_recognition_test/unknown_pictures/unknown.jpg,unknown_person\n```\n\nThere's one line in the output for each face. The data is comma-separated\nwith the filename and the name of the person found.\n\nAn `unknown_person` is a face in the image that didn't match anyone in\nyour folder of known people.\n\n#### `face_detection` command line tool\n\nThe `face_detection` command lets you find the location (pixel coordinatates) \nof any faces in an image.\n\nJust run the command `face_detection`, passing in a folder of images \nto check (or a single image):\n\n```bash\n$ face_detection  ./folder_with_pictures/\n\nexamples/image1.jpg,65,215,169,112\nexamples/image2.jpg,62,394,211,244\nexamples/image2.jpg,95,941,244,792\n```\n\nIt prints one line for each face that was detected. The coordinates\nreported are the top, right, bottom and left coordinates of the face (in pixels).\n \n##### Adjusting Tolerance / Sensitivity\n\nIf you are getting multiple matches for the same person, it might be that\nthe people in your photos look very similar and a lower tolerance value\nis needed to make face comparisons more strict.\n\nYou can do that with the `--tolerance` parameter. The default tolerance\nvalue is 0.6 and lower numbers make face comparisons more strict:\n\n```bash\n$ face_recognition --tolerance 0.54 ./pictures_of_people_i_know/ ./unknown_pictures/\n\n/unknown_pictures/unknown.jpg,Barack Obama\n/face_recognition_test/unknown_pictures/unknown.jpg,unknown_person\n```\n\nIf you want to see the face distance calculated for each match in order\nto adjust the tolerance setting, you can use `--show-distance true`:\n\n```bash\n$ face_recognition --show-distance true ./pictures_of_people_i_know/ ./unknown_pictures/\n\n/unknown_pictures/unknown.jpg,Barack Obama,0.378542298956785\n/face_recognition_test/unknown_pictures/unknown.jpg,unknown_person,None\n```\n\n##### More Examples\n\nIf you simply want to know the names of the people in each photograph but don't\ncare about file names, you could do this:\n\n```bash\n$ face_recognition ./pictures_of_people_i_know/ ./unknown_pictures/ | cut -d ',' -f2\n\nBarack Obama\nunknown_person\n```\n\n##### Speeding up Face Recognition\n\nFace recognition can be done in parallel if you have a computer with\nmultiple CPU cores. For example, if your system has 4 CPU cores, you can\nprocess about 4 times as many images in the same amount of time by using\nall your CPU cores in parallel.\n\nIf you are using Python 3.4 or newer, pass in a `--cpus <number_of_cpu_cores_to_use>` parameter:\n\n```bash\n$ face_recognition --cpus 4 ./pictures_of_people_i_know/ ./unknown_pictures/\n```\n\nYou can also pass in `--cpus -1` to use all CPU cores in your system.\n\n#### Python Module\n\nYou can import the `face_recognition` module and then easily manipulate\nfaces with just a couple of lines of code. It's super easy!\n\nAPI Docs: [https://face-recognition.readthedocs.io](https://face-recognition.readthedocs.io/en/latest/face_recognition.html).\n\n##### Automatically find all the faces in an image\n\n```python\nimport face_recognition\n\nimage = face_recognition.load_image_file(\"my_picture.jpg\")\nface_locations = face_recognition.face_locations(image)\n\n# face_locations is now an array listing the co-ordinates of each face!\n```\n\nSee [this example](https://github.com/ageitgey/face_recognition/blob/master/examples/find_faces_in_picture.py)\n to try it out.\n\nYou can also opt-in to a somewhat more accurate deep-learning-based face detection model.\n\nNote: GPU acceleration (via NVidia's CUDA library) is required for good\nperformance with this model. You'll also want to enable CUDA support\nwhen compliling `dlib`.\n\n```python\nimport face_recognition\n\nimage = face_recognition.load_image_file(\"my_picture.jpg\")\nface_locations = face_recognition.face_locations(image, model=\"cnn\")\n\n# face_locations is now an array listing the co-ordinates of each face!\n```\n\nSee [this example](https://github.com/ageitgey/face_recognition/blob/master/examples/find_faces_in_picture_cnn.py)\n to try it out.\n\nIf you have a lot of images and a GPU, you can also\n[find faces in batches](https://github.com/ageitgey/face_recognition/blob/master/examples/find_faces_in_batches.py).\n\n##### Automatically locate the facial features of a person in an image\n\n```python\nimport face_recognition\n\nimage = face_recognition.load_image_file(\"my_picture.jpg\")\nface_landmarks_list = face_recognition.face_landmarks(image)\n\n# face_landmarks_list is now an array with the locations of each facial feature in each face.\n# face_landmarks_list[0]['left_eye'] would be the location and outline of the first person's left eye.\n```\n\nSee [this example](https://github.com/ageitgey/face_recognition/blob/master/examples/find_facial_features_in_picture.py)\n to try it out.\n\n##### Recognize faces in images and identify who they are\n\n```python\nimport face_recognition\n\npicture_of_me = face_recognition.load_image_file(\"me.jpg\")\nmy_face_encoding = face_recognition.face_encodings(picture_of_me)[0]\n\n# my_face_encoding now contains a universal 'encoding' of my facial features that can be compared to any other picture of a face!\n\nunknown_picture = face_recognition.load_image_file(\"unknown.jpg\")\nunknown_face_encoding = face_recognition.face_encodings(unknown_picture)[0]\n\n# Now we can see the two face encodings are of the same person with `compare_faces`!\n\nresults = face_recognition.compare_faces([my_face_encoding], unknown_face_encoding)\n\nif results[0] == True:\n    print(\"It's a picture of me!\")\nelse:\n    print(\"It's not a picture of me!\")\n```\n\nSee [this example](https://github.com/ageitgey/face_recognition/blob/master/examples/recognize_faces_in_pictures.py)\n to try it out.\n\n## Python Code Examples\n\nAll the examples are available [here](https://github.com/ageitgey/face_recognition/tree/master/examples).\n\n\n#### Face Detection\n\n* [Find faces in a photograph](https://github.com/ageitgey/face_recognition/blob/master/examples/find_faces_in_picture.py)\n* [Find faces in a photograph (using deep learning)](https://github.com/ageitgey/face_recognition/blob/master/examples/find_faces_in_picture_cnn.py)\n* [Find faces in batches of images w/ GPU (using deep learning)](https://github.com/ageitgey/face_recognition/blob/master/examples/find_faces_in_batches.py)\n* [Blur all the faces in a live video using your webcam (Requires OpenCV to be installed)](https://github.com/ageitgey/face_recognition/blob/master/examples/blur_faces_on_webcam.py)\n\n#### Facial Features\n\n* [Identify specific facial features in a photograph](https://github.com/ageitgey/face_recognition/blob/master/examples/find_facial_features_in_picture.py)\n* [Apply (horribly ugly) digital make-up](https://github.com/ageitgey/face_recognition/blob/master/examples/digital_makeup.py)\n\n#### Facial Recognition\n\n* [Find and recognize unknown faces in a photograph based on photographs of known people](https://github.com/ageitgey/face_recognition/blob/master/examples/recognize_faces_in_pictures.py)\n* [Identify and draw boxes around each person in a photo](https://github.com/ageitgey/face_recognition/blob/master/examples/identify_and_draw_boxes_on_faces.py)\n* [Compare faces by numeric face distance instead of only True/False matches](https://github.com/ageitgey/face_recognition/blob/master/examples/face_distance.py)\n* [Recognize faces in live video using your webcam - Simple / Slower Version (Requires OpenCV to be installed)](https://github.com/ageitgey/face_recognition/blob/master/examples/facerec_from_webcam.py)\n* [Recognize faces in live video using your webcam - Faster Version (Requires OpenCV to be installed)](https://github.com/ageitgey/face_recognition/blob/master/examples/facerec_from_webcam_faster.py)\n* [Recognize faces in a video file and write out new video file (Requires OpenCV to be installed)](https://github.com/ageitgey/face_recognition/blob/master/examples/facerec_from_video_file.py)\n* [Recognize faces on a Raspberry Pi w/ camera](https://github.com/ageitgey/face_recognition/blob/master/examples/facerec_on_raspberry_pi.py)\n* [Run a web service to recognize faces via HTTP (Requires Flask to be installed)](https://github.com/ageitgey/face_recognition/blob/master/examples/web_service_example.py)\n* [Recognize faces with a K-nearest neighbors classifier](https://github.com/ageitgey/face_recognition/blob/master/examples/face_recognition_knn.py)\n* [Train multiple images per person then recognize faces using a SVM](https://github.com/ageitgey/face_recognition/blob/master/examples/face_recognition_svm.py)\n\n## Creating a Standalone Executable\nIf you want to create a standalone executable that can run without the need to install `python` or `face_recognition`, you can use [PyInstaller](https://github.com/pyinstaller/pyinstaller). However, it requires some custom configuration to work with this library. See [this issue](https://github.com/ageitgey/face_recognition/issues/357) for how to do it.\n\n## Articles and Guides that cover `face_recognition`\n\n- My article on how Face Recognition works: [Modern Face Recognition with Deep Learning](https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78)\n  - Covers the algorithms and how they generally work\n- [Face recognition with OpenCV, Python, and deep learning](https://www.pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/) by Adrian Rosebrock\n  - Covers how to use face recognition in practice\n- [Raspberry Pi Face Recognition](https://www.pyimagesearch.com/2018/06/25/raspberry-pi-face-recognition/) by Adrian Rosebrock\n  - Covers how to use this on a Raspberry Pi\n- [Face clustering with Python](https://www.pyimagesearch.com/2018/07/09/face-clustering-with-python/) by Adrian Rosebrock\n  - Covers how to automatically cluster photos based on who appears in each photo using unsupervised learning\n\n## How Face Recognition Works\n\nIf you want to learn how face location and recognition work instead of\ndepending on a black box library, [read my article](https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78).\n\n## Caveats\n\n* The face recognition model is trained on adults and does not work very well on children. It tends to mix\n  up children quite easy using the default comparison threshold of 0.6.\n* Accuracy may vary between ethnic groups. Please see [this wiki page](https://github.com/ageitgey/face_recognition/wiki/Face-Recognition-Accuracy-Problems#question-face-recognition-works-well-with-european-individuals-but-overall-accuracy-is-lower-with-asian-individuals) for more details.\n\n## <a name=\"deployment\">Deployment to Cloud Hosts (Heroku, AWS, etc)</a>\n\nSince `face_recognition` depends on `dlib` which is written in C++, it can be tricky to deploy an app\nusing it to a cloud hosting provider like Heroku or AWS.\n\nTo make things easier, there's an example Dockerfile in this repo that shows how to run an app built with\n`face_recognition` in a [Docker](https://www.docker.com/) container. With that, you should be able to deploy\nto any service that supports Docker images.\n\nYou can try the Docker image locally by running: `docker-compose up --build`\n\nThere are also [several prebuilt Docker images.](docker/README.md)\n\nLinux users with a GPU (drivers >= 384.81) and [Nvidia-Docker](https://github.com/NVIDIA/nvidia-docker) installed can run the example on the GPU: Open the [docker-compose.yml](docker-compose.yml) file and uncomment the `dockerfile: Dockerfile.gpu` and `runtime: nvidia` lines.\n\n## Having problems?\n\nIf you run into problems, please read the [Common Errors](https://github.com/ageitgey/face_recognition/wiki/Common-Errors) section of the wiki before filing a github issue.\n\n## Thanks\n\n* Many, many thanks to [Davis King](https://github.com/davisking) ([@nulhom](https://twitter.com/nulhom))\n  for creating dlib and for providing the trained facial feature detection and face encoding models\n  used in this library. For more information on the ResNet that powers the face encodings, check out\n  his [blog post](http://blog.dlib.net/2017/02/high-quality-face-recognition-with-deep.html).\n* Thanks to everyone who works on all the awesome Python data science libraries like numpy, scipy, scikit-image,\n  pillow, etc, etc that makes this kind of stuff so easy and fun in Python.\n* Thanks to [Cookiecutter](https://github.com/audreyr/cookiecutter) and the\n  [audreyr/cookiecutter-pypackage](https://github.com/audreyr/cookiecutter-pypackage) project template\n  for making Python project packaging way more tolerable.\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 264818686,
    "name": "yolov5",
    "full_name": "ultralytics/yolov5",
    "description": "YOLOv5 \ud83d\ude80 in PyTorch > ONNX > CoreML > TFLite",
    "html_url": "https://github.com/ultralytics/yolov5",
    "clone_url": "https://github.com/ultralytics/yolov5.git",
    "owner_login": "ultralytics",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/26833451?v=4",
    "stargazers_count": 54909,
    "watchers_count": 54909,
    "forks_count": 17119,
    "open_issues_count": 292,
    "size": 16995,
    "language": "Python",
    "languages": {
      "Python": 790527,
      "Shell": 9071,
      "Dockerfile": 3405
    },
    "topics": [
      "coreml",
      "deep-learning",
      "ios",
      "machine-learning",
      "ml",
      "object-detection",
      "onnx",
      "pytorch",
      "tflite",
      "ultralytics",
      "yolo",
      "yolov3",
      "yolov5"
    ],
    "license_name": "GNU Affero General Public License v3.0",
    "created_at": "2020-05-18T03:45:11+00:00",
    "updated_at": "2025-08-06T00:26:21+00:00",
    "pushed_at": "2025-08-03T09:33:59+00:00",
    "contributors_count": 100,
    "readme_length": 49884,
    "readme_content": "<div align=\"center\">\n  <p>\n    <a href=\"https://www.ultralytics.com/blog/all-you-need-to-know-about-ultralytics-yolo11-and-its-applications\" target=\"_blank\">\n      <img width=\"100%\" src=\"https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png\" alt=\"Ultralytics YOLO banner\"></a>\n  </p>\n\n[\u4e2d\u6587](https://docs.ultralytics.com/zh) | [\ud55c\uad6d\uc5b4](https://docs.ultralytics.com/ko) | [\u65e5\u672c\u8a9e](https://docs.ultralytics.com/ja) | [\u0420\u0443\u0441\u0441\u043a\u0438\u0439](https://docs.ultralytics.com/ru) | [Deutsch](https://docs.ultralytics.com/de) | [Fran\u00e7ais](https://docs.ultralytics.com/fr) | [Espa\u00f1ol](https://docs.ultralytics.com/es) | [Portugu\u00eas](https://docs.ultralytics.com/pt) | [T\u00fcrk\u00e7e](https://docs.ultralytics.com/tr) | [Ti\u1ebfng Vi\u1ec7t](https://docs.ultralytics.com/vi) | [\u0627\u0644\u0639\u0631\u0628\u064a\u0629](https://docs.ultralytics.com/ar)\n\n<div>\n    <a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI Testing\"></a>\n    <a href=\"https://zenodo.org/badge/latestdoi/264818686\"><img src=\"https://zenodo.org/badge/264818686.svg\" alt=\"YOLOv5 Citation\"></a>\n    <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n    <a href=\"https://discord.com/invite/ultralytics\"><img alt=\"Discord\" src=\"https://img.shields.io/discord/1089800235347353640?logo=discord&logoColor=white&label=Discord&color=blue\"></a> <a href=\"https://community.ultralytics.com/\"><img alt=\"Ultralytics Forums\" src=\"https://img.shields.io/discourse/users?server=https%3A%2F%2Fcommunity.ultralytics.com&logo=discourse&label=Forums&color=blue\"></a> <a href=\"https://reddit.com/r/ultralytics\"><img alt=\"Ultralytics Reddit\" src=\"https://img.shields.io/reddit/subreddit-subscribers/ultralytics?style=flat&logo=reddit&logoColor=white&label=Reddit&color=blue\"></a>\n    <br>\n    <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a>\n    <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n    <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n  </div>\n  <br>\n\nUltralytics YOLOv5 \ud83d\ude80 is a cutting-edge, state-of-the-art (SOTA) computer vision model developed by [Ultralytics](https://www.ultralytics.com/). Based on the [PyTorch](https://pytorch.org/) framework, YOLOv5 is renowned for its ease of use, speed, and accuracy. It incorporates insights and best practices from extensive research and development, making it a popular choice for a wide range of vision AI tasks, including [object detection](https://docs.ultralytics.com/tasks/detect/), [image segmentation](https://docs.ultralytics.com/tasks/segment/), and [image classification](https://docs.ultralytics.com/tasks/classify/).\n\nWe hope the resources here help you get the most out of YOLOv5. Please browse the [YOLOv5 Docs](https://docs.ultralytics.com/yolov5/) for detailed information, raise an issue on [GitHub](https://github.com/ultralytics/yolov5/issues/new/choose) for support, and join our [Discord community](https://discord.com/invite/ultralytics) for questions and discussions!\n\nTo request an Enterprise License, please complete the form at [Ultralytics Licensing](https://www.ultralytics.com/license).\n\n<div align=\"center\">\n  <a href=\"https://github.com/ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png\" width=\"2%\" alt=\"Ultralytics GitHub\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"space\">\n  <a href=\"https://www.linkedin.com/company/ultralytics/\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png\" width=\"2%\" alt=\"Ultralytics LinkedIn\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"space\">\n  <a href=\"https://twitter.com/ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png\" width=\"2%\" alt=\"Ultralytics Twitter\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"space\">\n  <a href=\"https://youtube.com/ultralytics?sub_confirmation=1\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png\" width=\"2%\" alt=\"Ultralytics YouTube\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"space\">\n  <a href=\"https://www.tiktok.com/@ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png\" width=\"2%\" alt=\"Ultralytics TikTok\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"space\">\n  <a href=\"https://ultralytics.com/bilibili\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-bilibili.png\" width=\"2%\" alt=\"Ultralytics BiliBili\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"space\">\n  <a href=\"https://discord.com/invite/ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-discord.png\" width=\"2%\" alt=\"Ultralytics Discord\"></a>\n</div>\n\n</div>\n<br>\n\n## \ud83d\ude80 YOLO11: The Next Evolution\n\nWe are excited to announce the launch of **Ultralytics YOLO11** \ud83d\ude80, the latest advancement in our state-of-the-art (SOTA) vision models! Available now at the [Ultralytics YOLO GitHub repository](https://github.com/ultralytics/ultralytics), YOLO11 builds on our legacy of speed, precision, and ease of use. Whether you're tackling [object detection](https://docs.ultralytics.com/tasks/detect/), [instance segmentation](https://docs.ultralytics.com/tasks/segment/), [pose estimation](https://docs.ultralytics.com/tasks/pose/), [image classification](https://docs.ultralytics.com/tasks/classify/), or [oriented object detection (OBB)](https://docs.ultralytics.com/tasks/obb/), YOLO11 delivers the performance and versatility needed to excel in diverse applications.\n\nGet started today and unlock the full potential of YOLO11! Visit the [Ultralytics Docs](https://docs.ultralytics.com/) for comprehensive guides and resources:\n\n[![PyPI version](https://badge.fury.io/py/ultralytics.svg)](https://badge.fury.io/py/ultralytics) [![Downloads](https://static.pepy.tech/badge/ultralytics)](https://www.pepy.tech/projects/ultralytics)\n\n```bash\n# Install the ultralytics package\npip install ultralytics\n```\n\n<div align=\"center\">\n  <a href=\"https://www.ultralytics.com/yolo\" target=\"_blank\">\n  <img width=\"100%\" src=\"https://raw.githubusercontent.com/ultralytics/assets/refs/heads/main/yolo/performance-comparison.png\" alt=\"Ultralytics YOLO Performance Comparison\"></a>\n</div>\n\n## \ud83d\udcda Documentation\n\nSee the [YOLOv5 Docs](https://docs.ultralytics.com/yolov5/) for full documentation on training, testing, and deployment. See below for quickstart examples.\n\n<details open>\n<summary>Install</summary>\n\nClone the repository and install dependencies in a [**Python>=3.8.0**](https://www.python.org/) environment. Ensure you have [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) installed.\n\n```bash\n# Clone the YOLOv5 repository\ngit clone https://github.com/ultralytics/yolov5\n\n# Navigate to the cloned directory\ncd yolov5\n\n# Install required packages\npip install -r requirements.txt\n```\n\n</details>\n\n<details open>\n<summary>Inference with PyTorch Hub</summary>\n\nUse YOLOv5 via [PyTorch Hub](https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading/) for inference. [Models](https://github.com/ultralytics/yolov5/tree/master/models) are automatically downloaded from the latest YOLOv5 [release](https://github.com/ultralytics/yolov5/releases).\n\n```python\nimport torch\n\n# Load a YOLOv5 model (options: yolov5n, yolov5s, yolov5m, yolov5l, yolov5x)\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\")  # Default: yolov5s\n\n# Define the input image source (URL, local file, PIL image, OpenCV frame, numpy array, or list)\nimg = \"https://ultralytics.com/images/zidane.jpg\"  # Example image\n\n# Perform inference (handles batching, resizing, normalization automatically)\nresults = model(img)\n\n# Process the results (options: .print(), .show(), .save(), .crop(), .pandas())\nresults.print()  # Print results to console\nresults.show()  # Display results in a window\nresults.save()  # Save results to runs/detect/exp\n```\n\n</details>\n\n<details>\n<summary>Inference with detect.py</summary>\n\nThe `detect.py` script runs inference on various sources. It automatically downloads [models](https://github.com/ultralytics/yolov5/tree/master/models) from the latest YOLOv5 [release](https://github.com/ultralytics/yolov5/releases) and saves the results to the `runs/detect` directory.\n\n```bash\n# Run inference using a webcam\npython detect.py --weights yolov5s.pt --source 0\n\n# Run inference on a local image file\npython detect.py --weights yolov5s.pt --source img.jpg\n\n# Run inference on a local video file\npython detect.py --weights yolov5s.pt --source vid.mp4\n\n# Run inference on a screen capture\npython detect.py --weights yolov5s.pt --source screen\n\n# Run inference on a directory of images\npython detect.py --weights yolov5s.pt --source path/to/images/\n\n# Run inference on a text file listing image paths\npython detect.py --weights yolov5s.pt --source list.txt\n\n# Run inference on a text file listing stream URLs\npython detect.py --weights yolov5s.pt --source list.streams\n\n# Run inference using a glob pattern for images\npython detect.py --weights yolov5s.pt --source 'path/to/*.jpg'\n\n# Run inference on a YouTube video URL\npython detect.py --weights yolov5s.pt --source 'https://youtu.be/LNwODJXcvt4'\n\n# Run inference on an RTSP, RTMP, or HTTP stream\npython detect.py --weights yolov5s.pt --source 'rtsp://example.com/media.mp4'\n```\n\n</details>\n\n<details>\n<summary>Training</summary>\n\nThe commands below demonstrate how to reproduce YOLOv5 [COCO dataset](https://docs.ultralytics.com/datasets/detect/coco/) results. Both [models](https://github.com/ultralytics/yolov5/tree/master/models) and [datasets](https://github.com/ultralytics/yolov5/tree/master/data) are downloaded automatically from the latest YOLOv5 [release](https://github.com/ultralytics/yolov5/releases). Training times for YOLOv5n/s/m/l/x are approximately 1/2/4/6/8 days on a single [NVIDIA V100 GPU](https://www.nvidia.com/en-us/data-center/v100/). Using [Multi-GPU training](https://docs.ultralytics.com/yolov5/tutorials/multi_gpu_training/) can significantly reduce training time. Use the largest `--batch-size` your hardware allows, or use `--batch-size -1` for YOLOv5 [AutoBatch](https://github.com/ultralytics/yolov5/pull/5092). The batch sizes shown below are for V100-16GB GPUs.\n\n```bash\n# Train YOLOv5n on COCO for 300 epochs\npython train.py --data coco.yaml --epochs 300 --weights '' --cfg yolov5n.yaml --batch-size 128\n\n# Train YOLOv5s on COCO for 300 epochs\npython train.py --data coco.yaml --epochs 300 --weights '' --cfg yolov5s.yaml --batch-size 64\n\n# Train YOLOv5m on COCO for 300 epochs\npython train.py --data coco.yaml --epochs 300 --weights '' --cfg yolov5m.yaml --batch-size 40\n\n# Train YOLOv5l on COCO for 300 epochs\npython train.py --data coco.yaml --epochs 300 --weights '' --cfg yolov5l.yaml --batch-size 24\n\n# Train YOLOv5x on COCO for 300 epochs\npython train.py --data coco.yaml --epochs 300 --weights '' --cfg yolov5x.yaml --batch-size 16\n```\n\n<img width=\"800\" src=\"https://user-images.githubusercontent.com/26833433/90222759-949d8800-ddc1-11ea-9fa1-1c97eed2b963.png\" alt=\"YOLOv5 Training Results\">\n\n</details>\n\n<details open>\n<summary>Tutorials</summary>\n\n- **[Train Custom Data](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/)** \ud83d\ude80 **RECOMMENDED**: Learn how to train YOLOv5 on your own datasets.\n- **[Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/)** \u2618\ufe0f: Improve your model's performance with expert tips.\n- **[Multi-GPU Training](https://docs.ultralytics.com/yolov5/tutorials/multi_gpu_training/)**: Speed up training using multiple GPUs.\n- **[PyTorch Hub Integration](https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading/)** \ud83c\udf1f **NEW**: Easily load models using PyTorch Hub.\n- **[Model Export (TFLite, ONNX, CoreML, TensorRT)](https://docs.ultralytics.com/yolov5/tutorials/model_export/)** \ud83d\ude80: Convert your models to various deployment formats like [ONNX](https://onnx.ai/) or [TensorRT](https://developer.nvidia.com/tensorrt).\n- **[NVIDIA Jetson Deployment](https://docs.ultralytics.com/guides/nvidia-jetson/)** \ud83c\udf1f **NEW**: Deploy YOLOv5 on [NVIDIA Jetson](https://developer.nvidia.com/embedded-computing) devices.\n- **[Test-Time Augmentation (TTA)](https://docs.ultralytics.com/yolov5/tutorials/test_time_augmentation/)**: Enhance prediction accuracy with TTA.\n- **[Model Ensembling](https://docs.ultralytics.com/yolov5/tutorials/model_ensembling/)**: Combine multiple models for better performance.\n- **[Model Pruning/Sparsity](https://docs.ultralytics.com/yolov5/tutorials/model_pruning_and_sparsity/)**: Optimize models for size and speed.\n- **[Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/)**: Automatically find the best training hyperparameters.\n- **[Transfer Learning with Frozen Layers](https://docs.ultralytics.com/yolov5/tutorials/transfer_learning_with_frozen_layers/)**: Adapt pretrained models to new tasks efficiently using [transfer learning](https://www.ultralytics.com/glossary/transfer-learning).\n- **[Architecture Summary](https://docs.ultralytics.com/yolov5/tutorials/architecture_description/)** \ud83c\udf1f **NEW**: Understand the YOLOv5 model architecture.\n- **[Ultralytics HUB Training](https://www.ultralytics.com/hub)** \ud83d\ude80 **RECOMMENDED**: Train and deploy YOLO models using Ultralytics HUB.\n- **[ClearML Logging](https://docs.ultralytics.com/yolov5/tutorials/clearml_logging_integration/)**: Integrate with [ClearML](https://clear.ml/) for experiment tracking.\n- **[Neural Magic DeepSparse Integration](https://docs.ultralytics.com/yolov5/tutorials/neural_magic_pruning_quantization/)**: Accelerate inference with DeepSparse.\n- **[Comet Logging](https://docs.ultralytics.com/yolov5/tutorials/comet_logging_integration/)** \ud83c\udf1f **NEW**: Log experiments using [Comet ML](https://www.comet.com/site/).\n\n</details>\n\n## \ud83e\udde9 Integrations\n\nOur key integrations with leading AI platforms extend the functionality of Ultralytics' offerings, enhancing tasks like dataset labeling, training, visualization, and model management. Discover how Ultralytics, in collaboration with partners like [Weights & Biases](https://docs.ultralytics.com/integrations/weights-biases/), [Comet ML](https://docs.ultralytics.com/integrations/comet/), [Roboflow](https://docs.ultralytics.com/integrations/roboflow/), and [Intel OpenVINO](https://docs.ultralytics.com/integrations/openvino/), can optimize your AI workflow. Explore more at [Ultralytics Integrations](https://docs.ultralytics.com/integrations/).\n\n<a href=\"https://docs.ultralytics.com/integrations/\" target=\"_blank\">\n    <img width=\"100%\" src=\"https://github.com/ultralytics/assets/raw/main/yolov8/banner-integrations.png\" alt=\"Ultralytics active learning integrations\">\n</a>\n<br>\n<br>\n\n<div align=\"center\">\n  <a href=\"https://www.ultralytics.com/hub\">\n    <img src=\"https://github.com/ultralytics/assets/raw/main/partners/logo-ultralytics-hub.png\" width=\"10%\" alt=\"Ultralytics HUB logo\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"15%\" height=\"0\" alt=\"space\">\n  <a href=\"https://docs.ultralytics.com/integrations/weights-biases/\">\n    <img src=\"https://github.com/ultralytics/assets/raw/main/partners/logo-wb.png\" width=\"10%\" alt=\"Weights & Biases logo\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"15%\" height=\"0\" alt=\"space\">\n  <a href=\"https://docs.ultralytics.com/integrations/comet/\">\n    <img src=\"https://github.com/ultralytics/assets/raw/main/partners/logo-comet.png\" width=\"10%\" alt=\"Comet ML logo\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"15%\" height=\"0\" alt=\"space\">\n  <a href=\"https://docs.ultralytics.com/integrations/neural-magic/\">\n    <img src=\"https://github.com/ultralytics/assets/raw/main/partners/logo-neuralmagic.png\" width=\"10%\" alt=\"Neural Magic logo\"></a>\n</div>\n\n|                                                       Ultralytics HUB \ud83c\udf1f                                                        |                                                          Weights & Biases                                                           |                                                                              Comet                                                                              |                                                        Neural Magic                                                         |\n| :-----------------------------------------------------------------------------------------------------------------------------: | :---------------------------------------------------------------------------------------------------------------------------------: | :-------------------------------------------------------------------------------------------------------------------------------------------------------------: | :-------------------------------------------------------------------------------------------------------------------------: |\n| Streamline YOLO workflows: Label, train, and deploy effortlessly with [Ultralytics HUB](https://hub.ultralytics.com/). Try now! | Track experiments, hyperparameters, and results with [Weights & Biases](https://docs.ultralytics.com/integrations/weights-biases/). | Free forever, [Comet ML](https://docs.ultralytics.com/integrations/comet/) lets you save YOLO models, resume training, and interactively visualize predictions. | Run YOLO inference up to 6x faster with [Neural Magic DeepSparse](https://docs.ultralytics.com/integrations/neural-magic/). |\n\n## \u2b50 Ultralytics HUB\n\nExperience seamless AI development with [Ultralytics HUB](https://www.ultralytics.com/hub) \u2b50, the ultimate platform for building, training, and deploying [computer vision](https://www.ultralytics.com/glossary/computer-vision-cv) models. Visualize datasets, train [YOLOv5](https://docs.ultralytics.com/models/yolov5/) and [YOLOv8](https://docs.ultralytics.com/models/yolov8/) \ud83d\ude80 models, and deploy them to real-world applications without writing any code. Transform images into actionable insights using our cutting-edge tools and user-friendly [Ultralytics App](https://www.ultralytics.com/app-install). Start your journey for **Free** today!\n\n<a align=\"center\" href=\"https://www.ultralytics.com/hub\" target=\"_blank\">\n<img width=\"100%\" src=\"https://github.com/ultralytics/assets/raw/main/im/ultralytics-hub.png\" alt=\"Ultralytics HUB Platform Screenshot\"></a>\n\n## \ud83e\udd14 Why YOLOv5?\n\nYOLOv5 is designed for simplicity and ease of use. We prioritize real-world performance and accessibility.\n\n<p align=\"left\"><img width=\"800\" src=\"https://user-images.githubusercontent.com/26833433/155040763-93c22a27-347c-4e3c-847a-8094621d3f4e.png\" alt=\"YOLOv5 Performance Chart\"></p>\n<details>\n  <summary>YOLOv5-P5 640 Figure</summary>\n\n<p align=\"left\"><img width=\"800\" src=\"https://user-images.githubusercontent.com/26833433/155040757-ce0934a3-06a6-43dc-a979-2edbbd69ea0e.png\" alt=\"YOLOv5 P5 640 Performance Chart\"></p>\n</details>\n<details>\n  <summary>Figure Notes</summary>\n\n- **COCO AP val** denotes the [mean Average Precision (mAP)](https://www.ultralytics.com/glossary/mean-average-precision-map) at [Intersection over Union (IoU)](https://www.ultralytics.com/glossary/intersection-over-union-iou) thresholds from 0.5 to 0.95, measured on the 5,000-image [COCO val2017 dataset](https://docs.ultralytics.com/datasets/detect/coco/) across various inference sizes (256 to 1536 pixels).\n- **GPU Speed** measures the average inference time per image on the [COCO val2017 dataset](https://docs.ultralytics.com/datasets/detect/coco/) using an [AWS p3.2xlarge V100 instance](https://aws.amazon.com/ec2/instance-types/p4/) with a batch size of 32.\n- **EfficientDet** data is sourced from the [google/automl repository](https://github.com/google/automl) at batch size 8.\n- **Reproduce** these results using the command: `python val.py --task study --data coco.yaml --iou 0.7 --weights yolov5n6.pt yolov5s6.pt yolov5m6.pt yolov5l6.pt yolov5x6.pt`\n\n</details>\n\n### Pretrained Checkpoints\n\nThis table shows the performance metrics for various YOLOv5 models trained on the COCO dataset.\n\n| Model                                                                                                                                                                    | Size<br><sup>(pixels) | mAP<sup>val<br>50-95 | mAP<sup>val<br>50 | Speed<br><sup>CPU b1<br>(ms) | Speed<br><sup>V100 b1<br>(ms) | Speed<br><sup>V100 b32<br>(ms) | Params<br><sup>(M) | FLOPs<br><sup>@640 (B) |\n| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------- | -------------------- | ----------------- | ---------------------------- | ----------------------------- | ------------------------------ | ------------------ | ---------------------- |\n| [YOLOv5n](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n.pt)                                                                                       | 640                   | 28.0                 | 45.7              | **45**                       | **6.3**                       | **0.6**                        | **1.9**            | **4.5**                |\n| [YOLOv5s](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt)                                                                                       | 640                   | 37.4                 | 56.8              | 98                           | 6.4                           | 0.9                            | 7.2                | 16.5                   |\n| [YOLOv5m](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m.pt)                                                                                       | 640                   | 45.4                 | 64.1              | 224                          | 8.2                           | 1.7                            | 21.2               | 49.0                   |\n| [YOLOv5l](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l.pt)                                                                                       | 640                   | 49.0                 | 67.3              | 430                          | 10.1                          | 2.7                            | 46.5               | 109.1                  |\n| [YOLOv5x](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5x.pt)                                                                                       | 640                   | 50.7                 | 68.9              | 766                          | 12.1                          | 4.8                            | 86.7               | 205.7                  |\n|                                                                                                                                                                          |                       |                      |                   |                              |                               |                                |                    |                        |\n| [YOLOv5n6](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n6.pt)                                                                                     | 1280                  | 36.0                 | 54.4              | 153                          | 8.1                           | 2.1                            | 3.2                | 4.6                    |\n| [YOLOv5s6](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s6.pt)                                                                                     | 1280                  | 44.8                 | 63.7              | 385                          | 8.2                           | 3.6                            | 12.6               | 16.8                   |\n| [YOLOv5m6](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m6.pt)                                                                                     | 1280                  | 51.3                 | 69.3              | 887                          | 11.1                          | 6.8                            | 35.7               | 50.0                   |\n| [YOLOv5l6](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l6.pt)                                                                                     | 1280                  | 53.7                 | 71.3              | 1784                         | 15.8                          | 10.5                           | 76.8               | 111.4                  |\n| [YOLOv5x6](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5x6.pt)<br>+ [[TTA]](https://docs.ultralytics.com/yolov5/tutorials/test_time_augmentation/) | 1280<br>1536          | 55.0<br>**55.8**     | 72.7<br>**72.7**  | 3136<br>-                    | 26.2<br>-                     | 19.4<br>-                      | 140.7<br>-         | 209.8<br>-             |\n\n<details>\n  <summary>Table Notes</summary>\n\n- All checkpoints were trained for 300 epochs using default settings. Nano (n) and Small (s) models use [hyp.scratch-low.yaml](https://github.com/ultralytics/yolov5/blob/master/data/hyps/hyp.scratch-low.yaml) hyperparameters, while Medium (m), Large (l), and Extra-Large (x) models use [hyp.scratch-high.yaml](https://github.com/ultralytics/yolov5/blob/master/data/hyps/hyp.scratch-high.yaml).\n- **mAP<sup>val</sup>** values represent single-model, single-scale performance on the [COCO val2017 dataset](https://docs.ultralytics.com/datasets/detect/coco/).<br>Reproduce using: `python val.py --data coco.yaml --img 640 --conf 0.001 --iou 0.65`\n- **Speed** metrics are averaged over COCO val images using an [AWS p3.2xlarge V100 instance](https://aws.amazon.com/ec2/instance-types/p4/). Non-Maximum Suppression (NMS) time (~1 ms/image) is not included.<br>Reproduce using: `python val.py --data coco.yaml --img 640 --task speed --batch 1`\n- **TTA** ([Test Time Augmentation](https://docs.ultralytics.com/yolov5/tutorials/test_time_augmentation/)) includes reflection and scale augmentations for improved accuracy.<br>Reproduce using: `python val.py --data coco.yaml --img 1536 --iou 0.7 --augment`\n\n</details>\n\n## \ud83d\uddbc\ufe0f Segmentation\n\nThe YOLOv5 [release v7.0](https://github.com/ultralytics/yolov5/releases/v7.0) introduced [instance segmentation](https://docs.ultralytics.com/tasks/segment/) models that achieve state-of-the-art performance. These models are designed for easy training, validation, and deployment. For full details, see the [Release Notes](https://github.com/ultralytics/yolov5/releases/v7.0) and explore the [YOLOv5 Segmentation Colab Notebook](https://github.com/ultralytics/yolov5/blob/master/segment/tutorial.ipynb) for quickstart examples.\n\n<details>\n  <summary>Segmentation Checkpoints</summary>\n\n<div align=\"center\">\n<a align=\"center\" href=\"https://www.ultralytics.com/yolo\" target=\"_blank\">\n<img width=\"800\" src=\"https://user-images.githubusercontent.com/61612323/204180385-84f3aca9-a5e9-43d8-a617-dda7ca12e54a.png\" alt=\"YOLOv5 Segmentation Performance Chart\"></a>\n</div>\n\nYOLOv5 segmentation models were trained on the [COCO dataset](https://docs.ultralytics.com/datasets/segment/coco/) for 300 epochs at an image size of 640 pixels using A100 GPUs. Models were exported to [ONNX](https://onnx.ai/) FP32 for CPU speed tests and [TensorRT](https://developer.nvidia.com/tensorrt) FP16 for GPU speed tests. All speed tests were conducted on Google [Colab Pro](https://colab.research.google.com/signup) notebooks for reproducibility.\n\n| Model                                                                                      | Size<br><sup>(pixels) | mAP<sup>box<br>50-95 | mAP<sup>mask<br>50-95 | Train Time<br><sup>300 epochs<br>A100 (hours) | Speed<br><sup>ONNX CPU<br>(ms) | Speed<br><sup>TRT A100<br>(ms) | Params<br><sup>(M) | FLOPs<br><sup>@640 (B) |\n| ------------------------------------------------------------------------------------------ | --------------------- | -------------------- | --------------------- | --------------------------------------------- | ------------------------------ | ------------------------------ | ------------------ | ---------------------- |\n| [YOLOv5n-seg](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n-seg.pt) | 640                   | 27.6                 | 23.4                  | 80:17                                         | **62.7**                       | **1.2**                        | **2.0**            | **7.1**                |\n| [YOLOv5s-seg](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s-seg.pt) | 640                   | 37.6                 | 31.7                  | 88:16                                         | 173.3                          | 1.4                            | 7.6                | 26.4                   |\n| [YOLOv5m-seg](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m-seg.pt) | 640                   | 45.0                 | 37.1                  | 108:36                                        | 427.0                          | 2.2                            | 22.0               | 70.8                   |\n| [YOLOv5l-seg](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l-seg.pt) | 640                   | 49.0                 | 39.9                  | 66:43 (2x)                                    | 857.4                          | 2.9                            | 47.9               | 147.7                  |\n| [YOLOv5x-seg](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5x-seg.pt) | 640                   | **50.7**             | **41.4**              | 62:56 (3x)                                    | 1579.2                         | 4.5                            | 88.8               | 265.7                  |\n\n- All checkpoints were trained for 300 epochs using the SGD optimizer with `lr0=0.01` and `weight_decay=5e-5` at an image size of 640 pixels, using default settings.<br>Training runs are logged at [https://wandb.ai/glenn-jocher/YOLOv5_v70_official](https://wandb.ai/glenn-jocher/YOLOv5_v70_official).\n- **Accuracy** values represent single-model, single-scale performance on the COCO dataset.<br>Reproduce using: `python segment/val.py --data coco.yaml --weights yolov5s-seg.pt`\n- **Speed** metrics are averaged over 100 inference images using a [Colab Pro A100 High-RAM instance](https://colab.research.google.com/signup). Values indicate inference speed only (NMS adds approximately 1ms per image).<br>Reproduce using: `python segment/val.py --data coco.yaml --weights yolov5s-seg.pt --batch 1`\n- **Export** to ONNX (FP32) and TensorRT (FP16) was performed using `export.py`.<br>Reproduce using: `python export.py --weights yolov5s-seg.pt --include engine --device 0 --half`\n\n</details>\n\n<details>\n  <summary>Segmentation Usage Examples &nbsp;<a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/segment/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a></summary>\n\n### Train\n\nYOLOv5 segmentation training supports automatic download of the [COCO128-seg dataset](https://docs.ultralytics.com/datasets/segment/coco8-seg/) via the `--data coco128-seg.yaml` argument. For the full [COCO-segments dataset](https://docs.ultralytics.com/datasets/segment/coco/), download it manually using `bash data/scripts/get_coco.sh --train --val --segments` and then train with `python train.py --data coco.yaml`.\n\n```bash\n# Train on a single GPU\npython segment/train.py --data coco128-seg.yaml --weights yolov5s-seg.pt --img 640\n\n# Train using Multi-GPU Distributed Data Parallel (DDP)\npython -m torch.distributed.run --nproc_per_node 4 --master_port 1 segment/train.py --data coco128-seg.yaml --weights yolov5s-seg.pt --img 640 --device 0,1,2,3\n```\n\n### Val\n\nValidate the mask [mean Average Precision (mAP)](https://www.ultralytics.com/glossary/mean-average-precision-map) of YOLOv5s-seg on the COCO dataset:\n\n```bash\n# Download COCO validation segments split (780MB, 5000 images)\nbash data/scripts/get_coco.sh --val --segments\n\n# Validate the model\npython segment/val.py --weights yolov5s-seg.pt --data coco.yaml --img 640\n```\n\n### Predict\n\nUse the pretrained YOLOv5m-seg.pt model to perform segmentation on `bus.jpg`:\n\n```bash\n# Run prediction\npython segment/predict.py --weights yolov5m-seg.pt --source data/images/bus.jpg\n```\n\n```python\n# Load model from PyTorch Hub (Note: Inference support might vary)\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"custom\", \"yolov5m-seg.pt\")\n```\n\n| ![Zidane Segmentation Example](https://user-images.githubusercontent.com/26833433/203113421-decef4c4-183d-4a0a-a6c2-6435b33bc5d3.jpg) | ![Bus Segmentation Example](https://user-images.githubusercontent.com/26833433/203113416-11fe0025-69f7-4874-a0a6-65d0bfe2999a.jpg) |\n| :-----------------------------------------------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------------------------------------------: |\n\n### Export\n\nExport the YOLOv5s-seg model to ONNX and TensorRT formats:\n\n```bash\n# Export model\npython export.py --weights yolov5s-seg.pt --include onnx engine --img 640 --device 0\n```\n\n</details>\n\n## \ud83c\udff7\ufe0f Classification\n\nYOLOv5 [release v6.2](https://github.com/ultralytics/yolov5/releases/v6.2) introduced support for [image classification](https://docs.ultralytics.com/tasks/classify/) model training, validation, and deployment. Check the [Release Notes](https://github.com/ultralytics/yolov5/releases/v6.2) for details and the [YOLOv5 Classification Colab Notebook](https://github.com/ultralytics/yolov5/blob/master/classify/tutorial.ipynb) for quickstart guides.\n\n<details>\n  <summary>Classification Checkpoints</summary>\n\n<br>\n\nYOLOv5-cls classification models were trained on [ImageNet](https://docs.ultralytics.com/datasets/classify/imagenet/) for 90 epochs using a 4xA100 instance. [ResNet](https://arxiv.org/abs/1512.03385) and [EfficientNet](https://arxiv.org/abs/1905.11946) models were trained alongside under identical settings for comparison. Models were exported to [ONNX](https://onnx.ai/) FP32 (CPU speed tests) and [TensorRT](https://developer.nvidia.com/tensorrt) FP16 (GPU speed tests). All speed tests were run on Google [Colab Pro](https://colab.research.google.com/signup) for reproducibility.\n\n| Model                                                                                              | Size<br><sup>(pixels) | Acc<br><sup>top1 | Acc<br><sup>top5 | Training<br><sup>90 epochs<br>4xA100 (hours) | Speed<br><sup>ONNX CPU<br>(ms) | Speed<br><sup>TensorRT V100<br>(ms) | Params<br><sup>(M) | FLOPs<br><sup>@224 (B) |\n| -------------------------------------------------------------------------------------------------- | --------------------- | ---------------- | ---------------- | -------------------------------------------- | ------------------------------ | ----------------------------------- | ------------------ | ---------------------- |\n| [YOLOv5n-cls](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n-cls.pt)         | 224                   | 64.6             | 85.4             | 7:59                                         | **3.3**                        | **0.5**                             | **2.5**            | **0.5**                |\n| [YOLOv5s-cls](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s-cls.pt)         | 224                   | 71.5             | 90.2             | 8:09                                         | 6.6                            | 0.6                                 | 5.4                | 1.4                    |\n| [YOLOv5m-cls](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m-cls.pt)         | 224                   | 75.9             | 92.9             | 10:06                                        | 15.5                           | 0.9                                 | 12.9               | 3.9                    |\n| [YOLOv5l-cls](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l-cls.pt)         | 224                   | 78.0             | 94.0             | 11:56                                        | 26.9                           | 1.4                                 | 26.5               | 8.5                    |\n| [YOLOv5x-cls](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5x-cls.pt)         | 224                   | **79.0**         | **94.4**         | 15:04                                        | 54.3                           | 1.8                                 | 48.1               | 15.9                   |\n|                                                                                                    |                       |                  |                  |                                              |                                |                                     |                    |                        |\n| [ResNet18](https://github.com/ultralytics/yolov5/releases/download/v7.0/resnet18.pt)               | 224                   | 70.3             | 89.5             | **6:47**                                     | 11.2                           | 0.5                                 | 11.7               | 3.7                    |\n| [ResNet34](https://github.com/ultralytics/yolov5/releases/download/v7.0/resnet34.pt)               | 224                   | 73.9             | 91.8             | 8:33                                         | 20.6                           | 0.9                                 | 21.8               | 7.4                    |\n| [ResNet50](https://github.com/ultralytics/yolov5/releases/download/v7.0/resnet50.pt)               | 224                   | 76.8             | 93.4             | 11:10                                        | 23.4                           | 1.0                                 | 25.6               | 8.5                    |\n| [ResNet101](https://github.com/ultralytics/yolov5/releases/download/v7.0/resnet101.pt)             | 224                   | 78.5             | 94.3             | 17:10                                        | 42.1                           | 1.9                                 | 44.5               | 15.9                   |\n|                                                                                                    |                       |                  |                  |                                              |                                |                                     |                    |                        |\n| [EfficientNet_b0](https://github.com/ultralytics/yolov5/releases/download/v7.0/efficientnet_b0.pt) | 224                   | 75.1             | 92.4             | 13:03                                        | 12.5                           | 1.3                                 | 5.3                | 1.0                    |\n| [EfficientNet_b1](https://github.com/ultralytics/yolov5/releases/download/v7.0/efficientnet_b1.pt) | 224                   | 76.4             | 93.2             | 17:04                                        | 14.9                           | 1.6                                 | 7.8                | 1.5                    |\n| [EfficientNet_b2](https://github.com/ultralytics/yolov5/releases/download/v7.0/efficientnet_b2.pt) | 224                   | 76.6             | 93.4             | 17:10                                        | 15.9                           | 1.6                                 | 9.1                | 1.7                    |\n| [EfficientNet_b3](https://github.com/ultralytics/yolov5/releases/download/v7.0/efficientnet_b3.pt) | 224                   | 77.7             | 94.0             | 19:19                                        | 18.9                           | 1.9                                 | 12.2               | 2.4                    |\n\n<details>\n  <summary>Table Notes (click to expand)</summary>\n\n- All checkpoints were trained for 90 epochs using the SGD optimizer with `lr0=0.001` and `weight_decay=5e-5` at an image size of 224 pixels, using default settings.<br>Training runs are logged at [https://wandb.ai/glenn-jocher/YOLOv5-Classifier-v6-2](https://wandb.ai/glenn-jocher/YOLOv5-Classifier-v6-2).\n- **Accuracy** values (top-1 and top-5) represent single-model, single-scale performance on the [ImageNet-1k dataset](https://docs.ultralytics.com/datasets/classify/imagenet/).<br>Reproduce using: `python classify/val.py --data ../datasets/imagenet --img 224`\n- **Speed** metrics are averaged over 100 inference images using a Google [Colab Pro V100 High-RAM instance](https://colab.research.google.com/signup).<br>Reproduce using: `python classify/val.py --data ../datasets/imagenet --img 224 --batch 1`\n- **Export** to ONNX (FP32) and TensorRT (FP16) was performed using `export.py`.<br>Reproduce using: `python export.py --weights yolov5s-cls.pt --include engine onnx --imgsz 224`\n\n</details>\n</details>\n\n<details>\n  <summary>Classification Usage Examples &nbsp;<a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/classify/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a></summary>\n\n### Train\n\nYOLOv5 classification training supports automatic download for datasets like [MNIST](https://docs.ultralytics.com/datasets/classify/mnist/), [Fashion-MNIST](https://docs.ultralytics.com/datasets/classify/fashion-mnist/), [CIFAR10](https://docs.ultralytics.com/datasets/classify/cifar10/), [CIFAR100](https://docs.ultralytics.com/datasets/classify/cifar100/), [Imagenette](https://docs.ultralytics.com/datasets/classify/imagenette/), [Imagewoof](https://docs.ultralytics.com/datasets/classify/imagewoof/), and [ImageNet](https://docs.ultralytics.com/datasets/classify/imagenet/) using the `--data` argument. For example, start training on MNIST with `--data mnist`.\n\n```bash\n# Train on a single GPU using CIFAR-100 dataset\npython classify/train.py --model yolov5s-cls.pt --data cifar100 --epochs 5 --img 224 --batch 128\n\n# Train using Multi-GPU DDP on ImageNet dataset\npython -m torch.distributed.run --nproc_per_node 4 --master_port 1 classify/train.py --model yolov5s-cls.pt --data imagenet --epochs 5 --img 224 --device 0,1,2,3\n```\n\n### Val\n\nValidate the accuracy of the YOLOv5m-cls model on the ImageNet-1k validation dataset:\n\n```bash\n# Download ImageNet validation split (6.3GB, 50,000 images)\nbash data/scripts/get_imagenet.sh --val\n\n# Validate the model\npython classify/val.py --weights yolov5m-cls.pt --data ../datasets/imagenet --img 224\n```\n\n### Predict\n\nUse the pretrained YOLOv5s-cls.pt model to classify the image `bus.jpg`:\n\n```bash\n# Run prediction\npython classify/predict.py --weights yolov5s-cls.pt --source data/images/bus.jpg\n```\n\n```python\n# Load model from PyTorch Hub\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"custom\", \"yolov5s-cls.pt\")\n```\n\n### Export\n\nExport trained YOLOv5s-cls, ResNet50, and EfficientNet_b0 models to ONNX and TensorRT formats:\n\n```bash\n# Export models\npython export.py --weights yolov5s-cls.pt resnet50.pt efficientnet_b0.pt --include onnx engine --img 224\n```\n\n</details>\n\n## \u2601\ufe0f Environments\n\nGet started quickly with our pre-configured environments. Click the icons below for setup details.\n\n<div align=\"center\">\n  <a href=\"https://bit.ly/yolov5-paperspace-notebook\" title=\"Run on Paperspace Gradient\">\n    <img src=\"https://github.com/ultralytics/assets/releases/download/v0.0.0/logo-gradient.png\" width=\"10%\" /></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"5%\" alt=\"\" />\n  <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\" title=\"Open in Google Colab\">\n    <img src=\"https://github.com/ultralytics/assets/releases/download/v0.0.0/logo-colab-small.png\" width=\"10%\" /></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"5%\" alt=\"\" />\n  <a href=\"https://www.kaggle.com/models/ultralytics/yolov5\" title=\"Open in Kaggle\">\n    <img src=\"https://github.com/ultralytics/assets/releases/download/v0.0.0/logo-kaggle-small.png\" width=\"10%\" /></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"5%\" alt=\"\" />\n  <a href=\"https://hub.docker.com/r/ultralytics/yolov5\" title=\"Pull Docker Image\">\n    <img src=\"https://github.com/ultralytics/assets/releases/download/v0.0.0/logo-docker-small.png\" width=\"10%\" /></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"5%\" alt=\"\" />\n  <a href=\"https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/\" title=\"AWS Quickstart Guide\">\n    <img src=\"https://github.com/ultralytics/assets/releases/download/v0.0.0/logo-aws-small.png\" width=\"10%\" /></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"5%\" alt=\"\" />\n  <a href=\"https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/\" title=\"GCP Quickstart Guide\">\n    <img src=\"https://github.com/ultralytics/assets/releases/download/v0.0.0/logo-gcp-small.png\" width=\"10%\" /></a>\n</div>\n\n## \ud83e\udd1d Contribute\n\nWe welcome your contributions! Making YOLOv5 accessible and effective is a community effort. Please see our [Contributing Guide](https://docs.ultralytics.com/help/contributing/) to get started. Share your feedback through the [YOLOv5 Survey](https://www.ultralytics.com/survey?utm_source=github&utm_medium=social&utm_campaign=Survey). Thank you to all our contributors for making YOLOv5 better!\n\n[![Ultralytics open-source contributors](https://raw.githubusercontent.com/ultralytics/assets/main/im/image-contributors.png)](https://github.com/ultralytics/yolov5/graphs/contributors)\n\n## \ud83d\udcdc License\n\nUltralytics provides two licensing options to meet different needs:\n\n- **AGPL-3.0 License**: An [OSI-approved](https://opensource.org/license/agpl-v3) open-source license ideal for academic research, personal projects, and testing. It promotes open collaboration and knowledge sharing. See the [LICENSE](https://github.com/ultralytics/yolov5/blob/master/LICENSE) file for details.\n- **Enterprise License**: Tailored for commercial applications, this license allows seamless integration of Ultralytics software and AI models into commercial products and services, bypassing the open-source requirements of AGPL-3.0. For commercial use cases, please contact us via [Ultralytics Licensing](https://www.ultralytics.com/license).\n\n## \ud83d\udce7 Contact\n\nFor bug reports and feature requests related to YOLOv5, please visit [GitHub Issues](https://github.com/ultralytics/yolov5/issues). For general questions, discussions, and community support, join our [Discord server](https://discord.com/invite/ultralytics)!\n\n<br>\n<div align=\"center\">\n  <a href=\"https://github.com/ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png\" width=\"3%\" alt=\"Ultralytics GitHub\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"space\">\n  <a href=\"https://www.linkedin.com/company/ultralytics/\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png\" width=\"3%\" alt=\"Ultralytics LinkedIn\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"space\">\n  <a href=\"https://twitter.com/ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png\" width=\"3%\" alt=\"Ultralytics Twitter\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"space\">\n  <a href=\"https://youtube.com/ultralytics?sub_confirmation=1\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png\" width=\"3%\" alt=\"Ultralytics YouTube\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"space\">\n  <a href=\"https://www.tiktok.com/@ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png\" width=\"3%\" alt=\"Ultralytics TikTok\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"space\">\n  <a href=\"https://ultralytics.com/bilibili\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-bilibili.png\" width=\"3%\" alt=\"Ultralytics BiliBili\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"space\">\n  <a href=\"https://discord.com/invite/ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-discord.png\" width=\"3%\" alt=\"Ultralytics Discord\"></a>\n</div>\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 114747226,
    "name": "faceswap",
    "full_name": "deepfakes/faceswap",
    "description": "Deepfakes Software For All",
    "html_url": "https://github.com/deepfakes/faceswap",
    "clone_url": "https://github.com/deepfakes/faceswap.git",
    "owner_login": "deepfakes",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/34667098?v=4",
    "stargazers_count": 54353,
    "watchers_count": 54353,
    "forks_count": 13426,
    "open_issues_count": 39,
    "size": 203600,
    "language": "Python",
    "languages": {
      "Python": 3517980,
      "Shell": 30982,
      "NSIS": 16093
    },
    "topics": [
      "deep-face-swap",
      "deep-learning",
      "deep-neural-networks",
      "deepface",
      "deepfakes",
      "deeplearning",
      "face-swap",
      "faceswap",
      "fakeapp",
      "machine-learning",
      "myfakeapp",
      "neural-nets",
      "neural-networks",
      "openfaceswap"
    ],
    "license_name": "GNU General Public License v3.0",
    "created_at": "2017-12-19T09:44:13+00:00",
    "updated_at": "2025-08-05T20:34:45+00:00",
    "pushed_at": "2025-07-11T17:20:12+00:00",
    "contributors_count": 81,
    "readme_length": 12091,
    "readme_content": "# deepfakes_faceswap\n\n### Important information for **Patreon** and **PayPal** supporters. Please see this forum post: https://forum.faceswap.dev/viewtopic.php?f=14&t=3120\n\n<p align=\"center\">\n  <a href=\"https://faceswap.dev\"><img src=\"https://i.imgur.com/zHvjHnb.png\"></img></a>\n<br />FaceSwap is a tool that utilizes deep learning to recognize and swap faces in pictures and videos.\n</p>\n<p align=\"center\">\n<img src = \"https://i.imgur.com/nWHFLDf.jpg\"></img>\n</p>\n\n<p align=\"center\">\n<a href=\"https://www.patreon.com/bePatron?u=23238350\"><img src=\"https://c5.patreon.com/external/logo/become_a_patron_button.png\"></img></a>\n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"https://discord.gg/FC54sYg\"><img src=\"https://i.imgur.com/gIpztkv.png\"></img></a></p>\n\n<p align=\"center\">\n  <a href=\"https://www.dailymotion.com/video/x810mot\"><img src=\"https://user-images.githubusercontent.com/36920800/178301720-b69841bb-a1ca-4c20-91db-a2a10f5692ca.png\"></img></a>\n<br />Emma Stone/Scarlett Johansson FaceSwap using the Phaze-A model\n</p>\n\n<p align=\"center\">\n  <a href=\"https://www.youtube.com/watch?v=r1jng79a5xc\"><img src=\"https://img.youtube.com/vi/r1jng79a5xc/0.jpg\"></img></a>\n<br />Jennifer Lawrence/Steve Buscemi FaceSwap using the Villain model\n</p>\n\n\n![Build Status](https://github.com/deepfakes/faceswap/actions/workflows/pytest.yml/badge.svg) [![Documentation Status](https://readthedocs.org/projects/faceswap/badge/?version=latest)](https://faceswap.readthedocs.io/en/latest/?badge=latest)\n\nMake sure you check out [INSTALL.md](INSTALL.md) before getting started.\n\n- [deepfakes\\_faceswap](#deepfakes_faceswap)\n    - [Important information for **Patreon** and **PayPal** supporters. Please see this forum post: https://forum.faceswap.dev/viewtopic.php?f=14\\&t=3120](#important-information-for-patreon-and-paypal-supporters-please-see-this-forum-post-httpsforumfaceswapdevviewtopicphpf14t3120)\n- [Manifesto](#manifesto)\n  - [FaceSwap has ethical uses.](#faceswap-has-ethical-uses)\n- [How To setup and run the project](#how-to-setup-and-run-the-project)\n- [Overview](#overview)\n  - [Extract](#extract)\n  - [Train](#train)\n  - [Convert](#convert)\n  - [GUI](#gui)\n- [General notes:](#general-notes)\n- [Help I need support!](#help-i-need-support)\n  - [Discord Server](#discord-server)\n  - [FaceSwap Forum](#faceswap-forum)\n- [Donate](#donate)\n  - [Patreon](#patreon)\n  - [One time Donations](#one-time-donations)\n    - [@torzdf](#torzdf)\n    - [@andenixa](#andenixa)\n- [How to contribute](#how-to-contribute)\n  - [For people interested in the generative models](#for-people-interested-in-the-generative-models)\n  - [For devs](#for-devs)\n  - [For non-dev advanced users](#for-non-dev-advanced-users)\n  - [For end-users](#for-end-users)\n- [About machine learning](#about-machine-learning)\n  - [How does a computer know how to recognize/shape faces? How does machine learning work? What is a neural network?](#how-does-a-computer-know-how-to-recognizeshape-faces-how-does-machine-learning-work-what-is-a-neural-network)\n\n# Manifesto\n\n## FaceSwap has ethical uses.\n\nWhen faceswapping was first developed and published, the technology was groundbreaking, it was a huge step in AI development. It was also completely ignored outside of academia because the code was confusing and fragmentary. It required a thorough understanding of complicated AI techniques and took a lot of effort to figure it out. Until one individual brought it together into a single, cohesive collection. It ran, it worked, and as is so often the way with new technology emerging on the internet, it was immediately used to create inappropriate content. Despite the inappropriate uses the software was given originally, it was the first AI code that anyone could download, run and learn by experimentation without having a Ph.D. in math, computer theory, psychology, and more. Before \"deepfakes\" these techniques were like black magic, only practiced by those who could understand all of the inner workings as described in esoteric and endlessly complicated books and papers.\n\n\"Deepfakes\" changed all that and anyone could participate in AI development. To us, developers, the release of this code opened up a fantastic learning opportunity. It allowed us to build on ideas developed by others, collaborate with a variety of skilled coders, experiment with AI whilst learning new skills and ultimately contribute towards an emerging technology which will only see more mainstream use as it progresses.\n\nAre there some out there doing horrible things with similar software? Yes. And because of this, the developers have been following strict ethical standards. Many of us don't even use it to create videos, we just tinker with the code to see what it does. Sadly, the media concentrates only on the unethical uses of this software. That is, unfortunately, the nature of how it was first exposed to the public, but it is not representative of why it was created, how we use it now, or what we see in its future. Like any technology, it can be used for good or it can be abused. It is our intention to develop FaceSwap in a way that its potential for abuse is minimized whilst maximizing its potential as a tool for learning, experimenting and, yes, for legitimate faceswapping.\n\nWe are not trying to denigrate celebrities or to demean anyone. We are programmers, we are engineers, we are Hollywood VFX artists, we are activists, we are hobbyists, we are human beings. To this end, we feel that it's time to come out with a standard statement of what this software is and isn't as far as us developers are concerned.\n\n- FaceSwap is not for creating inappropriate content.\n- FaceSwap is not for changing faces without consent or with the intent of hiding its use.\n- FaceSwap is not for any illicit, unethical, or questionable purposes.\n- FaceSwap exists to experiment and discover AI techniques, for social or political commentary, for movies, and for any number of ethical and reasonable uses.\n\nWe are very troubled by the fact that FaceSwap can be used for unethical and disreputable things. However, we support the development of tools and techniques that can be used ethically as well as provide education and experience in AI for anyone who wants to learn it hands-on. We will take a zero tolerance approach to anyone using this software for any unethical purposes and will actively discourage any such uses.\n\n# How To setup and run the project\nFaceSwap is a Python program that will run on multiple Operating Systems including Windows, Linux, and MacOS.\n\nSee [INSTALL.md](INSTALL.md) for full installation instructions. You will need a modern GPU with CUDA support for best performance. Many AMD GPUs are supported through DirectML (Windows) and ROCm (Linux).\n\n# Overview\nThe project has multiple entry points. You will have to:\n - Gather photos and/or videos\n - **Extract** faces from your raw photos\n - **Train** a model on the faces extracted from the photos/videos\n - **Convert** your sources with the model\n\nCheck out [USAGE.md](USAGE.md) for more detailed instructions.\n\n## Extract\nFrom your setup folder, run `python faceswap.py extract`. This will take photos from `src` folder and extract faces into `extract` folder.\n\n## Train\nFrom your setup folder, run `python faceswap.py train`. This will take photos from two folders containing pictures of both faces and train a model that will be saved inside the `models` folder.\n\n## Convert\nFrom your setup folder, run `python faceswap.py convert`. This will take photos from `original` folder and apply new faces into `modified` folder.\n\n## GUI\nAlternatively, you can run the GUI by running `python faceswap.py gui`\n\n# General notes:\n- All of the scripts mentioned have `-h`/`--help` options with arguments that they will accept. You're smart, you can figure out how this works, right?!\n\nNB: there is a conversion tool for video. This can be accessed by running `python tools.py effmpeg -h`. Alternatively, you can use [ffmpeg](https://www.ffmpeg.org) to convert video into photos, process images, and convert images back to the video.\n\n\n**Some tips:**\n\nReusing existing models will train much faster than starting from nothing.\nIf there is not enough training data, start with someone who looks similar, then switch the data.\n\n# Help I need support!\n## Discord Server\nYour best bet is to join the [FaceSwap Discord server](https://discord.gg/FC54sYg) where there are plenty of users willing to help. Please note that, like this repo, this is a SFW Server!\n\n## FaceSwap Forum\nAlternatively, you can post questions in the [FaceSwap Forum](https://faceswap.dev/forum). Please do not post general support questions in this repo as they are liable to be deleted without response.\n\n# Donate\nThe developers work tirelessly to improve and develop FaceSwap. Many hours have been put in to provide the software as it is today, but this is an extremely time-consuming process with no financial reward. If you enjoy using the software, please consider donating to the devs, so they can spend more time implementing improvements.\n\n## Patreon\nThe best way to support us is through our Patreon page:\n\n[![become-a-patron](https://c5.patreon.com/external/logo/become_a_patron_button.png)](https://www.patreon.com/bePatron?u=23238350)\n\n## One time Donations\nAlternatively you can give a one off donation to any of our Devs:\n### @torzdf\n There is very little FaceSwap code that hasn't been touched by torzdf. He is responsible for implementing the GUI, FAN aligner, MTCNN detector and porting the Villain, DFL-H128 and DFaker models to FaceSwap, as well as significantly improving many areas of the code.\n\n**Bitcoin:** bc1qpm22suz59ylzk0j7qk5e4c7cnkjmve2rmtrnc6\n\n**Ethereum:** 0xd3e954dC241B87C4E8E1A801ada485DC1d530F01\n\n**Monero:** 45dLrtQZ2pkHizBpt3P3yyJKkhcFHnhfNYPMSnz3yVEbdWm3Hj6Kr5TgmGAn3Far8LVaQf1th2n3DJVTRkfeB5ZkHxWozSX\n\n**Paypal:** [![torzdf](https://www.paypalobjects.com/en_GB/i/btn/btn_donate_SM.gif)](https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=JZ8PP3YE9J62L)\n\n### @andenixa\nCreator of the Unbalanced and OHR models, as well as expanding various capabilities within the training process. Andenixa is currently working on new models and will take requests for donations.\n\n**Paypal:** [![andenixa](https://www.paypalobjects.com/en_GB/i/btn/btn_donate_SM.gif)](https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=NRVLQYGS6NWTU)\n\n# How to contribute\n\n## For people interested in the generative models\n - Go to the 'faceswap-model' to discuss/suggest/commit alternatives to the current algorithm.\n\n## For devs\n - Read this README entirely\n - Fork the repo\n - Play with it\n - Check issues with the 'dev' tag\n - For devs more interested in computer vision and openCV, look at issues with the 'opencv' tag. Also feel free to add your own alternatives/improvements\n\n## For non-dev advanced users\n - Read this README entirely\n - Clone the repo\n - Play with it\n - Check issues with the 'advuser' tag\n - Also go to the '[faceswap Forum](https://faceswap.dev/forum)' and help others.\n\n## For end-users\n - Get the code here and play with it if you can\n - You can also go to the [faceswap Forum](https://faceswap.dev/forum) and help or get help from others.\n - Be patient. This is a relatively new technology for developers as well. Much effort is already being put into making this program easy to use for the average user. It just takes time!\n - **Notice** Any issue related to running the code has to be opened in the [faceswap Forum](https://faceswap.dev/forum)!\n\n# About machine learning\n\n## How does a computer know how to recognize/shape faces? How does machine learning work? What is a neural network?\nIt's complicated. Here's a good video that makes the process understandable:\n[![How Machines Learn](https://img.youtube.com/vi/R9OHn5ZF4Uo/0.jpg)](https://www.youtube.com/watch?v=R9OHn5ZF4Uo)\n\nHere's a slightly more in depth video that tries to explain the basic functioning of a neural network:\n[![How Machines Learn](https://img.youtube.com/vi/aircAruvnKk/0.jpg)](https://www.youtube.com/watch?v=aircAruvnKk)\n\ntl;dr: training data + trial and error\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 323048702,
    "name": "OpenBB",
    "full_name": "OpenBB-finance/OpenBB",
    "description": "Investment Research for Everyone, Everywhere.",
    "html_url": "https://github.com/OpenBB-finance/OpenBB",
    "clone_url": "https://github.com/OpenBB-finance/OpenBB.git",
    "owner_login": "OpenBB-finance",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/80064875?v=4",
    "stargazers_count": 48237,
    "watchers_count": 48237,
    "forks_count": 4434,
    "open_issues_count": 64,
    "size": 2358995,
    "language": "Python",
    "languages": {
      "Python": 8001568
    },
    "topics": [
      "ai",
      "crypto",
      "derivatives",
      "economics",
      "equity",
      "finance",
      "fixed-income",
      "machine-learning",
      "openbb",
      "options",
      "python",
      "quantitative-finance",
      "stocks"
    ],
    "license_name": "Other",
    "created_at": "2020-12-20T10:46:38+00:00",
    "updated_at": "2025-08-06T02:12:43+00:00",
    "pushed_at": "2025-08-04T20:53:30+00:00",
    "contributors_count": 100,
    "readme_length": 10226,
    "readme_content": "<br />\n<img src=\"https://github.com/OpenBB-finance/OpenBB/blob/develop/images/platform-light.svg?raw=true#gh-light-mode-only\" alt=\"OpenBB Platform logo\" width=\"600\">\n<img src=\"https://github.com/OpenBB-finance/OpenBB/blob/develop/images/platform-dark.svg?raw=true#gh-dark-mode-only\" alt=\"OpenBB Platform logo\" width=\"600\">\n<br />\n<br />\n\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/openbb_finance.svg?style=social&label=Follow%20%40openbb_finance)](https://x.com/openbb_finance)\n[![Discord Shield](https://img.shields.io/discord/831165782750789672)](https://discord.com/invite/xPHTuHCmuV)\n[![Open in Dev Containers](https://img.shields.io/static/v1?label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/OpenBB-finance/OpenBB)\n<a href=\"https://codespaces.new/OpenBB-finance/OpenBB\">\n  <img src=\"https://github.com/codespaces/badge.svg\" height=\"20\" />\n</a>\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/OpenBB-finance/OpenBB/blob/develop/examples/googleColab.ipynb\">\n  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n[![PyPI](https://img.shields.io/pypi/v/openbb?color=blue&label=PyPI%20Package)](https://pypi.org/project/openbb/)\n\nThe first financial Platform that is open source.\n\nThe OpenBB Platform offers access to equity, options, crypto, forex, macro economy, fixed income, and more while also offering a broad range of extensions to enhance the user experience according to their needs.\n\nGet started with: `pip install openbb`\n\n```python\nfrom openbb import obb\noutput = obb.equity.price.historical(\"AAPL\")\ndf = output.to_dataframe()\n```\n\nYou can sign up to the [OpenBB Hub](https://my.openbb.co/login) to get the most out of the OpenBB ecosystem.\n\nData integrations available can be found here: <https://docs.openbb.co/platform/reference>\n\n---\n\n## OpenBB Workspace\n\nWhile the OpenBB Platform is all about an integration to dozens of different data vendors, the interface is either Python or a CLI.\n\nIf you want an enterprise UI to visualize this datasets and use AI agents on top, you can find OpenBB Workspace at <https://pro.openbb.co>.\n\n<a href=\"https://pro.openbb.co\">\n  <div align=\"center\">\n  <img src=\"https://openbb-cms.directus.app/assets/f69b6aaf-0821-4bc8-a43c-715e03a924ef.png\" alt=\"Logo\" width=\"1000\">\n  </div>\n</a>\n\nData integration:\n\n- You can learn more about adding data to the OpenBB workspace from the [docs](https://docs.openbb.co/workspace) or [this open source repository](https://github.com/OpenBB-finance/backends-for-openbb).\n\nAI Agents integration:\n\n- You can learn more about adding AI agents to the OpenBB workspace from [this open source repository](https://github.com/OpenBB-finance/agents-for-openbb).\n\n### Integrating OpenBB Platform to the OpenBB Workspace\n\nConnect this library to the OpenBB Workspace with a few simple commands, in a Python (3.9.21 - 3.12) environment.\n\n#### Run OpenBB Platform backend\n\n- Install the packages.\n\n```sh\npip install \"openbb[all]\"\n```\n\n- Start the API server over localhost.\n\n```sh\nopenbb-api\n```\n\nThis will launch a FastAPI server, via Uvicorn, at `127.0.0.1:6900`.\n\nYou can check that it works by going to <http://127.0.0.1:6900>.\n\n#### Integrate OpenBB Platform backend to OpenBB Workspace\n\nSign-in to the [OpenBB Workspace](https://pro.openbb.co/), and follow the following steps:\n\n![CleanShot 2025-05-17 at 09 51 56@2x](https://github.com/user-attachments/assets/75cffb4a-5e95-470a-b9d0-6ffd4067e069)\n\n1. Go to the \"Apps\" tab\n2. Click on \"Connect backend\"\n3. Fill in the form with:\n   Name: OpenBB Platform\n   URL: <http://127.0.0.1:6900>\n4. Click on \"Test\". You should get a \"Test successful\" with the number of apps found.\n5. Click on \"Add\".\n\nThat's it.\n\n---\n\n<!-- TABLE OF CONTENTS -->\n<details closed=\"closed\">\n  <summary><h2 style=\"display: inline-block\">Table of Contents</h2></summary>\n  <ol>\n    <li><a href=\"#1-installation\">Installation</a></li>\n    <li><a href=\"#2-contributing\">Contributing</a></li>\n    <li><a href=\"#3-license\">License</a></li>\n    <li><a href=\"#4-disclaimer\">Disclaimer</a></li>\n    <li><a href=\"#5-contacts\">Contacts</a></li>\n    <li><a href=\"#6-star-history\">Star History</a></li>\n    <li><a href=\"#7-contributors\">Contributors</a></li>\n  </ol>\n</details>\n\n## 1. Installation\n\nThe OpenBB Platform can be installed as a [PyPI package](https://pypi.org/project/openbb/) by running `pip install openbb`\n\nor by cloning the repository directly with `git clone https://github.com/OpenBB-finance/OpenBB.git`.\n\nPlease find more about the installation process, in the [OpenBB Documentation](https://docs.openbb.co/platform/installation).\n\n### OpenBB Platform CLI installation\n\nThe OpenBB Platform CLI is a command-line interface that allows you to access the OpenBB Platform directly from your command line.\n\nIt can be installed by running `pip install openbb-cli`\n\nor by cloning the repository directly with  `git clone https://github.com/OpenBB-finance/OpenBB.git`.\n\nPlease find more about the installation process in the [OpenBB Documentation](https://docs.openbb.co/cli/installation).\n\n## 2. Contributing\n\nThere are three main ways of contributing to this project. (Hopefully you have starred the project by now \u2b50\ufe0f)\n\n### Become a Contributor\n\n- More information on our [Contributing Documentation](https://docs.openbb.co/platform/developer_guide/misc/contributing).\n\n### Create a GitHub ticket\n\nBefore creating a ticket make sure the one you are creating doesn't exist already [here](https://github.com/OpenBB-finance/OpenBB/issues)\n\n- [Report bug](https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&labels=bug&template=bug_report.md&title=%5BBug%5D)\n- [Suggest improvement](https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&labels=enhancement&template=enhancement.md&title=%5BIMPROVE%5D)\n- [Request a feature](https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&labels=new+feature&template=feature_request.md&title=%5BFR%5D)\n\n### Provide feedback\n\nWe are most active on [our Discord](https://openbb.co/discord), but feel free to reach out to us in any of [our social media](https://openbb.co/links) for feedback.\n\n## 3. License\n\nDistributed under the AGPLv3 License. See\n[LICENSE](https://github.com/OpenBB-finance/OpenBB/blob/main/LICENSE) for more information.\n\n## 4. Disclaimer\n\nTrading in financial instruments involves high risks including the risk of losing some, or all, of your investment\namount, and may not be suitable for all investors.\n\nBefore deciding to trade in a financial instrument you should be fully informed of the risks and costs associated with trading the financial markets, carefully consider your investment objectives, level of experience, and risk appetite, and seek professional advice where needed.\n\nThe data contained in the OpenBB Platform is not necessarily accurate.\n\nOpenBB and any provider of the data contained in this website will not accept liability for any loss or damage as a result of your trading, or your reliance on the information displayed.\n\nAll names, logos, and brands of third parties that may be referenced in our sites, products or documentation are trademarks of their respective owners. Unless otherwise specified, OpenBB and its products and services are not endorsed by, sponsored by, or affiliated with these third parties.\n\nOur use of these names, logos, and brands is for identification purposes only, and does not imply any such endorsement, sponsorship, or affiliation.\n\n## 5. Contacts\n\nIf you have any questions about the platform or anything OpenBB, feel free to email us at `support@openbb.co`\n\nIf you want to say hi, or are interested in partnering with us, feel free to reach us at `hello@openbb.co`\n\nAny of our social media platforms: [openbb.co/links](https://openbb.co/links)\n\n## 6. Star History\n\nThis is a proxy of our growth and that we are just getting started.\n\nBut for more metrics important to us check [openbb.co/open](https://openbb.co/open).\n\n[![Star History Chart](https://api.star-history.com/svg?repos=openbb-finance/OpenBB&type=Date&theme=dark)](https://api.star-history.com/svg?repos=openbb-finance/OpenBB&type=Date&theme=dark)\n\n## 7. Contributors\n\nOpenBB wouldn't be OpenBB without you. If we are going to disrupt financial industry, every contribution counts. Thank you for being part of this journey.\n\n<a href=\"https://github.com/OpenBB-finance/OpenBB/graphs/contributors\">\n   <img src=\"https://contributors-img.web.app/image?repo=OpenBB-finance/OpenBB\" width=\"800\"/>\n</a>\n\n<!-- MARKDOWN LINKS & IMAGES -->\n<!-- https://www.markdownguide.org/basic-syntax/#reference-style-links -->\n\n[contributors-shield]: https://img.shields.io/github/contributors/OpenBB-finance/OpenBB.svg?style=for-the-badge\n[contributors-url]: https://github.com/OpenBB-finance/OpenBB/graphs/contributors\n[forks-shield]: https://img.shields.io/github/forks/OpenBB-finance/OpenBB.svg?style=for-the-badge\n[forks-url]: https://github.com/OpenBB-finance/OpenBB/network/members\n[stars-shield]: https://img.shields.io/github/stars/OpenBB-finance/OpenBB.svg?style=for-the-badge\n[stars-url]: https://github.com/OpenBB-finance/OpenBB/stargazers\n[issues-shield]: https://img.shields.io/github/issues/OpenBB-finance/OpenBB.svg?style=for-the-badge&color=blue\n[issues-url]: https://github.com/OpenBB-finance/OpenBB/issues\n[bugs-open-shield]: https://img.shields.io/github/issues/OpenBB-finance/OpenBB/bug.svg?style=for-the-badge&color=yellow\n[bugs-open-url]: https://github.com/OpenBB-finance/OpenBB/issues?q=is%3Aissue+label%3Abug+is%3Aopen\n[bugs-closed-shield]: https://img.shields.io/github/issues-closed/OpenBB-finance/OpenBB/bug.svg?style=for-the-badge&color=success\n[bugs-closed-url]: https://github.com/OpenBB-finance/OpenBB/issues?q=is%3Aissue+label%3Abug+is%3Aclosed\n[license-shield]: https://img.shields.io/github/license/OpenBB-finance/OpenBB.svg?style=for-the-badge\n[license-url]: https://github.com/OpenBB-finance/OpenBB/blob/main/LICENSE.txt\n[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=for-the-badge&logo=linkedin&colorB=555\n[linkedin-url]: https://linkedin.com/in/DidierRLopes\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 139824423,
    "name": "100-Days-Of-ML-Code",
    "full_name": "Avik-Jain/100-Days-Of-ML-Code",
    "description": "100 Days of ML Coding",
    "html_url": "https://github.com/Avik-Jain/100-Days-Of-ML-Code",
    "clone_url": "https://github.com/Avik-Jain/100-Days-Of-ML-Code.git",
    "owner_login": "Avik-Jain",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/30073708?v=4",
    "stargazers_count": 47857,
    "watchers_count": 47857,
    "forks_count": 11028,
    "open_issues_count": 65,
    "size": 10955,
    "language": null,
    "languages": {},
    "topics": [
      "100-days-of-code-log",
      "100daysofcode",
      "deep-learning",
      "implementation",
      "infographics",
      "linear-algebra",
      "linear-regression",
      "logistic-regression",
      "machine-learning",
      "machine-learning-algorithms",
      "naive-bayes-classifier",
      "python",
      "scikit-learn",
      "siraj-raval",
      "siraj-raval-challenge",
      "support-vector-machines",
      "svm",
      "tutorial"
    ],
    "license_name": "MIT License",
    "created_at": "2018-07-05T09:11:43+00:00",
    "updated_at": "2025-08-06T00:20:35+00:00",
    "pushed_at": "2023-12-29T07:57:53+00:00",
    "contributors_count": 6,
    "readme_length": 18245,
    "readme_content": "# 100-Days-Of-ML-Code\n\n100 Days of Machine Learning Coding as proposed by [Siraj Raval](https://github.com/llSourcell)\n\nGet the datasets from [here](https://github.com/Avik-Jain/100-Days-Of-ML-Code/tree/master/datasets)\n\n## Data PreProcessing | Day 1\nCheck out the code from [here](https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Code/Day%201_Data%20PreProcessing.md).\n\n<p align=\"center\">\n  <img src=\"https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%201.jpg\">\n</p>\n\n## Simple Linear Regression | Day 2\nCheck out the code from [here](https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Code/Day2_Simple_Linear_Regression.md).\n\n<p align=\"center\">\n  <img src=\"https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%202.jpg\">\n</p>\n\n## Multiple Linear Regression | Day 3\nCheck out the code from [here](https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Code/Day3_Multiple_Linear_Regression.md).\n\n<p align=\"center\">\n  <img src=\"https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%203.jpg\">\n</p>\n\n## Logistic Regression | Day 4\n\n<p align=\"center\">\n  <img src=\"https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%204.jpg\">\n</p>\n\n## Logistic Regression | Day 5\nMoving forward into #100DaysOfMLCode today I dived into the deeper depth of what Logistic Regression actually is and what is the math involved behind it. Learned how cost function is calculated and then how to apply gradient descent algorithm to cost function to minimize the error in prediction.  \nDue to less time I will now be posting an infographic on alternate days.\nAlso if someone wants to help me out in documentaion of code and already has some experince in the field and knows Markdown for github please contact me on LinkedIn :) .\n\n## Implementing Logistic Regression | Day 6\nCheck out the Code [here](https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Code/Day%206%20Logistic%20Regression.md)\n\n## K Nearest Neighbours | Day 7\n<p align=\"center\">\n  <img src=\"https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%207.jpg\">\n</p>\n\n## Math Behind Logistic Regression | Day 8 \n\n#100DaysOfMLCode To clear my insights on logistic regression I was searching on the internet for some resource or article and I came across this article (https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc) by Saishruthi Swaminathan. \n\nIt gives a detailed description of Logistic Regression. Do check it out.\n\n## Support Vector Machines | Day 9\nGot an intution on what SVM is and how it is used to solve Classification problem.\n\n## SVM and KNN | Day 10\nLearned more about how SVM works and implementing the K-NN algorithm.\n\n## Implementation of K-NN | Day 11  \n\nImplemented the K-NN algorithm for classification. #100DaysOfMLCode \nSupport Vector Machine Infographic is halfway complete. Will update it tomorrow.\n\n## Support Vector Machines | Day 12\n<p align=\"center\">\n  <img src=\"https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%2012.jpg\">\n</p>\n\n## Naive Bayes Classifier | Day 13\n\nContinuing with #100DaysOfMLCode today I went through the Naive Bayes classifier.\nI am also implementing the SVM in python using scikit-learn. Will update the code soon.\n\n## Implementation of SVM | Day 14\nToday I implemented SVM on linearly related data. Used Scikit-Learn library. In Scikit-Learn we have SVC classifier which we use to achieve this task. Will be using kernel-trick on next implementation.\nCheck the code [here](https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Code/Day%2013%20SVM.md).\n\n## Naive Bayes Classifier and Black Box Machine Learning | Day 15\nLearned about different types of naive bayes classifiers. Also started the lectures by [Bloomberg](https://bloomberg.github.io/foml/#home). First one in the playlist was Black Box Machine Learning. It gives the whole overview about prediction functions, feature extraction, learning algorithms, performance evaluation, cross-validation, sample bias, nonstationarity, overfitting, and hyperparameter tuning.\n\n## Implemented SVM using Kernel Trick | Day 16\nUsing Scikit-Learn library implemented SVM algorithm along with kernel function which maps our data points into higher dimension to find optimal hyperplane. \n\n## Started Deep learning Specialization on Coursera | Day 17\nCompleted the whole Week 1 and Week 2 on a single day. Learned Logistic regression as Neural Network. \n\n## Deep learning Specialization on Coursera | Day 18\nCompleted the Course 1 of the deep learning specialization. Implemented a neural net in python.\n\n## The Learning Problem , Professor Yaser Abu-Mostafa | Day 19\nStarted Lecture 1 of 18 of Caltech's Machine Learning Course - CS 156 by Professor Yaser Abu-Mostafa. It was basically an introduction to the upcoming lectures. He also explained Perceptron Algorithm.\n\n## Started Deep learning Specialization Course 2 | Day 20\nCompleted the Week 1 of Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization.\n\n## Web Scraping | Day 21\nWatched some tutorials on how to do web scraping using Beautiful Soup in order to collect data for building a model.\n\n## Is Learning Feasible? | Day 22\nLecture 2 of 18 of Caltech's Machine Learning Course - CS 156 by Professor Yaser Abu-Mostafa. Learned about Hoeffding Inequality.\n\n## Decision Trees | Day 23\n<p align=\"center\">\n  <img src=\"https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%2023.jpg\">\n</p>\n\n## Introduction To Statistical Learning Theory | Day 24\nLec 3 of Bloomberg ML course introduced some of the core concepts like input space, action space, outcome space, prediction functions, loss functions, and hypothesis spaces.\n\n## Implementing Decision Trees | Day 25\nCheck the code [here.](https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Code/Day%2025%20Decision%20Tree.md)\n\n## Jumped To Brush up Linear Algebra | Day 26\nFound an amazing [channel](https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw) on youtube 3Blue1Brown. It has a playlist called Essence of Linear Algebra. Started off by completing 4 videos which gave a complete overview of Vectors, Linear Combinations, Spans, Basis Vectors, Linear Transformations and Matrix Multiplication. \n\nLink to the playlist [here.](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)\n\n## Jumped To Brush up Linear Algebra | Day 27\nContinuing with the playlist completed next 4 videos discussing topics 3D Transformations, Determinants, Inverse Matrix, Column Space, Null Space and Non-Square Matrices.\n\nLink to the playlist [here.](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)\n\n## Jumped To Brush up Linear Algebra | Day 28\nIn the playlist of 3Blue1Brown completed another 3 videos from the essence of linear algebra. \nTopics covered were Dot Product and Cross Product.\n\nLink to the playlist [here.](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)\n\n\n## Jumped To Brush up Linear Algebra | Day 29\nCompleted the whole playlist today, videos 12-14. Really an amazing playlist to refresh the concepts of Linear Algebra.\nTopics covered were the change of basis, Eigenvectors and Eigenvalues, and Abstract Vector Spaces.\n\nLink to the playlist [here.](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)\n\n## Essence of calculus | Day 30\nCompleting the playlist - Essence of Linear Algebra by 3blue1brown a suggestion popped up by youtube regarding a series of videos again by the same channel 3Blue1Brown. Being already impressed by the previous series on Linear algebra I dived straight into it.\nCompleted about 5 videos on topics such as Derivatives, Chain Rule, Product Rule, and derivative of exponential.\n\nLink to the playlist [here.](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr)\n\n## Essence of calculus | Day 31\nWatched 2 Videos on topic Implicit Diffrentiation and Limits from the playlist Essence of Calculus.\n\nLink to the playlist [here.](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr)\n\n## Essence of calculus | Day 32\nWatched the remaining 4 videos covering topics Like Integration and Higher order derivatives.\n\nLink to the playlist [here.](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr)\n\n## Random Forests | Day 33\n<p align=\"center\">\n  <img src=\"https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%2033.jpg\">\n</p>\n\n## Implementing Random Forests | Day 34\nCheck the code [here.](https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Code/Day%2034%20Random_Forest.md)\n\n## But what *is* a Neural Network? | Deep learning, chapter 1  | Day 35\nAn Amazing Video on neural networks by 3Blue1Brown youtube channel. This video gives a good understanding of Neural Networks and uses Handwritten digit dataset to explain the concept. \nLink To the [video.](https://www.youtube.com/watch?v=aircAruvnKk&t=7s)\n\n## Gradient descent, how neural networks learn | Deep learning, chapter 2 | Day 36\nPart two of neural networks by 3Blue1Brown youtube channel. This video explains the concepts of Gradient Descent in an interesting way. 169 must watch and highly recommended.\nLink To the [video.](https://www.youtube.com/watch?v=IHZwWFHWa-w)\n\n## What is backpropagation really doing? | Deep learning, chapter 3 | Day 37\nPart three of neural networks by 3Blue1Brown youtube channel. This video mostly discusses the partial derivatives and backpropagation.\nLink To the [video.](https://www.youtube.com/watch?v=Ilg3gGewQ5U)\n\n## Backpropagation calculus | Deep learning, chapter 4 | Day 38\nPart four of neural networks by 3Blue1Brown youtube channel. The goal here is to represent, in somewhat more formal terms, the intuition for how backpropagation works and the video moslty discusses the partial derivatives and backpropagation.\nLink To the [video.](https://www.youtube.com/watch?v=tIeHLnjs5U8)\n\n## Deep Learning with Python, TensorFlow, and Keras tutorial | Day 39\nLink To the [video.](https://www.youtube.com/watch?v=wQ8BIBpya2k&t=19s&index=2&list=PLQVvvaa0QuDfhTox0AjmQ6tvTgMBZBEXN)\n\n## Loading in your own data - Deep Learning basics with Python, TensorFlow and Keras p.2 | Day 40\nLink To the [video.](https://www.youtube.com/watch?v=j-3vuBynnOE&list=PLQVvvaa0QuDfhTox0AjmQ6tvTgMBZBEXN&index=2)\n\n## Convolutional Neural Networks - Deep Learning basics with Python, TensorFlow and Keras p.3 | Day 41\nLink To the [video.](https://www.youtube.com/watch?v=WvoLTXIjBYU&list=PLQVvvaa0QuDfhTox0AjmQ6tvTgMBZBEXN&index=3)\n\n## Analyzing Models with TensorBoard - Deep Learning with Python, TensorFlow and Keras p.4 | Day 42\nLink To the [video.](https://www.youtube.com/watch?v=BqgTU7_cBnk&list=PLQVvvaa0QuDfhTox0AjmQ6tvTgMBZBEXN&index=4)\n\n## K Means Clustering | Day 43\nMoved to Unsupervised Learning and studied about Clustering.\nWorking on my website check it out [avikjain.me](http://www.avikjain.me/)\nAlso found a wonderful animation that can help to easily understand K - Means Clustering [Link](http://shabal.in/visuals/kmeans/6.html)\n\n<p align=\"center\">\n  <img src=\"https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%2043.jpg\">\n</p>\n\n## K Means Clustering Implementation | Day 44\nImplemented K Means Clustering. Check the code [here.]()\n\n## Digging Deeper | NUMPY  | Day 45\nGot a new book \"Python Data Science HandBook\" by JK VanderPlas Check the Jupyter notebooks [here.](https://github.com/jakevdp/PythonDataScienceHandbook)\n<br>Started with chapter 2 : Introduction to Numpy. Covered topics like Data Types, Numpy arrays and Computations on Numpy arrays.\n<br>Check the code - \n<br>[Introduction to NumPy](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.00-Introduction-to-NumPy.ipynb)\n<br>[Understanding Data Types in Python](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.01-Understanding-Data-Types.ipynb)\n<br>[The Basics of NumPy Arrays](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.02-The-Basics-Of-NumPy-Arrays.ipynb)\n<br>[Computation on NumPy Arrays: Universal Functions](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.03-Computation-on-arrays-ufuncs.ipynb)\n\n## Digging Deeper | NUMPY | Day 46\nChapter 2 : Aggregations, Comparisions and Broadcasting\n<br>Link to Notebook:\n<br>[Aggregations: Min, Max, and Everything In Between](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.04-Computation-on-arrays-aggregates.ipynb)\n<br>[Computation on Arrays: Broadcasting](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.05-Computation-on-arrays-broadcasting.ipynb)\n<br>[Comparisons, Masks, and Boolean Logic](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.06-Boolean-Arrays-and-Masks.ipynb)\n\n## Digging Deeper | NUMPY | Day 47\nChapter 2 : Fancy Indexing, sorting arrays, Struchered Data\n<br>Link to Notebook:\n<br>[Fancy Indexing](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.07-Fancy-Indexing.ipynb)\n<br>[Sorting Arrays](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.08-Sorting.ipynb)\n<br>[Structured Data: NumPy's Structured Arrays](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.09-<br>Structured-Data-NumPy.ipynb)\n\n## Digging Deeper | PANDAS | Day 48\nChapter 3 : Data Manipulation with Pandas\n<br> Covered Various topics like Pandas Objects, Data Indexing and Selection, Operating on Data, Handling Missing Data, Hierarchical Indexing, ConCat and Append.\n<br>Link To the Notebooks:\n<br>[Data Manipulation with Pandas](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.00-Introduction-to-Pandas.ipynb)\n<br>[Introducing Pandas Objects](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.01-Introducing-Pandas-Objects.ipynb)\n<br>[Data Indexing and Selection](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.02-Data-Indexing-and-Selection.ipynb)\n<br>[Operating on Data in Pandas](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.03-Operations-in-Pandas.ipynb)\n<br>[Handling Missing Data](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.04-Missing-Values.ipynb)\n<br>[Hierarchical Indexing](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.05-Hierarchical-Indexing.ipynb)\n<br>[Combining Datasets: Concat and Append](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.06-Concat-And-Append.ipynb)\n\n## Digging Deeper | PANDAS | Day 49\nChapter 3: Completed following topics- Merge and Join, Aggregation and grouping and Pivot Tables.\n<br>[Combining Datasets: Merge and Join](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.07-Merge-and-Join.ipynb)\n<br>[Aggregation and Grouping](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.08-Aggregation-and-Grouping.ipynb)\n<br>[Pivot Tables](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.09-Pivot-Tables.ipynb)\n\n## Digging Deeper | PANDAS | Day 50\nChapter 3: Vectorized Strings Operations, Working with Time Series\n<br>Links to Notebooks:\n<br>[Vectorized String Operations](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.10-Working-With-Strings.ipynb)\n<br>[Working with Time Series](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.11-Working-with-Time-Series.ipynb)\n<br>[High-Performance Pandas: eval() and query()](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.12-Performance-Eval-and-Query.ipynb)\n\n## Digging Deeper | MATPLOTLIB | Day 51\nChapter 4: Visualization with Matplotlib \nLearned about Simple Line Plots, Simple Scatter Plotsand Density and Contour Plots.\n<br>Links to Notebooks: \n<br>[Visualization with Matplotlib](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.00-Introduction-To-Matplotlib.ipynb)\n<br>[Simple Line Plots](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.01-Simple-Line-Plots.ipynb)\n<br>[Simple Scatter Plots](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.02-Simple-Scatter-Plots.ipynb)\n<br>[Visualizing Errors](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.03-Errorbars.ipynb)\n<br>[Density and Contour Plots](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.04-Density-and-Contour-Plots.ipynb)\n\n## Digging Deeper | MATPLOTLIB | Day 52\nChapter 4: Visualization with Matplotlib \nLearned about Histograms, How to customize plot legends, colorbars, and buliding Multiple Subplots.\n<br>Links to Notebooks: \n<br>[Histograms, Binnings, and Density](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.05-Histograms-and-Binnings.ipynb)\n<br>[Customizing Plot Legends](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.06-Customizing-Legends.ipynb)\n<br>[Customizing Colorbars](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.07-Customizing-Colorbars.ipynb)\n<br>[Multiple Subplots](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.08-Multiple-Subplots.ipynb)\n<br>[Text and Annotation](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.09-Text-and-Annotation.ipynb)\n\n## Digging Deeper | MATPLOTLIB | Day 53\nChapter 4: Covered Three Dimensional Plotting in Mathplotlib.\n<br>Links to Notebooks:\n<br>[Three-Dimensional Plotting in Matplotlib](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.12-Three-Dimensional-Plotting.ipynb)\n\n## Hierarchical Clustering | Day 54\nStudied about Hierarchical Clustering.\nCheck out this amazing [Visualization.](https://cdn-images-1.medium.com/max/800/1*ET8kCcPpr893vNZFs8j4xg.gif)\n<p align=\"center\">\n  <img src=\"https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%2054.jpg\">\n</p>\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 1644196,
    "name": "julia",
    "full_name": "JuliaLang/julia",
    "description": "The Julia Programming Language",
    "html_url": "https://github.com/JuliaLang/julia",
    "clone_url": "https://github.com/JuliaLang/julia.git",
    "owner_login": "JuliaLang",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/743164?v=4",
    "stargazers_count": 47434,
    "watchers_count": 47434,
    "forks_count": 5614,
    "open_issues_count": 5003,
    "size": 338777,
    "language": "Julia",
    "languages": {
      "Julia": 12661748,
      "C": 3567008,
      "C++": 2222821,
      "Scheme": 523338,
      "Makefile": 236389,
      "LLVM": 212513,
      "Clojure": 34978,
      "Shell": 32502,
      "Objective-C": 25812,
      "PHP": 15949,
      "Assembly": 14338,
      "Inno Setup": 7305,
      "Python": 6594,
      "Rich Text Format": 2851,
      "DTrace": 1487,
      "AppleScript": 175,
      "GDB": 76
    },
    "topics": [
      "hacktoberfest",
      "hpc",
      "julia",
      "julia-language",
      "julialang",
      "machine-learning",
      "numerical",
      "programming-language",
      "science",
      "scientific"
    ],
    "license_name": "MIT License",
    "created_at": "2011-04-21T07:01:50+00:00",
    "updated_at": "2025-08-05T23:14:07+00:00",
    "pushed_at": "2025-08-06T00:36:43+00:00",
    "contributors_count": 100,
    "readme_length": 7728,
    "readme_content": "<a name=\"logo\"/>\n<div align=\"center\">\n<a href=\"https://julialang.org/\" target=\"_blank\">\n<img src=\"doc/src/assets/logo.svg\" alt=\"Julia Logo\" width=\"210\" height=\"142\"></img>\n</a>\n</div>\n\n<table>\n    <!-- Docs -->\n    <tr>\n        <td>Documentation</td>\n        <td>\n            <a href=\"https://docs.julialang.org\"><img src='https://img.shields.io/badge/docs-v1-blue.svg'/></a>\n        </td>\n    </tr>\n    <!-- Continuous integration\n    To change the badge to point to a different pipeline, it is not sufficient to simply change the `?branch=` part.\n    You need to go to the Buildkite website and get the SVG URL for the correct pipeline. -->\n    <tr>\n        <td>Continuous integration</td>\n        <td>\n            <a href=\"https://buildkite.com/julialang/julia-master\"><img src='https://badge.buildkite.com/f28e0d28b345f9fad5856ce6a8d64fffc7c70df8f4f2685cd8.svg?branch=master'/></a>\n        </td>\n    </tr>\n    <!-- Coverage -->\n    <tr>\n        <td>Code coverage</td>\n        <td>\n            <a href='https://coveralls.io/github/JuliaLang/julia?branch=master'><img src='https://coveralls.io/repos/github/JuliaLang/julia/badge.svg?branch=master' alt='Coverage Status'/></a>\n            <a href=\"https://codecov.io/gh/JuliaLang/julia\"><img src=\"https://codecov.io/gh/JuliaLang/julia/branch/master/graph/badge.svg?token=TckCRxc7HS\"/></a>\n        </td>\n    </tr>\n</table>\n\n## The Julia Language\n\nJulia is a high-level, high-performance dynamic language for technical\ncomputing. The main homepage for Julia can be found at\n[julialang.org](https://julialang.org/). This is the GitHub\nrepository of Julia source code, including instructions for compiling\nand installing Julia, below.\n\n## Resources\n\n- **Homepage:** <https://julialang.org>\n- **Install:** <https://julialang.org/install/>\n- **Source code:** <https://github.com/JuliaLang/julia>\n- **Documentation:** <https://docs.julialang.org>\n- **Packages:** <https://julialang.org/packages/>\n- **Discussion forum:** <https://discourse.julialang.org>\n- **Zulip:** <https://julialang.zulipchat.com/>\n- **Slack:** <https://julialang.slack.com> (get an invite from <https://julialang.org/slack/>)\n- **YouTube:** <https://www.youtube.com/user/JuliaLanguage>\n- **Code coverage:** <https://coveralls.io/r/JuliaLang/julia>\n\nNew developers may find the notes in\n[CONTRIBUTING](https://github.com/JuliaLang/julia/blob/master/CONTRIBUTING.md)\nhelpful to start contributing to the Julia codebase.\n\n### Learning Julia\n\n- [**Learning resources**](https://julialang.org/learning/)\n\n## Binary Installation\n\nThe recommended way of installing Julia is to use `juliaup` which will install\nthe latest stable `julia` for you and help keep it up to date. It can also let\nyou install and run different Julia versions simultaneously. Instructions for\nthis can be found [here](https://julialang.org/install/). If you want to manually\ndownload specific Julia binaries, you can find those on the [downloads\npage](https://julialang.org/downloads/). The downloads page also provides\ndetails on the [different tiers of\nsupport](https://julialang.org/downloads/#supported_platforms) for OS and\nplatform combinations.\n\nIf everything works correctly, you will get a `julia` program and when you run\nit in a terminal or command prompt, you will see a Julia banner and an\ninteractive prompt into which you can enter expressions for evaluation. You can\nread about [getting\nstarted](https://docs.julialang.org/en/v1/manual/getting-started/) in the\nmanual.\n\n**Note**: Although some OS package managers provide Julia, such\ninstallations are neither maintained nor endorsed by the Julia\nproject. They may be outdated, broken and/or unmaintained. We\nrecommend you use the official Julia binaries instead.\n\n## Building Julia\n\nFirst, make sure you have all the [required\ndependencies](https://github.com/JuliaLang/julia/blob/master/doc/src/devdocs/build/build.md#required-build-tools-and-external-libraries) installed.\nThen, acquire the source code by cloning the git repository:\n\n    git clone https://github.com/JuliaLang/julia.git\n\nand then use the command prompt to change into the resulting julia directory. By default, you will be building the latest unstable version of\nJulia. However, most users should use the [most recent stable version](https://github.com/JuliaLang/julia/releases)\nof Julia. You can get this version by running:\n\n    git checkout v1.11.6\n\nTo build the `julia` executable, run `make` from within the julia directory.\n\nBuilding Julia requires 2GiB of disk space and approximately 4GiB of virtual memory.\n\n**Note:** The build process will fail badly if any of the build directory's parent directories have spaces or other shell meta-characters such as `$` or `:` in their names (this is due to a limitation in GNU make).\n\nOnce it is built, you can run the `julia` executable. From within the julia directory, run\n\n    ./julia\n\nYour first test of Julia determines whether your build is working\nproperly. From the julia\ndirectory, type `make testall`. You should see output that\nlists a series of running tests; if they complete without error, you\nshould be in good shape to start using Julia.\n\nYou can read about [getting\nstarted](https://docs.julialang.org/en/v1/manual/getting-started/)\nin the manual.\n\nDetailed build instructions, should they be necessary,\nare included in the [build documentation](https://github.com/JuliaLang/julia/blob/master/doc/src/devdocs/build/build.md).\n\n### Uninstalling Julia\n\nBy default, Julia does not install anything outside the directory it was cloned\ninto and `~/.julia`. Julia and the vast majority of Julia packages can be\ncompletely uninstalled by deleting these two directories.\n\n## Source Code Organization\n\nThe Julia source code is organized as follows:\n\n| Directory         | Contents                                                           |\n| -                 | -                                                                  |\n| `base/`           | source code for the Base module (part of Julia's standard library) |\n| `cli/`            | source for the command line interface/REPL                         |\n| `contrib/`        | miscellaneous scripts                                              |\n| `deps/`           | external dependencies                                              |\n| `doc/src/`        | source for the user manual                                         |\n| `etc/`            | contains `startup.jl`                                              |\n| `src/`            | source for Julia language core                                     |\n| `stdlib/`         | source code for other standard library packages                    |\n| `test/`           | test suites                                                        |\n\n## Terminal, Editors and IDEs\n\nThe Julia REPL is quite powerful. See the section in the manual on\n[the Julia REPL](https://docs.julialang.org/en/v1/stdlib/REPL/)\nfor more details.\n\nOn Windows, we highly recommend running Julia in a modern terminal,\nsuch as [Windows Terminal from the Microsoft Store](https://aka.ms/terminal).\n\nSupport for editing Julia is available for many\n[widely used editors](https://github.com/JuliaEditorSupport):\n[Emacs](https://github.com/JuliaEditorSupport/julia-emacs),\n[Vim](https://github.com/JuliaEditorSupport/julia-vim),\n[Sublime Text](https://github.com/JuliaEditorSupport/Julia-sublime), and many\nothers.\n\nFor users who prefer IDEs, we recommend using VS Code with the\n[julia-vscode](https://www.julia-vscode.org/) plugin.\\\nFor notebook users, [Jupyter](https://jupyter.org/) notebook support is available through the\n[IJulia](https://github.com/JuliaLang/IJulia.jl) package, and\nthe [Pluto.jl](https://github.com/fonsp/Pluto.jl) package provides Pluto notebooks.\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 535360445,
    "name": "ultralytics",
    "full_name": "ultralytics/ultralytics",
    "description": "Ultralytics YOLO \ud83d\ude80",
    "html_url": "https://github.com/ultralytics/ultralytics",
    "clone_url": "https://github.com/ultralytics/ultralytics.git",
    "owner_login": "ultralytics",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/26833451?v=4",
    "stargazers_count": 44031,
    "watchers_count": 44031,
    "forks_count": 8605,
    "open_issues_count": 363,
    "size": 34463,
    "language": "Python",
    "languages": {
      "Python": 2533210,
      "Shell": 4718,
      "HTML": 4261,
      "Dockerfile": 3718
    },
    "topics": [
      "cli",
      "computer-vision",
      "deep-learning",
      "hub",
      "image-classification",
      "instance-segmentation",
      "machine-learning",
      "object-detection",
      "pose-estimation",
      "python",
      "pytorch",
      "rotated-object-detection",
      "segment-anything",
      "tracking",
      "ultralytics",
      "yolo",
      "yolo-world",
      "yolo11",
      "yolov10",
      "yolov8"
    ],
    "license_name": "GNU Affero General Public License v3.0",
    "created_at": "2022-09-11T16:39:45+00:00",
    "updated_at": "2025-08-06T01:35:10+00:00",
    "pushed_at": "2025-08-05T22:35:26+00:00",
    "contributors_count": 100,
    "readme_length": 33066,
    "readme_content": "<div align=\"center\">\n  <p>\n    <a href=\"https://www.ultralytics.com/blog/ultralytics-yolo11-has-arrived-redefine-whats-possible-in-ai\" target=\"_blank\">\n      <img width=\"100%\" src=\"https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png\" alt=\"Ultralytics YOLO banner\"></a>\n  </p>\n\n[\u4e2d\u6587](https://docs.ultralytics.com/zh/) | [\ud55c\uad6d\uc5b4](https://docs.ultralytics.com/ko/) | [\u65e5\u672c\u8a9e](https://docs.ultralytics.com/ja/) | [\u0420\u0443\u0441\u0441\u043a\u0438\u0439](https://docs.ultralytics.com/ru/) | [Deutsch](https://docs.ultralytics.com/de/) | [Fran\u00e7ais](https://docs.ultralytics.com/fr/) | [Espa\u00f1ol](https://docs.ultralytics.com/es) | [Portugu\u00eas](https://docs.ultralytics.com/pt/) | [T\u00fcrk\u00e7e](https://docs.ultralytics.com/tr/) | [Ti\u1ebfng Vi\u1ec7t](https://docs.ultralytics.com/vi/) | [\u0627\u0644\u0639\u0631\u0628\u064a\u0629](https://docs.ultralytics.com/ar/) <br>\n\n<div>\n    <a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n    <a href=\"https://pepy.tech/projects/ultralytics\"><img src=\"https://static.pepy.tech/badge/ultralytics\" alt=\"Ultralytics Downloads\"></a>\n    <a href=\"https://zenodo.org/badge/latestdoi/264818686\"><img src=\"https://zenodo.org/badge/264818686.svg\" alt=\"Ultralytics YOLO Citation\"></a>\n    <a href=\"https://discord.com/invite/ultralytics\"><img alt=\"Ultralytics Discord\" src=\"https://img.shields.io/discord/1089800235347353640?logo=discord&logoColor=white&label=Discord&color=blue\"></a>\n    <a href=\"https://community.ultralytics.com/\"><img alt=\"Ultralytics Forums\" src=\"https://img.shields.io/discourse/users?server=https%3A%2F%2Fcommunity.ultralytics.com&logo=discourse&label=Forums&color=blue\"></a>\n    <a href=\"https://www.reddit.com/r/ultralytics/\"><img alt=\"Ultralytics Reddit\" src=\"https://img.shields.io/reddit/subreddit-subscribers/ultralytics?style=flat&logo=reddit&logoColor=white&label=Reddit&color=blue\"></a>\n    <br>\n    <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run Ultralytics on Gradient\"></a>\n    <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open Ultralytics In Colab\"></a>\n    <a href=\"https://www.kaggle.com/models/ultralytics/yolo11\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open Ultralytics In Kaggle\"></a>\n    <a href=\"https://mybinder.org/v2/gh/ultralytics/ultralytics/HEAD?labpath=examples%2Ftutorial.ipynb\"><img src=\"https://mybinder.org/badge_logo.svg\" alt=\"Open Ultralytics In Binder\"></a>\n</div>\n</div>\n<br>\n\n[Ultralytics](https://www.ultralytics.com/) creates cutting-edge, state-of-the-art (SOTA) [YOLO models](https://www.ultralytics.com/yolo) built on years of foundational research in computer vision and AI. Constantly updated for performance and flexibility, our models are **fast**, **accurate**, and **easy to use**. They excel at [object detection](https://docs.ultralytics.com/tasks/detect/), [tracking](https://docs.ultralytics.com/modes/track/), [instance segmentation](https://docs.ultralytics.com/tasks/segment/), [image classification](https://docs.ultralytics.com/tasks/classify/), and [pose estimation](https://docs.ultralytics.com/tasks/pose/) tasks.\n\nFind detailed documentation in the [Ultralytics Docs](https://docs.ultralytics.com/). Get support via [GitHub Issues](https://github.com/ultralytics/ultralytics/issues/new/choose). Join discussions on [Discord](https://discord.com/invite/ultralytics), [Reddit](https://www.reddit.com/r/ultralytics/), and the [Ultralytics Community Forums](https://community.ultralytics.com/)!\n\nRequest an Enterprise License for commercial use at [Ultralytics Licensing](https://www.ultralytics.com/license).\n\n<a href=\"https://docs.ultralytics.com/models/yolo11/\" target=\"_blank\">\n  <img width=\"100%\" src=\"https://raw.githubusercontent.com/ultralytics/assets/refs/heads/main/yolo/performance-comparison.png\" alt=\"YOLO11 performance plots\">\n</a>\n\n<div align=\"center\">\n  <a href=\"https://github.com/ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png\" width=\"2%\" alt=\"Ultralytics GitHub\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"space\">\n  <a href=\"https://www.linkedin.com/company/ultralytics/\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png\" width=\"2%\" alt=\"Ultralytics LinkedIn\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"space\">\n  <a href=\"https://twitter.com/ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png\" width=\"2%\" alt=\"Ultralytics Twitter\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"space\">\n  <a href=\"https://youtube.com/ultralytics?sub_confirmation=1\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png\" width=\"2%\" alt=\"Ultralytics YouTube\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"space\">\n  <a href=\"https://www.tiktok.com/@ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png\" width=\"2%\" alt=\"Ultralytics TikTok\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"space\">\n  <a href=\"https://ultralytics.com/bilibili\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-bilibili.png\" width=\"2%\" alt=\"Ultralytics BiliBili\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"space\">\n  <a href=\"https://discord.com/invite/ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-discord.png\" width=\"2%\" alt=\"Ultralytics Discord\"></a>\n</div>\n\n## \ud83d\udcc4 Documentation\n\nSee below for quickstart installation and usage examples. For comprehensive guidance on training, validation, prediction, and deployment, refer to our full [Ultralytics Docs](https://docs.ultralytics.com/).\n\n<details open>\n<summary>Install</summary>\n\nInstall the `ultralytics` package, including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml), in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/).\n\n[![PyPI - Version](https://img.shields.io/pypi/v/ultralytics?logo=pypi&logoColor=white)](https://pypi.org/project/ultralytics/) [![Ultralytics Downloads](https://static.pepy.tech/badge/ultralytics)](https://www.pepy.tech/projects/ultralytics) [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/ultralytics?logo=python&logoColor=gold)](https://pypi.org/project/ultralytics/)\n\n```bash\npip install ultralytics\n```\n\nFor alternative installation methods, including [Conda](https://anaconda.org/conda-forge/ultralytics), [Docker](https://hub.docker.com/r/ultralytics/ultralytics), and building from source via Git, please consult the [Quickstart Guide](https://docs.ultralytics.com/quickstart/).\n\n[![Conda Version](https://img.shields.io/conda/vn/conda-forge/ultralytics?logo=condaforge)](https://anaconda.org/conda-forge/ultralytics) [![Docker Image Version](https://img.shields.io/docker/v/ultralytics/ultralytics?sort=semver&logo=docker)](https://hub.docker.com/r/ultralytics/ultralytics) [![Ultralytics Docker Pulls](https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker)](https://hub.docker.com/r/ultralytics/ultralytics)\n\n</details>\n\n<details open>\n<summary>Usage</summary>\n\n### CLI\n\nYou can use Ultralytics YOLO directly from the Command Line Interface (CLI) with the `yolo` command:\n\n```bash\n# Predict using a pretrained YOLO model (e.g., YOLO11n) on an image\nyolo predict model=yolo11n.pt source='https://ultralytics.com/images/bus.jpg'\n```\n\nThe `yolo` command supports various tasks and modes, accepting additional arguments like `imgsz=640`. Explore the YOLO [CLI Docs](https://docs.ultralytics.com/usage/cli/) for more examples.\n\n### Python\n\nUltralytics YOLO can also be integrated directly into your Python projects. It accepts the same [configuration arguments](https://docs.ultralytics.com/usage/cfg/) as the CLI:\n\n```python\nfrom ultralytics import YOLO\n\n# Load a pretrained YOLO11n model\nmodel = YOLO(\"yolo11n.pt\")\n\n# Train the model on the COCO8 dataset for 100 epochs\ntrain_results = model.train(\n    data=\"coco8.yaml\",  # Path to dataset configuration file\n    epochs=100,  # Number of training epochs\n    imgsz=640,  # Image size for training\n    device=\"cpu\",  # Device to run on (e.g., 'cpu', 0, [0,1,2,3])\n)\n\n# Evaluate the model's performance on the validation set\nmetrics = model.val()\n\n# Perform object detection on an image\nresults = model(\"path/to/image.jpg\")  # Predict on an image\nresults[0].show()  # Display results\n\n# Export the model to ONNX format for deployment\npath = model.export(format=\"onnx\")  # Returns the path to the exported model\n```\n\nDiscover more examples in the YOLO [Python Docs](https://docs.ultralytics.com/usage/python/).\n\n</details>\n\n## \u2728 Models\n\nUltralytics supports a wide range of YOLO models, from early versions like [YOLOv3](https://docs.ultralytics.com/models/yolov3/) to the latest [YOLO11](https://docs.ultralytics.com/models/yolo11/). The tables below showcase YOLO11 models pretrained on the [COCO](https://docs.ultralytics.com/datasets/detect/coco/) dataset for [Detection](https://docs.ultralytics.com/tasks/detect/), [Segmentation](https://docs.ultralytics.com/tasks/segment/), and [Pose Estimation](https://docs.ultralytics.com/tasks/pose/). Additionally, [Classification](https://docs.ultralytics.com/tasks/classify/) models pretrained on the [ImageNet](https://docs.ultralytics.com/datasets/classify/imagenet/) dataset are available. [Tracking](https://docs.ultralytics.com/modes/track/) mode is compatible with all Detection, Segmentation, and Pose models. All [Models](https://docs.ultralytics.com/models/) are automatically downloaded from the latest Ultralytics [release](https://github.com/ultralytics/assets/releases) upon first use.\n\n<a href=\"https://docs.ultralytics.com/tasks/\" target=\"_blank\">\n    <img width=\"100%\" src=\"https://github.com/ultralytics/docs/releases/download/0/ultralytics-yolov8-tasks-banner.avif\" alt=\"Ultralytics YOLO supported tasks\">\n</a>\n<br>\n<br>\n\n<details open><summary>Detection (COCO)</summary>\n\nExplore the [Detection Docs](https://docs.ultralytics.com/tasks/detect/) for usage examples. These models are trained on the [COCO dataset](https://cocodataset.org/), featuring 80 object classes.\n\n| Model                                                                                | size<br><sup>(pixels) | mAP<sup>val<br>50-95 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>T4 TensorRT10<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\n| ------------------------------------------------------------------------------------ | --------------------- | -------------------- | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |\n| [YOLO11n](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt) | 640                   | 39.5                 | 56.1 \u00b1 0.8                     | 1.5 \u00b1 0.0                           | 2.6                | 6.5               |\n| [YOLO11s](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s.pt) | 640                   | 47.0                 | 90.0 \u00b1 1.2                     | 2.5 \u00b1 0.0                           | 9.4                | 21.5              |\n| [YOLO11m](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m.pt) | 640                   | 51.5                 | 183.2 \u00b1 2.0                    | 4.7 \u00b1 0.1                           | 20.1               | 68.0              |\n| [YOLO11l](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l.pt) | 640                   | 53.4                 | 238.6 \u00b1 1.4                    | 6.2 \u00b1 0.1                           | 25.3               | 86.9              |\n| [YOLO11x](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x.pt) | 640                   | 54.7                 | 462.8 \u00b1 6.7                    | 11.3 \u00b1 0.2                          | 56.9               | 194.9             |\n\n- **mAP<sup>val</sup>** values refer to single-model single-scale performance on the [COCO val2017](https://cocodataset.org/) dataset. See [YOLO Performance Metrics](https://docs.ultralytics.com/guides/yolo-performance-metrics/) for details. <br>Reproduce with `yolo val detect data=coco.yaml device=0`\n- **Speed** metrics are averaged over COCO val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance. CPU speeds measured with [ONNX](https://onnx.ai/) export. GPU speeds measured with [TensorRT](https://developer.nvidia.com/tensorrt) export. <br>Reproduce with `yolo val detect data=coco.yaml batch=1 device=0|cpu`\n\n</details>\n\n<details><summary>Segmentation (COCO)</summary>\n\nRefer to the [Segmentation Docs](https://docs.ultralytics.com/tasks/segment/) for usage examples. These models are trained on [COCO-Seg](https://docs.ultralytics.com/datasets/segment/coco/), including 80 classes.\n\n| Model                                                                                        | size<br><sup>(pixels) | mAP<sup>box<br>50-95 | mAP<sup>mask<br>50-95 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>T4 TensorRT10<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\n| -------------------------------------------------------------------------------------------- | --------------------- | -------------------- | --------------------- | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |\n| [YOLO11n-seg](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-seg.pt) | 640                   | 38.9                 | 32.0                  | 65.9 \u00b1 1.1                     | 1.8 \u00b1 0.0                           | 2.9                | 10.4              |\n| [YOLO11s-seg](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s-seg.pt) | 640                   | 46.6                 | 37.8                  | 117.6 \u00b1 4.9                    | 2.9 \u00b1 0.0                           | 10.1               | 35.5              |\n| [YOLO11m-seg](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-seg.pt) | 640                   | 51.5                 | 41.5                  | 281.6 \u00b1 1.2                    | 6.3 \u00b1 0.1                           | 22.4               | 123.3             |\n| [YOLO11l-seg](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l-seg.pt) | 640                   | 53.4                 | 42.9                  | 344.2 \u00b1 3.2                    | 7.8 \u00b1 0.2                           | 27.6               | 142.2             |\n| [YOLO11x-seg](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x-seg.pt) | 640                   | 54.7                 | 43.8                  | 664.5 \u00b1 3.2                    | 15.8 \u00b1 0.7                          | 62.1               | 319.0             |\n\n- **mAP<sup>val</sup>** values are for single-model single-scale on the [COCO val2017](https://cocodataset.org/) dataset. See [YOLO Performance Metrics](https://docs.ultralytics.com/guides/yolo-performance-metrics/) for details. <br>Reproduce with `yolo val segment data=coco.yaml device=0`\n- **Speed** metrics are averaged over COCO val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance. CPU speeds measured with [ONNX](https://onnx.ai/) export. GPU speeds measured with [TensorRT](https://developer.nvidia.com/tensorrt) export. <br>Reproduce with `yolo val segment data=coco.yaml batch=1 device=0|cpu`\n\n</details>\n\n<details><summary>Classification (ImageNet)</summary>\n\nConsult the [Classification Docs](https://docs.ultralytics.com/tasks/classify/) for usage examples. These models are trained on [ImageNet](https://docs.ultralytics.com/datasets/classify/imagenet/), covering 1000 classes.\n\n| Model                                                                                        | size<br><sup>(pixels) | acc<br><sup>top1 | acc<br><sup>top5 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>T4 TensorRT10<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) at 224 |\n| -------------------------------------------------------------------------------------------- | --------------------- | ---------------- | ---------------- | ------------------------------ | ----------------------------------- | ------------------ | ------------------------ |\n| [YOLO11n-cls](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-cls.pt) | 224                   | 70.0             | 89.4             | 5.0 \u00b1 0.3                      | 1.1 \u00b1 0.0                           | 1.6                | 0.5                      |\n| [YOLO11s-cls](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s-cls.pt) | 224                   | 75.4             | 92.7             | 7.9 \u00b1 0.2                      | 1.3 \u00b1 0.0                           | 5.5                | 1.6                      |\n| [YOLO11m-cls](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-cls.pt) | 224                   | 77.3             | 93.9             | 17.2 \u00b1 0.4                     | 2.0 \u00b1 0.0                           | 10.4               | 5.0                      |\n| [YOLO11l-cls](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l-cls.pt) | 224                   | 78.3             | 94.3             | 23.2 \u00b1 0.3                     | 2.8 \u00b1 0.0                           | 12.9               | 6.2                      |\n| [YOLO11x-cls](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x-cls.pt) | 224                   | 79.5             | 94.9             | 41.4 \u00b1 0.9                     | 3.8 \u00b1 0.0                           | 28.4               | 13.7                     |\n\n- **acc** values represent model accuracy on the [ImageNet](https://www.image-net.org/) dataset validation set. <br>Reproduce with `yolo val classify data=path/to/ImageNet device=0`\n- **Speed** metrics are averaged over ImageNet val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance. CPU speeds measured with [ONNX](https://onnx.ai/) export. GPU speeds measured with [TensorRT](https://developer.nvidia.com/tensorrt) export. <br>Reproduce with `yolo val classify data=path/to/ImageNet batch=1 device=0|cpu`\n\n</details>\n\n<details><summary>Pose (COCO)</summary>\n\nSee the [Pose Estimation Docs](https://docs.ultralytics.com/tasks/pose/) for usage examples. These models are trained on [COCO-Pose](https://docs.ultralytics.com/datasets/pose/coco/), focusing on the 'person' class.\n\n| Model                                                                                          | size<br><sup>(pixels) | mAP<sup>pose<br>50-95 | mAP<sup>pose<br>50 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>T4 TensorRT10<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\n| ---------------------------------------------------------------------------------------------- | --------------------- | --------------------- | ------------------ | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |\n| [YOLO11n-pose](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-pose.pt) | 640                   | 50.0                  | 81.0               | 52.4 \u00b1 0.5                     | 1.7 \u00b1 0.0                           | 2.9                | 7.6               |\n| [YOLO11s-pose](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s-pose.pt) | 640                   | 58.9                  | 86.3               | 90.5 \u00b1 0.6                     | 2.6 \u00b1 0.0                           | 9.9                | 23.2              |\n| [YOLO11m-pose](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-pose.pt) | 640                   | 64.9                  | 89.4               | 187.3 \u00b1 0.8                    | 4.9 \u00b1 0.1                           | 20.9               | 71.7              |\n| [YOLO11l-pose](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l-pose.pt) | 640                   | 66.1                  | 89.9               | 247.7 \u00b1 1.1                    | 6.4 \u00b1 0.1                           | 26.2               | 90.7              |\n| [YOLO11x-pose](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x-pose.pt) | 640                   | 69.5                  | 91.1               | 488.0 \u00b1 13.9                   | 12.1 \u00b1 0.2                          | 58.8               | 203.3             |\n\n- **mAP<sup>val</sup>** values are for single-model single-scale on the [COCO Keypoints val2017](https://docs.ultralytics.com/datasets/pose/coco/) dataset. See [YOLO Performance Metrics](https://docs.ultralytics.com/guides/yolo-performance-metrics/) for details. <br>Reproduce with `yolo val pose data=coco-pose.yaml device=0`\n- **Speed** metrics are averaged over COCO val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance. CPU speeds measured with [ONNX](https://onnx.ai/) export. GPU speeds measured with [TensorRT](https://developer.nvidia.com/tensorrt) export. <br>Reproduce with `yolo val pose data=coco-pose.yaml batch=1 device=0|cpu`\n\n</details>\n\n<details><summary>Oriented Bounding Boxes (DOTAv1)</summary>\n\nCheck the [OBB Docs](https://docs.ultralytics.com/tasks/obb/) for usage examples. These models are trained on [DOTAv1](https://docs.ultralytics.com/datasets/obb/dota-v2/#dota-v10/), including 15 classes.\n\n| Model                                                                                        | size<br><sup>(pixels) | mAP<sup>test<br>50 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>T4 TensorRT10<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\n| -------------------------------------------------------------------------------------------- | --------------------- | ------------------ | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |\n| [YOLO11n-obb](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-obb.pt) | 1024                  | 78.4               | 117.6 \u00b1 0.8                    | 4.4 \u00b1 0.0                           | 2.7                | 17.2              |\n| [YOLO11s-obb](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s-obb.pt) | 1024                  | 79.5               | 219.4 \u00b1 4.0                    | 5.1 \u00b1 0.0                           | 9.7                | 57.5              |\n| [YOLO11m-obb](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-obb.pt) | 1024                  | 80.9               | 562.8 \u00b1 2.9                    | 10.1 \u00b1 0.4                          | 20.9               | 183.5             |\n| [YOLO11l-obb](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l-obb.pt) | 1024                  | 81.0               | 712.5 \u00b1 5.0                    | 13.5 \u00b1 0.6                          | 26.2               | 232.0             |\n| [YOLO11x-obb](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x-obb.pt) | 1024                  | 81.3               | 1408.6 \u00b1 7.7                   | 28.6 \u00b1 1.0                          | 58.8               | 520.2             |\n\n- **mAP<sup>test</sup>** values are for single-model multiscale performance on the [DOTAv1 test set](https://captain-whu.github.io/DOTA/dataset.html). <br>Reproduce by `yolo val obb data=DOTAv1.yaml device=0 split=test` and submit merged results to the [DOTA evaluation server](https://captain-whu.github.io/DOTA/evaluation.html).\n- **Speed** metrics are averaged over [DOTAv1 val images](https://docs.ultralytics.com/datasets/obb/dota-v2/#dota-v10) using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance. CPU speeds measured with [ONNX](https://onnx.ai/) export. GPU speeds measured with [TensorRT](https://developer.nvidia.com/tensorrt) export. <br>Reproduce by `yolo val obb data=DOTAv1.yaml batch=1 device=0|cpu`\n\n</details>\n\n## \ud83e\udde9 Integrations\n\nOur key integrations with leading AI platforms extend the functionality of Ultralytics' offerings, enhancing tasks like dataset labeling, training, visualization, and model management. Discover how Ultralytics, in collaboration with partners like [Weights & Biases](https://docs.ultralytics.com/integrations/weights-biases/), [Comet ML](https://docs.ultralytics.com/integrations/comet/), [Roboflow](https://docs.ultralytics.com/integrations/roboflow/), and [Intel OpenVINO](https://docs.ultralytics.com/integrations/openvino/), can optimize your AI workflow. Explore more at [Ultralytics Integrations](https://docs.ultralytics.com/integrations/).\n\n<a href=\"https://docs.ultralytics.com/integrations/\" target=\"_blank\">\n    <img width=\"100%\" src=\"https://github.com/ultralytics/assets/raw/main/yolov8/banner-integrations.png\" alt=\"Ultralytics active learning integrations\">\n</a>\n<br>\n<br>\n\n<div align=\"center\">\n  <a href=\"https://www.ultralytics.com/hub\">\n    <img src=\"https://github.com/ultralytics/assets/raw/main/partners/logo-ultralytics-hub.png\" width=\"10%\" alt=\"Ultralytics HUB logo\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"15%\" height=\"0\" alt=\"space\">\n  <a href=\"https://docs.ultralytics.com/integrations/weights-biases/\">\n    <img src=\"https://github.com/ultralytics/assets/raw/main/partners/logo-wb.png\" width=\"10%\" alt=\"Weights & Biases logo\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"15%\" height=\"0\" alt=\"space\">\n  <a href=\"https://docs.ultralytics.com/integrations/comet/\">\n    <img src=\"https://github.com/ultralytics/assets/raw/main/partners/logo-comet.png\" width=\"10%\" alt=\"Comet ML logo\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"15%\" height=\"0\" alt=\"space\">\n  <a href=\"https://docs.ultralytics.com/integrations/neural-magic/\">\n    <img src=\"https://github.com/ultralytics/assets/raw/main/partners/logo-neuralmagic.png\" width=\"10%\" alt=\"Neural Magic logo\"></a>\n</div>\n\n|                                                       Ultralytics HUB \ud83c\udf1f                                                        |                                                          Weights & Biases                                                           |                                                                              Comet                                                                              |                                                        Neural Magic                                                         |\n| :-----------------------------------------------------------------------------------------------------------------------------: | :---------------------------------------------------------------------------------------------------------------------------------: | :-------------------------------------------------------------------------------------------------------------------------------------------------------------: | :-------------------------------------------------------------------------------------------------------------------------: |\n| Streamline YOLO workflows: Label, train, and deploy effortlessly with [Ultralytics HUB](https://hub.ultralytics.com/). Try now! | Track experiments, hyperparameters, and results with [Weights & Biases](https://docs.ultralytics.com/integrations/weights-biases/). | Free forever, [Comet ML](https://docs.ultralytics.com/integrations/comet/) lets you save YOLO models, resume training, and interactively visualize predictions. | Run YOLO inference up to 6x faster with [Neural Magic DeepSparse](https://docs.ultralytics.com/integrations/neural-magic/). |\n\n## \ud83c\udf1f Ultralytics HUB\n\nExperience seamless AI with [Ultralytics HUB](https://hub.ultralytics.com/), the all-in-one platform for data visualization, training YOLO models, and deployment\u2014no coding required. Transform images into actionable insights and bring your AI visions to life effortlessly using our cutting-edge platform and user-friendly [Ultralytics App](https://www.ultralytics.com/app-install). Start your journey for **Free** today!\n\n<a href=\"https://www.ultralytics.com/hub\" target=\"_blank\">\n<img width=\"100%\" src=\"https://github.com/ultralytics/assets/raw/main/im/ultralytics-hub.png\" alt=\"Ultralytics HUB preview image\"></a>\n\n## \ud83e\udd1d Contribute\n\nWe thrive on community collaboration! Ultralytics YOLO wouldn't be the SOTA framework it is without contributions from developers like you. Please see our [Contributing Guide](https://docs.ultralytics.com/help/contributing/) to get started. We also welcome your feedback\u2014share your experience by completing our [Survey](https://www.ultralytics.com/survey?utm_source=github&utm_medium=social&utm_campaign=Survey). A huge **Thank You** \ud83d\ude4f to everyone who contributes!\n\n<!-- SVG image from https://opencollective.com/ultralytics/contributors.svg?width=1280 -->\n\n[![Ultralytics open-source contributors](https://raw.githubusercontent.com/ultralytics/assets/main/im/image-contributors.png)](https://github.com/ultralytics/ultralytics/graphs/contributors)\n\nWe look forward to your contributions to help make the Ultralytics ecosystem even better!\n\n## \ud83d\udcdc License\n\nUltralytics offers two licensing options to suit different needs:\n\n- **AGPL-3.0 License**: This [OSI-approved](https://opensource.org/license) open-source license is perfect for students, researchers, and enthusiasts. It encourages open collaboration and knowledge sharing. See the [LICENSE](https://github.com/ultralytics/ultralytics/blob/main/LICENSE) file for full details.\n- **Ultralytics Enterprise License**: Designed for commercial use, this license allows for the seamless integration of Ultralytics software and AI models into commercial products and services, bypassing the open-source requirements of AGPL-3.0. If your use case involves commercial deployment, please contact us via [Ultralytics Licensing](https://www.ultralytics.com/license).\n\n## \ud83d\udcde Contact\n\nFor bug reports and feature requests related to Ultralytics software, please visit [GitHub Issues](https://github.com/ultralytics/ultralytics/issues). For questions, discussions, and community support, join our active communities on [Discord](https://discord.com/invite/ultralytics), [Reddit](https://www.reddit.com/r/ultralytics/), and the [Ultralytics Community Forums](https://community.ultralytics.com/). We're here to help with all things Ultralytics!\n\n<br>\n<div align=\"center\">\n  <a href=\"https://github.com/ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png\" width=\"3%\" alt=\"Ultralytics GitHub\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"space\">\n  <a href=\"https://www.linkedin.com/company/ultralytics/\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png\" width=\"3%\" alt=\"Ultralytics LinkedIn\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"space\">\n  <a href=\"https://twitter.com/ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png\" width=\"3%\" alt=\"Ultralytics Twitter\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"space\">\n  <a href=\"https://youtube.com/ultralytics?sub_confirmation=1\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png\" width=\"3%\" alt=\"Ultralytics YouTube\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"space\">\n  <a href=\"https://www.tiktok.com/@ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png\" width=\"3%\" alt=\"Ultralytics TikTok\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"space\">\n  <a href=\"https://ultralytics.com/bilibili\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-bilibili.png\" width=\"3%\" alt=\"Ultralytics BiliBili\"></a>\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"space\">\n  <a href=\"https://discord.com/invite/ultralytics\"><img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-discord.png\" width=\"3%\" alt=\"Ultralytics Discord\"></a>\n</div>\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 33884891,
    "name": "airflow",
    "full_name": "apache/airflow",
    "description": "Apache Airflow - A platform to programmatically author, schedule, and monitor workflows",
    "html_url": "https://github.com/apache/airflow",
    "clone_url": "https://github.com/apache/airflow.git",
    "owner_login": "apache",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/47359?v=4",
    "stargazers_count": 41399,
    "watchers_count": 41399,
    "forks_count": 15413,
    "open_issues_count": 1531,
    "size": 424606,
    "language": "Python",
    "languages": {
      "Python": 42915318,
      "TypeScript": 2208987,
      "JavaScript": 329955,
      "Shell": 232931,
      "Dockerfile": 119831,
      "Jinja": 76265,
      "Go": 63158,
      "HTML": 43330,
      "CSS": 15733,
      "Jupyter Notebook": 7288,
      "HCL": 3786,
      "Mako": 2684,
      "Java": 1443
    },
    "topics": [
      "airflow",
      "apache",
      "apache-airflow",
      "automation",
      "dag",
      "data-engineering",
      "data-integration",
      "data-orchestrator",
      "data-pipelines",
      "data-science",
      "elt",
      "etl",
      "machine-learning",
      "mlops",
      "orchestration",
      "python",
      "scheduler",
      "workflow",
      "workflow-engine",
      "workflow-orchestration"
    ],
    "license_name": "Apache License 2.0",
    "created_at": "2015-04-13T18:04:58+00:00",
    "updated_at": "2025-08-06T01:54:13+00:00",
    "pushed_at": "2025-08-06T01:54:08+00:00",
    "contributors_count": 100,
    "readme_length": 37751,
    "readme_content": "<!--\n Licensed to the Apache Software Foundation (ASF) under one\n or more contributor license agreements.  See the NOTICE file\n distributed with this work for additional information\n regarding copyright ownership.  The ASF licenses this file\n to you under the Apache License, Version 2.0 (the\n \"License\"); you may not use this file except in compliance\n with the License.  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing,\n software distributed under the License is distributed on an\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n KIND, either express or implied.  See the License for the\n specific language governing permissions and limitations\n under the License.\n-->\n\n<!-- START Apache Airflow, please keep comment here to allow auto update of PyPI readme.md -->\n# Apache Airflow\n\n| Badges     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n|------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| License    | [![License](https://img.shields.io/:license-Apache%202-blue.svg)](https://www.apache.org/licenses/LICENSE-2.0.txt)                                                                                                                                                                                                                                                                                                                                               |\n| PyPI       | [![PyPI version](https://badge.fury.io/py/apache-airflow.svg)](https://badge.fury.io/py/apache-airflow) [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/apache-airflow.svg)](https://pypi.org/project/apache-airflow/) [![PyPI - Downloads](https://img.shields.io/pypi/dm/apache-airflow)](https://pypi.org/project/apache-airflow/)                                                                                                           |\n| Containers | [![Docker Pulls](https://img.shields.io/docker/pulls/apache/airflow.svg)](https://hub.docker.com/r/apache/airflow) [![Docker Stars](https://img.shields.io/docker/stars/apache/airflow.svg)](https://hub.docker.com/r/apache/airflow) [![Artifact HUB](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/apache-airflow)](https://artifacthub.io/packages/search?repo=apache-airflow)                                                  |\n| Community  | [![Contributors](https://img.shields.io/github/contributors/apache/airflow)](https://github.com/apache/airflow/graphs/contributors) [![Slack Status](https://img.shields.io/badge/slack-join_chat-white.svg?logo=slack&style=social)](https://s.apache.org/airflow-slack) ![Commit Activity](https://img.shields.io/github/commit-activity/m/apache/airflow) [![OSSRank](https://shields.io/endpoint?url=https://ossrank.com/shield/6)](https://ossrank.com/p/6) |\n\n\n\n| Version | Build Status                                                                                                                                                                                                                                                                                                            |\n|---------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Main    | [![GitHub Build main](https://github.com/apache/airflow/actions/workflows/ci-amd.yml/badge.svg)](https://github.com/apache/airflow/actions) [![GitHub Build main](https://github.com/apache/airflow/actions/workflows/ci-arm.yml/badge.svg)](https://github.com/apache/airflow/actions)                                 |\n| 3.x     | [![GitHub Build 3.0](https://github.com/apache/airflow/actions/workflows/ci-amd.yml/badge.svg?branch=v3-0-test)](https://github.com/apache/airflow/actions) [![GitHub Build 3.0](https://github.com/apache/airflow/actions/workflows/ci-arm.yml/badge.svg?branch=v3-0-test)](https://github.com/apache/airflow/actions) |\n| 2.x     | [![GitHub Build 2.11](https://github.com/apache/airflow/actions/workflows/ci.yml/badge.svg?branch=v2-11-test)](https://github.com/apache/airflow/actions)                                                                                                                                                               |\n\n\n\n<picture width=\"500\">\n  <img\n    src=\"https://github.com/apache/airflow/blob/19ebcac2395ef9a6b6ded3a2faa29dc960c1e635/docs/apache-airflow/img/logos/wordmark_1.png?raw=true\"\n    alt=\"Apache Airflow logo\"\n  />\n</picture>\n\n[Apache Airflow](https://airflow.apache.org/docs/apache-airflow/stable/) (or simply Airflow) is a platform to programmatically author, schedule, and monitor workflows.\n\nWhen workflows are defined as code, they become more maintainable, versionable, testable, and collaborative.\n\nUse Airflow to author workflows (Dags) that orchestrate tasks. The Airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.\n\n<!-- END Apache Airflow, please keep comment here to allow auto update of PyPI readme.md -->\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n**Table of contents**\n\n- [Project Focus](#project-focus)\n- [Principles](#principles)\n- [Requirements](#requirements)\n- [Getting started](#getting-started)\n- [Installing from PyPI](#installing-from-pypi)\n- [Installation](#installation)\n- [Official source code](#official-source-code)\n- [Convenience packages](#convenience-packages)\n- [User Interface](#user-interface)\n- [Semantic versioning](#semantic-versioning)\n- [Version Life Cycle](#version-life-cycle)\n- [Support for Python and Kubernetes versions](#support-for-python-and-kubernetes-versions)\n- [Base OS support for reference Airflow images](#base-os-support-for-reference-airflow-images)\n- [Approach to dependencies of Airflow](#approach-to-dependencies-of-airflow)\n- [Contributing](#contributing)\n- [Voting Policy](#voting-policy)\n- [Who uses Apache Airflow?](#who-uses-apache-airflow)\n- [Who maintains Apache Airflow?](#who-maintains-apache-airflow)\n- [What goes into the next release?](#what-goes-into-the-next-release)\n- [Can I use the Apache Airflow logo in my presentation?](#can-i-use-the-apache-airflow-logo-in-my-presentation)\n- [Links](#links)\n- [Sponsors](#sponsors)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n\n## Project Focus\n\nAirflow works best with workflows that are mostly static and slowly changing. When the DAG structure is similar from one run to the next, it clarifies the unit of work and continuity. Other similar projects include [Luigi](https://github.com/spotify/luigi), [Oozie](https://oozie.apache.org/) and [Azkaban](https://azkaban.github.io/).\n\nAirflow is commonly used to process data, but has the opinion that tasks should ideally be idempotent (i.e., results of the task will be the same, and will not create duplicated data in a destination system), and should not pass large quantities of data from one task to the next (though tasks can pass metadata using Airflow's [XCom feature](https://airflow.apache.org/docs/apache-airflow/stable/concepts/xcoms.html)). For high-volume, data-intensive tasks, a best practice is to delegate to external services specializing in that type of work.\n\nAirflow is not a streaming solution, but it is often used to process real-time data, pulling data off streams in batches.\n\n## Principles\n\n- **Dynamic**: Pipelines are defined in code, enabling dynamic dag generation and parameterization.\n- **Extensible**: The Airflow framework includes a wide range of built-in operators and can be extended to fit your needs.\n- **Flexible**: Airflow leverages the [**Jinja**](https://jinja.palletsprojects.com) templating engine, allowing rich customizations.\n\n<!-- START Requirements, please keep comment here to allow auto update of PyPI readme.md -->\n## Requirements\n\nApache Airflow is tested with:\n\n|            | Main version (dev)     | Stable version (3.0.3) |\n|------------|------------------------|------------------------|\n| Python     | 3.10, 3.11, 3.12, 3.13 | 3.9, 3.10, 3.11, 3.12  |\n| Platform   | AMD64/ARM64(\\*)        | AMD64/ARM64(\\*)        |\n| Kubernetes | 1.30, 1.31, 1.32, 1.33 | 1.30, 1.31, 1.32, 1.33 |\n| PostgreSQL | 13, 14, 15, 16, 17     | 13, 14, 15, 16, 17     |\n| MySQL      | 8.0, 8.4, Innovation   | 8.0, 8.4, Innovation   |\n| SQLite     | 3.15.0+                | 3.15.0+                |\n\n\\* Experimental\n\n**Note**: MariaDB is not tested/recommended.\n\n**Note**: SQLite is used in Airflow tests. Do not use it in production. We recommend\nusing the latest stable version of SQLite for local development.\n\n**Note**: Airflow currently can be run on POSIX-compliant Operating Systems. For development, it is regularly\ntested on fairly modern Linux Distros and recent versions of macOS.\nOn Windows you can run it via WSL2 (Windows Subsystem for Linux 2) or via Linux Containers.\nThe work to add Windows support is tracked via [#10388](https://github.com/apache/airflow/issues/10388), but\nit is not a high priority. You should only use Linux-based distros as \"Production\" execution environment\nas this is the only environment that is supported. The only distro that is used in our CI tests and that\nis used in the [Community managed DockerHub image](https://hub.docker.com/p/apache/airflow) is\n`Debian Bookworm`.\n\n<!-- END Requirements, please keep comment here to allow auto update of PyPI readme.md -->\n<!-- START Getting started, please keep comment here to allow auto update of PyPI readme.md -->\n## Getting started\n\nVisit the official Airflow website documentation (latest **stable** release) for help with\n[installing Airflow](https://airflow.apache.org/docs/apache-airflow/stable/installation/),\n[getting started](https://airflow.apache.org/docs/apache-airflow/stable/start.html), or walking\nthrough a more complete [tutorial](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/).\n\n> Note: If you're looking for documentation for the main branch (latest development branch): you can find it on [s.apache.org/airflow-docs](https://s.apache.org/airflow-docs/).\n\nFor more information on Airflow Improvement Proposals (AIPs), visit\nthe [Airflow Wiki](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals).\n\nDocumentation for dependent projects like provider distributions, Docker image, Helm Chart, you'll find it in [the documentation index](https://airflow.apache.org/docs/).\n\n<!-- END Getting started, please keep comment here to allow auto update of PyPI readme.md -->\n<!-- START Installing from PyPI, please keep comment here to allow auto update of PyPI readme.md -->\n\n## Installing from PyPI\n\nWe publish Apache Airflow as `apache-airflow` package in PyPI. Installing it however might be sometimes tricky\nbecause Airflow is a bit of both a library and application. Libraries usually keep their dependencies open, and\napplications usually pin them, but we should do neither and both simultaneously. We decided to keep\nour dependencies as open as possible (in `pyproject.toml`) so users can install different versions of libraries\nif needed. This means that `pip install apache-airflow` will not work from time to time or will\nproduce unusable Airflow installation.\n\nTo have repeatable installation, however, we keep a set of \"known-to-be-working\" constraint\nfiles in the orphan `constraints-main` and `constraints-2-0` branches. We keep those \"known-to-be-working\"\nconstraints files separately per major/minor Python version.\nYou can use them as constraint files when installing Airflow from PyPI. Note that you have to specify\ncorrect Airflow tag/version/branch and Python versions in the URL.\n\n1. Installing just Airflow:\n\n> Note: Only `pip` installation is currently officially supported.\n\nWhile it is possible to install Airflow with tools like [Poetry](https://python-poetry.org) or\n[pip-tools](https://pypi.org/project/pip-tools), they do not share the same workflow as\n`pip` - especially when it comes to constraint vs. requirements management.\nInstalling via `Poetry` or `pip-tools` is not currently supported.\n\nThere are known issues with ``bazel`` that might lead to circular dependencies when using it to install\nAirflow. Please switch to ``pip`` if you encounter such problems. ``Bazel`` community works on fixing\nthe problem in `this PR <https://github.com/bazelbuild/rules_python/pull/1166>`_ so it might be that\nnewer versions of ``bazel`` will handle it.\n\nIf you wish to install Airflow using those tools, you should use the constraint files and convert\nthem to the appropriate format and workflow that your tool requires.\n\n\n```bash\npip install 'apache-airflow==3.0.3' \\\n --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-3.0.3/constraints-3.10.txt\"\n```\n\n2. Installing with extras (i.e., postgres, google)\n\n```bash\npip install 'apache-airflow[postgres,google]==3.0.3' \\\n --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-3.0.3/constraints-3.10.txt\"\n```\n\nFor information on installing provider distributions, check\n[providers](http://airflow.apache.org/docs/apache-airflow-providers/index.html).\n\n<!-- END Installing from PyPI, please keep comment here to allow auto update of PyPI readme.md -->\n\n## Installation\n\nFor comprehensive instructions on setting up your local development environment and installing Apache Airflow, please refer to the [INSTALLING.md](INSTALLING.md) file.\n\n<!-- START Official source code, please keep comment here to allow auto update of PyPI readme.md -->\n## Official source code\n\nApache Airflow is an [Apache Software Foundation](https://www.apache.org) (ASF) project,\nand our official source code releases:\n\n- Follow the [ASF Release Policy](https://www.apache.org/legal/release-policy.html)\n- Can be downloaded from [the ASF Distribution Directory](https://downloads.apache.org/airflow)\n- Are cryptographically signed by the release manager\n- Are officially voted on by the PMC members during the\n  [Release Approval Process](https://www.apache.org/legal/release-policy.html#release-approval)\n\nFollowing the ASF rules, the source packages released must be sufficient for a user to build and test the\nrelease provided they have access to the appropriate platform and tools.\n\n<!-- END Official source code, please keep comment here to allow auto update of PyPI readme.md -->\n## Convenience packages\n\nThere are other ways of installing and using Airflow. Those are \"convenience\" methods - they are\nnot \"official releases\" as stated by the `ASF Release Policy`, but they can be used by the users\nwho do not want to build the software themselves.\n\nThose are - in the order of most common ways people install Airflow:\n\n- [PyPI releases](https://pypi.org/project/apache-airflow/) to install Airflow using standard `pip` tool\n- [Docker Images](https://hub.docker.com/r/apache/airflow) to install airflow via\n  `docker` tool, use them in Kubernetes, Helm Charts, `docker-compose`, `docker swarm`, etc. You can\n  read more about using, customizing, and extending the images in the\n  [Latest docs](https://airflow.apache.org/docs/docker-stack/index.html), and\n  learn details on the internals in the [images](https://airflow.apache.org/docs/docker-stack/index.html) document.\n- [Tags in GitHub](https://github.com/apache/airflow/tags) to retrieve the git project sources that\n  were used to generate official source packages via git\n\nAll those artifacts are not official releases, but they are prepared using officially released sources.\nSome of those artifacts are \"development\" or \"pre-release\" ones, and they are clearly marked as such\nfollowing the ASF Policy.\n\n## User Interface\n\n- **DAGs**: Overview of all DAGs in your environment.\n\n  ![DAGs](https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/dags.png)\n\n- **Assets**: Overview of Assets with dependencies.\n\n  ![Asset Dependencies](https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/assets_graph.png)\n\n- **Grid**: Grid representation of a DAG that spans across time.\n\n  ![Grid](https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/grid.png)\n\n- **Graph**: Visualization of a DAG's dependencies and their current status for a specific run.\n\n  ![Graph](https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/graph.png)\n\n- **Home**: Summary statistics of your Airflow environment.\n\n  ![Home](https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/home.png)\n\n- **Backfill**: Backfilling a DAG for a specific date range.\n\n  ![Backfill](https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/backfill.png)\n\n- **Code**: Quick way to view source code of a DAG.\n\n  ![Code](https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/code.png)\n\n## Semantic versioning\n\nAs of Airflow 2.0.0, we support a strict [SemVer](https://semver.org/) approach for all packages released.\n\nThere are few specific rules that we agreed to that define details of versioning of the different\npackages:\n\n* **Airflow**: SemVer rules apply to core airflow only (excludes any changes to providers).\n  Changing limits for versions of Airflow dependencies is not a breaking change on its own.\n* **Airflow Providers**: SemVer rules apply to changes in the particular provider's code only.\n  SemVer MAJOR and MINOR versions for the packages are independent of the Airflow version.\n  For example, `google 4.1.0` and `amazon 3.0.3` providers can happily be installed\n  with `Airflow 2.1.2`. If there are limits of cross-dependencies between providers and Airflow packages,\n  they are present in providers as `install_requires` limitations. We aim to keep backwards\n  compatibility of providers with all previously released Airflow 2 versions but\n  there will sometimes be breaking changes that might make some, or all\n  providers, have minimum Airflow version specified.\n* **Airflow Helm Chart**: SemVer rules apply to changes in the chart only. SemVer MAJOR and MINOR\n  versions for the chart are independent of the Airflow version. We aim to keep backwards\n  compatibility of the Helm Chart with all released Airflow 2 versions, but some new features might\n  only work starting from specific Airflow releases. We might however limit the Helm\n  Chart to depend on minimal Airflow version.\n* **Airflow API clients**: Their versioning is independent from Airflow versions. They follow their own\n  SemVer rules for breaking changes and new features - which for example allows to change the way we generate\n  the clients.\n\n## Version Life Cycle\n\nApache Airflow version life cycle:\n\n<!-- This table is automatically updated by pre-commit scripts/ci/pre_commit/supported_versions.py -->\n<!-- Beginning of auto-generated table -->\n\n| Version   | Current Patch/Minor   | State     | First Release   | Limited Maintenance   | EOL/Terminated   |\n|-----------|-----------------------|-----------|-----------------|-----------------------|------------------|\n| 3         | 3.0.3                 | Supported | Apr 22, 2025    | TBD                   | TBD              |\n| 2         | 2.11.0                | Supported | Dec 17, 2020    | Oct 22, 2025          | Apr 22, 2026     |\n| 1.10      | 1.10.15               | EOL       | Aug 27, 2018    | Dec 17, 2020          | June 17, 2021    |\n| 1.9       | 1.9.0                 | EOL       | Jan 03, 2018    | Aug 27, 2018          | Aug 27, 2018     |\n| 1.8       | 1.8.2                 | EOL       | Mar 19, 2017    | Jan 03, 2018          | Jan 03, 2018     |\n| 1.7       | 1.7.1.2               | EOL       | Mar 28, 2016    | Mar 19, 2017          | Mar 19, 2017     |\n\n<!-- End of auto-generated table -->\n\nLimited support versions will be supported with security and critical bug fix only.\nEOL versions will not get any fixes nor support.\nWe always recommend that all users run the latest available minor release for whatever major version is in use.\nWe **highly** recommend upgrading to the latest Airflow major release at the earliest convenient time and before the EOL date.\n\n## Support for Python and Kubernetes versions\n\nAs of Airflow 2.0, we agreed to certain rules we follow for Python and Kubernetes support.\nThey are based on the official release schedule of Python and Kubernetes, nicely summarized in the\n[Python Developer's Guide](https://devguide.python.org/#status-of-python-branches) and\n[Kubernetes version skew policy](https://kubernetes.io/docs/setup/release/version-skew-policy/).\n\n1. We drop support for Python and Kubernetes versions when they reach EOL. Except for Kubernetes, a\n   version stays supported by Airflow if two major cloud providers still provide support for it. We drop\n   support for those EOL versions in main right after EOL date, and it is effectively removed when we release\n   the first new MINOR (Or MAJOR if there is no new MINOR version) of Airflow. For example, for Python 3.10 it\n   means that we will drop support in main right after 27.06.2023, and the first MAJOR or MINOR version of\n   Airflow released after will not have it.\n\n2. We support a new version of Python/Kubernetes in main after they are officially released, as soon as we\n   make them work in our CI pipeline (which might not be immediate due to dependencies catching up with\n   new versions of Python mostly) we release new images/support in Airflow based on the working CI setup.\n\n3. This policy is best-effort which means there may be situations where we might terminate support earlier\n   if circumstances require it.\n\n## Base OS support for reference Airflow images\n\nThe Airflow Community provides conveniently packaged container images that are published whenever\nwe publish an Apache Airflow release. Those images contain:\n\n* Base OS with necessary packages to install Airflow (stable Debian OS)\n* Base Python installation in versions supported at the time of release for the MINOR version of\n  Airflow released (so there could be different versions for 2.3 and 2.2 line for example)\n* Libraries required to connect to supported Databases (again the set of databases supported depends\n  on the MINOR version of Airflow)\n* Predefined set of popular providers (for details see the [Dockerfile](https://raw.githubusercontent.com/apache/airflow/main/Dockerfile)).\n* Possibility of building your own, custom image where the user can choose their own set of providers\n  and libraries (see [Building the image](https://airflow.apache.org/docs/docker-stack/build.html))\n* In the future Airflow might also support a \"slim\" version without providers nor database clients installed\n\nThe version of the base OS image is the stable version of Debian. Airflow supports using all currently active\nstable versions - as soon as all Airflow dependencies support building, and we set up the CI pipeline for\nbuilding and testing the OS version. Approximately 6 months before the end-of-regular support of a\nprevious stable version of the OS, Airflow switches the images released to use the latest supported\nversion of the OS.\n\nFor example switch from ``Debian Bullseye`` to ``Debian Bookworm`` has been implemented\nbefore 2.8.0 release in October 2023 and ``Debian Bookworm`` will be the only option supported as of\nAirflow 2.10.0.\n\nUsers will continue to be able to build their images using stable Debian releases until the end of regular\nsupport and building and verifying of the images happens in our CI but no unit tests were executed using\nthis image in the `main` branch.\n\n## Approach to dependencies of Airflow\n\nAirflow has a lot of dependencies - direct and transitive, also Airflow is both - library and application,\ntherefore our policies to dependencies has to include both - stability of installation of application,\nbut also ability to install newer version of dependencies for those users who develop DAGs. We developed\nthe approach where `constraints` are used to make sure airflow can be installed in a repeatable way, while\nwe do not limit our users to upgrade most of the dependencies. As a result we decided not to upper-bound\nversion of Airflow dependencies by default, unless we have good reasons to believe upper-bounding them is\nneeded because of importance of the dependency as well as risk it involves to upgrade specific dependency.\nWe also upper-bound the dependencies that we know cause problems.\n\nThe constraint mechanism of ours takes care about finding and upgrading all the non-upper bound dependencies\nautomatically (providing that all the tests pass). Our `main` build failures will indicate in case there\nare versions of dependencies that break our tests - indicating that we should either upper-bind them or\nthat we should fix our code/tests to account for the upstream changes from those dependencies.\n\nWhenever we upper-bound such a dependency, we should always comment why we are doing it - i.e. we should have\na good reason why dependency is upper-bound. And we should also mention what is the condition to remove the\nbinding.\n\n### Approach for dependencies for Airflow Core\n\nThose dependencies are maintained in ``pyproject.toml``.\n\nThere are few dependencies that we decided are important enough to upper-bound them by default, as they are\nknown to follow predictable versioning scheme, and we know that new versions of those are very likely to\nbring breaking changes. We commit to regularly review and attempt to upgrade to the newer versions of\nthe dependencies as they are released, but this is manual process.\n\nThe important dependencies are:\n\n* `SQLAlchemy`: upper-bound to specific MINOR version (SQLAlchemy is known to remove deprecations and\n   introduce breaking changes especially that support for different Databases varies and changes at\n   various speed)\n* `Alembic`: it is important to handle our migrations in predictable and performant way. It is developed\n   together with SQLAlchemy. Our experience with Alembic is that it very stable in MINOR version\n* `Flask`: We are using Flask as the back-bone of our web UI and API. We know major version of Flask\n   are very likely to introduce breaking changes across those so limiting it to MAJOR version makes sense\n* `werkzeug`: the library is known to cause problems in new versions. It is tightly coupled with Flask\n   libraries, and we should update them together\n* `celery`: Celery is a crucial component of Airflow as it used for CeleryExecutor (and similar). Celery\n   [follows SemVer](https://docs.celeryq.dev/en/stable/contributing.html?highlight=semver#versions), so\n   we should upper-bound it to the next MAJOR version. Also, when we bump the upper version of the library,\n   we should make sure Celery Provider minimum Airflow version is updated.\n* `kubernetes`: Kubernetes is a crucial component of Airflow as it is used for the KubernetesExecutor\n   (and similar). Kubernetes Python library [follows SemVer](https://github.com/kubernetes-client/python#compatibility),\n   so we should upper-bound it to the next MAJOR version. Also, when we bump the upper version of the library,\n   we should make sure Kubernetes Provider minimum Airflow version is updated.\n\n### Approach for dependencies in Airflow Providers and extras\n\nThe main part of the Airflow is the Airflow Core, but the power of Airflow also comes from a number of\nproviders that extend the core functionality and are released separately, even if we keep them (for now)\nin the same monorepo for convenience. You can read more about the providers in the\n[Providers documentation](https://airflow.apache.org/docs/apache-airflow-providers/index.html). We also\nhave set of policies implemented for maintaining and releasing community-managed providers as well\nas the approach for community vs. 3rd party providers in the [providers](https://github.com/apache/airflow/blob/main/PROVIDERS.rst) document.\n\nThose `extras` and `providers` dependencies are maintained in `provider.yaml` of each provider.\n\nBy default, we should not upper-bound dependencies for providers, however each provider's maintainer\nmight decide to add additional limits (and justify them with comment).\n\n<!-- START Contributing, please keep comment here to allow auto update of PyPI readme.md -->\n\n## Contributing\n\nWant to help build Apache Airflow? Check out our [contributors' guide](https://github.com/apache/airflow/blob/main/contributing-docs/README.rst) for a comprehensive overview of how to contribute, including setup instructions, coding standards, and pull request guidelines.\n\nIf you can't wait to contribute, and want to get started asap, check out the [contribution quickstart](https://github.com/apache/airflow/blob/main/contributing-docs/03_contributors_quick_start.rst) here!\n\nOfficial Docker (container) images for Apache Airflow are described in [images](https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/02_images.md).\n\n<!-- END Contributing, please keep comment here to allow auto update of PyPI readme.md -->\n<!-- START Who uses Apache Airflow, please keep comment here to allow auto update of PyPI readme.md -->\n\n## Voting Policy\n\n* Commits need a +1 vote from a committer who is not the author\n* When we do AIP voting, both PMC member's and committer's `+1s` are considered a binding vote.\n\n## Who uses Apache Airflow?\n\nWe know about around 500 organizations that are using Apache Airflow (but there are likely many more)\n[in the wild](https://github.com/apache/airflow/blob/main/INTHEWILD.md).\n\nIf you use Airflow - feel free to make a PR to add your organisation to the list.\n\n<!-- END Who uses Apache Airflow, please keep comment here to allow auto update of PyPI readme.md -->\n<!-- START Who maintains Apache Airflow, please keep comment here to allow auto update of PyPI readme.md -->\n\n## Who maintains Apache Airflow?\n\nAirflow is the work of the [community](https://github.com/apache/airflow/graphs/contributors),\nbut the [core committers/maintainers](https://people.apache.org/committers-by-project.html#airflow)\nare responsible for reviewing and merging PRs as well as steering conversations around new feature requests.\nIf you would like to become a maintainer, please review the Apache Airflow\n[committer requirements](https://github.com/apache/airflow/blob/main/COMMITTERS.rst#guidelines-to-become-an-airflow-committer).\n\n<!-- END Who maintains Apache Airflow, please keep comment here to allow auto update of PyPI readme.md -->\n\n## What goes into the next release?\n\nOften you will see an issue that is assigned to specific milestone with Airflow version, or a PR that gets merged\nto the main branch and you might wonder which release the merged PR(s) will be released in or which release the fixed\nissues will be in. The answer to this is as usual - it depends on various scenarios. The answer is different for PRs and Issues.\n\nTo add a bit of context, we are following the [Semver](https://semver.org/) versioning scheme as described in\n[Airflow release process](https://airflow.apache.org/docs/apache-airflow/stable/release-process.html). More\ndetails are explained in detail in this README under the [Semantic versioning](#semantic-versioning) chapter, but\nin short, we have `MAJOR.MINOR.PATCH` versions of Airflow.\n\n* `MAJOR` version is incremented in case of breaking changes\n* `MINOR` version is incremented when there are new features added\n* `PATCH` version is incremented when there are only bug-fixes and doc-only changes\n\nGenerally we release `MINOR` versions of Airflow from a branch that is named after the MINOR version. For example\n`2.7.*` releases are released from `v2-7-stable` branch, `2.8.*` releases are released from `v2-8-stable`\nbranch, etc.\n\n1. Most of the time in our release cycle, when the branch for next `MINOR` branch is not yet created, all\nPRs merged to `main` (unless they get reverted), will find their way to the next `MINOR` release. For example\nif the last release is `2.7.3` and `v2-8-stable` branch is not created yet, the next `MINOR` release\nis `2.8.0` and all PRs merged to main will be released in `2.8.0`. However, some PRs (bug-fixes and\ndoc-only changes) when merged, can be cherry-picked to current `MINOR` branch and released in the\nnext `PATCHLEVEL` release. For example, if `2.8.1` is already released and we are working on `2.9.0dev`,  then\nmarking a PR with `2.8.2` milestone means that it will be cherry-picked to `v2-8-test` branch and\nreleased in `2.8.2rc1`, and eventually in `2.8.2`.\n\n2. When we prepare for the next `MINOR` release, we cut new `v2-*-test` and `v2-*-stable` branch\nand prepare `alpha`, `beta` releases for the next `MINOR` version, the PRs merged to main will still be\nreleased in the next `MINOR` release until `rc` version is cut. This is happening because the `v2-*-test`\nand `v2-*-stable` branches are rebased on top of main when next `beta` and `rc` releases are prepared.\nFor example, when we cut `2.10.0beta1` version, anything merged to main before `2.10.0rc1` is released,\nwill find its way to 2.10.0rc1.\n\n3. Then, once we prepare the first RC candidate for the MINOR release, we stop moving the `v2-*-test` and\n`v2-*-stable` branches and the PRs merged to main will be released in the next `MINOR` release.\nHowever, some PRs (bug-fixes and doc-only changes) when merged, can be cherry-picked to current `MINOR`\nbranch and released in the next `PATCHLEVEL` release - for example when the last released version from `v2-10-stable`\nbranch is `2.10.0rc1`, some of the PRs from main can be marked as `2.10.0` milestone by committers,\nthe release manager will try to cherry-pick them into the release branch.\nIf successful, they will be released in `2.10.0rc2` and subsequently in `2.10.0`. This also applies to\nsubsequent `PATCHLEVEL` versions. When for example `2.10.1` is already released, marking a PR with\n`2.10.2` milestone will mean that it will be cherry-picked to `v2-10-stable` branch and released in `2.10.2rc1`\nand eventually in `2.10.2`.\n\nThe final decision about cherry-picking is made by the release manager.\n\nMarking issues with a milestone is a bit different. Maintainers do not mark issues with a milestone usually,\nnormally they are only marked in PRs. If PR linked to the issue (and \"fixing it\") gets merged and released\nin a specific version following the process described above, the issue will be automatically closed, no\nmilestone will be set for the issue, you need to check the PR that fixed the issue to see which version\nit was released in.\n\nHowever, sometimes maintainers mark issues with specific milestone, which means that the\nissue is important to become a candidate to take a look when the release is being prepared. Since this is an\nOpen-Source project, where basically all contributors volunteer their time, there is no guarantee that specific\nissue will be fixed in specific version. We do not want to hold the release because some issue is not fixed,\nso in such case release manager will reassign such unfixed issues to the next milestone in case they are not\nfixed in time for the current release. Therefore, the milestone for issue is more of an intent that it should be\nlooked at, than promise it will be fixed in the version.\n\nMore context and **FAQ** about the patchlevel release can be found in the\n[What goes into the next release](dev/WHAT_GOES_INTO_THE_NEXT_RELEASE.md) document in the `dev` folder of the\nrepository.\n\n## Can I use the Apache Airflow logo in my presentation?\n\nYes! Be sure to abide by the Apache Foundation [trademark policies](https://www.apache.org/foundation/marks/#books) and the Apache Airflow [Brandbook](https://cwiki.apache.org/confluence/display/AIRFLOW/Brandbook). The most up-to-date logos are found in [this repo](https://github.com/apache/airflow/tree/main/airflow-core/docs/img/logos/) and on the Apache Software Foundation [website](https://www.apache.org/logos/about.html).\n\n## Links\n\n- [Documentation](https://airflow.apache.org/docs/apache-airflow/stable/)\n- [Chat](https://s.apache.org/airflow-slack)\n- [Community Information](https://airflow.apache.org/community/)\n\n## Sponsors\n\nThe CI infrastructure for Apache Airflow has been sponsored by:\n\n<!-- Ordered by most recently \"funded\" -->\n\n<a href=\"https://astronomer.io\"><img src=\"https://assets2.astronomer.io/logos/logoForLIGHTbackground.png\" alt=\"astronomer.io\" width=\"250px\"></a>\n<a href=\"https://aws.amazon.com/opensource/\"><img src=\"https://github.com/apache/airflow/blob/main/providers/amazon/docs/integration-logos/AWS-Cloud-alt_light-bg@4x.png?raw=true\" alt=\"AWS OpenSource\" width=\"130px\"></a>\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 204086862,
    "name": "streamlit",
    "full_name": "streamlit/streamlit",
    "description": "Streamlit \u2014 A faster way to build and share data apps.",
    "html_url": "https://github.com/streamlit/streamlit",
    "clone_url": "https://github.com/streamlit/streamlit.git",
    "owner_login": "streamlit",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/45109972?v=4",
    "stargazers_count": 40774,
    "watchers_count": 40774,
    "forks_count": 3635,
    "open_issues_count": 1239,
    "size": 727262,
    "language": "Python",
    "languages": {
      "Python": 6535799,
      "TypeScript": 4437246,
      "JavaScript": 94504,
      "Makefile": 14164,
      "HTML": 9760,
      "SCSS": 3714,
      "Shell": 3688,
      "Dockerfile": 1121,
      "Batchfile": 676,
      "CSS": 146
    },
    "topics": [
      "data-analysis",
      "data-science",
      "data-visualization",
      "deep-learning",
      "developer-tools",
      "machine-learning",
      "python",
      "streamlit"
    ],
    "license_name": "Apache License 2.0",
    "created_at": "2019-08-24T00:14:52+00:00",
    "updated_at": "2025-08-06T01:44:22+00:00",
    "pushed_at": "2025-08-06T01:15:00+00:00",
    "contributors_count": 100,
    "readme_length": 6181,
    "readme_content": "<br>\n\n<img src=\"https://user-images.githubusercontent.com/7164864/217935870-c0bc60a3-6fc0-4047-b011-7b4c59488c91.png\" alt=\"Streamlit logo\" style=\"margin-top:50px\"></img>\n\n# Welcome to Streamlit \ud83d\udc4b\n\n**A faster way to build and share data apps.**\n\n## What is Streamlit?\n\nStreamlit lets you transform Python scripts into interactive web apps in minutes, instead of weeks. Build dashboards, generate reports, or create chat apps. Once you\u2019ve created an app, you can use our [Community Cloud platform](https://streamlit.io/cloud) to deploy, manage, and share your app.\n\n### Why choose Streamlit?\n\n- **Simple and Pythonic:** Write beautiful, easy-to-read code.\n- **Fast, interactive prototyping:** Let others interact with your data and provide feedback quickly.\n- **Live editing:** See your app update instantly as you edit your script.\n- **Open-source and free:** Join a vibrant community and contribute to Streamlit's future.\n\n## Installation\n\nOpen a terminal and run:\n\n```bash\n$ pip install streamlit\n$ streamlit hello\n```\n\nIf this opens our sweet _Streamlit Hello_ app in your browser, you're all set! If not, head over to [our docs](https://docs.streamlit.io/get-started) for specific installs.\n\nThe app features a bunch of examples of what you can do with Streamlit. Jump to the [quickstart](#quickstart) section to understand how that all works.\n\n<img src=\"https://user-images.githubusercontent.com/7164864/217936487-1017784e-68ec-4e0d-a7f6-6b97525ddf88.gif\" alt=\"Streamlit Hello\" width=500 href=\"none\"></img>\n\n## Quickstart\n\n### A little example\n\nCreate a new file named `streamlit_app.py` in your project directory with the following code:\n```python\nimport streamlit as st\nx = st.slider(\"Select a value\")\nst.write(x, \"squared is\", x * x)\n```\n\nNow run it to open the app!\n```\n$ streamlit run streamlit_app.py\n```\n\n<img src=\"https://user-images.githubusercontent.com/7164864/215172915-cf087c56-e7ae-449a-83a4-b5fa0328d954.gif\" width=300 alt=\"Little example\"></img>\n\n### Give me more!\n\nStreamlit comes in with [a ton of additional powerful elements](https://docs.streamlit.io/develop/api-reference) to spice up your data apps and delight your viewers. Some examples:\n\n<table border=\"0\">\n  <tr>\n    <td>\n      <a target=\"_blank\" href=\"https://docs.streamlit.io/develop/api-reference/widgets\">\n        <img src=\"https://user-images.githubusercontent.com/7164864/217936099-12c16f8c-7fe4-44b1-889a-1ac9ee6a1b44.png\" style=\"max-height:150px; width:auto; display:block;\">\n      </a>\n    </td>\n    <td>\n      <a target=\"_blank\" href=\"https://docs.streamlit.io/develop/api-reference/data/st.dataframe\">\n        <img src=\"https://user-images.githubusercontent.com/7164864/215110064-5eb4e294-8f30-4933-9563-0275230e52b5.gif\" style=\"max-height:150px; width:auto; display:block;\">\n      </a>\n    </td>\n    <td>\n      <a target=\"_blank\" href=\"https://docs.streamlit.io/develop/api-reference/charts\">\n        <img src=\"https://user-images.githubusercontent.com/7164864/215174472-bca8a0d7-cf4b-4268-9c3b-8c03dad50bcd.gif\" style=\"max-height:150px; width:auto; display:block;\">\n      </a>\n    </td>\n    <td>\n      <a target=\"_blank\" href=\"https://docs.streamlit.io/develop/api-reference/layout\">\n        <img src=\"https://user-images.githubusercontent.com/7164864/217936149-a35c35be-0d96-4c63-8c6a-1c4b52aa8f60.png\" style=\"max-height:150px; width:auto; display:block;\">\n      </a>\n    </td>\n    <td>\n      <a target=\"_blank\" href=\"https://docs.streamlit.io/develop/concepts/multipage-apps\">\n        <img src=\"https://user-images.githubusercontent.com/7164864/215173883-eae0de69-7c1d-4d78-97d0-3bc1ab865e5b.gif\" style=\"max-height:150px; width:auto; display:block;\">\n      </a>\n    </td>\n    <td>\n      <a target=\"_blank\" href=\"https://streamlit.io/gallery\">\n        <img src=\"https://user-images.githubusercontent.com/7164864/215109229-6ae9111f-e5c1-4f0b-b3a2-87a79268ccc9.gif\" style=\"max-height:150px; width:auto; display:block;\">\n      </a>\n    </td>\n  </tr>\n  <tr>\n    <td>Input widgets</td>\n    <td>Dataframes</td>\n    <td>Charts</td>\n    <td>Layout</td>\n    <td>Multi-page apps</td>\n    <td>Fun</td>\n  </tr>\n</table>\n\n\nOur vibrant creators community also extends Streamlit capabilities using \u00a0\ud83e\udde9 [Streamlit Components](https://streamlit.io/components).\n\n## Get inspired\n\nThere's so much you can build with Streamlit:\n- \ud83e\udd16\u00a0\u00a0[LLMs & chatbot apps](https://streamlit.io/gallery?category=llms)\n- \ud83e\uddec\u00a0\u00a0[Science & technology apps](https://streamlit.io/gallery?category=science-technology)\n- \ud83d\udcac\u00a0\u00a0[NLP & language apps](https://streamlit.io/gallery?category=nlp-language)\n- \ud83c\udfe6\u00a0\u00a0[Finance & business apps](https://streamlit.io/gallery?category=finance-business)\n- \ud83d\uddfa\u00a0\u00a0[Geography & society apps](https://streamlit.io/gallery?category=geography-society)\n- and more!\n\n**Check out [our gallery!](https://streamlit.io/gallery)** \ud83c\udf88\n\n## Community Cloud\n\nDeploy, manage and share your apps for free using our [Community Cloud](https://streamlit.io/cloud)! Sign-up [here](https://share.streamlit.io/signup). <br><br>\n<img src=\"https://user-images.githubusercontent.com/7164864/214965336-64500db3-0d79-4a20-8052-2dda883902d2.gif\" width=\"400\"></img>\n\n## Resources\n\n- Explore our [docs](https://docs.streamlit.io) to learn how Streamlit works.\n- Ask questions and get help in our [community forum](https://discuss.streamlit.io).\n- Read our [blog](https://blog.streamlit.io) for tips from developers and creators.\n- Extend Streamlit's capabilities by installing or creating your own [Streamlit Components](https://streamlit.io/components).\n- Help others find and play with your app by using the Streamlit GitHub badge in your repository:\n```markdown\n[![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](URL_TO_YOUR_APP)\n```\n[![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://share.streamlit.io/streamlit/roadmap)\n\n## Contribute\n\n\ud83c\udf89 Thanks for your interest in helping improve Streamlit! \ud83c\udf89\n\nBefore contributing, please read our guidelines here: https://github.com/streamlit/streamlit/wiki/Contributing\n\n## License\n\nStreamlit is completely free and open-source and licensed under the [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0) license.\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 235860204,
    "name": "DeepSpeed",
    "full_name": "deepspeedai/DeepSpeed",
    "description": "DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.",
    "html_url": "https://github.com/deepspeedai/DeepSpeed",
    "clone_url": "https://github.com/deepspeedai/DeepSpeed.git",
    "owner_login": "deepspeedai",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/74068820?v=4",
    "stargazers_count": 39626,
    "watchers_count": 39626,
    "forks_count": 4501,
    "open_issues_count": 1184,
    "size": 227260,
    "language": "Python",
    "languages": {
      "Python": 5697793,
      "C++": 1499931,
      "Cuda": 713389,
      "Shell": 28837,
      "C": 25866,
      "Dockerfile": 7942,
      "Makefile": 713,
      "Batchfile": 388
    },
    "topics": [
      "billion-parameters",
      "compression",
      "data-parallelism",
      "deep-learning",
      "gpu",
      "inference",
      "machine-learning",
      "mixture-of-experts",
      "model-parallelism",
      "pipeline-parallelism",
      "pytorch",
      "trillion-parameters",
      "zero"
    ],
    "license_name": "Apache License 2.0",
    "created_at": "2020-01-23T18:35:18+00:00",
    "updated_at": "2025-08-06T02:12:12+00:00",
    "pushed_at": "2025-08-04T18:36:40+00:00",
    "contributors_count": 100,
    "readme_length": 36470,
    "readme_content": "[![License Apache 2.0](https://badgen.net/badge/license/apache2.0/blue)](https://github.com/deepspeedai/DeepSpeed/blob/master/LICENSE)\n[![PyPI version](https://badge.fury.io/py/deepspeed.svg)](https://pypi.org/project/deepspeed/)\n[![Downloads](https://static.pepy.tech/badge/deepspeed)](https://pepy.tech/project/deepspeed)\n[![Build](https://badgen.net/badge/build/check-status/blue)](#build-pipeline-status)\n[![OpenSSF Best Practices](https://www.bestpractices.dev/projects/9530/badge)](https://www.bestpractices.dev/projects/9530)\n[![Twitter](https://img.shields.io/twitter/follow/DeepSpeedAI)](https://twitter.com/intent/follow?screen_name=DeepSpeedAI)\n[![Japanese Twitter](https://img.shields.io/badge/%E6%97%A5%E6%9C%AC%E8%AA%9ETwitter-%40DeepSpeedAI_JP-blue)](https://twitter.com/DeepSpeedAI_JP)\n[![Chinese Zhihu](https://img.shields.io/badge/%E7%9F%A5%E4%B9%8E-%E5%BE%AE%E8%BD%AFDeepSpeed-blue)](https://www.zhihu.com/people/deepspeed)\n[![Slack](https://img.shields.io/badge/Slack-4A154B?style=for-the-badge&logo=slack&logoColor=white)](https://join.slack.com/t/deepspeedworkspace/shared_invite/zt-3a8pjd8dd-PCj2hMvR4Y2syPwVnjEoww)\n\n\n<div align=\"center\">\n <img src=\"docs/assets/images/DeepSpeed_light.svg#gh-light-mode-only\" width=\"400px\">\n <img src=\"docs/assets/images/DeepSpeed_dark_transparent.svg#gh-dark-mode-only\" width=\"400px\">\n</div>\n\n## Latest News\n<b> <span style=\"color:orange\" > DeepSpeed empowers ChatGPT-like model training with a single click, offering 15x speedup over SOTA RLHF systems with unprecedented cost reduction at all scales; [learn how](https://github.com/deepspeedai/DeepSpeed/tree/master/blogs/deepspeed-chat)</span>.</b>\n* [2025/06] [Arctic Long Sequence Training (ALST) with DeepSpeed: Scalable And Efficient Training For Multi-Million Token Sequences](https://www.snowflake.com/en/engineering-blog/arctic-long-sequence-training-multi-million-token-ai/)\n* [2025/04] [DeepCompile: Unlocking Compiler Optimization for Distributed Training](https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepcompile/README.md)\n* [2025/03] [DeepSpeed-AutoTP: Automatic Tensor Parallel Training of Hugging Face models](https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/huggingface-tp/README.md)\n* [2024/12] [Ulysses-Offload: Democratizing Long Context LLM Training ](https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/ulysses-offload/README.md)\n* [2024/12] [DeepSpeed-Domino: Communication-Free LLM Training Engine](https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-domino/README.md)\n* [2024/08] [DeepSpeed on Windows](https://github.com/deepspeedai/DeepSpeed/tree/master/blogs/windows/08-2024/README.md) [[\u65e5\u672c\u8a9e](https://github.com/deepspeedai/DeepSpeed/tree/master/blogs/windows/08-2024/japanese/README.md)]  [[\u4e2d\u6587](https://github.com/deepspeedai/DeepSpeed/tree/master/blogs/windows/08-2024/chinese/README.md)]\n\n<!-- NOTE: we must use html for news items otherwise links will be broken in the 'more news' section -->\n<details>\n <summary>More news</summary>\n <ul>\n   <li> [2024/08] <a href=\"https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-gds/README.md\"> DeepNVMe: Improving DL Applications through I/O Optimizations</a> [<a href=\"ttps://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-gds/japanese/README.md\"> \u65e5\u672c\u8a9e </a>] [<a href=\"https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-gds/japanese/README.md\"> \u4e2d\u6587 </a>]</li>\n\n    <li> [2024/07] <a href=\"https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-ucp/README.md\"> DeepSpeed Universal Checkpointing: Efficient and Flexible Checkpointing for Large Scale Distributed Training</a> [<a href=\"https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-ucp/japanese/README.md\"> \u65e5\u672c\u8a9e </a>] </li>\n\n   <li> [2024/03] <a href=\"https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fp6/03-05-2024/README.md\"> DeepSpeed-FP6: The Power of FP6-Centric Serving for Large Language Models</a> [<a href=\"https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fp6/03-05-2024/README-Chinese.md\"> \u4e2d\u6587 </a>] </li>\n\n </ul>\n</details>\n\n---\n\n# Extreme Speed and Scale for DL Training and Inference\n\n***[DeepSpeed](https://www.deepspeed.ai/) enabled the world's most powerful language models (at the time of this writing) such as [MT-530B](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/) and [BLOOM](https://huggingface.co/blog/bloom-megatron-deepspeed)***. It is an easy-to-use deep learning optimization software suite that powers unprecedented scale and speed for both training and inference. With DeepSpeed you can:\n\n* Train/Inference dense or sparse models with billions or trillions of parameters\n* Achieve excellent system throughput and efficiently scale to thousands of GPUs\n* Train/Inference on resource constrained GPU systems\n* Achieve unprecedented low latency and high throughput for inference\n* Achieve extreme compression for an unparalleled inference latency and model size reduction with low costs\n\n---\n\n# DeepSpeed's four innovation pillars\n\n<img src=\"docs/assets/images/DeepSpeed-pillars.png\" width=\"800px\">\n\n\n## DeepSpeed-Training\n\nDeepSpeed offers a confluence of system innovations, that has made large scale DL training effective, and efficient, greatly improved ease of use, and redefined the DL training landscape in terms of scale that is possible. These innovations such as ZeRO, 3D-Parallelism, DeepSpeed-MoE, ZeRO-Infinity, etc. fall under the training pillar. Learn more: [DeepSpeed-Training](https://www.deepspeed.ai/training/)\n\n## DeepSpeed-Inference\n\nDeepSpeed brings together innovations in parallelism technology such as tensor, pipeline, expert and ZeRO-parallelism, and combines them with high performance custom inference kernels, communication optimizations and heterogeneous memory technologies to enable inference at an unprecedented scale, while achieving unparalleled latency, throughput and cost reduction. This systematic composition of system technologies for inference falls under the inference pillar. Learn more: [DeepSpeed-Inference](https://www.deepspeed.ai/inference)\n\n\n## DeepSpeed-Compression\n\nTo further increase the inference efficiency, DeepSpeed offers easy-to-use and flexible-to-compose compression techniques for researchers and practitioners to compress their models while delivering faster speed, smaller model size, and significantly reduced compression cost. Moreover, SoTA innovations on compression like ZeroQuant and XTC are included under the compression pillar. Learn more: [DeepSpeed-Compression](https://www.deepspeed.ai/compression)\n\n## DeepSpeed4Science\n\nIn line with Microsoft's mission to solve humanity's most pressing challenges, the DeepSpeed team at Microsoft is responding to this opportunity by launching a new initiative called *DeepSpeed4Science*, aiming to build unique capabilities through AI system technology innovations to help domain experts to unlock today's biggest science mysteries. Learn more: [tutorials](https://www.deepspeed.ai/deepspeed4science/)\n\n---\n\n# DeepSpeed Software Suite\n\n## DeepSpeed Library\n\n   The [DeepSpeed](https://github.com/deepspeedai/deepspeed) library (this repository) implements and packages the innovations and technologies in DeepSpeed Training, Inference and Compression Pillars into a single easy-to-use, open-sourced repository. It allows for easy composition of multitude of features within a single training, inference or compression pipeline. The DeepSpeed Library is heavily adopted by the DL community, and has been used to enable some of the most powerful models (see [DeepSpeed Adoption](#deepspeed-adoption)).\n\n## Model Implementations for Inference (MII)\n\n   [Model Implementations for Inference (MII)](https://github.com/deepspeedai/deepspeed-mii) is an open-sourced repository for making low-latency and high-throughput inference accessible to all data scientists by alleviating the need to apply complex system optimization techniques themselves. Out-of-box, MII offers support for thousands of widely used DL models, optimized using DeepSpeed-Inference, that can be deployed with a few lines of code, while achieving significant latency reduction compared to their vanilla open-sourced versions.\n\n## DeepSpeed on Azure\n\n   DeepSpeed users are diverse and have access to different environments. We recommend to try DeepSpeed on Azure as it is the simplest and easiest method. The recommended method to try DeepSpeed on Azure is through AzureML [recipes](https://github.com/Azure/azureml-examples/tree/main/v1/python-sdk/workflows/train/deepspeed). The job submission and data preparation scripts have been made available [here](https://github.com/deepspeedai/Megatron-DeepSpeed/tree/main/examples_deepspeed/azureml). For more details on how to use DeepSpeed on Azure, please follow the [Azure tutorial](https://www.deepspeed.ai/tutorials/azure/).\n\n---\n\n# DeepSpeed Adoption\n\nDeepSpeed was an important part of Microsoft\u2019s\n[AI at Scale](https://www.microsoft.com/en-us/research/project/ai-at-scale/)\ninitiative to enable next-generation AI capabilities at scale, where you can find more\ninformation [here](https://innovation.microsoft.com/en-us/exploring-ai-at-scale).\n\nDeepSpeed has been used to train many different large-scale models, below is a list of several examples that we are aware of (if you'd like to include your model please submit a PR):\n\n  * [Megatron-Turing NLG (530B)](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/)\n  * [Jurassic-1 (178B)](https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf)\n  * [BLOOM (176B)](https://huggingface.co/blog/bloom-megatron-deepspeed)\n  * [GLM (130B)](https://github.com/THUDM/GLM-130B)\n  * [xTrimoPGLM (100B)](https://www.biorxiv.org/content/10.1101/2023.07.05.547496v2)\n  * [YaLM (100B)](https://github.com/yandex/YaLM-100B)\n  * [GPT-NeoX (20B)](https://github.com/EleutherAI/gpt-neox)\n  * [AlexaTM (20B)](https://www.amazon.science/blog/20b-parameter-alexa-model-sets-new-marks-in-few-shot-learning)\n  * [Turing NLG (17B)](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/)\n  * [METRO-LM (5.4B)](https://arxiv.org/pdf/2204.06644.pdf)\n\nDeepSpeed has been integrated with several different popular open-source DL frameworks such as:\n\n|                                                                                                | Documentation                                |\n| ---------------------------------------------------------------------------------------------- | -------------------------------------------- |\n<img src=\"docs/assets/images/transformers-light.png#gh-light-mode-only\" width=\"250px\"><img src=\"docs/assets/images/transformers-dark.png#gh-dark-mode-only\" width=\"250px\"> | [Transformers with DeepSpeed](https://huggingface.co/docs/transformers/deepspeed) |\n| <img src=\"docs/assets/images/accelerate-light.png#gh-light-mode-only\" width=\"250px\"><img src=\"docs/assets/images/accelerate-dark.png#gh-dark-mode-only\" width=\"250px\"> | [Accelerate with DeepSpeed](https://huggingface.co/docs/accelerate/usage_guides/deepspeed) |\n| <img src=\"docs/assets/images/lightning-light.svg#gh-light-mode-only\" width=\"200px\"><img src=\"docs/assets/images/lightning-dark.svg#gh-dark-mode-only\" width=\"200px\"> | [Lightning with DeepSpeed](https://lightning.ai/docs/pytorch/stable/advanced/model_parallel.html#deepspeed) |\n| <img src=\"docs/assets/images/mosaicml.svg\" width=\"200px\"> | [MosaicML with DeepSpeed](https://docs.mosaicml.com/projects/composer/en/latest/trainer/using_the_trainer.html?highlight=deepspeed#deepspeed-integration) |\n| <img src=\"docs/assets/images/determined.svg\" width=\"225px\"> | [Determined with DeepSpeed](https://docs.determined.ai/latest/training/apis-howto/deepspeed/overview.html) |\n| <img src=\"https://user-images.githubusercontent.com/58739961/187154444-fce76639-ac8d-429b-9354-c6fac64b7ef8.jpg\" width=150> | [MMEngine with DeepSpeed](https://mmengine.readthedocs.io/en/latest/common_usage/large_model_training.html#deepspeed) |\n\n---\n\n# Build Pipeline Status\n\n| Description | Status |\n| ----------- | ------ |\n| NVIDIA | [![nv-torch-latest-v100](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-torch-latest-v100.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-torch-latest-v100.yml)  [![nv-inference](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-inference.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-inference.yml) [![nv-nightly](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-nightly.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-nightly.yml) |\n| AMD | [![amd-mi200](https://github.com/deepspeedai/DeepSpeed/actions/workflows/amd-mi200.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/amd-mi200.yml) |\n| CPU | [![torch-latest-cpu](https://github.com/deepspeedai/DeepSpeed/actions/workflows/cpu-torch-latest.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/cpu-torch-latest.yml) |\n| Intel Gaudi | [![hpu-gaudi2](https://github.com/deepspeedai/DeepSpeed/actions/workflows/hpu-gaudi2.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/hpu-gaudi2.yml) |\n| Intel XPU | [![xpu-max1100](https://github.com/deepspeedai/DeepSpeed/actions/workflows/xpu-max1100.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/xpu-max1100.yml) |\n| PyTorch Nightly | [![nv-torch-nightly-v100](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-torch-nightly-v100.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-torch-nightly-v100.yml) |\n| Integrations | [![nv-transformers-v100](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-transformers-v100.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-transformers-v100.yml) [![nv-lightning-v100](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-lightning-v100.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-lightning-v100.yml) [![nv-accelerate-v100](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-accelerate-v100.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-accelerate-v100.yml) [![nv-mii](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-mii.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-mii.yml) [![nv-ds-chat](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-ds-chat.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-ds-chat.yml) [![nv-sd](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-sd.yml/badge.svg)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-sd.yml) |\n| Misc | [![Formatting](https://github.com/deepspeedai/DeepSpeed/actions/workflows/formatting.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/formatting.yml) [![pages-build-deployment](https://github.com/deepspeedai/DeepSpeed/actions/workflows/pages/pages-build-deployment/badge.svg)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/pages/pages-build-deployment) [![Documentation Status](https://readthedocs.org/projects/deepspeed/badge/?version=latest)](https://deepspeed.readthedocs.io/en/latest/?badge=latest)[![python](https://github.com/deepspeedai/DeepSpeed/actions/workflows/python.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/python.yml) |\n| Huawei Ascend NPU | [![Huawei Ascend NPU](https://github.com/Ascend/Ascend-CI/actions/workflows/deepspeed.yaml/badge.svg?branch=main)](https://github.com/Ascend/Ascend-CI/actions/workflows/deepspeed.yaml) |\n\n# Installation\n\nThe quickest way to get started with DeepSpeed is via pip, this will install\nthe latest release of DeepSpeed which is not tied to specific PyTorch or CUDA\nversions. DeepSpeed includes several C++/CUDA extensions that we commonly refer\nto as our 'ops'.  By default, all of these extensions/ops will be built\njust-in-time (JIT) using [torch's JIT C++ extension loader that relies on\nninja](https://pytorch.org/docs/stable/cpp_extension.html) to build and\ndynamically link them at runtime.\n\n## Requirements\n* [PyTorch](https://pytorch.org/) must be installed _before_ installing DeepSpeed.\n* For full feature support we recommend a version of PyTorch that is >= 1.9 and ideally the latest PyTorch stable release.\n* A CUDA or ROCm compiler such as [nvcc](https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/#introduction) or [hipcc](https://github.com/ROCm-Developer-Tools/HIPCC) used to compile C++/CUDA/HIP extensions.\n* Specific GPUs we develop and test against are listed below, this doesn't mean your GPU will not work if it doesn't fall into this category it's just DeepSpeed is most well tested on the following:\n  * NVIDIA: Pascal, Volta, Ampere, and Hopper architectures\n  * AMD: MI100 and MI200\n\n## Contributed HW support\n* DeepSpeed now support various HW accelerators.\n\n| Contributor | Hardware                            | Accelerator Name | Contributor validated | Upstream validated |\n|-------------|-------------------------------------|------------------| --------------------- |--------------------|\n| Huawei      | Huawei Ascend NPU                   | npu              | Yes | No                 |\n| Intel       | Intel(R) Gaudi(R) 2 AI accelerator  | hpu              | Yes | Yes                |\n| Intel       | Intel(R) Xeon(R) Processors         | cpu              | Yes | Yes                |\n| Intel       | Intel(R) Data Center GPU Max series | xpu              | Yes | Yes                |\n| Tecorigin   | Scalable Data Analytics Accelerator | sdaa             | Yes | No                 |\n\n## PyPI\nWe regularly push releases to [PyPI](https://pypi.org/project/deepspeed/) and encourage users to install from there in most cases.\n\n```bash\npip install deepspeed\n```\n\nAfter installation, you can validate your install and see which extensions/ops\nyour machine is compatible with via the DeepSpeed environment report.\n\n```bash\nds_report\n```\n\nIf you would like to pre-install any of the DeepSpeed extensions/ops (instead\nof JIT compiling) or install pre-compiled ops via PyPI please see our [advanced\ninstallation instructions](https://www.deepspeed.ai/tutorials/advanced-install/).\n\n## Windows\nMany DeepSpeed features are supported on Windows for both training and inference. You can read more about this in the original blog post [here](https://github.com/deepspeedai/DeepSpeed/tree/master/blogs/windows/08-2024/README.md). Among features that are currently not supported are async io (AIO) and GDS (which does not support Windows).\n1. Install PyTorch, such as pytorch 2.3+cu121.\n2. Install Visual C++ build tools, such as VS2022 C++ x64/x86 build tools.\n3. Launch Cmd console with Administrator permissions for creating required symlink folders and ensure MSVC tools are added to your PATH or launch the Developer Command Prompt for Visual Studio 2022 with administrator permissions.\n4. Run `build_win.bat` to build wheel in `dist` folder.\n\n# Features\n\nPlease checkout [DeepSpeed-Training](https://www.deepspeed.ai/training), [DeepSpeed-Inference](https://www.deepspeed.ai/inference) and [DeepSpeed-Compression](https://www.deepspeed.ai/compression) pages for full set of features offered along each of these three pillars.\n\n# Further Reading\n\nAll DeepSpeed documentation, tutorials, and blogs can be found on our website: [deepspeed.ai](https://www.deepspeed.ai/)\n\n\n|                                                                                                | Description                                  |\n| ---------------------------------------------------------------------------------------------- | -------------------------------------------- |\n| [Getting Started](https://www.deepspeed.ai/getting-started/)                                   |  First steps with DeepSpeed                  |\n| [DeepSpeed JSON Configuration](https://www.deepspeed.ai/docs/config-json/)                     |  Configuring DeepSpeed                       |\n| [API Documentation](https://deepspeed.readthedocs.io/en/latest/)                               |  Generated DeepSpeed API documentation       |\n| [Tutorials](https://www.deepspeed.ai/tutorials/)                                               |  Tutorials                                   |\n| [Blogs](https://www.deepspeed.ai/posts/)                                                       |  Blogs                                   |\n\n\n# Contributing\nDeepSpeed welcomes your contributions! Please see our\n[contributing](CONTRIBUTING.md) guide for more details on formatting, testing,\netc.<br/>\nThanks so much to all of our amazing contributors!\n\n<a href=\"https://github.com/deepspeedai/DeepSpeed/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=microsoft/DeepSpeed&r=\"  width=\"800px\"/>\n</a>\n\n## Contributor License Agreement\nThis project welcomes contributions and suggestions. Most contributions require you to\nagree to a Contributor License Agreement (CLA) declaring that you have the right to, and\nactually do, grant us the rights to use your contribution. For details, visit\nhttps://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need\nto provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply\nfollow the instructions provided by the bot. You will only need to do this once across\nall repos using our CLA.\n\n## Code of Conduct\nThis project has adopted the [Microsoft Open Source Code of\nConduct](https://opensource.microsoft.com/codeofconduct/). For more information see the\n[Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact\n[opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n# Publications\n1. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He. (2019) ZeRO: memory optimizations toward training trillion parameter models. [arXiv:1910.02054](https://arxiv.org/abs/1910.02054) and [In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC '20)](https://dl.acm.org/doi/10.5555/3433701.3433727).\n2. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. (2020) DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters. [In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD '20, Tutorial)](https://dl.acm.org/doi/10.1145/3394486.3406703).\n3. Minjia Zhang, Yuxiong He. (2020) Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping. [arXiv:2010.13369](https://arxiv.org/abs/2010.13369) and [NeurIPS 2020](https://proceedings.neurips.cc/paper/2020/hash/a1140a3d0df1c81e24ae954d935e8926-Abstract.html).\n4. Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, Yuxiong He. (2021) ZeRO-Offload: Democratizing Billion-Scale Model Training. [arXiv:2101.06840](https://arxiv.org/abs/2101.06840) and [USENIX ATC 2021](https://www.usenix.org/conference/atc21/presentation/ren-jie). [[paper]](https://arxiv.org/abs/2101.06840) [[slides]](https://www.usenix.org/system/files/atc21_slides_ren-jie.pdf) [[blog]](https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/)\n5. Hanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari, Conglong Li, Xiangru Lian, Ji Liu, Ce Zhang, Yuxiong He. (2021) 1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed. [arXiv:2102.02888](https://arxiv.org/abs/2102.02888) and [ICML 2021](http://proceedings.mlr.press/v139/tang21a.html).\n6. Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, Yuxiong He. (2021) ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning. [arXiv:2104.07857](https://arxiv.org/abs/2104.07857) and [SC 2021](https://dl.acm.org/doi/abs/10.1145/3458817.3476205). [[paper]](https://arxiv.org/abs/2104.07857) [[slides]](docs/assets/files/SC21-ZeRO-Infinity.pdf) [[blog]](https://www.microsoft.com/en-us/research/blog/zero-infinity-and-deepspeed-unlocking-unprecedented-model-scale-for-deep-learning-training/)\n7. Conglong Li, Ammar Ahmad Awan, Hanlin Tang, Samyam Rajbhandari, Yuxiong He. (2021) 1-bit LAMB: Communication Efficient Large-Scale Large-Batch Training with LAMB's Convergence Speed. [arXiv:2104.06069](https://arxiv.org/abs/2104.06069) and [HiPC 2022](https://hipc.org/advance-program/).\n8. Conglong Li, Minjia Zhang, Yuxiong He. (2021) The Stability-Efficiency Dilemma: Investigating Sequence Length Warmup for Training GPT Models. [arXiv:2108.06084](https://arxiv.org/abs/2108.06084) and [NeurIPS 2022](https://openreview.net/forum?id=JpZ5du_Kdh).\n9. Yucheng Lu, Conglong Li, Minjia Zhang, Christopher De Sa, Yuxiong He. (2022) Maximizing Communication Efficiency for Large-scale Training via 0/1 Adam. [arXiv:2202.06009](https://arxiv.org/abs/2202.06009).\n10. Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, Yuxiong He. (2022) DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale [arXiv:2201.05596](https://arxiv.org/abs/2201.05596) and [ICML 2022](https://proceedings.mlr.press/v162/rajbhandari22a.html). [[pdf]](https://arxiv.org/abs/2201.05596) [[slides]](docs/assets/files/ICML-5mins.pdf) [[blog]](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/)\n11. Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, Bryan Catanzaro. (2022) Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model [arXiv:2201.11990](https://arxiv.org/abs/2201.11990).\n12. Xiaoxia Wu, Zhewei Yao, Minjia Zhang, Conglong Li, Yuxiong He. (2022) Extreme Compression for Pre-trained Transformers Made Simple and Efficient. [arXiv:2206.01859](https://arxiv.org/abs/2206.01859) and [NeurIPS 2022](https://openreview.net/forum?id=xNeAhc2CNAl).\n13. Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, Yuxiong He. (2022) ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers. [arXiv:2206.01861](https://arxiv.org/abs/2206.01861) and [NeurIPS 2022](https://openreview.net/forum?id=f-fVCElZ-G1) [[slides]](docs/assets/files/zeroquant_series.pdf) [[blog]](https://www.microsoft.com/en-us/research/blog/deepspeed-compression-a-composable-library-for-extreme-compression-and-zero-cost-quantization/)\n14. Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase, Yuxiong He. (2022) DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale. [arXiv:2207.00032](https://arxiv.org/abs/2207.00032) and [SC 2022](https://dl.acm.org/doi/abs/10.5555/3571885.3571946). [[paper]](https://arxiv.org/abs/2207.00032) [[slides]](docs/assets/files/sc22-ds-inference.pdf) [[blog]](https://www.microsoft.com/en-us/research/blog/deepspeed-accelerating-large-scale-model-inference-and-training-via-system-optimizations-and-compression/)\n15. Zhewei Yao, Xiaoxia Wu, Conglong Li, Connor Holmes, Minjia Zhang, Cheng Li, Yuxiong He. (2022) Random-LTD: Random and Layerwise Token Dropping Brings Efficient Training for Large-scale Transformers. [arXiv:2211.11586](https://arxiv.org/abs/2211.11586).\n16. Conglong Li, Zhewei Yao, Xiaoxia Wu, Minjia Zhang, Yuxiong He. (2022) DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing. [arXiv:2212.03597](https://arxiv.org/abs/2212.03597) [ENLSP2023 Workshop at NeurIPS2023](https://neurips2023-enlsp.github.io/)\n17. Xiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao, Yuxiong He. (2023) Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases. [arXiv:2301.12017](https://arxiv.org/abs/2301.12017) and [ICML2023](https://icml.cc/Conferences/2023).\n18. Syed Zawad, Cheng Li, Zhewei Yao, Elton Zheng, Yuxiong He, Feng Yan. (2023) DySR: Adaptive Super-Resolution via Algorithm and System Co-design. [ICLR:2023](https://openreview.net/forum?id=Pgtn4l6eKjv).\n19. Sheng Shen, Zhewei Yao, Chunyuan Li, Trevor Darrell, Kurt Keutzer, Yuxiong He. (2023) Scaling Vision-Language Models with Sparse Mixture of Experts. [arXiv:2303.07226](https://arxiv.org/abs/2303.07226) and [Finding at EMNLP2023](https://2023.emnlp.org/).\n20. Quentin Anthony, Ammar Ahmad Awan, Jeff Rasley, Yuxiong He, Aamir Shafi, Mustafa Abduljabbar, Hari Subramoni, Dhabaleswar Panda. (2023) MCR-DL: Mix-and-Match Communication Runtime for Deep Learning [arXiv:2303.08374](https://arxiv.org/abs/2303.08374) and will appear at IPDPS 2023.\n21. Siddharth Singh, Olatunji Ruwase, Ammar Ahmad Awan, Samyam Rajbhandari, Yuxiong He, Abhinav Bhatele. (2023) A Hybrid Tensor-Expert-Data Parallelism Approach to Optimize Mixture-of-Experts Training [arXiv:2303.06318](https://arxiv.org/abs/2303.06318) and [ICS 2023](https://dl.acm.org/doi/10.1145/3577193.3593704).\n22. Guanhua Wang, Heyang Qin, Sam Ade Jacobs, Xiaoxia Wu, Connor Holmes, Zhewei Yao, Samyam Rajbhandari, Olatunji Ruwase, Feng Yan, Lei Yang, Yuxiong He. (2023) ZeRO++: Extremely Efficient Collective Communication for Giant Model Training [arXiv:2306.10209](https://arxiv.org/abs/2306.10209) and [ML for Sys Workshop at NeurIPS2023](http://mlforsystems.org/) [[blog]](https://www.microsoft.com/en-us/research/blog/deepspeed-zero-a-leap-in-speed-for-llm-and-chat-model-training-with-4x-less-communication/)\n23. Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, Yuxiong He. (2023) ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation [arXiv:2303.08302](https://arxiv.org/abs/2303.08302) and [ENLSP2023 Workshop at NeurIPS2023](https://neurips2023-enlsp.github.io/) [[slides]](docs/assets/files/zeroquant_series.pdf)\n24. Pareesa Ameneh Golnari, Zhewei Yao, Yuxiong He. (2023) Selective Guidance: Are All the Denoising Steps of Guided Diffusion Important? [arXiv:2305.09847](https://arxiv.org/abs/2305.09847)\n25. Zhewei Yao, Reza Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajbhandari, Xiaoxia Wu, Ammar Ahmad Awan, Jeff Rasley, Minjia Zhang, Conglong Li, Connor Holmes, Zhongzhu Zhou, Michael Wyatt, Molly Smith, Lev Kurilenko, Heyang Qin, Masahiro Tanaka, Shuai Che, Shuaiwen Leon Song, Yuxiong He. (2023) DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales [arXiv:2308.01320](https://arxiv.org/abs/2308.01320).\n26. Xiaoxia Wu, Zhewei Yao, Yuxiong He. (2023) ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats [arXiv:2307.09782](https://arxiv.org/abs/2307.09782) and [ENLSP2023 Workshop at NeurIPS2023](https://neurips2023-enlsp.github.io/) [[slides]](docs/assets/files/zeroquant_series.pdf)\n27. Zhewei Yao, Xiaoxia Wu, Conglong Li, Minjia Zhang, Heyang Qin, Olatunji Ruwase, Ammar Ahmad Awan, Samyam Rajbhandari, Yuxiong He. (2023) DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention [arXiv:2309.14327](https://arxiv.org/pdf/2309.14327.pdf)\n28. Shuaiwen Leon Song, Bonnie Kruft, Minjia Zhang, Conglong Li, Shiyang Chen, Chengming Zhang, Masahiro Tanaka, Xiaoxia Wu, Jeff Rasley, Ammar Ahmad Awan, Connor Holmes, Martin Cai, Adam Ghanem, Zhongzhu Zhou, Yuxiong He, et al. (2023) DeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery through Sophisticated AI System Technologies [arXiv:2310.04610](https://arxiv.org/abs/2310.04610) [[blog]](https://www.microsoft.com/en-us/research/blog/announcing-the-deepspeed4science-initiative-enabling-large-scale-scientific-discovery-through-sophisticated-ai-system-technologies/)\n29. Zhewei Yao, Reza Yazdani Aminabadi, Stephen Youn, Xiaoxia Wu, Elton Zheng, Yuxiong He. (2023) ZeroQuant-HERO: Hardware-Enhanced Robust Optimized Post-Training Quantization Framework for W8A8 Transformers [arXiv:2310.17723](https://arxiv.org/abs/2310.17723)\n\n30. Xiaoxia Wu, Haojun Xia, Stephen Youn, Zhen Zheng, Shiyang Chen, Arash Bakhtiari, Michael Wyatt, Reza Yazdani Aminabadi, Yuxiong He, Olatunji Ruwase, Leon Song, Zhewei Yao (2023) ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric Strategy for Diverse Generative Tasks [arXiv:2312.08583](https://arxiv.org/abs/2312.08583)\n\n31. Haojun Xia, Zhen Zheng, Xiaoxia Wu, Shiyang Chen, Zhewei Yao, Stephen Youn, Arash Bakhtiari, Michael Wyatt, Donglin Zhuang, Zhongzhu Zhou, Olatunji Ruwase, Yuxiong He, Shuaiwen Leon Song. (2024) FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design  [arXiv:2401.14112](https://arxiv.org/abs/2401.14112)\n32. Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Reza Yazdani Aminadabi, Shuaiwen Leon Song, Samyam Rajbhandari, Yuxiong He. (2024) [System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models](https://dl.acm.org/doi/10.1145/3662158.3662806)\n33. Xinyu Lian, Sam Ade Jacobs, Lev Kurilenko, Masahiro Tanaka, Stas Bekman, Olatunji Ruwase, Minjia Zhang. (2024) Universal Checkpointing: Efficient and Flexible Checkpointing for Large Scale Distributed Training [arXiv:2406.18820](https://arxiv.org/abs/2406.18820)\n34. Stas Bekman, Samyam Rajbhandari, Michael Wyatt, Jeff Rasley, Tunji Ruwase, Zhewei Yao, Aurick Qiao, Yuxiong He. (2025) Arctic Long Sequence Training: Scalable And Efficient Training For Multi-Million Token Sequences [arXiv:2506.13996](https://arxiv.org/abs/2506.13996)\n\n\n# Videos\n1. DeepSpeed KDD 2020 Tutorial\n    1. [Overview](https://www.youtube.com/watch?v=CaseqC45DNc&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=29)\n    2. [ZeRO + large model training](https://www.youtube.com/watch?v=y4_bCiAsIAk&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=28)\n    3. [17B T-NLG demo](https://www.youtube.com/watch?v=9V-ZbP92drg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=27)\n    4. [Fastest BERT training + RScan tuning](https://www.youtube.com/watch?v=o1K-ZG9F6u0&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=26)\n    5. DeepSpeed hands on deep dive: [part 1](https://www.youtube.com/watch?v=_NOk-mBwDYg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=92), [part 2](https://www.youtube.com/watch?v=sG6_c4VXLww&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=94), [part 3](https://www.youtube.com/watch?v=k9yPkBTayos&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=93)\n    6. [FAQ](https://www.youtube.com/watch?v=nsHu6vEgPew&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=24)\n2. Microsoft Research Webinar\n    * Registration is free and all videos are available on-demand.\n    * [ZeRO & Fastest BERT: Increasing the scale and speed of deep learning training in DeepSpeed](https://note.microsoft.com/MSR-Webinar-DeepSpeed-Registration-On-Demand.html).\n3. [DeepSpeed on AzureML](https://youtu.be/yBVXR8G8Bg8)\n4. [Large Model Training and Inference with DeepSpeed // Samyam Rajbhandari // LLMs in Prod Conference](https://www.youtube.com/watch?v=cntxC3g22oU) [[slides]](docs/assets/files/presentation-mlops.pdf)\n5. Community Tutorials\n    * [DeepSpeed: All the tricks to scale to gigantic models (Mark Saroufim)](https://www.youtube.com/watch?v=pDGI668pNg0)\n    * [Turing-NLG, DeepSpeed and the ZeRO optimizer (Yannic Kilcher)](https://www.youtube.com/watch?v=tC01FRB0M7w)\n    * [Ultimate Guide To Scaling ML Models (The AI Epiphany)](https://www.youtube.com/watch?v=hc0u4avAkuM)\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 162405963,
    "name": "gradio",
    "full_name": "gradio-app/gradio",
    "description": "Build and share delightful machine learning apps, all in Python. \ud83c\udf1f Star to support our work!",
    "html_url": "https://github.com/gradio-app/gradio",
    "clone_url": "https://github.com/gradio-app/gradio.git",
    "owner_login": "gradio-app",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/51063788?v=4",
    "stargazers_count": 39351,
    "watchers_count": 39351,
    "forks_count": 3014,
    "open_issues_count": 438,
    "size": 298956,
    "language": "Python",
    "languages": {
      "Python": 3485603,
      "Svelte": 1447013,
      "TypeScript": 1322285,
      "mdsvex": 225523,
      "CSS": 70796,
      "JavaScript": 63728,
      "Jupyter Notebook": 32113,
      "HTML": 24584,
      "Batchfile": 6427,
      "Shell": 6052,
      "MDX": 1670,
      "Dockerfile": 512
    },
    "topics": [
      "data-analysis",
      "data-science",
      "data-visualization",
      "deep-learning",
      "deploy",
      "gradio",
      "gradio-interface",
      "hacktoberfest",
      "interface",
      "machine-learning",
      "models",
      "python",
      "python-notebook",
      "ui",
      "ui-components"
    ],
    "license_name": "Apache License 2.0",
    "created_at": "2018-12-19T08:24:04+00:00",
    "updated_at": "2025-08-06T02:11:22+00:00",
    "pushed_at": "2025-08-05T21:45:03+00:00",
    "contributors_count": 100,
    "readme_length": 13313,
    "readme_content": "<!-- DO NOT EDIT THIS FILE DIRECTLY. INSTEAD EDIT THE `readme_template.md` OR `guides/01_getting-started/01_quickstart.md` TEMPLATES AND THEN RUN `render_readme.py` SCRIPT. -->\n\n<div align=\"center\">\n<a href=\"https://gradio.app\">\n<img src=\"readme_files/gradio.svg\" alt=\"gradio\" width=350>\n</a>\n</div>\n\n<div align=\"center\">\n<span>\n<a href=\"https://www.producthunt.com/posts/gradio-5-0?embed=true&utm_source=badge-featured&utm_medium=badge&utm_souce=badge-gradio&#0045;5&#0045;0\" target=\"_blank\"><img src=\"https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=501906&theme=light\" alt=\"Gradio&#0032;5&#0046;0 - the&#0032;easiest&#0032;way&#0032;to&#0032;build&#0032;AI&#0032;web&#0032;apps | Product Hunt\" style=\"width: 150px; height: 54px;\" width=\"150\" height=\"54\" /></a>\n<a href=\"https://trendshift.io/repositories/2145\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/2145\" alt=\"gradio-app%2Fgradio | Trendshift\" style=\"width: 150px; height: 55px;\" width=\"150\" height=\"55\"/></a>\n</span>\n\n[![gradio-backend](https://github.com/gradio-app/gradio/actions/workflows/test-python.yml/badge.svg)](https://github.com/gradio-app/gradio/actions/workflows/test-python.yml)\n[![gradio-ui](https://github.com/gradio-app/gradio/actions/workflows/tests-js.yml/badge.svg)](https://github.com/gradio-app/gradio/actions/workflows/tests-js.yml) \n[![PyPI](https://img.shields.io/pypi/v/gradio)](https://pypi.org/project/gradio/)\n[![PyPI downloads](https://img.shields.io/pypi/dm/gradio)](https://pypi.org/project/gradio/)\n![Python version](https://img.shields.io/badge/python-3.10+-important)\n[![Twitter follow](https://img.shields.io/twitter/follow/gradio?style=social&label=follow)](https://twitter.com/gradio)\n\n[Website](https://gradio.app)\n| [Documentation](https://gradio.app/docs/)\n| [Guides](https://gradio.app/guides/)\n| [Getting Started](https://gradio.app/getting_started/)\n| [Examples](demo/)\n\n</div>\n\n<div align=\"center\">\n\nEnglish | [\u4e2d\u6587](readme_files/zh-cn#readme)\n\n</div>\n\n# Gradio: Build Machine Learning Web Apps \u2014 in Python\n\n\n\nGradio is an open-source Python package that allows you to quickly **build** a demo or web application for your machine learning model, API, or any arbitrary Python function. You can then **share** a link to your demo or web application in just a few seconds using Gradio's built-in sharing features. *No JavaScript, CSS, or web hosting experience needed!*\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/gif-version.gif\" style=\"padding-bottom: 10px\">\n\nIt just takes a few lines of Python to create your own demo, so let's get started \ud83d\udcab\n\n\n### Installation\n\n**Prerequisite**: Gradio requires [Python 3.10 or higher](https://www.python.org/downloads/).\n\n\nWe recommend installing Gradio using `pip`, which is included by default in Python. Run this in your terminal or command prompt:\n\n```bash\npip install --upgrade gradio\n```\n\n\n> [!TIP]\n > It is best to install Gradio in a virtual environment. Detailed installation instructions for all common operating systems <a href=\"https://www.gradio.app/main/guides/installing-gradio-in-a-virtual-environment\">are provided here</a>. \n\n### Building Your First Demo\n\nYou can run Gradio in your favorite code editor, Jupyter notebook, Google Colab, or anywhere else you write Python. Let's write your first Gradio app:\n\n\n```python\nimport gradio as gr\n\ndef greet(name, intensity):\n    return \"Hello, \" + name + \"!\" * int(intensity)\n\ndemo = gr.Interface(\n    fn=greet,\n    inputs=[\"text\", \"slider\"],\n    outputs=[\"text\"],\n)\n\ndemo.launch()\n```\n\n\n\n> [!TIP]\n > We shorten the imported name from <code>gradio</code> to <code>gr</code>. This is a widely adopted convention for better readability of code. \n\nNow, run your code. If you've written the Python code in a file named `app.py`, then you would run `python app.py` from the terminal.\n\nThe demo below will open in a browser on [http://localhost:7860](http://localhost:7860) if running from a file. If you are running within a notebook, the demo will appear embedded within the notebook.\n\n![`hello_world_4` demo](demo/hello_world_4/screenshot.gif)\n\nType your name in the textbox on the left, drag the slider, and then press the Submit button. You should see a friendly greeting on the right.\n\n> [!TIP]\n > When developing locally, you can run your Gradio app in <strong>hot reload mode</strong>, which automatically reloads the Gradio app whenever you make changes to the file. To do this, simply type in <code>gradio</code> before the name of the file instead of <code>python</code>. In the example above, you would type: `gradio app.py` in your terminal. You can also enable <strong>vibe mode</strong> by using the <code>--vibe</code> flag, e.g. <code>gradio --vibe app.py</code>, which provides an in-browser chat that can be used to write or edit your Gradio app using natural language. Learn more in the <a href=\"https://www.gradio.app/guides/developing-faster-with-reload-mode\">Hot Reloading Guide</a>.\n\n\n**Understanding the `Interface` Class**\n\nYou'll notice that in order to make your first demo, you created an instance of the `gr.Interface` class. The `Interface` class is designed to create demos for machine learning models which accept one or more inputs, and return one or more outputs. \n\nThe `Interface` class has three core arguments:\n\n- `fn`: the function to wrap a user interface (UI) around\n- `inputs`: the Gradio component(s) to use for the input. The number of components should match the number of arguments in your function.\n- `outputs`: the Gradio component(s) to use for the output. The number of components should match the number of return values from your function.\n\nThe `fn` argument is very flexible -- you can pass *any* Python function that you want to wrap with a UI. In the example above, we saw a relatively simple function, but the function could be anything from a music generator to a tax calculator to the prediction function of a pretrained machine learning model.\n\nThe `inputs` and `outputs` arguments take one or more Gradio components. As we'll see, Gradio includes more than [30 built-in components](https://www.gradio.app/docs/gradio/introduction) (such as the `gr.Textbox()`, `gr.Image()`, and `gr.HTML()` components) that are designed for machine learning applications. \n\n> [!TIP]\n > For the `inputs` and `outputs` arguments, you can pass in the name of these components as a string (`\"textbox\"`) or an instance of the class (`gr.Textbox()`).\n\nIf your function accepts more than one argument, as is the case above, pass a list of input components to `inputs`, with each input component corresponding to one of the arguments of the function, in order. The same holds true if your function returns more than one value: simply pass in a list of components to `outputs`. This flexibility makes the `Interface` class a very powerful way to create demos.\n\nWe'll dive deeper into the `gr.Interface` on our series on [building Interfaces](https://www.gradio.app/main/guides/the-interface-class).\n\n### Sharing Your Demo\n\nWhat good is a beautiful demo if you can't share it? Gradio lets you easily share a machine learning demo without having to worry about the hassle of hosting on a web server. Simply set `share=True` in `launch()`, and a publicly accessible URL will be created for your demo. Let's revisit our example demo,  but change the last line as follows:\n\n```python\nimport gradio as gr\n\ndef greet(name):\n    return \"Hello \" + name + \"!\"\n\ndemo = gr.Interface(fn=greet, inputs=\"textbox\", outputs=\"textbox\")\n    \ndemo.launch(share=True)  # Share your demo with just 1 extra parameter \ud83d\ude80\n```\n\nWhen you run this code, a public URL will be generated for your demo in a matter of seconds, something like:\n\n\ud83d\udc49 &nbsp; `https://a23dsf231adb.gradio.live`\n\nNow, anyone around the world can try your Gradio demo from their browser, while the machine learning model and all computation continues to run locally on your computer.\n\nTo learn more about sharing your demo, read our dedicated guide on [sharing your Gradio application](https://www.gradio.app/guides/sharing-your-app).\n\n\n### An Overview of Gradio\n\nSo far, we've been discussing the `Interface` class, which is a high-level class that lets to build demos quickly with Gradio. But what else does Gradio include?\n\n#### Custom Demos with `gr.Blocks`\n\nGradio offers a low-level approach for designing web apps with more customizable layouts and data flows with the `gr.Blocks` class. Blocks supports things like controlling where components appear on the page, handling multiple data flows and more complex interactions (e.g. outputs can serve as inputs to other functions), and updating properties/visibility of components based on user interaction \u2014 still all in Python. \n\nYou can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners).\n\n#### Chatbots with `gr.ChatInterface`\n\nGradio includes another high-level class, `gr.ChatInterface`, which is specifically designed to create Chatbot UIs. Similar to `Interface`, you supply a function and Gradio creates a fully working Chatbot UI. If you're interested in creating a chatbot, you can jump straight to [our dedicated guide on `gr.ChatInterface`](https://www.gradio.app/guides/creating-a-chatbot-fast).\n\n#### The Gradio Python & JavaScript Ecosystem\n\nThat's the gist of the core `gradio` Python library, but Gradio is actually so much more! It's an entire ecosystem of Python and JavaScript libraries that let you build machine learning applications, or query them programmatically, in Python or JavaScript. Here are other related parts of the Gradio ecosystem:\n\n* [Gradio Python Client](https://www.gradio.app/guides/getting-started-with-the-python-client) (`gradio_client`): query any Gradio app programmatically in Python.\n* [Gradio JavaScript Client](https://www.gradio.app/guides/getting-started-with-the-js-client) (`@gradio/client`): query any Gradio app programmatically in JavaScript.\n* [Gradio-Lite](https://www.gradio.app/guides/gradio-lite) (`@gradio/lite`): write Gradio apps in Python that run entirely in the browser (no server needed!), thanks to Pyodide. \n* [Hugging Face Spaces](https://huggingface.co/spaces): the most popular place to host Gradio applications \u2014 for free!\n\n### What's Next?\n\nKeep learning about Gradio sequentially using the Gradio Guides, which include explanations as well as example code and embedded interactive demos. Next up: [let's dive deeper into the Interface class](https://www.gradio.app/guides/the-interface-class).\n\nOr, if you already know the basics and are looking for something specific, you can search the more [technical API documentation](https://www.gradio.app/docs/).\n\n\n### Gradio Sketch\n\nYou can also build Gradio applications without writing any code. Simply type `gradio sketch` into your terminal to open up an editor that lets you define and modify Gradio components, adjust their layouts, add events, all through a web editor. Or [use this hosted version of Gradio Sketch, running on Hugging Face Spaces](https://huggingface.co/spaces/aliabid94/Sketch).\n\n## Questions?\n\nIf you'd like to report a bug or have a feature request, please create an [issue on GitHub](https://github.com/gradio-app/gradio/issues/new/choose). For general questions about usage, we are available on [our Discord server](https://discord.com/invite/feTf9x3ZSB) and happy to help.\n\nIf you like Gradio, please leave us a \u2b50 on GitHub!\n\n## Open Source Stack\n\nGradio is built on top of many wonderful open-source libraries!\n\n[<img src=\"readme_files/huggingface_mini.svg\" alt=\"huggingface\" height=40>](https://huggingface.co)\n[<img src=\"readme_files/python.svg\" alt=\"python\" height=40>](https://www.python.org)\n[<img src=\"readme_files/fastapi.svg\" alt=\"fastapi\" height=40>](https://fastapi.tiangolo.com)\n[<img src=\"readme_files/encode.svg\" alt=\"encode\" height=40>](https://www.encode.io)\n[<img src=\"readme_files/svelte.svg\" alt=\"svelte\" height=40>](https://svelte.dev)\n[<img src=\"readme_files/vite.svg\" alt=\"vite\" height=40>](https://vitejs.dev)\n[<img src=\"readme_files/pnpm.svg\" alt=\"pnpm\" height=40>](https://pnpm.io)\n[<img src=\"readme_files/tailwind.svg\" alt=\"tailwind\" height=40>](https://tailwindcss.com)\n[<img src=\"readme_files/storybook.svg\" alt=\"storybook\" height=40>](https://storybook.js.org/)\n[<img src=\"readme_files/chromatic.svg\" alt=\"chromatic\" height=40>](https://www.chromatic.com/)\n\n## License\n\nGradio is licensed under the Apache License 2.0 found in the [LICENSE](LICENSE) file in the root directory of this repository.\n\n## Citation\n\nAlso check out the paper _[Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild](https://arxiv.org/abs/1906.02569), ICML HILL 2019_, and please cite it if you use Gradio in your work.\n\n```\n@article{abid2019gradio,\n  title = {Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild},\n  author = {Abid, Abubakar and Abdalla, Ali and Abid, Ali and Khan, Dawood and Alfozan, Abdulrahman and Zou, James},\n  journal = {arXiv preprint arXiv:1906.02569},\n  year = {2019},\n}\n```\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 71932349,
    "name": "ray",
    "full_name": "ray-project/ray",
    "description": "Ray is an AI compute engine. Ray consists of a core distributed runtime and a set of AI Libraries for accelerating ML workloads.",
    "html_url": "https://github.com/ray-project/ray",
    "clone_url": "https://github.com/ray-project/ray.git",
    "owner_login": "ray-project",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/22125274?v=4",
    "stargazers_count": 38332,
    "watchers_count": 38332,
    "forks_count": 6683,
    "open_issues_count": 2992,
    "size": 579235,
    "language": "Python",
    "languages": {
      "Python": 30933773,
      "C++": 7972030,
      "Java": 1122874,
      "TypeScript": 650552,
      "Starlark": 553234,
      "Cython": 394870,
      "Shell": 154280,
      "C": 39275,
      "Dockerfile": 32580,
      "Jinja": 9660,
      "CSS": 2887,
      "Linker Script": 2821,
      "PowerShell": 2649,
      "HTML": 1831,
      "Roff": 1220,
      "JavaScript": 875
    },
    "topics": [
      "data-science",
      "deep-learning",
      "deployment",
      "distributed",
      "hyperparameter-optimization",
      "hyperparameter-search",
      "large-language-models",
      "llm",
      "llm-inference",
      "llm-serving",
      "machine-learning",
      "optimization",
      "parallel",
      "python",
      "pytorch",
      "ray",
      "reinforcement-learning",
      "rllib",
      "serving",
      "tensorflow"
    ],
    "license_name": "Apache License 2.0",
    "created_at": "2016-10-25T19:38:30+00:00",
    "updated_at": "2025-08-06T02:06:10+00:00",
    "pushed_at": "2025-08-06T01:13:04+00:00",
    "contributors_count": 100,
    "readme_length": 6509,
    "readme_content": ".. image:: https://github.com/ray-project/ray/raw/master/doc/source/images/ray_header_logo.png\n\n.. image:: https://readthedocs.org/projects/ray/badge/?version=master\n    :target: http://docs.ray.io/en/master/?badge=master\n\n.. image:: https://img.shields.io/badge/Ray-Join%20Slack-blue\n    :target: https://www.ray.io/join-slack\n\n.. image:: https://img.shields.io/badge/Discuss-Ask%20Questions-blue\n    :target: https://discuss.ray.io/\n\n.. image:: https://img.shields.io/twitter/follow/raydistributed.svg?style=social&logo=twitter\n    :target: https://x.com/raydistributed\n\n.. image:: https://img.shields.io/badge/Get_started_for_free-3C8AE9?logo=data%3Aimage%2Fpng%3Bbase64%2CiVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8%2F9hAAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAEKADAAQAAAABAAAAEAAAAAA0VXHyAAABKElEQVQ4Ea2TvWoCQRRGnWCVWChIIlikC9hpJdikSbGgaONbpAoY8gKBdAGfwkfwKQypLQ1sEGyMYhN1Pd%2B6A8PqwBZeOHt%2FvsvMnd3ZXBRFPQjBZ9K6OY8ZxF%2B0IYw9PW3qz8aY6lk92bZ%2BVqSI3oC9T7%2FyCVnrF1ngj93us%2B540sf5BrCDfw9b6jJ5lx%2FyjtGKBBXc3cnqx0INN4ImbI%2Bl%2BPnI8zWfFEr4chLLrWHCp9OO9j19Kbc91HX0zzzBO8EbLK2Iv4ZvNO3is3h6jb%2BCwO0iL8AaWqB7ILPTxq3kDypqvBuYuwswqo6wgYJbT8XxBPZ8KS1TepkFdC79TAHHce%2F7LbVioi3wEfTpmeKtPRGEeoldSP%2FOeoEftpP4BRbgXrYZefsAI%2BP9JU7ImyEAAAAASUVORK5CYII%3D\n   :target: https://www.anyscale.com/ray-on-anyscale?utm_source=github&utm_medium=ray_readme&utm_campaign=get_started_badge\n\nRay is a unified framework for scaling AI and Python applications. Ray consists of a core distributed runtime and a set of AI libraries for simplifying ML compute:\n\n.. image:: https://github.com/ray-project/ray/raw/master/doc/source/images/what-is-ray-padded.svg\n\n..\n  https://docs.google.com/drawings/d/1Pl8aCYOsZCo61cmp57c7Sja6HhIygGCvSZLi_AuBuqo/edit\n\nLearn more about `Ray AI Libraries`_:\n\n- `Data`_: Scalable Datasets for ML\n- `Train`_: Distributed Training\n- `Tune`_: Scalable Hyperparameter Tuning\n- `RLlib`_: Scalable Reinforcement Learning\n- `Serve`_: Scalable and Programmable Serving\n\nOr more about `Ray Core`_ and its key abstractions:\n\n- `Tasks`_: Stateless functions executed in the cluster.\n- `Actors`_: Stateful worker processes created in the cluster.\n- `Objects`_: Immutable values accessible across the cluster.\n\nLearn more about Monitoring and Debugging:\n\n- Monitor Ray apps and clusters with the `Ray Dashboard <https://docs.ray.io/en/latest/ray-core/ray-dashboard.html>`__.\n- Debug Ray apps with the `Ray Distributed Debugger <https://docs.ray.io/en/latest/ray-observability/ray-distributed-debugger.html>`__.\n\nRay runs on any machine, cluster, cloud provider, and Kubernetes, and features a growing\n`ecosystem of community integrations`_.\n\nInstall Ray with: ``pip install ray``. For nightly wheels, see the\n`Installation page <https://docs.ray.io/en/latest/ray-overview/installation.html>`__.\n\n.. _`Serve`: https://docs.ray.io/en/latest/serve/index.html\n.. _`Data`: https://docs.ray.io/en/latest/data/dataset.html\n.. _`Workflow`: https://docs.ray.io/en/latest/workflows/\n.. _`Train`: https://docs.ray.io/en/latest/train/train.html\n.. _`Tune`: https://docs.ray.io/en/latest/tune/index.html\n.. _`RLlib`: https://docs.ray.io/en/latest/rllib/index.html\n.. _`ecosystem of community integrations`: https://docs.ray.io/en/latest/ray-overview/ray-libraries.html\n\n\nWhy Ray?\n--------\n\nToday's ML workloads are increasingly compute-intensive. As convenient as they are, single-node development environments such as your laptop cannot scale to meet these demands.\n\nRay is a unified way to scale Python and AI applications from a laptop to a cluster.\n\nWith Ray, you can seamlessly scale the same code from a laptop to a cluster. Ray is designed to be general-purpose, meaning that it can performantly run any kind of workload. If your application is written in Python, you can scale it with Ray, no other infrastructure required.\n\nMore Information\n----------------\n\n- `Documentation`_\n- `Ray Architecture whitepaper`_\n- `Exoshuffle: large-scale data shuffle in Ray`_\n- `Ownership: a distributed futures system for fine-grained tasks`_\n- `RLlib paper`_\n- `Tune paper`_\n\n*Older documents:*\n\n- `Ray paper`_\n- `Ray HotOS paper`_\n- `Ray Architecture v1 whitepaper`_\n\n.. _`Ray AI Libraries`: https://docs.ray.io/en/latest/ray-air/getting-started.html\n.. _`Ray Core`: https://docs.ray.io/en/latest/ray-core/walkthrough.html\n.. _`Tasks`: https://docs.ray.io/en/latest/ray-core/tasks.html\n.. _`Actors`: https://docs.ray.io/en/latest/ray-core/actors.html\n.. _`Objects`: https://docs.ray.io/en/latest/ray-core/objects.html\n.. _`Documentation`: http://docs.ray.io/en/latest/index.html\n.. _`Ray Architecture v1 whitepaper`: https://docs.google.com/document/d/1lAy0Owi-vPz2jEqBSaHNQcy2IBSDEHyXNOQZlGuj93c/preview\n.. _`Ray Architecture whitepaper`: https://docs.google.com/document/d/1tBw9A4j62ruI5omIJbMxly-la5w4q_TjyJgJL_jN2fI/preview\n.. _`Exoshuffle: large-scale data shuffle in Ray`: https://arxiv.org/abs/2203.05072\n.. _`Ownership: a distributed futures system for fine-grained tasks`: https://www.usenix.org/system/files/nsdi21-wang.pdf\n.. _`Ray paper`: https://arxiv.org/abs/1712.05889\n.. _`Ray HotOS paper`: https://arxiv.org/abs/1703.03924\n.. _`RLlib paper`: https://arxiv.org/abs/1712.09381\n.. _`Tune paper`: https://arxiv.org/abs/1807.05118\n\nGetting Involved\n----------------\n\n.. list-table::\n   :widths: 25 50 25 25\n   :header-rows: 1\n\n   * - Platform\n     - Purpose\n     - Estimated Response Time\n     - Support Level\n   * - `Discourse Forum`_\n     - For discussions about development and questions about usage.\n     - < 1 day\n     - Community\n   * - `GitHub Issues`_\n     - For reporting bugs and filing feature requests.\n     - < 2 days\n     - Ray OSS Team\n   * - `Slack`_\n     - For collaborating with other Ray users.\n     - < 2 days\n     - Community\n   * - `StackOverflow`_\n     - For asking questions about how to use Ray.\n     - 3-5 days\n     - Community\n   * - `Meetup Group`_\n     - For learning about Ray projects and best practices.\n     - Monthly\n     - Ray DevRel\n   * - `Twitter`_\n     - For staying up-to-date on new features.\n     - Daily\n     - Ray DevRel\n\n.. _`Discourse Forum`: https://discuss.ray.io/\n.. _`GitHub Issues`: https://github.com/ray-project/ray/issues\n.. _`StackOverflow`: https://stackoverflow.com/questions/tagged/ray\n.. _`Meetup Group`: https://www.meetup.com/Bay-Area-Ray-Meetup/\n.. _`Twitter`: https://x.com/raydistributed\n.. _`Slack`: https://www.ray.io/join-slack?utm_source=github&utm_medium=ray_readme&utm_campaign=getting_involved\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 577603990,
    "name": "Open-Assistant",
    "full_name": "LAION-AI/Open-Assistant",
    "description": "OpenAssistant is a chat-based assistant that understands tasks, can interact with third-party systems, and retrieve information dynamically to do so.",
    "html_url": "https://github.com/LAION-AI/Open-Assistant",
    "clone_url": "https://github.com/LAION-AI/Open-Assistant.git",
    "owner_login": "LAION-AI",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/92627801?v=4",
    "stargazers_count": 37434,
    "watchers_count": 37434,
    "forks_count": 3289,
    "open_issues_count": 293,
    "size": 35477,
    "language": "Python",
    "languages": {
      "Python": 1851071,
      "TypeScript": 702999,
      "JavaScript": 21756,
      "Shell": 14985,
      "Mako": 1052,
      "HTML": 387,
      "Dockerfile": 296,
      "CSS": 100
    },
    "topics": [
      "ai",
      "assistant",
      "chatgpt",
      "discord-bot",
      "language-model",
      "machine-learning",
      "nextjs",
      "python",
      "rlhf"
    ],
    "license_name": "Apache License 2.0",
    "created_at": "2022-12-13T05:24:17+00:00",
    "updated_at": "2025-08-05T15:09:56+00:00",
    "pushed_at": "2024-08-17T01:55:35+00:00",
    "contributors_count": 100,
    "readme_length": 9283,
    "readme_content": "<h1 align=\"center\">\n    <span>Open-Assistant</span>\n  <img width=\"auto\" height=\"50px\" src=\"https://github.com/LAION-AI/Open-Assistant/blob/main/assets/logo_crop.png\"/>\n</h1>\n\n<blockquote>\n<p>:memo: <strong>NOTE</strong>: OpenAssistant is completed, and the project is now finished. Thank you to everyone who contributed! Check out our <a href=\"https://projects.laion.ai/Open-Assistant/blog/2023/10/25/open-assistant-is-completed\">blog post</a> for more information. The final published oasst2 dataset can be found on HuggingFace at <a href=\"https://huggingface.co/datasets/OpenAssistant/oasst2\">OpenAssistant/oasst2</a></p>\n</blockquote>\n\n<div align=\"center\">\n\n<a href=\"https://github.com/LAION-AI/Open-Assistant/stargazers\">![GitHub Repo stars](https://img.shields.io/github/stars/LAION-AI/Open-Assistant?style=social)</a>\n<a href=\"https://laion-ai.github.io/Open-Assistant/\">![Docs](https://img.shields.io/badge/docs-laion--ai.github.io%2FOpen--Assistant%2F-green)</a>\n<a href=\"https://github.com/LAION-AI/Open-Assistant/actions/workflows/build-frontend.yaml\">![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/LAION-AI/Open-Assistant/build-frontend.yaml?label=build-frontend)</a>\n<a href=\"https://github.com/LAION-AI/Open-Assistant/actions/workflows/build-postgres.yaml\">![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/LAION-AI/Open-Assistant/build-postgres.yaml?label=build-postgres)</a>\n<a href=\"https://github.com/LAION-AI/Open-Assistant/actions/workflows/pre-commit.yaml\">![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/LAION-AI/Open-Assistant/pre-commit.yaml?label=pre-commit)</a>\n<a href=\"https://github.com/LAION-AI/Open-Assistant/actions/workflows/test-api-contract.yaml\">![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/LAION-AI/Open-Assistant/test-api-contract.yaml?label=tests-api)</a>\n<a href=\"https://github.com/LAION-AI/Open-Assistant/actions/workflows/test-e2e.yaml\">![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/LAION-AI/Open-Assistant/test-e2e.yaml?label=tests-web)</a>\n<a href=\"https://github.com/LAION-AI/Open-Assistant/actions/workflows/deploy-docs-site.yaml\">![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/LAION-AI/Open-Assistant/deploy-docs-site.yaml?label=deploy-docs)</a>\n<a href=\"https://github.com/LAION-AI/Open-Assistant/actions/workflows/production-deploy.yaml\">![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/LAION-AI/Open-Assistant/production-deploy.yaml?label=deploy-production)</a>\n<a href=\"https://github.com/LAION-AI/Open-Assistant/actions/workflows/release.yaml\">![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/LAION-AI/Open-Assistant/release.yaml?label=deploy-release)</a>\n<a href=\"https://github.com/LAION-AI/Open-Assistant/releases\">![GitHub release (latest by date)](https://img.shields.io/github/v/release/LAION-AI/Open-Assistant)</a>\n<a href=\"https://github-com.translate.goog/LAION-AI/Open-Assistant/blob/main/README.md?_x_tr_sl=auto&_x_tr_tl=en&_x_tr_hl=en&_x_tr_pto=wapp\">![Translate](https://img.shields.io/badge/Translate-blue)</a>\n\n</div>\n\n# Table of Contents\n\n- [What is Open Assistant?](#what-is-open-assistant)\n- [Useful Links](#useful-links)\n- [How To Try It Out](#how-to-try-it-out)\n- [The Vision](#the-vision)\n- [The Plan](#the-plan)\n- [How You Can Help](#how-you-can-help)\n\n---\n\n## What is Open Assistant?\n\n<p align=\"center\">\nOpen Assistant is a project meant to give everyone access to a great chat based\nlarge language model.\n</p>\n\nWe believe that by doing this we will create a revolution in innovation in\nlanguage. In the same way that stable-diffusion helped the world make art and\nimages in new ways we hope Open Assistant can help improve the world by\nimproving language itself.\n\n# Useful Links\n\n- [Data Collection](https://open-assistant.io)\n\n- [Chat](https://open-assistant.io/chat)\n\n- [Project Documentation](https://projects.laion.ai/Open-Assistant/)\n\n## How To Try It Out\n\n### Chatting with the AI\n\nThe chat frontend is now live [here](https://open-assistant.io/chat). Log in and\nstart chatting! Please try to react with a thumbs up or down for the assistant's\nresponses when chatting.\n\n### Contributing to Data Collection\n\nThe data collection frontend is now live [here](https://open-assistant.io/). Log\nin and start taking on tasks! We want to collect a high volume of quality data.\nBy submitting, ranking, and labelling model prompts and responses you will be\ndirectly helping to improve the capabilities of Open Assistant.\n\n### Running the Development Setup Locally (without chat)\n\n**You do not need to run the project locally unless you are contributing to the\ndevelopment process. The website link above will take you to the public website\nwhere you can use the data collection app and the chat.**\n\nIf you would like to run the data collection app locally for development, you\ncan set up an entire stack needed to run **Open-Assistant**, including the\nwebsite, backend, and associated dependent services, with Docker.\n\nTo start the demo, run this in the root directory of the repository (check\n[this FAQ](https://projects.laion.ai/Open-Assistant/docs/faq#docker-compose-instead-of-docker-compose)\nif you have problems):\n\n```sh\ndocker compose --profile ci up --build --attach-dependencies\n```\n\n> **Note:** when running on MacOS with an M1 chip you have to use:\n> `DB_PLATFORM=linux/x86_64 docker compose ...`\n\nThen, navigate to `http://localhost:3000` (It may take some time to boot up) and\ninteract with the website.\n\n> **Note:** If an issue occurs with the build, please head to the\n> [FAQ](https://projects.laion.ai/Open-Assistant/docs/faq) and check out the\n> entries about Docker.\n\n> **Note:** When logging in via email, navigate to `http://localhost:1080` to\n> get the magic email login link.\n\n> **Note:** If you would like to run this in a standardized development\n> environment (a\n> [\"devcontainer\"](https://code.visualstudio.com/docs/devcontainers/containers))\n> using\n> [vscode locally](https://code.visualstudio.com/docs/devcontainers/create-dev-container#_create-a-devcontainerjson-file)\n> or in a web browser using\n> [GitHub Codespaces](https://github.com/features/codespaces), you can use the\n> provided [`.devcontainer`](.devcontainer/) folder.\n\n### Running the Development Setup Locally for Chat\n\n**You do not need to run the project locally unless you are contributing to the\ndevelopment process. The website link above will take you to the public website\nwhere you can use the data collection app and the chat.**\n\n**Also note that the local setup is only for development and is not meant to be\nused as a local chatbot, unless you know what you are doing.**\n\nIf you _do_ know what you are doing, then see the `inference` folder for getting\nthe inference system up and running, or have a look at `--profile inference` in\naddition to `--profile ci` in the above command.\n\n## The Vision\n\nWe are not going to stop at replicating ChatGPT. We want to build the assistant\nof the future, able to not only write email and cover letters, but do meaningful\nwork, use APIs, dynamically research information, and much more, with the\nability to be personalized and extended by anyone. And we want to do this in a\nway that is open and accessible, which means we must not only build a great\nassistant, but also make it small and efficient enough to run on consumer\nhardware.\n\n## The Plan\n\n##### We want to get to an initial MVP as fast as possible, by following the 3-steps outlined in the [InstructGPT paper](https://arxiv.org/abs/2203.02155)\n\n1. Collect high-quality human generated Instruction-Fulfillment samples\n   (prompt + response), goal >50k. We design a crowdsourced process to collect\n   and reviewed prompts. We do not want to train on\n   flooding/toxic/spam/junk/personal information data. We will have a\n   leaderboard to motivate the community that shows progress and the most active\n   users. Swag will be given to the top-contributors.\n2. For each of the collected prompts we will sample multiple completions.\n   Completions of one prompt will then be shown randomly to users to rank them\n   from best to worst. Again this should happen crowd-sourced, e.g. we need to\n   deal with unreliable potentially malicious users. At least multiple votes by\n   independent users have to be collected to measure the overall agreement. The\n   gathered ranking-data will be used to train a reward model.\n3. Now follows the RLHF training phase based on the prompts and the reward\n   model.\n\nWe can then take the resulting model and continue with completion sampling step\n2 for a next iteration.\n\n### Slide Decks\n\n[Vision & Roadmap](https://docs.google.com/presentation/d/1n7IrAOVOqwdYgiYrXc8Sj0He8krn5MVZO_iLkCjTtu0/edit?usp=sharing)\n\n[Important Data Structures](https://docs.google.com/presentation/d/1iaX_nxasVWlvPiSNs0cllR9L_1neZq0RJxd6MFEalUY/edit?usp=sharing)\n\n## How You Can Help\n\nAll open source projects begin with people like you. Open source is the belief\nthat if we collaborate we can together gift our knowledge and technology to the\nworld for the benefit of humanity.\n\nCheck out our [contributing guide](CONTRIBUTING.md) to get started.\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 571613891,
    "name": "supervision",
    "full_name": "roboflow/supervision",
    "description": "We write your reusable computer vision tools. \ud83d\udc9c",
    "html_url": "https://github.com/roboflow/supervision",
    "clone_url": "https://github.com/roboflow/supervision.git",
    "owner_login": "roboflow",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/53104118?v=4",
    "stargazers_count": 33339,
    "watchers_count": 33339,
    "forks_count": 2685,
    "open_issues_count": 124,
    "size": 2547985,
    "language": "Python",
    "languages": {
      "Python": 1225715
    },
    "topics": [
      "classification",
      "coco",
      "computer-vision",
      "deep-learning",
      "hacktoberfest",
      "image-processing",
      "instance-segmentation",
      "low-code",
      "machine-learning",
      "metrics",
      "object-detection",
      "oriented-bounding-box",
      "pascal-voc",
      "python",
      "pytorch",
      "tensorflow",
      "tracking",
      "video-processing",
      "yolo"
    ],
    "license_name": "MIT License",
    "created_at": "2022-11-28T14:08:44+00:00",
    "updated_at": "2025-08-06T02:04:33+00:00",
    "pushed_at": "2025-08-06T01:36:50+00:00",
    "contributors_count": 100,
    "readme_length": 11840,
    "readme_content": "<div align=\"center\">\n  <p>\n    <a align=\"center\" href=\"\" target=\"https://supervision.roboflow.com\">\n      <img\n        width=\"100%\"\n        src=\"https://media.roboflow.com/open-source/supervision/rf-supervision-banner.png?updatedAt=1678995927529\"\n      >\n    </a>\n  </p>\n\n<br>\n\n[notebooks](https://github.com/roboflow/notebooks) | [inference](https://github.com/roboflow/inference) | [autodistill](https://github.com/autodistill/autodistill) | [maestro](https://github.com/roboflow/multimodal-maestro)\n\n<br>\n\n[![version](https://badge.fury.io/py/supervision.svg)](https://badge.fury.io/py/supervision)\n[![downloads](https://img.shields.io/pypi/dm/supervision)](https://pypistats.org/packages/supervision)\n[![snyk](https://snyk.io/advisor/python/supervision/badge.svg)](https://snyk.io/advisor/python/supervision)\n[![license](https://img.shields.io/pypi/l/supervision)](https://github.com/roboflow/supervision/blob/main/LICENSE.md)\n[![python-version](https://img.shields.io/pypi/pyversions/supervision)](https://badge.fury.io/py/supervision)\n[![colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow/supervision/blob/main/demo.ipynb)\n[![gradio](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/Roboflow/Annotators)\n[![discord](https://img.shields.io/discord/1159501506232451173?logo=discord&label=discord&labelColor=fff&color=5865f2&link=https%3A%2F%2Fdiscord.gg%2FGbfgXGJ8Bk)](https://discord.gg/GbfgXGJ8Bk)\n[![built-with-material-for-mkdocs](https://img.shields.io/badge/Material_for_MkDocs-526CFE?logo=MaterialForMkDocs&logoColor=white)](https://squidfunk.github.io/mkdocs-material/)\n\n  <div align=\"center\">\n    <a href=\"https://trendshift.io/repositories/124\"  target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/124\" alt=\"roboflow%2Fsupervision | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n  </div>\n\n</div>\n\n## \ud83d\udc4b hello\n\n**We write your reusable computer vision tools.** Whether you need to load your dataset from your hard drive, draw detections on an image or video, or count how many detections are in a zone. You can count on us! \ud83e\udd1d\n\n## \ud83d\udcbb install\n\nPip install the supervision package in a\n[**Python>=3.9**](https://www.python.org/) environment.\n\n```bash\npip install supervision\n```\n\nRead more about conda, mamba, and installing from source in our [guide](https://roboflow.github.io/supervision/).\n\n## \ud83d\udd25 quickstart\n\n### models\n\nSupervision was designed to be model agnostic. Just plug in any classification, detection, or segmentation model. For your convenience, we have created [connectors](https://supervision.roboflow.com/latest/detection/core/#detections) for the most popular libraries like Ultralytics, Transformers, or MMDetection.\n\n```python\nimport cv2\nimport supervision as sv\nfrom ultralytics import YOLO\n\nimage = cv2.imread(...)\nmodel = YOLO(\"yolov8s.pt\")\nresult = model(image)[0]\ndetections = sv.Detections.from_ultralytics(result)\n\nlen(detections)\n# 5\n```\n\n<details>\n<summary>\ud83d\udc49 more model connectors</summary>\n\n- inference\n\n  Running with [Inference](https://github.com/roboflow/inference) requires a [Roboflow API KEY](https://docs.roboflow.com/api-reference/authentication#retrieve-an-api-key).\n\n  ```python\n  import cv2\n  import supervision as sv\n  from inference import get_model\n\n  image = cv2.imread(...)\n  model = get_model(model_id=\"yolov8s-640\", api_key=<ROBOFLOW API KEY>)\n  result = model.infer(image)[0]\n  detections = sv.Detections.from_inference(result)\n\n  len(detections)\n  # 5\n  ```\n\n</details>\n\n### annotators\n\nSupervision offers a wide range of highly customizable [annotators](https://supervision.roboflow.com/latest/detection/annotators/), allowing you to compose the perfect visualization for your use case.\n\n```python\nimport cv2\nimport supervision as sv\n\nimage = cv2.imread(...)\ndetections = sv.Detections(...)\n\nbox_annotator = sv.BoxAnnotator()\nannotated_frame = box_annotator.annotate(\n  scene=image.copy(),\n  detections=detections)\n```\n\nhttps://github.com/roboflow/supervision/assets/26109316/691e219c-0565-4403-9218-ab5644f39bce\n\n### datasets\n\nSupervision provides a set of [utils](https://supervision.roboflow.com/latest/datasets/core/) that allow you to load, split, merge, and save datasets in one of the supported formats.\n\n```python\nimport supervision as sv\nfrom roboflow import Roboflow\n\nproject = Roboflow().workspace(<WORKSPACE_ID>).project(<PROJECT_ID>)\ndataset = project.version(<PROJECT_VERSION>).download(\"coco\")\n\nds = sv.DetectionDataset.from_coco(\n    images_directory_path=f\"{dataset.location}/train\",\n    annotations_path=f\"{dataset.location}/train/_annotations.coco.json\",\n)\n\npath, image, annotation = ds[0]\n    # loads image on demand\n\nfor path, image, annotation in ds:\n    # loads image on demand\n```\n\n<details close>\n<summary>\ud83d\udc49 more dataset utils</summary>\n\n- load\n\n  ```python\n  dataset = sv.DetectionDataset.from_yolo(\n      images_directory_path=...,\n      annotations_directory_path=...,\n      data_yaml_path=...\n  )\n\n  dataset = sv.DetectionDataset.from_pascal_voc(\n      images_directory_path=...,\n      annotations_directory_path=...\n  )\n\n  dataset = sv.DetectionDataset.from_coco(\n      images_directory_path=...,\n      annotations_path=...\n  )\n  ```\n\n- split\n\n  ```python\n  train_dataset, test_dataset = dataset.split(split_ratio=0.7)\n  test_dataset, valid_dataset = test_dataset.split(split_ratio=0.5)\n\n  len(train_dataset), len(test_dataset), len(valid_dataset)\n  # (700, 150, 150)\n  ```\n\n- merge\n\n  ```python\n  ds_1 = sv.DetectionDataset(...)\n  len(ds_1)\n  # 100\n  ds_1.classes\n  # ['dog', 'person']\n\n  ds_2 = sv.DetectionDataset(...)\n  len(ds_2)\n  # 200\n  ds_2.classes\n  # ['cat']\n\n  ds_merged = sv.DetectionDataset.merge([ds_1, ds_2])\n  len(ds_merged)\n  # 300\n  ds_merged.classes\n  # ['cat', 'dog', 'person']\n  ```\n\n- save\n\n  ```python\n  dataset.as_yolo(\n      images_directory_path=...,\n      annotations_directory_path=...,\n      data_yaml_path=...\n  )\n\n  dataset.as_pascal_voc(\n      images_directory_path=...,\n      annotations_directory_path=...\n  )\n\n  dataset.as_coco(\n      images_directory_path=...,\n      annotations_path=...\n  )\n  ```\n\n- convert\n\n  ```python\n  sv.DetectionDataset.from_yolo(\n      images_directory_path=...,\n      annotations_directory_path=...,\n      data_yaml_path=...\n  ).as_pascal_voc(\n      images_directory_path=...,\n      annotations_directory_path=...\n  )\n  ```\n\n</details>\n\n## \ud83c\udfac tutorials\n\nWant to learn how to use Supervision? Explore our [how-to guides](https://supervision.roboflow.com/develop/how_to/detect_and_annotate/), [end-to-end examples](https://github.com/roboflow/supervision/tree/develop/examples), [cheatsheet](https://roboflow.github.io/cheatsheet-supervision/), and [cookbooks](https://supervision.roboflow.com/develop/cookbooks/)!\n\n<br/>\n\n<p align=\"left\">\n<a href=\"https://youtu.be/hAWpsIuem10\" title=\"Dwell Time Analysis with Computer Vision | Real-Time Stream Processing\"><img src=\"https://github.com/SkalskiP/SkalskiP/assets/26109316/a742823d-c158-407d-b30f-063a5d11b4e1\" alt=\"Dwell Time Analysis with Computer Vision | Real-Time Stream Processing\" width=\"300px\" align=\"left\" /></a>\n<a href=\"https://youtu.be/hAWpsIuem10\" title=\"Dwell Time Analysis with Computer Vision | Real-Time Stream Processing\"><strong>Dwell Time Analysis with Computer Vision | Real-Time Stream Processing</strong></a>\n<div><strong>Created: 5 Apr 2024</strong></div>\n<br/>Learn how to use computer vision to analyze wait times and optimize processes. This tutorial covers object detection, tracking, and calculating time spent in designated zones. Use these techniques to improve customer experience in retail, traffic management, or other scenarios.</p>\n\n<br/>\n\n<p align=\"left\">\n<a href=\"https://youtu.be/uWP6UjDeZvY\" title=\"Speed Estimation & Vehicle Tracking | Computer Vision | Open Source\"><img src=\"https://github.com/SkalskiP/SkalskiP/assets/26109316/61a444c8-b135-48ce-b979-2a5ab47c5a91\" alt=\"Speed Estimation & Vehicle Tracking | Computer Vision | Open Source\" width=\"300px\" align=\"left\" /></a>\n<a href=\"https://youtu.be/uWP6UjDeZvY\" title=\"Speed Estimation & Vehicle Tracking | Computer Vision | Open Source\"><strong>Speed Estimation & Vehicle Tracking | Computer Vision | Open Source</strong></a>\n<div><strong>Created: 11 Jan 2024</strong></div>\n<br/>Learn how to track and estimate the speed of vehicles using YOLO, ByteTrack, and Roboflow Inference. This comprehensive tutorial covers object detection, multi-object tracking, filtering detections, perspective transformation, speed estimation, visualization improvements, and more.</p>\n\n## \ud83d\udc9c built with supervision\n\nDid you build something cool using supervision? [Let us know!](https://github.com/roboflow/supervision/discussions/categories/built-with-supervision)\n\nhttps://user-images.githubusercontent.com/26109316/207858600-ee862b22-0353-440b-ad85-caa0c4777904.mp4\n\nhttps://github.com/roboflow/supervision/assets/26109316/c9436828-9fbf-4c25-ae8c-60e9c81b3900\n\nhttps://github.com/roboflow/supervision/assets/26109316/3ac6982f-4943-4108-9b7f-51787ef1a69f\n\n## \ud83d\udcda documentation\n\nVisit our [documentation](https://roboflow.github.io/supervision) page to learn how supervision can help you build computer vision applications faster and more reliably.\n\n## \ud83c\udfc6 contribution\n\nWe love your input! Please see our [contributing guide](https://github.com/roboflow/supervision/blob/main/CONTRIBUTING.md) to get started. Thank you \ud83d\ude4f to all our contributors!\n\n<p align=\"center\">\n    <a href=\"https://github.com/roboflow/supervision/graphs/contributors\">\n      <img src=\"https://contrib.rocks/image?repo=roboflow/supervision\" />\n    </a>\n</p>\n\n<br>\n\n<div align=\"center\">\n\n<div align=\"center\">\n      <a href=\"https://youtube.com/roboflow\">\n          <img\n            src=\"https://media.roboflow.com/notebooks/template/icons/purple/youtube.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949634652\"\n            width=\"3%\"\n          />\n      </a>\n      <img src=\"https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png\" width=\"3%\"/>\n      <a href=\"https://roboflow.com\">\n          <img\n            src=\"https://media.roboflow.com/notebooks/template/icons/purple/roboflow-app.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949746649\"\n            width=\"3%\"\n          />\n      </a>\n      <img src=\"https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png\" width=\"3%\"/>\n      <a href=\"https://www.linkedin.com/company/roboflow-ai/\">\n          <img\n            src=\"https://media.roboflow.com/notebooks/template/icons/purple/linkedin.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949633691\"\n            width=\"3%\"\n          />\n      </a>\n      <img src=\"https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png\" width=\"3%\"/>\n      <a href=\"https://docs.roboflow.com\">\n          <img\n            src=\"https://media.roboflow.com/notebooks/template/icons/purple/knowledge.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949634511\"\n            width=\"3%\"\n          />\n      </a>\n      <img src=\"https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png\" width=\"3%\"/>\n      <a href=\"https://discuss.roboflow.com\">\n          <img\n            src=\"https://media.roboflow.com/notebooks/template/icons/purple/forum.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949633584\"\n            width=\"3%\"\n          />\n      <img src=\"https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png\" width=\"3%\"/>\n      <a href=\"https://blog.roboflow.com\">\n          <img\n            src=\"https://media.roboflow.com/notebooks/template/icons/purple/blog.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949633605\"\n            width=\"3%\"\n          />\n      </a>\n      </a>\n  </div>\n</div>\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 21467110,
    "name": "spaCy",
    "full_name": "explosion/spaCy",
    "description": "\ud83d\udcab Industrial-strength Natural Language Processing (NLP) in Python",
    "html_url": "https://github.com/explosion/spaCy",
    "clone_url": "https://github.com/explosion/spaCy.git",
    "owner_login": "explosion",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/20011530?v=4",
    "stargazers_count": 32095,
    "watchers_count": 32095,
    "forks_count": 4552,
    "open_issues_count": 195,
    "size": 203297,
    "language": "Python",
    "languages": {
      "Python": 4076046,
      "MDX": 2351912,
      "Cython": 791001,
      "JavaScript": 197404,
      "Sass": 57825,
      "TypeScript": 26865,
      "Jinja": 16352,
      "C": 9571,
      "HTML": 6927,
      "Makefile": 1512,
      "Shell": 1417,
      "Dockerfile": 425,
      "C++": 187
    },
    "topics": [
      "ai",
      "artificial-intelligence",
      "cython",
      "data-science",
      "deep-learning",
      "entity-linking",
      "machine-learning",
      "named-entity-recognition",
      "natural-language-processing",
      "neural-network",
      "neural-networks",
      "nlp",
      "nlp-library",
      "python",
      "spacy",
      "text-classification",
      "tokenization"
    ],
    "license_name": "MIT License",
    "created_at": "2014-07-03T15:15:40+00:00",
    "updated_at": "2025-08-06T01:31:04+00:00",
    "pushed_at": "2025-05-28T15:28:05+00:00",
    "contributors_count": 100,
    "readme_length": 23666,
    "readme_content": "<a href=\"https://explosion.ai\"><img src=\"https://explosion.ai/assets/img/logo.svg\" width=\"125\" height=\"125\" align=\"right\" /></a>\n\n# spaCy: Industrial-strength NLP\n\nspaCy is a library for **advanced Natural Language Processing** in Python and\nCython. It's built on the very latest research, and was designed from day one to\nbe used in real products.\n\nspaCy comes with [pretrained pipelines](https://spacy.io/models) and currently\nsupports tokenization and training for **70+ languages**. It features\nstate-of-the-art speed and **neural network models** for tagging, parsing,\n**named entity recognition**, **text classification** and more, multi-task\nlearning with pretrained **transformers** like BERT, as well as a\nproduction-ready [**training system**](https://spacy.io/usage/training) and easy\nmodel packaging, deployment and workflow management. spaCy is commercial\nopen-source software, released under the\n[MIT license](https://github.com/explosion/spaCy/blob/master/LICENSE).\n\n\ud83d\udcab **Version 3.8 out now!**\n[Check out the release notes here.](https://github.com/explosion/spaCy/releases)\n\n[![tests](https://github.com/explosion/spaCy/actions/workflows/tests.yml/badge.svg)](https://github.com/explosion/spaCy/actions/workflows/tests.yml)\n[![Current Release Version](https://img.shields.io/github/release/explosion/spacy.svg?style=flat-square&logo=github)](https://github.com/explosion/spaCy/releases)\n[![pypi Version](https://img.shields.io/pypi/v/spacy.svg?style=flat-square&logo=pypi&logoColor=white)](https://pypi.org/project/spacy/)\n[![conda Version](https://img.shields.io/conda/vn/conda-forge/spacy.svg?style=flat-square&logo=conda-forge&logoColor=white)](https://anaconda.org/conda-forge/spacy)\n[![Python wheels](https://img.shields.io/badge/wheels-%E2%9C%93-4c1.svg?longCache=true&style=flat-square&logo=python&logoColor=white)](https://github.com/explosion/wheelwright/releases)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg?style=flat-square)](https://github.com/ambv/black)\n<br />\n[![PyPi downloads](https://static.pepy.tech/personalized-badge/spacy?period=total&units=international_system&left_color=grey&right_color=orange&left_text=pip%20downloads)](https://pypi.org/project/spacy/)\n[![Conda downloads](https://img.shields.io/conda/dn/conda-forge/spacy?label=conda%20downloads)](https://anaconda.org/conda-forge/spacy)\n\n## \ud83d\udcd6 Documentation\n\n| Documentation                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                              |\n| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| \u2b50\ufe0f **[spaCy 101]**                                                                                                                                                                                                       | New to spaCy? Here's everything you need to know!                                                                                                                                                                                                                                                                                            |\n| \ud83d\udcda **[Usage Guides]**                                                                                                                                                                                                     | How to use spaCy and its features.                                                                                                                                                                                                                                                                                                           |\n| \ud83d\ude80 **[New in v3.0]**                                                                                                                                                                                                      | New features, backwards incompatibilities and migration guide.                                                                                                                                                                                                                                                                               |\n| \ud83e\ude90 **[Project Templates]**                                                                                                                                                                                                | End-to-end workflows you can clone, modify and run.                                                                                                                                                                                                                                                                                          |\n| \ud83c\udf9b **[API Reference]**                                                                                                                                                                                                     | The detailed reference for spaCy's API.                                                                                                                                                                                                                                                                                                      |\n| \u23e9 **[GPU Processing]**                                                                                                                                                                                                    | Use spaCy with CUDA-compatible GPU processing.                                                                                                                                                                                                                                                                                               |\n| \ud83d\udce6 **[Models]**                                                                                                                                                                                                           | Download trained pipelines for spaCy.                                                                                                                                                                                                                                                                                                        |\n| \ud83e\udd99 **[Large Language Models]**                                                                                                                                                                                            | Integrate LLMs into spaCy pipelines.                                                                                                                                                                                                                                                                                                        |\n| \ud83c\udf0c **[Universe]**                                                                                                                                                                                                         | Plugins, extensions, demos and books from the spaCy ecosystem.                                                                                                                                                                                                                                                                               |\n| \u2699\ufe0f **[spaCy VS Code Extension]**                                                                                                                                                                                          | Additional tooling and features for working with spaCy's config files.                                                                                                                                                                                                                                                                       |\n| \ud83d\udc69\u200d\ud83c\udfeb **[Online Course]**                                                                                                                                                                                                    | Learn spaCy in this free and interactive online course.                                                                                                                                                                                                                                                                                      |\n| \ud83d\udcf0 **[Blog]**                                                                                                                                                                                                             | Read about current spaCy and Prodigy development, releases, talks and more from Explosion.                                                                                                                                                                                                                 |\n| \ud83d\udcfa **[Videos]**                                                                                                                                                                                                           | Our YouTube channel with video tutorials, talks and more.                                                                                                                                                                                                                                                                                    |\n| \ud83d\udd34 **[Live Stream]**                                                                                                                                                                                                       | Join Matt as he works on spaCy and chat about NLP, live every week.                                                                                                                                                                                                                                                                         |\n| \ud83d\udee0 **[Changelog]**                                                                                                                                                                                                         | Changes and version history.                                                                                                                                                                                                                                                                                                                 |\n| \ud83d\udc9d **[Contribute]**                                                                                                                                                                                                       | How to contribute to the spaCy project and code base.                                                                                                                                                                                                                                                                                        |\n| \ud83d\udc55 **[Swag]**                                                                                                                                                                                                             | Support us and our work with unique, custom-designed swag!                                                                                                                                                                                                                                                                                   |\n| <a href=\"https://explosion.ai/tailored-solutions\"><img src=\"https://github.com/explosion/spaCy/assets/13643239/36d2a42e-98c0-4599-90e1-788ef75181be\" width=\"150\" alt=\"Tailored Solutions\"/></a> | Custom NLP consulting, implementation and strategic advice by spaCy\u2019s core development team. Streamlined, production-ready, predictable and maintainable. Send us an email or take our 5-minute questionnaire, and well'be in touch! **[Learn more &rarr;](https://explosion.ai/tailored-solutions)**                 |\n\n[spacy 101]: https://spacy.io/usage/spacy-101\n[new in v3.0]: https://spacy.io/usage/v3\n[usage guides]: https://spacy.io/usage/\n[api reference]: https://spacy.io/api/\n[gpu processing]: https://spacy.io/usage#gpu\n[models]: https://spacy.io/models\n[large language models]: https://spacy.io/usage/large-language-models\n[universe]: https://spacy.io/universe\n[spacy vs code extension]: https://github.com/explosion/spacy-vscode\n[videos]: https://www.youtube.com/c/ExplosionAI\n[live stream]: https://www.youtube.com/playlist?list=PLBmcuObd5An5_iAxNYLJa_xWmNzsYce8c\n[online course]: https://course.spacy.io\n[blog]: https://explosion.ai\n[project templates]: https://github.com/explosion/projects\n[changelog]: https://spacy.io/usage#changelog\n[contribute]: https://github.com/explosion/spaCy/blob/master/CONTRIBUTING.md\n[swag]: https://explosion.ai/merch\n\n## \ud83d\udcac Where to ask questions\n\nThe spaCy project is maintained by the [spaCy team](https://explosion.ai/about).\nPlease understand that we won't be able to provide individual support via email.\nWe also believe that help is much more valuable if it's shared publicly, so that\nmore people can benefit from it.\n\n| Type                            | Platforms                               |\n| ------------------------------- | --------------------------------------- |\n| \ud83d\udea8 **Bug Reports**              | [GitHub Issue Tracker]                  |\n| \ud83c\udf81 **Feature Requests & Ideas** | [GitHub Discussions] \u00b7 [Live Stream]    |\n| \ud83d\udc69\u200d\ud83d\udcbb **Usage Questions**          | [GitHub Discussions] \u00b7 [Stack Overflow] |\n| \ud83d\uddef **General Discussion**        | [GitHub Discussions] \u00b7 [Live Stream]   |\n\n[github issue tracker]: https://github.com/explosion/spaCy/issues\n[github discussions]: https://github.com/explosion/spaCy/discussions\n[stack overflow]: https://stackoverflow.com/questions/tagged/spacy\n[live stream]: https://www.youtube.com/playlist?list=PLBmcuObd5An5_iAxNYLJa_xWmNzsYce8c\n\n## Features\n\n- Support for **70+ languages**\n- **Trained pipelines** for different languages and tasks\n- Multi-task learning with pretrained **transformers** like BERT\n- Support for pretrained **word vectors** and embeddings\n- State-of-the-art speed\n- Production-ready **training system**\n- Linguistically-motivated **tokenization**\n- Components for named **entity recognition**, part-of-speech-tagging,\n  dependency parsing, sentence segmentation, **text classification**,\n  lemmatization, morphological analysis, entity linking and more\n- Easily extensible with **custom components** and attributes\n- Support for custom models in **PyTorch**, **TensorFlow** and other frameworks\n- Built in **visualizers** for syntax and NER\n- Easy **model packaging**, deployment and workflow management\n- Robust, rigorously evaluated accuracy\n\n\ud83d\udcd6 **For more details, see the\n[facts, figures and benchmarks](https://spacy.io/usage/facts-figures).**\n\n## \u23f3 Install spaCy\n\nFor detailed installation instructions, see the\n[documentation](https://spacy.io/usage).\n\n- **Operating system**: macOS / OS X \u00b7 Linux \u00b7 Windows (Cygwin, MinGW, Visual\n  Studio)\n- **Python version**: Python >=3.7, <3.13 (only 64 bit)\n- **Package managers**: [pip] \u00b7 [conda] (via `conda-forge`)\n\n[pip]: https://pypi.org/project/spacy/\n[conda]: https://anaconda.org/conda-forge/spacy\n\n### pip\n\nUsing pip, spaCy releases are available as source packages and binary wheels.\nBefore you install spaCy and its dependencies, make sure that your `pip`,\n`setuptools` and `wheel` are up to date.\n\n```bash\npip install -U pip setuptools wheel\npip install spacy\n```\n\nTo install additional data tables for lemmatization and normalization you can\nrun `pip install spacy[lookups]` or install\n[`spacy-lookups-data`](https://github.com/explosion/spacy-lookups-data)\nseparately. The lookups package is needed to create blank models with\nlemmatization data, and to lemmatize in languages that don't yet come with\npretrained models and aren't powered by third-party libraries.\n\nWhen using pip it is generally recommended to install packages in a virtual\nenvironment to avoid modifying system state:\n\n```bash\npython -m venv .env\nsource .env/bin/activate\npip install -U pip setuptools wheel\npip install spacy\n```\n\n### conda\n\nYou can also install spaCy from `conda` via the `conda-forge` channel. For the\nfeedstock including the build recipe and configuration, check out\n[this repository](https://github.com/conda-forge/spacy-feedstock).\n\n```bash\nconda install -c conda-forge spacy\n```\n\n### Updating spaCy\n\nSome updates to spaCy may require downloading new statistical models. If you're\nrunning spaCy v2.0 or higher, you can use the `validate` command to check if\nyour installed models are compatible and if not, print details on how to update\nthem:\n\n```bash\npip install -U spacy\npython -m spacy validate\n```\n\nIf you've trained your own models, keep in mind that your training and runtime\ninputs must match. After updating spaCy, we recommend **retraining your models**\nwith the new version.\n\n\ud83d\udcd6 **For details on upgrading from spaCy 2.x to spaCy 3.x, see the\n[migration guide](https://spacy.io/usage/v3#migrating).**\n\n## \ud83d\udce6 Download model packages\n\nTrained pipelines for spaCy can be installed as **Python packages**. This means\nthat they're a component of your application, just like any other module. Models\ncan be installed using spaCy's [`download`](https://spacy.io/api/cli#download)\ncommand, or manually by pointing pip to a path or URL.\n\n| Documentation              |                                                                  |\n| -------------------------- | ---------------------------------------------------------------- |\n| **[Available Pipelines]**  | Detailed pipeline descriptions, accuracy figures and benchmarks. |\n| **[Models Documentation]** | Detailed usage and installation instructions.                    |\n| **[Training]**             | How to train your own pipelines on your data.                    |\n\n[available pipelines]: https://spacy.io/models\n[models documentation]: https://spacy.io/usage/models\n[training]: https://spacy.io/usage/training\n\n```bash\n# Download best-matching version of specific model for your spaCy installation\npython -m spacy download en_core_web_sm\n\n# pip install .tar.gz archive or .whl from path or URL\npip install /Users/you/en_core_web_sm-3.0.0.tar.gz\npip install /Users/you/en_core_web_sm-3.0.0-py3-none-any.whl\npip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz\n```\n\n### Loading and using models\n\nTo load a model, use [`spacy.load()`](https://spacy.io/api/top-level#spacy.load)\nwith the model name or a path to the model data directory.\n\n```python\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"This is a sentence.\")\n```\n\nYou can also `import` a model directly via its full name and then call its\n`load()` method with no arguments.\n\n```python\nimport spacy\nimport en_core_web_sm\n\nnlp = en_core_web_sm.load()\ndoc = nlp(\"This is a sentence.\")\n```\n\n\ud83d\udcd6 **For more info and examples, check out the\n[models documentation](https://spacy.io/docs/usage/models).**\n\n## \u2692 Compile from source\n\nThe other way to install spaCy is to clone its\n[GitHub repository](https://github.com/explosion/spaCy) and build it from\nsource. That is the common way if you want to make changes to the code base.\nYou'll need to make sure that you have a development environment consisting of a\nPython distribution including header files, a compiler,\n[pip](https://pip.pypa.io/en/latest/installing/),\n[virtualenv](https://virtualenv.pypa.io/en/latest/) and\n[git](https://git-scm.com) installed. The compiler part is the trickiest. How to\ndo that depends on your system.\n\n| Platform    |                                                                                                                                                                                                                                                                     |\n| ----------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| **Ubuntu**  | Install system-level dependencies via `apt-get`: `sudo apt-get install build-essential python-dev git` .                                                                                                                                                            |\n| **Mac**     | Install a recent version of [XCode](https://developer.apple.com/xcode/), including the so-called \"Command Line Tools\". macOS and OS X ship with Python and git preinstalled.                                                                                        |\n| **Windows** | Install a version of the [Visual C++ Build Tools](https://visualstudio.microsoft.com/visual-cpp-build-tools/) or [Visual Studio Express](https://visualstudio.microsoft.com/vs/express/) that matches the version that was used to compile your Python interpreter. |\n\nFor more details and instructions, see the documentation on\n[compiling spaCy from source](https://spacy.io/usage#source) and the\n[quickstart widget](https://spacy.io/usage#section-quickstart) to get the right\ncommands for your platform and Python version.\n\n```bash\ngit clone https://github.com/explosion/spaCy\ncd spaCy\n\npython -m venv .env\nsource .env/bin/activate\n\n# make sure you are using the latest pip\npython -m pip install -U pip setuptools wheel\n\npip install -r requirements.txt\npip install --no-build-isolation --editable .\n```\n\nTo install with extras:\n\n```bash\npip install --no-build-isolation --editable .[lookups,cuda102]\n```\n\n## \ud83d\udea6 Run tests\n\nspaCy comes with an [extensive test suite](spacy/tests). In order to run the\ntests, you'll usually want to clone the repository and build spaCy from source.\nThis will also install the required development dependencies and test utilities\ndefined in the [`requirements.txt`](requirements.txt).\n\nAlternatively, you can run `pytest` on the tests from within the installed\n`spacy` package. Don't forget to also install the test utilities via spaCy's\n[`requirements.txt`](requirements.txt):\n\n```bash\npip install -r requirements.txt\npython -m pytest --pyargs spacy\n```\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 178626720,
    "name": "pytorch-lightning",
    "full_name": "Lightning-AI/pytorch-lightning",
    "description": "Pretrain, finetune ANY AI model of ANY size on multiple GPUs, TPUs with zero code changes.",
    "html_url": "https://github.com/Lightning-AI/pytorch-lightning",
    "clone_url": "https://github.com/Lightning-AI/pytorch-lightning.git",
    "owner_login": "Lightning-AI",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/58386951?v=4",
    "stargazers_count": 29905,
    "watchers_count": 29905,
    "forks_count": 3551,
    "open_issues_count": 1037,
    "size": 132913,
    "language": "Python",
    "languages": {
      "Python": 4778589,
      "Shell": 10985,
      "Dockerfile": 10841,
      "Makefile": 1822
    },
    "topics": [
      "ai",
      "artificial-intelligence",
      "data-science",
      "deep-learning",
      "machine-learning",
      "python",
      "pytorch"
    ],
    "license_name": "Apache License 2.0",
    "created_at": "2019-03-31T00:45:57+00:00",
    "updated_at": "2025-08-05T20:38:41+00:00",
    "pushed_at": "2025-08-05T14:15:03+00:00",
    "contributors_count": 100,
    "readme_length": 26528,
    "readme_content": "<div align=\"center\">\n\n<img alt=\"Lightning\" src=\"https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/ptl_banner.png\" width=\"800px\" style=\"max-width: 100%;\">\n\n<br/>\n<br/>\n\n**The deep learning framework to pretrain, finetune and deploy AI models.**\n\n**NEW- Deploying models? Check out [LitServe](https://github.com/Lightning-AI/litserve), the PyTorch Lightning for model serving**\n\n______________________________________________________________________\n\n<p align=\"center\">\n    <a href=\"#quick-start\" style=\"margin: 0 10px;\">Quick start</a> \u2022\n  <a href=\"#examples\">Examples</a> \u2022\n  <a href=\"#why-pytorch-lightning\">PyTorch Lightning</a> \u2022\n  <a href=\"#lightning-fabric-expert-control\">Fabric</a> \u2022\n  <a href=\"https://lightning.ai/\">Lightning AI</a> \u2022   \n  <a href=\"#community\">Community</a> \u2022\n  <a href=\"https://pytorch-lightning.readthedocs.io/en/stable/\">Docs</a>\n</p>\n\n<!-- DO NOT ADD CONDA DOWNLOADS... README CHANGES MUST BE APPROVED BY EDEN OR WILL -->\n\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/pytorch-lightning)](https://pypi.org/project/pytorch-lightning/)\n[![PyPI Status](https://badge.fury.io/py/pytorch-lightning.svg)](https://badge.fury.io/py/pytorch-lightning)\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/pytorch-lightning)](https://pepy.tech/project/pytorch-lightning)\n[![Conda](https://img.shields.io/conda/v/conda-forge/lightning?label=conda&color=success)](https://anaconda.org/conda-forge/lightning)\n[![codecov](https://codecov.io/gh/Lightning-AI/pytorch-lightning/graph/badge.svg?token=SmzX8mnKlA)](https://codecov.io/gh/Lightning-AI/pytorch-lightning)\n\n[![Discord](https://img.shields.io/discord/1077906959069626439?style=plastic)](https://discord.gg/VptPCZkGNa)\n![GitHub commit activity](https://img.shields.io/github/commit-activity/w/lightning-ai/lightning)\n[![license](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/Lightning-AI/pytorch-lightning/blob/master/LICENSE)\n\n<!--\n[![CodeFactor](https://www.codefactor.io/repository/github/Lightning-AI/lightning/badge)](https://www.codefactor.io/repository/github/Lightning-AI/lightning)\n-->\n\n</div>\n\n<div align=\"center\">\n  \n<p align=\"center\">\n\n&nbsp;\n  \n<a target=\"_blank\" href=\"https://lightning.ai/docs/pytorch/latest/starter/introduction.html#define-a-lightningmodule\">\n  <img src=\"https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/get-started-badge.svg\" height=\"36px\" alt=\"Get started\"/>\n</a>\n\n</p>\n\n</div>\n\n&nbsp;\n\n# Why PyTorch Lightning?   \n\nTraining models in plain PyTorch is tedious and error-prone - you have to manually handle things like backprop, mixed precision, multi-GPU, and distributed training, often rewriting code for every new project. PyTorch Lightning organizes PyTorch code to automate those complexities so you can focus on your model and data, while keeping full control and scaling from CPU to multi-node without changing your core code. But if you want control of those things, you can still opt into more DIY.   \n\nFun analogy: If PyTorch is Javascript, PyTorch Lightning is ReactJS or NextJS.\n\n# Lightning has 2 core packages\n\n[PyTorch Lightning: Train and deploy PyTorch at scale](#why-pytorch-lightning).\n<br/>\n[Lightning Fabric: Expert control](#lightning-fabric-expert-control).\n\nLightning gives you granular control over how much abstraction you want to add over PyTorch.\n\n<div align=\"center\">\n    <img src=\"https://pl-public-data.s3.amazonaws.com/assets_lightning/continuum.png\" width=\"80%\">\n</div>\n\n&nbsp;\n\n# Quick start\nInstall Lightning:\n\n```bash\npip install lightning\n```\n\n<!-- following section will be skipped from PyPI description -->\n\n<details>\n  <summary>Advanced install options</summary>\n    <!-- following section will be skipped from PyPI description -->\n\n#### Install with optional dependencies\n\n```bash\npip install lightning['extra']\n```\n\n#### Conda\n\n```bash\nconda install lightning -c conda-forge\n```\n\n#### Install stable version\n\nInstall future release from the source\n\n```bash\npip install https://github.com/Lightning-AI/lightning/archive/refs/heads/release/stable.zip -U\n```\n\n#### Install bleeding-edge\n\nInstall nightly from the source (no guarantees)\n\n```bash\npip install https://github.com/Lightning-AI/lightning/archive/refs/heads/master.zip -U\n```\n\nor from testing PyPI\n\n```bash\npip install -iU https://test.pypi.org/simple/ pytorch-lightning\n```\n\n</details>\n<!-- end skipping PyPI description -->\n\n### PyTorch Lightning example\nDefine the training workflow. Here's a toy example ([explore real examples](https://lightning.ai/lightning-ai/studios?view=public&section=featured&query=pytorch+lightning)):\n\n```python\n# main.py\n# ! pip install torchvision\nimport torch, torch.nn as nn, torch.utils.data as data, torchvision as tv, torch.nn.functional as F\nimport lightning as L\n\n# --------------------------------\n# Step 1: Define a LightningModule\n# --------------------------------\n# A LightningModule (nn.Module subclass) defines a full *system*\n# (ie: an LLM, diffusion model, autoencoder, or simple image classifier).\n\n\nclass LitAutoEncoder(L.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.encoder = nn.Sequential(nn.Linear(28 * 28, 128), nn.ReLU(), nn.Linear(128, 3))\n        self.decoder = nn.Sequential(nn.Linear(3, 128), nn.ReLU(), nn.Linear(128, 28 * 28))\n\n    def forward(self, x):\n        # in lightning, forward defines the prediction/inference actions\n        embedding = self.encoder(x)\n        return embedding\n\n    def training_step(self, batch, batch_idx):\n        # training_step defines the train loop. It is independent of forward\n        x, _ = batch\n        x = x.view(x.size(0), -1)\n        z = self.encoder(x)\n        x_hat = self.decoder(z)\n        loss = F.mse_loss(x_hat, x)\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n        return optimizer\n\n\n# -------------------\n# Step 2: Define data\n# -------------------\ndataset = tv.datasets.MNIST(\".\", download=True, transform=tv.transforms.ToTensor())\ntrain, val = data.random_split(dataset, [55000, 5000])\n\n# -------------------\n# Step 3: Train\n# -------------------\nautoencoder = LitAutoEncoder()\ntrainer = L.Trainer()\ntrainer.fit(autoencoder, data.DataLoader(train), data.DataLoader(val))\n```\n\nRun the model on your terminal\n\n```bash\npip install torchvision\npython main.py\n```\n\n&nbsp;\n\n\n# Why PyTorch Lightning?\n\nPyTorch Lightning is just organized PyTorch - Lightning disentangles PyTorch code to decouple the science from the engineering.\n\n![PT to PL](docs/source-pytorch/_static/images/general/pl_quick_start_full_compressed.gif)\n\n&nbsp;\n\n----\n\n### Examples\nExplore various types of training possible with PyTorch Lightning. Pretrain and finetune ANY kind of model to perform ANY task like classification, segmentation, summarization and more:    \n\n| Task                                                                                                        | Description                                                    | Run |\n|-------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------|---|\n| [Hello world](#hello-simple-model)                                                                          | Pretrain - Hello world example                                 | <a target=\"_blank\" href=\"https://lightning.ai/lightning-ai/studios/pytorch-lightning-hello-world\"><img src=\"https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/studio-badge.svg\" alt=\"Open In Studio\"/></a> |\n| [Image classification](https://lightning.ai/lightning-ai/studios/image-classification-with-pytorch-lightning) | Finetune - ResNet-34 model to classify images of cars          | <a target=\"_blank\" href=\"https://lightning.ai/lightning-ai/studios/image-classification-with-pytorch-lightning\"><img src=\"https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/studio-badge.svg\" alt=\"Open In Studio\"/></a> |   \n| [Image segmentation](https://lightning.ai/lightning-ai/studios/image-segmentation-with-pytorch-lightning)   | Finetune - ResNet-50 model to segment images                   | <a target=\"_blank\" href=\"https://lightning.ai/lightning-ai/studios/image-segmentation-with-pytorch-lightning\"><img src=\"https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/studio-badge.svg\" alt=\"Open In Studio\"/></a> |   \n| [Object detection](https://lightning.ai/lightning-ai/studios/object-detection-with-pytorch-lightning)       | Finetune - Faster R-CNN model to detect objects                   | <a target=\"_blank\" href=\"https://lightning.ai/lightning-ai/studios/object-detection-with-pytorch-lightning\"><img src=\"https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/studio-badge.svg\" alt=\"Open In Studio\"/></a> |\n| [Text classification](https://lightning.ai/lightning-ai/studios/text-classification-with-pytorch-lightning) | Finetune - text classifier (BERT model)                        | <a target=\"_blank\" href=\"https://lightning.ai/lightning-ai/studios/text-classification-with-pytorch-lightning\"><img src=\"https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/studio-badge.svg\" alt=\"Open In Studio\"/></a> |   \n| [Text summarization](https://lightning.ai/lightning-ai/studios/text-summarization-with-pytorch-lightning)   | Finetune - text summarization (Hugging Face transformer model) | <a target=\"_blank\" href=\"https://lightning.ai/lightning-ai/studios/text-summarization-with-pytorch-lightning\"><img src=\"https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/studio-badge.svg\" alt=\"Open In Studio\"/></a> |   \n| [Audio generation](https://lightning.ai/lightning-ai/studios/finetune-a-personal-ai-music-generator)        | Finetune - audio generator (transformer model)                 | <a target=\"_blank\" href=\"https://lightning.ai/lightning-ai/studios/finetune-a-personal-ai-music-generator\"><img src=\"https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/studio-badge.svg\" alt=\"Open In Studio\"/></a> |   \n| [LLM finetuning](https://lightning.ai/lightning-ai/studios/finetune-an-llm-with-pytorch-lightning)          | Finetune - LLM (Meta Llama 3.1 8B)                | <a target=\"_blank\" href=\"https://lightning.ai/lightning-ai/studios/finetune-an-llm-with-pytorch-lightning\"><img src=\"https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/studio-badge.svg\" alt=\"Open In Studio\"/></a> | \n| [Image generation](https://lightning.ai/lightning-ai/studios/train-a-diffusion-model-with-pytorch-lightning)          | Pretrain - Image generator (diffusion model)                | <a target=\"_blank\" href=\"https://lightning.ai/lightning-ai/studios/train-a-diffusion-model-with-pytorch-lightning\"><img src=\"https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/studio-badge.svg\" alt=\"Open In Studio\"/></a> | \n| [Recommendation system](https://lightning.ai/lightning-ai/studios/recommendation-system-with-pytorch-lightning)  | Train - recommendation system (factorization and embedding)    | <a target=\"_blank\" href=\"https://lightning.ai/lightning-ai/studios/recommendation-system-with-pytorch-lightning\"><img src=\"https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/studio-badge.svg\" alt=\"Open In Studio\"/></a> | \n| [Time-series forecasting](https://lightning.ai/lightning-ai/studios/time-series-forecasting-with-pytorch-lightning) | Train - Time-series forecasting with LSTM               | <a target=\"_blank\" href=\"https://lightning.ai/lightning-ai/studios/time-series-forecasting-with-pytorch-lightning\"><img src=\"https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/studio-badge.svg\" alt=\"Open In Studio\"/></a> | \n\n______________________________________________________________________\n\n## Advanced features\n\nLightning has over [40+ advanced features](https://lightning.ai/docs/pytorch/stable/common/trainer.html#trainer-flags) designed for professional AI research at scale.\n\nHere are some examples:\n\n<div align=\"center\">\n    <img src=\"https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/features_2.jpg\" max-height=\"600px\">\n  </div>\n\n<details>\n  <summary>Train on 1000s of GPUs without code changes</summary>\n\n```python\n# 8 GPUs\n# no code changes needed\ntrainer = Trainer(accelerator=\"gpu\", devices=8)\n\n# 256 GPUs\ntrainer = Trainer(accelerator=\"gpu\", devices=8, num_nodes=32)\n```\n\n</details>\n\n<details>\n  <summary>Train on other accelerators like TPUs without code changes</summary>\n\n```python\n# no code changes needed\ntrainer = Trainer(accelerator=\"tpu\", devices=8)\n```\n\n</details>\n\n<details>\n  <summary>16-bit precision</summary>\n\n```python\n# no code changes needed\ntrainer = Trainer(precision=16)\n```\n\n</details>\n\n<details>\n  <summary>Experiment managers</summary>\n\n```python\nfrom lightning import loggers\n\n# tensorboard\ntrainer = Trainer(logger=TensorBoardLogger(\"logs/\"))\n\n# weights and biases\ntrainer = Trainer(logger=loggers.WandbLogger())\n\n# comet\ntrainer = Trainer(logger=loggers.CometLogger())\n\n# mlflow\ntrainer = Trainer(logger=loggers.MLFlowLogger())\n\n# neptune\ntrainer = Trainer(logger=loggers.NeptuneLogger())\n\n# ... and dozens more\n```\n\n</details>\n\n<details>\n\n<summary>Early Stopping</summary>\n\n```python\nes = EarlyStopping(monitor=\"val_loss\")\ntrainer = Trainer(callbacks=[es])\n```\n\n</details>\n\n<details>\n  <summary>Checkpointing</summary>\n\n```python\ncheckpointing = ModelCheckpoint(monitor=\"val_loss\")\ntrainer = Trainer(callbacks=[checkpointing])\n```\n\n</details>\n\n<details>\n  <summary>Export to torchscript (JIT) (production use)</summary>\n\n```python\n# torchscript\nautoencoder = LitAutoEncoder()\ntorch.jit.save(autoencoder.to_torchscript(), \"model.pt\")\n```\n\n</details>\n\n<details>\n  <summary>Export to ONNX (production use)</summary>\n\n```python\n# onnx\nwith tempfile.NamedTemporaryFile(suffix=\".onnx\", delete=False) as tmpfile:\n    autoencoder = LitAutoEncoder()\n    input_sample = torch.randn((1, 64))\n    autoencoder.to_onnx(tmpfile.name, input_sample, export_params=True)\n    os.path.isfile(tmpfile.name)\n```\n\n</details>\n\n______________________________________________________________________\n\n## Advantages over unstructured PyTorch\n\n- Models become hardware agnostic\n- Code is clear to read because engineering code is abstracted away\n- Easier to reproduce\n- Make fewer mistakes because lightning handles the tricky engineering\n- Keeps all the flexibility (LightningModules are still PyTorch modules), but removes a ton of boilerplate\n- Lightning has dozens of integrations with popular machine learning tools.\n- [Tested rigorously with every new PR](https://github.com/Lightning-AI/lightning/tree/master/tests). We test every combination of PyTorch and Python supported versions, every OS, multi GPUs and even TPUs.\n- Minimal running speed overhead (about 300 ms per epoch compared with pure PyTorch).\n\n______________________________________________________________________\n\n<div align=\"center\">\n    <a href=\"https://lightning.ai/docs/pytorch/stable/\">Read the PyTorch Lightning docs</a>\n</div>\n\n______________________________________________________________________\n\n&nbsp;\n&nbsp;\n\n# Lightning Fabric: Expert control\n\nRun on any device at any scale with expert-level control over PyTorch training loop and scaling strategy. You can even write your own Trainer.\n\nFabric is designed for the most complex models like foundation model scaling, LLMs, diffusion, transformers, reinforcement learning, active learning. Of any size.\n\n<table>\n<tr>\n<th>What to change</th>\n<th>Resulting Fabric Code (copy me!)</th>\n</tr>\n<tr>\n<td>\n<sub>\n\n```diff\n+ import lightning as L\n  import torch; import torchvision as tv\n\n dataset = tv.datasets.CIFAR10(\"data\", download=True,\n                               train=True,\n                               transform=tv.transforms.ToTensor())\n\n+ fabric = L.Fabric()\n+ fabric.launch()\n\n  model = tv.models.resnet18()\n  optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n- device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n- model.to(device)\n+ model, optimizer = fabric.setup(model, optimizer)\n\n  dataloader = torch.utils.data.DataLoader(dataset, batch_size=8)\n+ dataloader = fabric.setup_dataloaders(dataloader)\n\n  model.train()\n  num_epochs = 10\n  for epoch in range(num_epochs):\n      for batch in dataloader:\n          inputs, labels = batch\n-         inputs, labels = inputs.to(device), labels.to(device)\n          optimizer.zero_grad()\n          outputs = model(inputs)\n          loss = torch.nn.functional.cross_entropy(outputs, labels)\n-         loss.backward()\n+         fabric.backward(loss)\n          optimizer.step()\n          print(loss.data)\n```\n\n</sub>\n<td>\n<sub>\n\n```Python\nimport lightning as L\nimport torch; import torchvision as tv\n\ndataset = tv.datasets.CIFAR10(\"data\", download=True,\n                              train=True,\n                              transform=tv.transforms.ToTensor())\n\nfabric = L.Fabric()\nfabric.launch()\n\nmodel = tv.models.resnet18()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001)\nmodel, optimizer = fabric.setup(model, optimizer)\n\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=8)\ndataloader = fabric.setup_dataloaders(dataloader)\n\nmodel.train()\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        inputs, labels = batch\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = torch.nn.functional.cross_entropy(outputs, labels)\n        fabric.backward(loss)\n        optimizer.step()\n        print(loss.data)\n```\n\n</sub>\n</td>\n</tr>\n</table>\n\n## Key features\n\n<details>\n  <summary>Easily switch from running on CPU to GPU (Apple Silicon, CUDA, \u2026), TPU, multi-GPU or even multi-node training</summary>\n\n```python\n# Use your available hardware\n# no code changes needed\nfabric = Fabric()\n\n# Run on GPUs (CUDA or MPS)\nfabric = Fabric(accelerator=\"gpu\")\n\n# 8 GPUs\nfabric = Fabric(accelerator=\"gpu\", devices=8)\n\n# 256 GPUs, multi-node\nfabric = Fabric(accelerator=\"gpu\", devices=8, num_nodes=32)\n\n# Run on TPUs\nfabric = Fabric(accelerator=\"tpu\")\n```\n\n</details>\n\n<details>\n  <summary>Use state-of-the-art distributed training strategies (DDP, FSDP, DeepSpeed) and mixed precision out of the box</summary>\n\n```python\n# Use state-of-the-art distributed training techniques\nfabric = Fabric(strategy=\"ddp\")\nfabric = Fabric(strategy=\"deepspeed\")\nfabric = Fabric(strategy=\"fsdp\")\n\n# Switch the precision\nfabric = Fabric(precision=\"16-mixed\")\nfabric = Fabric(precision=\"64\")\n```\n\n</details>\n\n<details>\n  <summary>All the device logic boilerplate is handled for you</summary>\n\n```diff\n  # no more of this!\n- model.to(device)\n- batch.to(device)\n```\n\n</details>\n\n<details>\n  <summary>Build your own custom Trainer using Fabric primitives for training checkpointing, logging, and more</summary>\n\n```python\nimport lightning as L\n\n\nclass MyCustomTrainer:\n    def __init__(self, accelerator=\"auto\", strategy=\"auto\", devices=\"auto\", precision=\"32-true\"):\n        self.fabric = L.Fabric(accelerator=accelerator, strategy=strategy, devices=devices, precision=precision)\n\n    def fit(self, model, optimizer, dataloader, max_epochs):\n        self.fabric.launch()\n\n        model, optimizer = self.fabric.setup(model, optimizer)\n        dataloader = self.fabric.setup_dataloaders(dataloader)\n        model.train()\n\n        for epoch in range(max_epochs):\n            for batch in dataloader:\n                input, target = batch\n                optimizer.zero_grad()\n                output = model(input)\n                loss = loss_fn(output, target)\n                self.fabric.backward(loss)\n                optimizer.step()\n```\n\nYou can find a more extensive example in our [examples](examples/fabric/build_your_own_trainer)\n\n</details>\n\n______________________________________________________________________\n\n<div align=\"center\">\n    <a href=\"https://lightning.ai/docs/fabric/stable/\">Read the Lightning Fabric docs</a>\n</div>\n\n______________________________________________________________________\n\n&nbsp;\n&nbsp;\n\n## Examples\n\n###### Self-supervised Learning\n\n- [CPC transforms](https://lightning-bolts.readthedocs.io/en/stable/transforms/self_supervised.html#cpc-transforms)\n- [Moco v2 transforms](https://lightning-bolts.readthedocs.io/en/stable/transforms/self_supervised.html#moco-v2-transforms)\n- [SimCLR transforms](https://lightning-bolts.readthedocs.io/en/stable/transforms/self_supervised.html#simclr-transforms)\n\n###### Convolutional Architectures\n\n- [GPT-2](https://lightning-bolts.readthedocs.io/en/stable/models/convolutional.html#gpt-2)\n- [UNet](https://lightning-bolts.readthedocs.io/en/stable/models/convolutional.html#unet)\n\n###### Reinforcement Learning\n\n- [DQN Loss](https://lightning-bolts.readthedocs.io/en/stable/losses.html#dqn-loss)\n- [Double DQN Loss](https://lightning-bolts.readthedocs.io/en/stable/losses.html#double-dqn-loss)\n- [Per DQN Loss](https://lightning-bolts.readthedocs.io/en/stable/losses.html#per-dqn-loss)\n\n###### GANs\n\n- [Basic GAN](https://lightning-bolts.readthedocs.io/en/stable/models/gans.html#basic-gan)\n- [DCGAN](https://lightning-bolts.readthedocs.io/en/stable/models/gans.html#dcgan)\n\n###### Classic ML\n\n- [Logistic Regression](https://lightning-bolts.readthedocs.io/en/stable/models/classic_ml.html#logistic-regression)\n- [Linear Regression](https://lightning-bolts.readthedocs.io/en/stable/models/classic_ml.html#linear-regression)\n\n&nbsp;\n&nbsp;\n\n## Continuous Integration\n\nLightning is rigorously tested across multiple CPUs, GPUs and TPUs and against major Python and PyTorch versions.\n\n###### \\*Codecov is > 90%+ but build delays may show less\n\n<details>\n  <summary>Current build statuses</summary>\n\n<center>\n\n|       System / PyTorch ver.        | 1.13                                                                                                                                                                                                                            | 2.0                                                                                                                                                                                                                             |                                                                                                               2.1                                                                                                               |\n| :--------------------------------: | :-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|\n|        Linux py3.9 \\[GPUs\\]        |  |  | [![Build Status](https://dev.azure.com/Lightning-AI/lightning/_apis/build/status%2Fpytorch-lightning%20%28GPUs%29?branchName=master)](https://dev.azure.com/Lightning-AI/lightning/_build/latest?definitionId=24&branchName=master) |\n|  Linux (multiple Python versions)  | [![Test PyTorch](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml)                                 | [![Test PyTorch](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml)                                 |                 [![Test PyTorch](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml)                 |\n|   OSX (multiple Python versions)   | [![Test PyTorch](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml)                                 | [![Test PyTorch](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml)                                 |                 [![Test PyTorch](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml)                 |\n| Windows (multiple Python versions) | [![Test PyTorch](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml)                                 | [![Test PyTorch](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml)                                 |                 [![Test PyTorch](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml)                 |\n\n</center>\n</details>\n\n&nbsp;\n&nbsp;\n\n## Community\n\nThe lightning community is maintained by\n\n- [10+ core contributors](https://lightning.ai/docs/pytorch/latest/community/governance.html) who are all a mix of professional engineers, Research Scientists, and Ph.D. students from top AI labs.\n- 800+ community contributors.\n\nWant to help us build Lightning and reduce boilerplate for thousands of researchers? [Learn how to make your first contribution here](https://lightning.ai/docs/pytorch/stable/generated/CONTRIBUTING.html)\n\nLightning is also part of the [PyTorch ecosystem](https://pytorch.org/ecosystem/) which requires projects to have solid testing, documentation and support.\n\n### Asking for help\n\nIf you have any questions please:\n\n1. [Read the docs](https://lightning.ai/docs).\n1. [Search through existing Discussions](https://github.com/Lightning-AI/lightning/discussions), or [add a new question](https://github.com/Lightning-AI/lightning/discussions/new)\n1. [Join our discord](https://discord.com/invite/tfXFetEZxv).\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 614765452,
    "name": "AutoGPT",
    "full_name": "Significant-Gravitas/AutoGPT",
    "description": "AutoGPT is the vision of accessible AI for everyone, to use and to build on. Our mission is to provide the tools, so that you can focus on what matters.",
    "html_url": "https://github.com/Significant-Gravitas/AutoGPT",
    "clone_url": "https://github.com/Significant-Gravitas/AutoGPT.git",
    "owner_login": "Significant-Gravitas",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/130738209?v=4",
    "stargazers_count": 177520,
    "watchers_count": 177520,
    "forks_count": 45916,
    "open_issues_count": 206,
    "size": 270708,
    "language": "Python",
    "languages": {
      "Python": 4004826,
      "TypeScript": 1500788,
      "Dart": 203562,
      "PLpgSQL": 54094,
      "Jinja": 31626,
      "C++": 23419,
      "CMake": 18862,
      "Shell": 17650,
      "JavaScript": 15907,
      "CSS": 11266,
      "Dockerfile": 4782,
      "Batchfile": 4101,
      "HTML": 3023,
      "Ruby": 2803,
      "Swift": 2384,
      "C": 1425,
      "Elixir": 1068,
      "Kotlin": 140,
      "Objective-C": 38
    },
    "topics": [
      "ai",
      "artificial-intelligence",
      "autonomous-agents",
      "gpt-4",
      "llama-api",
      "openai",
      "python"
    ],
    "license_name": "Other",
    "created_at": "2023-03-16T09:21:07+00:00",
    "updated_at": "2025-08-06T01:53:34+00:00",
    "pushed_at": "2025-08-05T23:54:36+00:00",
    "contributors_count": 100,
    "readme_length": 11144,
    "readme_content": "# AutoGPT: Build, Deploy, and Run AI Agents\n\n[![Discord Follow](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fdiscord.com%2Fapi%2Finvites%2Fautogpt%3Fwith_counts%3Dtrue&query=%24.approximate_member_count&label=total%20members&logo=discord&logoColor=white&color=7289da)](https://discord.gg/autogpt) &ensp;\n[![Twitter Follow](https://img.shields.io/twitter/follow/Auto_GPT?style=social)](https://twitter.com/Auto_GPT) &ensp;\n\n**AutoGPT** is a powerful platform that allows you to create, deploy, and manage continuous AI agents that automate complex workflows. \n\n## Hosting Options \n   - Download to self-host (Free!)\n   - [Join the Waitlist](https://bit.ly/3ZDijAI) for the cloud-hosted beta (Closed Beta - Public release Coming Soon!)\n\n## How to Self-Host the AutoGPT Platform\n> [!NOTE]\n> Setting up and hosting the AutoGPT Platform yourself is a technical process. \n> If you'd rather something that just works, we recommend [joining the waitlist](https://bit.ly/3ZDijAI) for the cloud-hosted beta.\n\n### System Requirements\n\nBefore proceeding with the installation, ensure your system meets the following requirements:\n\n#### Hardware Requirements\n- CPU: 4+ cores recommended\n- RAM: Minimum 8GB, 16GB recommended\n- Storage: At least 10GB of free space\n\n#### Software Requirements\n- Operating Systems:\n  - Linux (Ubuntu 20.04 or newer recommended)\n  - macOS (10.15 or newer)\n  - Windows 10/11 with WSL2\n- Required Software (with minimum versions):\n  - Docker Engine (20.10.0 or newer)\n  - Docker Compose (2.0.0 or newer)\n  - Git (2.30 or newer)\n  - Node.js (16.x or newer)\n  - npm (8.x or newer)\n  - VSCode (1.60 or newer) or any modern code editor\n\n#### Network Requirements\n- Stable internet connection\n- Access to required ports (will be configured in Docker)\n- Ability to make outbound HTTPS connections\n\n### Updated Setup Instructions:\nWe've moved to a fully maintained and regularly updated documentation site.\n\n\ud83d\udc49 [Follow the official self-hosting guide here](https://docs.agpt.co/platform/getting-started/)\n\n\nThis tutorial assumes you have Docker, VSCode, git and npm installed.\n\n---\n\n#### \u26a1 Quick Setup with One-Line Script (Recommended for Local Hosting)\n\nSkip the manual steps and get started in minutes using our automatic setup script.\n\nFor macOS/Linux:\n```\ncurl -fsSL https://setup.agpt.co/install.sh -o install.sh && bash install.sh\n```\n\nFor Windows (PowerShell):\n```\npowershell -c \"iwr https://setup.agpt.co/install.bat -o install.bat; ./install.bat\"\n```\n\nThis will install dependencies, configure Docker, and launch your local instance \u2014 all in one go.\n\n### \ud83e\uddf1 AutoGPT Frontend\n\nThe AutoGPT frontend is where users interact with our powerful AI automation platform. It offers multiple ways to engage with and leverage our AI agents. This is the interface where you'll bring your AI automation ideas to life:\n\n   **Agent Builder:** For those who want to customize, our intuitive, low-code interface allows you to design and configure your own AI agents. \n   \n   **Workflow Management:** Build, modify, and optimize your automation workflows with ease. You build your agent by connecting blocks, where each block     performs a single action.\n   \n   **Deployment Controls:** Manage the lifecycle of your agents, from testing to production.\n   \n   **Ready-to-Use Agents:** Don't want to build? Simply select from our library of pre-configured agents and put them to work immediately.\n   \n   **Agent Interaction:** Whether you've built your own or are using pre-configured agents, easily run and interact with them through our user-friendly      interface.\n\n   **Monitoring and Analytics:** Keep track of your agents' performance and gain insights to continually improve your automation processes.\n\n[Read this guide](https://docs.agpt.co/platform/new_blocks/) to learn how to build your own custom blocks.\n\n### \ud83d\udcbd AutoGPT Server\n\nThe AutoGPT Server is the powerhouse of our platform This is where your agents run. Once deployed, agents can be triggered by external sources and can operate continuously. It contains all the essential components that make AutoGPT run smoothly.\n\n   **Source Code:** The core logic that drives our agents and automation processes.\n   \n   **Infrastructure:** Robust systems that ensure reliable and scalable performance.\n   \n   **Marketplace:** A comprehensive marketplace where you can find and deploy a wide range of pre-built agents.\n\n### \ud83d\udc19 Example Agents\n\nHere are two examples of what you can do with AutoGPT:\n\n1. **Generate Viral Videos from Trending Topics**\n   - This agent reads topics on Reddit.\n   - It identifies trending topics.\n   - It then automatically creates a short-form video based on the content. \n\n2. **Identify Top Quotes from Videos for Social Media**\n   - This agent subscribes to your YouTube channel.\n   - When you post a new video, it transcribes it.\n   - It uses AI to identify the most impactful quotes to generate a summary.\n   - Then, it writes a post to automatically publish to your social media. \n\nThese examples show just a glimpse of what you can achieve with AutoGPT! You can create customized workflows to build agents for any use case.\n\n---\n\n### **License Overview:**\n\n\ud83d\udee1\ufe0f **Polyform Shield License:**\nAll code and content within the `autogpt_platform` folder is licensed under the Polyform Shield License. This new project is our in-developlemt platform for building, deploying and managing agents.</br>_[Read more about this effort](https://agpt.co/blog/introducing-the-autogpt-platform)_\n\n\ud83e\udd89 **MIT License:**\nAll other portions of the AutoGPT repository (i.e., everything outside the `autogpt_platform` folder) are licensed under the MIT License. This includes the original stand-alone AutoGPT Agent, along with projects such as [Forge](https://github.com/Significant-Gravitas/AutoGPT/tree/master/classic/forge), [agbenchmark](https://github.com/Significant-Gravitas/AutoGPT/tree/master/classic/benchmark) and the [AutoGPT Classic GUI](https://github.com/Significant-Gravitas/AutoGPT/tree/master/classic/frontend).</br>We also publish additional work under the MIT Licence in other repositories, such as [GravitasML](https://github.com/Significant-Gravitas/gravitasml) which is developed for and used in the AutoGPT Platform. See also our MIT Licenced [Code Ability](https://github.com/Significant-Gravitas/AutoGPT-Code-Ability) project.\n\n---\n### Mission\nOur mission is to provide the tools, so that you can focus on what matters:\n\n- \ud83c\udfd7\ufe0f **Building** - Lay the foundation for something amazing.\n- \ud83e\uddea **Testing** - Fine-tune your agent to perfection.\n- \ud83e\udd1d **Delegating** - Let AI work for you, and have your ideas come to life.\n\nBe part of the revolution! **AutoGPT** is here to stay, at the forefront of AI innovation.\n\n**\ud83d\udcd6 [Documentation](https://docs.agpt.co)**\n&ensp;|&ensp;\n**\ud83d\ude80 [Contributing](CONTRIBUTING.md)**\n\n---\n## \ud83e\udd16 AutoGPT Classic\n> Below is information about the classic version of AutoGPT.\n\n**\ud83d\udee0\ufe0f [Build your own Agent - Quickstart](classic/FORGE-QUICKSTART.md)**\n\n### \ud83c\udfd7\ufe0f Forge\n\n**Forge your own agent!** &ndash; Forge is a ready-to-go toolkit to build your own agent application. It handles most of the boilerplate code, letting you channel all your creativity into the things that set *your* agent apart. All tutorials are located [here](https://medium.com/@aiedge/autogpt-forge-e3de53cc58ec). Components from [`forge`](/classic/forge/) can also be used individually to speed up development and reduce boilerplate in your agent project.\n\n\ud83d\ude80 [**Getting Started with Forge**](https://github.com/Significant-Gravitas/AutoGPT/blob/master/classic/forge/tutorials/001_getting_started.md) &ndash;\nThis guide will walk you through the process of creating your own agent and using the benchmark and user interface.\n\n\ud83d\udcd8 [Learn More](https://github.com/Significant-Gravitas/AutoGPT/tree/master/classic/forge) about Forge\n\n### \ud83c\udfaf Benchmark\n\n**Measure your agent's performance!** The `agbenchmark` can be used with any agent that supports the agent protocol, and the integration with the project's [CLI] makes it even easier to use with AutoGPT and forge-based agents. The benchmark offers a stringent testing environment. Our framework allows for autonomous, objective performance evaluations, ensuring your agents are primed for real-world action.\n\n<!-- TODO: insert visual demonstrating the benchmark -->\n\n\ud83d\udce6 [`agbenchmark`](https://pypi.org/project/agbenchmark/) on Pypi\n&ensp;|&ensp;\n\ud83d\udcd8 [Learn More](https://github.com/Significant-Gravitas/AutoGPT/tree/master/classic/benchmark) about the Benchmark\n\n### \ud83d\udcbb UI\n\n**Makes agents easy to use!** The `frontend` gives you a user-friendly interface to control and monitor your agents. It connects to agents through the [agent protocol](#-agent-protocol), ensuring compatibility with many agents from both inside and outside of our ecosystem.\n\n<!-- TODO: insert screenshot of front end -->\n\nThe frontend works out-of-the-box with all agents in the repo. Just use the [CLI] to run your agent of choice!\n\n\ud83d\udcd8 [Learn More](https://github.com/Significant-Gravitas/AutoGPT/tree/master/classic/frontend) about the Frontend\n\n### \u2328\ufe0f CLI\n\n[CLI]: #-cli\n\nTo make it as easy as possible to use all of the tools offered by the repository, a CLI is included at the root of the repo:\n\n```shell\n$ ./run\nUsage: cli.py [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  --help  Show this message and exit.\n\nCommands:\n  agent      Commands to create, start and stop agents\n  benchmark  Commands to start the benchmark and list tests and categories\n  setup      Installs dependencies needed for your system.\n```\n\nJust clone the repo, install dependencies with `./run setup`, and you should be good to go!\n\n## \ud83e\udd14 Questions? Problems? Suggestions?\n\n### Get help - [Discord \ud83d\udcac](https://discord.gg/autogpt)\n\n[![Join us on Discord](https://invidget.switchblade.xyz/autogpt)](https://discord.gg/autogpt)\n\nTo report a bug or request a feature, create a [GitHub Issue](https://github.com/Significant-Gravitas/AutoGPT/issues/new/choose). Please ensure someone else hasn't created an issue for the same topic.\n\n## \ud83e\udd1d Sister projects\n\n### \ud83d\udd04 Agent Protocol\n\nTo maintain a uniform standard and ensure seamless compatibility with many current and future applications, AutoGPT employs the [agent protocol](https://agentprotocol.ai/) standard by the AI Engineer Foundation. This standardizes the communication pathways from your agent to the frontend and benchmark.\n\n---\n\n## Stars stats\n\n<p align=\"center\">\n<a href=\"https://star-history.com/#Significant-Gravitas/AutoGPT\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=Significant-Gravitas/AutoGPT&type=Date&theme=dark\" />\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=Significant-Gravitas/AutoGPT&type=Date\" />\n    <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=Significant-Gravitas/AutoGPT&type=Date\" />\n  </picture>\n</a>\n</p>\n\n\n## \u26a1 Contributors\n\n<a href=\"https://github.com/Significant-Gravitas/AutoGPT/graphs/contributors\" alt=\"View Contributors\">\n  <img src=\"https://contrib.rocks/image?repo=Significant-Gravitas/AutoGPT&max=1000&columns=10\" alt=\"Contributors\" />\n</a>\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 695864515,
    "name": "Deep-Live-Cam",
    "full_name": "hacksider/Deep-Live-Cam",
    "description": "real time face swap and one-click video deepfake with only a single image",
    "html_url": "https://github.com/hacksider/Deep-Live-Cam",
    "clone_url": "https://github.com/hacksider/Deep-Live-Cam.git",
    "owner_login": "hacksider",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/1267200?v=4",
    "stargazers_count": 72304,
    "watchers_count": 72304,
    "forks_count": 10416,
    "open_issues_count": 82,
    "size": 155835,
    "language": "Python",
    "languages": {
      "Python": 106532,
      "Batchfile": 79
    },
    "topics": [
      "ai",
      "ai-deep-fake",
      "ai-face",
      "ai-webcam",
      "artificial-intelligence",
      "deep-fake",
      "deepfake",
      "deepfake-webcam",
      "faceswap",
      "fake-webcam",
      "gan",
      "real-time-deepfake",
      "realtime",
      "realtime-deepfake",
      "realtime-face-changer",
      "video-deepfake",
      "webcam",
      "webcamera"
    ],
    "license_name": "GNU Affero General Public License v3.0",
    "created_at": "2023-09-24T13:19:31+00:00",
    "updated_at": "2025-08-06T01:55:56+00:00",
    "pushed_at": "2025-08-01T18:56:26+00:00",
    "contributors_count": 50,
    "readme_length": 15556,
    "readme_content": "<h1 align=\"center\">Deep-Live-Cam</h1>\n\n<p align=\"center\">\n  Real-time face swap and video deepfake with a single click and only a single image.\n</p>\n\n<p align=\"center\">\n<a href=\"https://trendshift.io/repositories/11395\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/11395\" alt=\"hacksider%2FDeep-Live-Cam | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n</p>\n\n<p align=\"center\">\n  <img src=\"media/demo.gif\" alt=\"Demo GIF\" width=\"800\">\n</p>\n\n##  Disclaimer\n\nThis deepfake software is designed to be a productive tool for the AI-generated media industry. It can assist artists in animating custom characters, creating engaging content, and even using models for clothing design.\n\nWe are aware of the potential for unethical applications and are committed to preventative measures. A built-in check prevents the program from processing inappropriate media (nudity, graphic content, sensitive material like war footage, etc.). We will continue to develop this project responsibly, adhering to the law and ethics. We may shut down the project or add watermarks if legally required.\n\n- Ethical Use: Users are expected to use this software responsibly and legally. If using a real person's face, obtain their consent and clearly label any output as a deepfake when sharing online.\n\n- Content Restrictions: The software includes built-in checks to prevent processing inappropriate media, such as nudity, graphic content, or sensitive material.\n\n- Legal Compliance: We adhere to all relevant laws and ethical guidelines. If legally required, we may shut down the project or add watermarks to the output.\n\n- User Responsibility: We are not responsible for end-user actions. Users must ensure their use of the software aligns with ethical standards and legal requirements.\n\nBy using this software, you agree to these terms and commit to using it in a manner that respects the rights and dignity of others.\n\nUsers are expected to use this software responsibly and legally. If using a real person's face, obtain their consent and clearly label any output as a deepfake when sharing online. We are not responsible for end-user actions.\n\n## Exclusive v2.1 Quick Start - Pre-built (Windows/Mac Silicon)\n\n  <a href=\"https://deeplivecam.net/index.php/quickstart\"> <img src=\"media/Download.png\" width=\"285\" height=\"77\" />\n\n##### This is the fastest build you can get if you have a discrete NVIDIA or AMD GPU or Mac Silicon, And you'll receive special priority support.\n \n###### These Pre-builts are perfect for non-technical users or those who don't have time to, or can't manually install all the requirements. Just a heads-up: this is an open-source project, so you can also install it manually. \n\n## TLDR; Live Deepfake in just 3 Clicks\n![easysteps](https://github.com/user-attachments/assets/af825228-852c-411b-b787-ffd9aac72fc6)\n1. Select a face\n2. Select which camera to use\n3. Press live!\n\n## Features & Uses - Everything is in real-time\n\n### Mouth Mask\n\n**Retain your original mouth for accurate movement using Mouth Mask**\n\n<p align=\"center\">\n  <img src=\"media/ludwig.gif\" alt=\"resizable-gif\">\n</p>\n\n### Face Mapping\n\n**Use different faces on multiple subjects simultaneously**\n\n<p align=\"center\">\n  <img src=\"media/streamers.gif\" alt=\"face_mapping_source\">\n</p>\n\n### Your Movie, Your Face\n\n**Watch movies with any face in real-time**\n\n<p align=\"center\">\n  <img src=\"media/movie.gif\" alt=\"movie\">\n</p>\n\n### Live Show\n\n**Run Live shows and performances**\n\n<p align=\"center\">\n  <img src=\"media/live_show.gif\" alt=\"show\">\n</p>\n\n### Memes\n\n**Create Your Most Viral Meme Yet**\n\n<p align=\"center\">\n  <img src=\"media/meme.gif\" alt=\"show\" width=\"450\"> \n  <br>\n  <sub>Created using Many Faces feature in Deep-Live-Cam</sub>\n</p>\n\n### Omegle\n\n**Surprise people on Omegle**\n\n<p align=\"center\">\n  <video src=\"https://github.com/user-attachments/assets/2e9b9b82-fa04-4b70-9f56-b1f68e7672d0\" width=\"450\" controls></video>\n</p>\n\n## Installation (Manual)\n\n**Please be aware that the installation requires technical skills and is not for beginners. Consider downloading the quickstart version.**\n\n<details>\n<summary>Click to see the process</summary>\n\n### Installation\n\nThis is more likely to work on your computer but will be slower as it utilizes the CPU.\n\n**1. Set up Your Platform**\n\n-   Python (3.11 recommended)\n-   pip\n-   git\n-   [ffmpeg](https://www.youtube.com/watch?v=OlNWCpFdVMA) - ```iex (irm ffmpeg.tc.ht)```\n-   [Visual Studio 2022 Runtimes (Windows)](https://visualstudio.microsoft.com/visual-cpp-build-tools/)\n\n**2. Clone the Repository**\n\n```bash\ngit clone https://github.com/hacksider/Deep-Live-Cam.git\ncd Deep-Live-Cam\n```\n\n**3. Download the Models**\n\n1. [GFPGANv1.4](https://huggingface.co/hacksider/deep-live-cam/resolve/main/GFPGANv1.4.pth)\n2. [inswapper\\_128\\_fp16.onnx](https://huggingface.co/hacksider/deep-live-cam/resolve/main/inswapper_128_fp16.onnx)\n\nPlace these files in the \"**models**\" folder.\n\n**4. Install Dependencies**\n\nWe highly recommend using a `venv` to avoid issues.\n\n\nFor Windows:\n```bash\npython -m venv venv\nvenv\\Scripts\\activate\npip install -r requirements.txt\n```\nFor Linux:\n```bash\n# Ensure you use the installed Python 3.10\npython3 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n```\n\n**For macOS:**\n\nApple Silicon (M1/M2/M3) requires specific setup:\n\n```bash\n# Install Python 3.11 (specific version is important)\nbrew install python@3.11\n\n# Install tkinter package (required for the GUI)\nbrew install python-tk@3.10\n\n# Create and activate virtual environment with Python 3.11\npython3.11 -m venv venv\nsource venv/bin/activate\n\n# Install dependencies\npip install -r requirements.txt\n```\n\n** In case something goes wrong and you need to reinstall the virtual environment **\n\n```bash\n# Deactivate the virtual environment\nrm -rf venv\n\n# Reinstall the virtual environment\npython -m venv venv\nsource venv/bin/activate\n\n# install the dependencies again\npip install -r requirements.txt\n```\n\n**Run:** If you don't have a GPU, you can run Deep-Live-Cam using `python run.py`. Note that initial execution will download models (~300MB).\n\n### GPU Acceleration\n\n**CUDA Execution Provider (Nvidia)**\n\n1. Install [CUDA Toolkit 12.8.0](https://developer.nvidia.com/cuda-12-8-0-download-archive)\n2. Install [cuDNN v8.9.7 for CUDA 12.x](https://developer.nvidia.com/rdp/cudnn-archive) (required for onnxruntime-gpu):\n   - Download cuDNN v8.9.7 for CUDA 12.x\n   - Make sure the cuDNN bin directory is in your system PATH\n3. Install dependencies:\n\n```bash\npip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\npip uninstall onnxruntime onnxruntime-gpu\npip install onnxruntime-gpu==1.21.0\n```\n\n3. Usage:\n\n```bash\npython run.py --execution-provider cuda\n```\n\n**CoreML Execution Provider (Apple Silicon)**\n\nApple Silicon (M1/M2/M3) specific installation:\n\n1. Make sure you've completed the macOS setup above using Python 3.10.\n2. Install dependencies:\n\n```bash\npip uninstall onnxruntime onnxruntime-silicon\npip install onnxruntime-silicon==1.13.1\n```\n\n3. Usage (important: specify Python 3.10):\n\n```bash\npython3.10 run.py --execution-provider coreml\n```\n\n**Important Notes for macOS:**\n- You **must** use Python 3.10, not newer versions like 3.11 or 3.13\n- Always run with `python3.10` command not just `python` if you have multiple Python versions installed\n- If you get error about `_tkinter` missing, reinstall the tkinter package: `brew reinstall python-tk@3.10`\n- If you get model loading errors, check that your models are in the correct folder\n- If you encounter conflicts with other Python versions, consider uninstalling them:\n  ```bash\n  # List all installed Python versions\n  brew list | grep python\n  \n  # Uninstall conflicting versions if needed\n  brew uninstall --ignore-dependencies python@3.11 python@3.13\n  \n  # Keep only Python 3.11\n  brew cleanup\n  ```\n\n**CoreML Execution Provider (Apple Legacy)**\n\n1. Install dependencies:\n\n```bash\npip uninstall onnxruntime onnxruntime-coreml\npip install onnxruntime-coreml==1.21.0\n```\n\n2. Usage:\n\n```bash\npython run.py --execution-provider coreml\n```\n\n**DirectML Execution Provider (Windows)**\n\n1. Install dependencies:\n\n```bash\npip uninstall onnxruntime onnxruntime-directml\npip install onnxruntime-directml==1.21.0\n```\n\n2. Usage:\n\n```bash\npython run.py --execution-provider directml\n```\n\n**OpenVINO\u2122 Execution Provider (Intel)**\n\n1. Install dependencies:\n\n```bash\npip uninstall onnxruntime onnxruntime-openvino\npip install onnxruntime-openvino==1.21.0\n```\n\n2. Usage:\n\n```bash\npython run.py --execution-provider openvino\n```\n</details>\n\n## Usage\n\n**1. Image/Video Mode**\n\n-   Execute `python run.py`.\n-   Choose a source face image and a target image/video.\n-   Click \"Start\".\n-   The output will be saved in a directory named after the target video.\n\n**2. Webcam Mode**\n\n-   Execute `python run.py`.\n-   Select a source face image.\n-   Click \"Live\".\n-   Wait for the preview to appear (10-30 seconds).\n-   Use a screen capture tool like OBS to stream.\n-   To change the face, select a new source image.\n\n## Command Line Arguments (Unmaintained)\n\n```\noptions:\n  -h, --help                                               show this help message and exit\n  -s SOURCE_PATH, --source SOURCE_PATH                     select a source image\n  -t TARGET_PATH, --target TARGET_PATH                     select a target image or video\n  -o OUTPUT_PATH, --output OUTPUT_PATH                     select output file or directory\n  --frame-processor FRAME_PROCESSOR [FRAME_PROCESSOR ...]  frame processors (choices: face_swapper, face_enhancer, ...)\n  --keep-fps                                               keep original fps\n  --keep-audio                                             keep original audio\n  --keep-frames                                            keep temporary frames\n  --many-faces                                             process every face\n  --map-faces                                              map source target faces\n  --mouth-mask                                             mask the mouth region\n  --video-encoder {libx264,libx265,libvpx-vp9}             adjust output video encoder\n  --video-quality [0-51]                                   adjust output video quality\n  --live-mirror                                            the live camera display as you see it in the front-facing camera frame\n  --live-resizable                                         the live camera frame is resizable\n  --max-memory MAX_MEMORY                                  maximum amount of RAM in GB\n  --execution-provider {cpu} [{cpu} ...]                   available execution provider (choices: cpu, ...)\n  --execution-threads EXECUTION_THREADS                    number of execution threads\n  -v, --version                                            show program's version number and exit\n```\n\nLooking for a CLI mode? Using the -s/--source argument will make the run program in cli mode.\n\n## Press\n\n**We are always open to criticism and are ready to improve, that's why we didn't cherry-pick anything.**\n\n - [*\"Deep-Live-Cam goes viral, allowing anyone to become a digital doppelganger\"*](https://arstechnica.com/information-technology/2024/08/new-ai-tool-enables-real-time-face-swapping-on-webcams-raising-fraud-concerns/) - Ars Technica\n - [*\"Thanks Deep Live Cam, shapeshifters are among us now\"*](https://dataconomy.com/2024/08/15/what-is-deep-live-cam-github-deepfake/) - Dataconomy\n - [*\"This free AI tool lets you become anyone during video-calls\"*](https://www.newsbytesapp.com/news/science/deep-live-cam-ai-impersonation-tool-goes-viral/story) - NewsBytes\n - [*\"OK, this viral AI live stream software is truly terrifying\"*](https://www.creativebloq.com/ai/ok-this-viral-ai-live-stream-software-is-truly-terrifying) - Creative Bloq\n - [*\"Deepfake AI Tool Lets You Become Anyone in a Video Call With Single Photo\"*](https://petapixel.com/2024/08/14/deep-live-cam-deepfake-ai-tool-lets-you-become-anyone-in-a-video-call-with-single-photo-mark-zuckerberg-jd-vance-elon-musk/) - PetaPixel\n - [*\"Deep-Live-Cam Uses AI to Transform Your Face in Real-Time, Celebrities Included\"*](https://www.techeblog.com/deep-live-cam-ai-transform-face/) - TechEBlog\n - [*\"An AI tool that \"makes you look like anyone\" during a video call is going viral online\"*](https://telegrafi.com/en/a-tool-that-makes-you-look-like-anyone-during-a-video-call-is-going-viral-on-the-Internet/) - Telegrafi\n - [*\"This Deepfake Tool Turning Images Into Livestreams is Topping the GitHub Charts\"*](https://decrypt.co/244565/this-deepfake-tool-turning-images-into-livestreams-is-topping-the-github-charts) - Emerge\n - [*\"New Real-Time Face-Swapping AI Allows Anyone to Mimic Famous Faces\"*](https://www.digitalmusicnews.com/2024/08/15/face-swapping-ai-real-time-mimic/) - Digital Music News\n - [*\"This real-time webcam deepfake tool raises alarms about the future of identity theft\"*](https://www.diyphotography.net/this-real-time-webcam-deepfake-tool-raises-alarms-about-the-future-of-identity-theft/) - DIYPhotography\n - [*\"That's Crazy, Oh God. That's Fucking Freaky Dude... That's So Wild Dude\"*](https://www.youtube.com/watch?time_continue=1074&v=py4Tc-Y8BcY) - SomeOrdinaryGamers\n - [*\"Alright look look look, now look chat, we can do any face we want to look like chat\"*](https://www.youtube.com/live/mFsCe7AIxq8?feature=shared&t=2686) - IShowSpeed\n - [*\"They do a pretty good job matching poses, expression and even the lighting\"*](https://www.youtube.com/watch?v=wnCghLjqv3s&t=551s) - TechLinked (LTT)\n\n\n## Credits\n\n-   [ffmpeg](https://ffmpeg.org/): for making video-related operations easy\n-   [deepinsight](https://github.com/deepinsight): for their [insightface](https://github.com/deepinsight/insightface) project which provided a well-made library and models. Please be reminded that the [use of the model is for non-commercial research purposes only](https://github.com/deepinsight/insightface?tab=readme-ov-file#license).\n-   [havok2-htwo](https://github.com/havok2-htwo): for sharing the code for webcam\n-   [GosuDRM](https://github.com/GosuDRM): for the open version of roop\n-   [pereiraroland26](https://github.com/pereiraroland26): Multiple faces support\n-   [vic4key](https://github.com/vic4key): For supporting/contributing to this project\n-   [kier007](https://github.com/kier007): for improving the user experience\n-   [qitianai](https://github.com/qitianai): for multi-lingual support\n-   and [all developers](https://github.com/hacksider/Deep-Live-Cam/graphs/contributors) behind libraries used in this project.\n-   Footnote: Please be informed that the base author of the code is [s0md3v](https://github.com/s0md3v/roop)\n-   All the wonderful users who helped make this project go viral by starring the repo \u2764\ufe0f\n\n[![Stargazers](https://reporoster.com/stars/hacksider/Deep-Live-Cam)](https://github.com/hacksider/Deep-Live-Cam/stargazers)\n\n## Contributions\n\n![Alt](https://repobeats.axiom.co/api/embed/fec8e29c45dfdb9c5916f3a7830e1249308d20e1.svg \"Repobeats analytics image\")\n\n## Stars to the Moon \ud83d\ude80\n\n<a href=\"https://star-history.com/#hacksider/deep-live-cam&Date\">\n <picture>\n   <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=hacksider/deep-live-cam&type=Date&theme=dark\" />\n   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=hacksider/deep-live-cam&type=Date\" />\n   <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=hacksider/deep-live-cam&type=Date\" />\n </picture>\n</a>\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 771302083,
    "name": "OpenHands",
    "full_name": "All-Hands-AI/OpenHands",
    "description": "\ud83d\ude4c OpenHands: Code Less, Make More",
    "html_url": "https://github.com/All-Hands-AI/OpenHands",
    "clone_url": "https://github.com/All-Hands-AI/OpenHands.git",
    "owner_login": "All-Hands-AI",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/169105795?v=4",
    "stargazers_count": 61760,
    "watchers_count": 61760,
    "forks_count": 7341,
    "open_issues_count": 407,
    "size": 220201,
    "language": "Python",
    "languages": {
      "Python": 5239167,
      "TypeScript": 1524028,
      "Shell": 121121,
      "Jinja": 89109,
      "JavaScript": 34600,
      "Makefile": 15668,
      "CSS": 8606,
      "Dockerfile": 8170,
      "HTML": 1849
    },
    "topics": [
      "agent",
      "artificial-intelligence",
      "chatgpt",
      "claude-ai",
      "cli",
      "developer-tools",
      "gpt",
      "llm",
      "openai"
    ],
    "license_name": "MIT License",
    "created_at": "2024-03-13T03:33:31+00:00",
    "updated_at": "2025-08-06T02:09:32+00:00",
    "pushed_at": "2025-08-06T02:11:49+00:00",
    "contributors_count": 100,
    "readme_length": 9521,
    "readme_content": "<a name=\"readme-top\"></a>\n\n<div align=\"center\">\n  <img src=\"./docs/static/img/logo.png\" alt=\"Logo\" width=\"200\">\n  <h1 align=\"center\">OpenHands: Code Less, Make More</h1>\n</div>\n\n\n<div align=\"center\">\n  <a href=\"https://github.com/All-Hands-AI/OpenHands/graphs/contributors\"><img src=\"https://img.shields.io/github/contributors/All-Hands-AI/OpenHands?style=for-the-badge&color=blue\" alt=\"Contributors\"></a>\n  <a href=\"https://github.com/All-Hands-AI/OpenHands/stargazers\"><img src=\"https://img.shields.io/github/stars/All-Hands-AI/OpenHands?style=for-the-badge&color=blue\" alt=\"Stargazers\"></a>\n  <a href=\"https://github.com/All-Hands-AI/OpenHands/blob/main/LICENSE\"><img src=\"https://img.shields.io/github/license/All-Hands-AI/OpenHands?style=for-the-badge&color=blue\" alt=\"MIT License\"></a>\n  <br/>\n  <a href=\"https://join.slack.com/t/openhands-ai/shared_invite/zt-3847of6xi-xuYJIPa6YIPg4ElbDWbtSA\"><img src=\"https://img.shields.io/badge/Slack-Join%20Us-red?logo=slack&logoColor=white&style=for-the-badge\" alt=\"Join our Slack community\"></a>\n  <a href=\"https://discord.gg/ESHStjSjD4\"><img src=\"https://img.shields.io/badge/Discord-Join%20Us-purple?logo=discord&logoColor=white&style=for-the-badge\" alt=\"Join our Discord community\"></a>\n  <a href=\"https://github.com/All-Hands-AI/OpenHands/blob/main/CREDITS.md\"><img src=\"https://img.shields.io/badge/Project-Credits-blue?style=for-the-badge&color=FFE165&logo=github&logoColor=white\" alt=\"Credits\"></a>\n  <br/>\n  <a href=\"https://docs.all-hands.dev/usage/getting-started\"><img src=\"https://img.shields.io/badge/Documentation-000?logo=googledocs&logoColor=FFE165&style=for-the-badge\" alt=\"Check out the documentation\"></a>\n  <a href=\"https://arxiv.org/abs/2407.16741\"><img src=\"https://img.shields.io/badge/Paper%20on%20Arxiv-000?logoColor=FFE165&logo=arxiv&style=for-the-badge\" alt=\"Paper on Arxiv\"></a>\n  <a href=\"https://docs.google.com/spreadsheets/d/1wOUdFCMyY6Nt0AIqF705KN4JKOWgeI4wUGUP60krXXs/edit?gid=0#gid=0\"><img src=\"https://img.shields.io/badge/Benchmark%20score-000?logoColor=FFE165&logo=huggingface&style=for-the-badge\" alt=\"Evaluation Benchmark Score\"></a>\n\n  <!-- Keep these links. Translations will automatically update with the README. -->\n  <a href=\"https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=de\">Deutsch</a> |\n  <a href=\"https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=es\">Espa\u00f1ol</a> |\n  <a href=\"https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=fr\">fran\u00e7ais</a> |\n  <a href=\"https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=ja\">\u65e5\u672c\u8a9e</a> |\n  <a href=\"https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=ko\">\ud55c\uad6d\uc5b4</a> |\n  <a href=\"https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=pt\">Portugu\u00eas</a> |\n  <a href=\"https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=ru\">\u0420\u0443\u0441\u0441\u043a\u0438\u0439</a> |\n  <a href=\"https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=zh\">\u4e2d\u6587</a>\n\n  <hr>\n</div>\n\nWelcome to OpenHands (formerly OpenDevin), a platform for software development agents powered by AI.\n\nOpenHands agents can do anything a human developer can: modify code, run commands, browse the web,\ncall APIs, and yes\u2014even copy code snippets from StackOverflow.\n\nLearn more at [docs.all-hands.dev](https://docs.all-hands.dev), or [sign up for OpenHands Cloud](https://app.all-hands.dev) to get started.\n\n> [!IMPORTANT]\n> Using OpenHands for work? We'd love to chat! Fill out\n> [this short form](https://docs.google.com/forms/d/e/1FAIpQLSet3VbGaz8z32gW9Wm-Grl4jpt5WgMXPgJ4EDPVmCETCBpJtQ/viewform)\n> to join our Design Partner program, where you'll get early access to commercial features and the opportunity to provide input on our product roadmap.\n\n![App screenshot](./docs/static/img/screenshot.png)\n\n## \u2601\ufe0f OpenHands Cloud\nThe easiest way to get started with OpenHands is on [OpenHands Cloud](https://app.all-hands.dev),\nwhich comes with $20 in free credits for new users.\n\n## \ud83d\udcbb Running OpenHands Locally\n\nOpenHands can also run on your local system using Docker.\nSee the [Running OpenHands](https://docs.all-hands.dev/usage/installation) guide for\nsystem requirements and more information.\n\n> [!WARNING]\n> On a public network? See our [Hardened Docker Installation Guide](https://docs.all-hands.dev/usage/runtimes/docker#hardened-docker-installation)\n> to secure your deployment by restricting network binding and implementing additional security measures.\n\n\n```bash\ndocker pull docker.all-hands.dev/all-hands-ai/runtime:0.51-nikolaik\n\ndocker run -it --rm --pull=always \\\n    -e SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:0.51-nikolaik \\\n    -e LOG_ALL_EVENTS=true \\\n    -v /var/run/docker.sock:/var/run/docker.sock \\\n    -v ~/.openhands:/.openhands \\\n    -p 3000:3000 \\\n    --add-host host.docker.internal:host-gateway \\\n    --name openhands-app \\\n    docker.all-hands.dev/all-hands-ai/openhands:0.51\n```\n\n> **Note**: If you used OpenHands before version 0.44, you may want to run `mv ~/.openhands-state ~/.openhands` to migrate your conversation history to the new location.\n\nYou'll find OpenHands running at [http://localhost:3000](http://localhost:3000)!\n\nWhen you open the application, you'll be asked to choose an LLM provider and add an API key.\n[Anthropic's Claude Sonnet 4](https://www.anthropic.com/api) (`anthropic/claude-sonnet-4-20250514`)\nworks best, but you have [many options](https://docs.all-hands.dev/usage/llms).\n\n## \ud83d\udca1 Other ways to run OpenHands\n\n> [!WARNING]\n> OpenHands is meant to be run by a single user on their local workstation.\n> It is not appropriate for multi-tenant deployments where multiple users share the same instance. There is no built-in authentication, isolation, or scalability.\n>\n> If you're interested in running OpenHands in a multi-tenant environment, check out the source-available, commercially-licensed\n> [OpenHands Cloud Helm Chart](https://github.com/all-Hands-AI/OpenHands-cloud)\n\nYou can [connect OpenHands to your local filesystem](https://docs.all-hands.dev/usage/runtimes/docker#connecting-to-your-filesystem),\nrun OpenHands in a scriptable [headless mode](https://docs.all-hands.dev/usage/how-to/headless-mode),\ninteract with it via a [friendly CLI](https://docs.all-hands.dev/usage/how-to/cli-mode),\nor run it on tagged issues with [a github action](https://docs.all-hands.dev/usage/how-to/github-action).\n\nVisit [Running OpenHands](https://docs.all-hands.dev/usage/installation) for more information and setup instructions.\n\nIf you want to modify the OpenHands source code, check out [Development.md](https://github.com/All-Hands-AI/OpenHands/blob/main/Development.md).\n\nHaving issues? The [Troubleshooting Guide](https://docs.all-hands.dev/usage/troubleshooting) can help.\n\n## \ud83d\udcd6 Documentation\n  <a href=\"https://deepwiki.com/All-Hands-AI/OpenHands\"><img src=\"https://deepwiki.com/badge.svg\" alt=\"Ask DeepWiki\" title=\"Autogenerated Documentation by DeepWiki\"></a>\n\nTo learn more about the project, and for tips on using OpenHands,\ncheck out our [documentation](https://docs.all-hands.dev/usage/getting-started).\n\nThere you'll find resources on how to use different LLM providers,\ntroubleshooting resources, and advanced configuration options.\n\n## \ud83e\udd1d How to Join the Community\n\nOpenHands is a community-driven project, and we welcome contributions from everyone. We do most of our communication\nthrough Slack, so this is the best place to start, but we also are happy to have you contact us on Discord or Github:\n\n- [Join our Slack workspace](https://join.slack.com/t/openhands-ai/shared_invite/zt-3847of6xi-xuYJIPa6YIPg4ElbDWbtSA) - Here we talk about research, architecture, and future development.\n- [Join our Discord server](https://discord.gg/ESHStjSjD4) - This is a community-run server for general discussion, questions, and feedback.\n- [Read or post Github Issues](https://github.com/All-Hands-AI/OpenHands/issues) - Check out the issues we're working on, or add your own ideas.\n\nSee more about the community in [COMMUNITY.md](./COMMUNITY.md) or find details on contributing in [CONTRIBUTING.md](./CONTRIBUTING.md).\n\n## \ud83d\udcc8 Progress\n\nSee the monthly OpenHands roadmap [here](https://github.com/orgs/All-Hands-AI/projects/1) (updated at the maintainer's meeting at the end of each month).\n\n<p align=\"center\">\n  <a href=\"https://star-history.com/#All-Hands-AI/OpenHands&Date\">\n    <img src=\"https://api.star-history.com/svg?repos=All-Hands-AI/OpenHands&type=Date\" width=\"500\" alt=\"Star History Chart\">\n  </a>\n</p>\n\n## \ud83d\udcdc License\n\nDistributed under the MIT License. See [`LICENSE`](./LICENSE) for more information.\n\n## \ud83d\ude4f Acknowledgements\n\nOpenHands is built by a large number of contributors, and every contribution is greatly appreciated! We also build upon other open source projects, and we are deeply thankful for their work.\n\nFor a list of open source projects and licenses used in OpenHands, please see our [CREDITS.md](./CREDITS.md) file.\n\n## \ud83d\udcda Cite\n\n```\n@inproceedings{\n  wang2025openhands,\n  title={OpenHands: An Open Platform for {AI} Software Developers as Generalist Agents},\n  author={Xingyao Wang and Boxuan Li and Yufan Song and Frank F. Xu and Xiangru Tang and Mingchen Zhuge and Jiayi Pan and Yueqi Song and Bowen Li and Jaskirat Singh and Hoang H. Tran and Fuqiang Li and Ren Ma and Mingzhang Zheng and Bill Qian and Yanjun Shao and Niklas Muennighoff and Yizhe Zhang and Binyuan Hui and Junyang Lin and Robert Brennan and Hao Peng and Heng Ji and Graham Neubig},\n  booktitle={The Thirteenth International Conference on Learning Representations},\n  year={2025},\n  url={https://openreview.net/forum?id=OJd3ayDDoF}\n}\n```\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 621803253,
    "name": "Flowise",
    "full_name": "FlowiseAI/Flowise",
    "description": "Build AI Agents, Visually",
    "html_url": "https://github.com/FlowiseAI/Flowise",
    "clone_url": "https://github.com/FlowiseAI/Flowise.git",
    "owner_login": "FlowiseAI",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/128289781?v=4",
    "stargazers_count": 42334,
    "watchers_count": 42334,
    "forks_count": 21691,
    "open_issues_count": 670,
    "size": 64557,
    "language": "TypeScript",
    "languages": {
      "TypeScript": 5364021,
      "JavaScript": 3385656,
      "HTML": 690065,
      "Handlebars": 637739,
      "CSS": 17493,
      "SCSS": 6717,
      "Dockerfile": 2587,
      "Shell": 195,
      "Batchfile": 60
    },
    "topics": [
      "agentic-ai",
      "agentic-workflow",
      "agents",
      "artificial-intelligence",
      "chatbot",
      "chatgpt",
      "javascript",
      "langchain",
      "large-language-models",
      "low-code",
      "multiagent-systems",
      "no-code",
      "openai",
      "rag",
      "react",
      "typescript",
      "workflow-automation"
    ],
    "license_name": "Other",
    "created_at": "2023-03-31T12:23:09+00:00",
    "updated_at": "2025-08-06T02:06:14+00:00",
    "pushed_at": "2025-08-05T17:53:56+00:00",
    "contributors_count": 100,
    "readme_length": 7152,
    "readme_content": "<!-- markdownlint-disable MD030 -->\n\n<p align=\"center\">\n<img src=\"https://github.com/FlowiseAI/Flowise/blob/main/images/flowise_white.svg#gh-light-mode-only\">\n<img src=\"https://github.com/FlowiseAI/Flowise/blob/main/images/flowise_dark.svg#gh-dark-mode-only\">\n</p>\n\n<div align=\"center\">\n\n[![Release Notes](https://img.shields.io/github/release/FlowiseAI/Flowise)](https://github.com/FlowiseAI/Flowise/releases)\n[![Discord](https://img.shields.io/discord/1087698854775881778?label=Discord&logo=discord)](https://discord.gg/jbaHfsRVBW)\n[![Twitter Follow](https://img.shields.io/twitter/follow/FlowiseAI?style=social)](https://twitter.com/FlowiseAI)\n[![GitHub star chart](https://img.shields.io/github/stars/FlowiseAI/Flowise?style=social)](https://star-history.com/#FlowiseAI/Flowise)\n[![GitHub fork](https://img.shields.io/github/forks/FlowiseAI/Flowise?style=social)](https://github.com/FlowiseAI/Flowise/fork)\n\nEnglish | [\u7e41\u9ad4\u4e2d\u6587](./i18n/README-TW.md) | [\u7b80\u4f53\u4e2d\u6587](./i18n/README-ZH.md) | [\u65e5\u672c\u8a9e](./i18n/README-JA.md) | [\ud55c\uad6d\uc5b4](./i18n/README-KR.md)\n\n</div>\n\n<h3>Build AI Agents, Visually</h3>\n<a href=\"https://github.com/FlowiseAI/Flowise\">\n<img width=\"100%\" src=\"https://github.com/FlowiseAI/Flowise/blob/main/images/flowise_agentflow.gif?raw=true\"></a>\n\n## \ud83d\udcda Table of Contents\n\n- [\u26a1 Quick Start](#-quick-start)\n- [\ud83d\udc33 Docker](#-docker)\n- [\ud83d\udc68\u200d\ud83d\udcbb Developers](#-developers)\n- [\ud83c\udf31 Env Variables](#-env-variables)\n- [\ud83d\udcd6 Documentation](#-documentation)\n- [\ud83c\udf10 Self Host](#-self-host)\n- [\u2601\ufe0f Flowise Cloud](#\ufe0f-flowise-cloud)\n- [\ud83d\ude4b Support](#-support)\n- [\ud83d\ude4c Contributing](#-contributing)\n- [\ud83d\udcc4 License](#-license)\n\n## \u26a1Quick Start\n\nDownload and Install [NodeJS](https://nodejs.org/en/download) >= 18.15.0\n\n1. Install Flowise\n    ```bash\n    npm install -g flowise\n    ```\n2. Start Flowise\n\n    ```bash\n    npx flowise start\n    ```\n\n3. Open [http://localhost:3000](http://localhost:3000)\n\n## \ud83d\udc33 Docker\n\n### Docker Compose\n\n1. Clone the Flowise project\n2. Go to `docker` folder at the root of the project\n3. Copy `.env.example` file, paste it into the same location, and rename to `.env` file\n4. `docker compose up -d`\n5. Open [http://localhost:3000](http://localhost:3000)\n6. You can bring the containers down by `docker compose stop`\n\n### Docker Image\n\n1. Build the image locally:\n   \n    ```bash\n    docker build --no-cache -t flowise .\n    ```\n2. Run image:\n   \n    ```bash\n    docker run -d --name flowise -p 3000:3000 flowise\n    ```\n\n3. Stop image:\n   \n    ```bash\n    docker stop flowise\n    ```\n\n## \ud83d\udc68\u200d\ud83d\udcbb Developers\n\nFlowise has 3 different modules in a single mono repository.\n\n-   `server`: Node backend to serve API logics\n-   `ui`: React frontend\n-   `components`: Third-party nodes integrations\n-   `api-documentation`: Auto-generated swagger-ui API docs from express\n\n### Prerequisite\n\n-   Install [PNPM](https://pnpm.io/installation)\n    ```bash\n    npm i -g pnpm\n    ```\n\n### Setup\n\n1.  Clone the repository:\n\n    ```bash\n    git clone https://github.com/FlowiseAI/Flowise.git\n    ```\n\n2.  Go into repository folder:\n\n    ```bash\n    cd Flowise\n    ```\n\n3.  Install all dependencies of all modules:\n\n    ```bash\n    pnpm install\n    ```\n\n4.  Build all the code:\n\n    ```bash\n    pnpm build\n    ```\n\n    <details>\n    <summary>Exit code 134 (JavaScript heap out of memory)</summary>  \n      If you get this error when running the above `build` script, try increasing the Node.js heap size and run the script again:\n\n        export NODE_OPTIONS=\"--max-old-space-size=4096\"\n        pnpm build\n\n    </details>\n\n5.  Start the app:\n\n    ```bash\n    pnpm start\n    ```\n\n    You can now access the app on [http://localhost:3000](http://localhost:3000)\n\n6.  For development build:\n\n    -   Create `.env` file and specify the `VITE_PORT` (refer to `.env.example`) in `packages/ui`\n    -   Create `.env` file and specify the `PORT` (refer to `.env.example`) in `packages/server`\n    -   Run:\n\n        ```bash\n        pnpm dev\n        ```\n\n    Any code changes will reload the app automatically on [http://localhost:8080](http://localhost:8080)\n\n## \ud83c\udf31 Env Variables\n\nFlowise supports different environment variables to configure your instance. You can specify the following variables in the `.env` file inside `packages/server` folder. Read [more](https://github.com/FlowiseAI/Flowise/blob/main/CONTRIBUTING.md#-env-variables)\n\n## \ud83d\udcd6 Documentation\n\nYou can view the Flowise Docs [here](https://docs.flowiseai.com/)\n\n## \ud83c\udf10 Self Host\n\nDeploy Flowise self-hosted in your existing infrastructure, we support various [deployments](https://docs.flowiseai.com/configuration/deployment)\n\n-   [AWS](https://docs.flowiseai.com/configuration/deployment/aws)\n-   [Azure](https://docs.flowiseai.com/configuration/deployment/azure)\n-   [Digital Ocean](https://docs.flowiseai.com/configuration/deployment/digital-ocean)\n-   [GCP](https://docs.flowiseai.com/configuration/deployment/gcp)\n-   [Alibaba Cloud](https://computenest.console.aliyun.com/service/instance/create/default?type=user&ServiceName=Flowise\u793e\u533a\u7248)\n-   <details>\n      <summary>Others</summary>\n\n    -   [Railway](https://docs.flowiseai.com/configuration/deployment/railway)\n\n        [![Deploy on Railway](https://railway.app/button.svg)](https://railway.app/template/pn4G8S?referralCode=WVNPD9)\n\n    -   [Render](https://docs.flowiseai.com/configuration/deployment/render)\n\n        [![Deploy to Render](https://render.com/images/deploy-to-render-button.svg)](https://docs.flowiseai.com/configuration/deployment/render)\n\n    -   [HuggingFace Spaces](https://docs.flowiseai.com/deployment/hugging-face)\n\n        <a href=\"https://huggingface.co/spaces/FlowiseAI/Flowise\"><img src=\"https://huggingface.co/datasets/huggingface/badges/raw/main/open-in-hf-spaces-sm.svg\" alt=\"HuggingFace Spaces\"></a>\n\n    -   [Elestio](https://elest.io/open-source/flowiseai)\n\n        [![Deploy on Elestio](https://elest.io/images/logos/deploy-to-elestio-btn.png)](https://elest.io/open-source/flowiseai)\n\n    -   [Sealos](https://template.sealos.io/deploy?templateName=flowise)\n\n        [![Deploy on Sealos](https://sealos.io/Deploy-on-Sealos.svg)](https://template.sealos.io/deploy?templateName=flowise)\n\n    -   [RepoCloud](https://repocloud.io/details/?app_id=29)\n\n        [![Deploy on RepoCloud](https://d16t0pc4846x52.cloudfront.net/deploy.png)](https://repocloud.io/details/?app_id=29)\n\n      </details>\n\n## \u2601\ufe0f Flowise Cloud\n\nGet Started with [Flowise Cloud](https://flowiseai.com/).\n\n## \ud83d\ude4b Support\n\nFeel free to ask any questions, raise problems, and request new features in [Discussion](https://github.com/FlowiseAI/Flowise/discussions).\n\n## \ud83d\ude4c Contributing\n\nThanks go to these awesome contributors\n\n<a href=\"https://github.com/FlowiseAI/Flowise/graphs/contributors\">\n<img src=\"https://contrib.rocks/image?repo=FlowiseAI/Flowise\" />\n</a><br><br>\n\nSee [Contributing Guide](CONTRIBUTING.md). Reach out to us at [Discord](https://discord.gg/jbaHfsRVBW) if you have any questions or issues.\n\n[![Star History Chart](https://api.star-history.com/svg?repos=FlowiseAI/Flowise&type=Timeline)](https://star-history.com/#FlowiseAI/Flowise&Date)\n\n## \ud83d\udcc4 License\n\nSource code in this repository is made available under the [Apache License Version 2.0](LICENSE.md).\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 26783295,
    "name": "kong",
    "full_name": "Kong/kong",
    "description": "\ud83e\udd8d The Cloud-Native API Gateway and AI Gateway.",
    "html_url": "https://github.com/Kong/kong",
    "clone_url": "https://github.com/Kong/kong.git",
    "owner_login": "Kong",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/962416?v=4",
    "stargazers_count": 41468,
    "watchers_count": 41468,
    "forks_count": 4958,
    "open_issues_count": 144,
    "size": 95313,
    "language": "Lua",
    "languages": {
      "Lua": 10059805,
      "Perl": 587765,
      "Raku": 343862,
      "Starlark": 110446,
      "Shell": 98520,
      "Python": 46550,
      "Makefile": 9727,
      "CSS": 5812,
      "Dockerfile": 3847,
      "Linker Script": 652
    },
    "topics": [
      "ai",
      "ai-gateway",
      "api-gateway",
      "api-management",
      "apis",
      "artificial-intelligence",
      "cloud-native",
      "devops",
      "kubernetes",
      "kubernetes-ingress",
      "kubernetes-ingress-controller",
      "llm-gateway",
      "llm-ops",
      "luajit",
      "microservice",
      "microservices",
      "nginx",
      "openai-proxy",
      "reverse-proxy",
      "serverless"
    ],
    "license_name": "Apache License 2.0",
    "created_at": "2014-11-17T23:56:08+00:00",
    "updated_at": "2025-08-06T02:03:21+00:00",
    "pushed_at": "2025-08-05T21:21:30+00:00",
    "contributors_count": 100,
    "readme_length": 8628,
    "readme_content": "[![][kong-logo]][kong-url]\n\n![Stars](https://img.shields.io/github/stars/Kong/kong?style=flat-square) ![GitHub commit activity](https://img.shields.io/github/commit-activity/m/Kong/kong?style=flat-square) ![Docker Pulls](https://img.shields.io/docker/pulls/_/kong?style=flat-square) [![Build Status][badge-action-image]][badge-action-url] ![Version](https://img.shields.io/github/v/release/Kong/kong?color=green&label=Version&style=flat-square)  ![License](https://img.shields.io/badge/License-Apache%202.0-blue?style=flat-square)  ![Twitter Follow](https://img.shields.io/twitter/follow/thekonginc?style=social)\n\n\n**Kong** or **Kong API Gateway** is a cloud-native, platform-agnostic, scalable API Gateway distinguished for its high performance and extensibility via plugins. It also provides advanced AI capabilities with multi-LLM support.\n\nBy providing functionality for proxying, routing, load balancing, health checking, authentication (and [more](#features)), Kong serves as the central layer for orchestrating microservices or conventional API traffic with ease.\n\nKong runs natively on Kubernetes thanks to its official [Kubernetes Ingress Controller](https://github.com/Kong/kubernetes-ingress-controller).\n\n---\n\n[Installation](https://konghq.com/install/#kong-community) | [Documentation](https://docs.konghq.com) | [Discussions](https://github.com/Kong/kong/discussions) | [Forum](https://discuss.konghq.com) | [Blog](https://konghq.com/blog) | [Builds][kong-master-builds] | [Cloud Hosted Kong](https://konghq.com/kong-konnect/)\n\n---\n\n## Getting Started\n\nIf you prefer to use a cloud-hosted Kong, you can [sign up for a free trial of Kong Konnect](https://konghq.com/products/kong-konnect/register?utm_medium=Referral&utm_source=Github&utm_campaign=kong-gateway&utm_content=konnect-promo-in-gateway&utm_term=get-started) and get started in minutes. If not, you can follow the instructions below to get started with Kong on your own infrastructure.\n\nLet\u2019s test drive Kong by adding authentication to an API in under 5 minutes.\n\nWe suggest using the docker-compose distribution via the instructions below, but there is also a [docker installation](https://docs.konghq.com/gateway/latest/install/docker/#install-kong-gateway-in-db-less-mode) procedure if you\u2019d prefer to run the Kong API Gateway in DB-less mode.\n\nWhether you\u2019re running in the cloud, on bare metal, or using containers, you can find every supported distribution on our [official installation](https://konghq.com/install/#kong-community) page.\n\n1) To start, clone the Docker repository and navigate to the compose folder.\n```cmd\n  $ git clone https://github.com/Kong/docker-kong\n  $ cd docker-kong/compose/\n```\n\n2) Start the Gateway stack using:\n```cmd\n  $ KONG_DATABASE=postgres docker-compose --profile database up\n```\n\nThe Gateway is now available on the following ports on localhost:\n\n- `:8000` - send traffic to your service via Kong\n- `:8001` - configure Kong using Admin API or via [decK](https://github.com/kong/deck)\n- `:8002` - access Kong's management Web UI ([Kong Manager](https://github.com/Kong/kong-manager)) on [localhost:8002](http://localhost:8002)\n\nNext, follow the [quick start guide](https://docs.konghq.com/gateway-oss/latest/getting-started/configuring-a-service/\n) to tour the Gateway features.\n\n## Features\n\nBy centralizing common API functionality across all your organization's services, the Kong API Gateway creates more freedom for engineering teams to focus on the challenges that matter most.\n\nThe top Kong features include:\n\n- Advanced routing, load balancing, health checking - all configurable via a RESTful admin API or declarative configuration.\n- Authentication and authorization for APIs using methods like JWT, basic auth, OAuth, ACLs and more.\n- Proxy, SSL/TLS termination, and connectivity support for L4 or L7 traffic.\n- Plugins for enforcing traffic controls, rate limiting, req/res transformations, logging, monitoring and including a plugin developer hub.\n- Plugins for AI traffic to support multi-LLM implementations and no-code AI use cases, with advanced AI prompt engineering, AI observability, AI security and more.\n- Sophisticated deployment models like Declarative Databaseless Deployment and Hybrid Deployment (control plane/data plane separation) without any vendor lock-in.\n- Native [ingress controller](https://github.com/Kong/kubernetes-ingress-controller) support for serving Kubernetes.\n\n[![][kong-benefits]][kong-url]\n\n### Plugin Hub\n\nPlugins provide advanced functionality that extends the use of the Gateway. Many of the Kong Inc. and community-developed plugins like AWS Lambda, Correlation ID, and Response Transformer are showcased at the [Plugin Hub](https://docs.konghq.com/hub/).\n\nContribute to the Plugin Hub and ensure your next innovative idea is published and available to the broader community!\n\n## Contributing\n\nWe \u2764\ufe0f pull requests, and we\u2019re continually working hard to make it as easy as possible for developers to contribute. Before beginning development with the Kong API Gateway, please familiarize yourself with the following developer resources:\n\n- Community Pledge ([COMMUNITY_PLEDGE.md](COMMUNITY_PLEDGE.md)) for our pledge to interact with you, the open source community.\n- Contributor Guide ([CONTRIBUTING.md](CONTRIBUTING.md)) to learn about how to contribute to Kong.\n- Development Guide ([DEVELOPER.md](DEVELOPER.md)): Setting up your development environment.\n- [CODE_OF_CONDUCT](CODE_OF_CONDUCT.md) and [COPYRIGHT](COPYRIGHT)\n\nUse the [Plugin Development Guide](https://docs.konghq.com/latest/plugin-development/) for building new and creative plugins, or browse the online version of Kong's source code documentation in the [Plugin Development Kit (PDK) Reference](https://docs.konghq.com/latest/pdk/). Developers can build plugins in [Lua](https://docs.konghq.com/gateway/latest/plugin-development/), [Go](https://docs.konghq.com/gateway-oss/latest/external-plugins/#developing-go-plugins) or [JavaScript](https://docs.konghq.com/gateway-oss/latest/external-plugins/#developing-javascript-plugins).\n\n## Releases\n\nPlease see the [Changelog](CHANGELOG.md) for more details about a given release. The [SemVer Specification](https://semver.org) is followed when versioning Gateway releases.\n\n## Join the Community\n\n- Check out the [docs](https://docs.konghq.com/)\n- Join the [Kong discussions forum](https://github.com/Kong/kong/discussions)\n- Join the Kong discussions at the Kong Nation forum: [https://discuss.konghq.com/](https://discuss.konghq.com/)\n- Join our [Community Slack](http://kongcommunity.slack.com/)\n- Read up on the latest happenings at our [blog](https://konghq.com/blog/)\n- Follow us on [X](https://x.com/thekonginc)\n- Subscribe to our [YouTube channel](https://www.youtube.com/c/KongInc/videos)\n- Visit our [homepage](https://konghq.com/) to learn more\n\n## Konnect Cloud\n\nKong Inc. offers commercial subscriptions that enhance the Kong API Gateway in a variety of ways. Customers of Kong's [Konnect Cloud](https://konghq.com/kong-konnect/) subscription take advantage of additional gateway functionality, commercial support, and access to Kong's managed (SaaS) control plane platform. The Konnect Cloud platform features include real-time analytics, a service catalog, developer portals, and so much more! [Get started](https://konghq.com/products/kong-konnect/register?utm_medium=Referral&utm_source=Github&utm_campaign=kong-gateway&utm_content=konnect-promo-in-gateway&utm_term=get-started) with Konnect Cloud.\n\n## License\n\n```\nCopyright 2016-2025 Kong Inc.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n```\n\n[kong-url]: https://konghq.com/\n[kong-logo]: https://konghq.com/wp-content/uploads/2018/05/kong-logo-github-readme.png\n[kong-benefits]: https://konghq.com/wp-content/uploads/2018/05/kong-benefits-github-readme.png\n[kong-master-builds]: https://hub.docker.com/r/kong/kong/tags\n[badge-action-url]: https://github.com/Kong/kong/actions\n[badge-action-image]: https://github.com/Kong/kong/actions/workflows/build_and_test.yml/badge.svg?branch=master&event=push\n\n[busted]: https://github.com/Olivine-Labs/busted\n[luacheck]: https://github.com/mpeterv/luacheck\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 344190478,
    "name": "AI-For-Beginners",
    "full_name": "microsoft/AI-For-Beginners",
    "description": "12 Weeks, 24 Lessons, AI for All!",
    "html_url": "https://github.com/microsoft/AI-For-Beginners",
    "clone_url": "https://github.com/microsoft/AI-For-Beginners.git",
    "owner_login": "microsoft",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/6154722?v=4",
    "stargazers_count": 39405,
    "watchers_count": 39405,
    "forks_count": 7594,
    "open_issues_count": 60,
    "size": 140015,
    "language": "Jupyter Notebook",
    "languages": {
      "Jupyter Notebook": 26193652,
      "Python": 36131,
      "HTML": 13907,
      "Vue": 5103,
      "JavaScript": 2433,
      "Dockerfile": 1006,
      "Shell": 921
    },
    "topics": [
      "ai",
      "artificial-intelligence",
      "cnn",
      "computer-vision",
      "deep-learning",
      "gan",
      "machine-learning",
      "microsoft-for-beginners",
      "nlp",
      "rnn"
    ],
    "license_name": "MIT License",
    "created_at": "2021-03-03T16:27:36+00:00",
    "updated_at": "2025-08-06T02:13:03+00:00",
    "pushed_at": "2025-06-25T19:07:05+00:00",
    "contributors_count": 52,
    "readme_length": 21449,
    "readme_content": "[![GitHub license](https://img.shields.io/github/license/microsoft/AI-For-Beginners.svg)](https://github.com/microsoft/AI-For-Beginners/blob/main/LICENSE)\n[![GitHub contributors](https://img.shields.io/github/contributors/microsoft/AI-For-Beginners.svg)](https://GitHub.com/microsoft/AI-For-Beginners/graphs/contributors/)\n[![GitHub issues](https://img.shields.io/github/issues/microsoft/AI-For-Beginners.svg)](https://GitHub.com/microsoft/AI-For-Beginners/issues/)\n[![GitHub pull-requests](https://img.shields.io/github/issues-pr/microsoft/AI-For-Beginners.svg)](https://GitHub.com/microsoft/AI-For-Beginners/pulls/)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)\n\n[![GitHub watchers](https://img.shields.io/github/watchers/microsoft/AI-For-Beginners.svg?style=social&label=Watch)](https://GitHub.com/microsoft/AI-For-Beginners/watchers/)\n[![GitHub forks](https://img.shields.io/github/forks/microsoft/AI-For-Beginners.svg?style=social&label=Fork)](https://GitHub.com/microsoft/AI-For-Beginners/network/)\n[![GitHub stars](https://img.shields.io/github/stars/microsoft/AI-For-Beginners.svg?style=social&label=Star)](https://GitHub.com/microsoft/AI-For-Beginners/stargazers/)\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/microsoft/ai-for-beginners/HEAD)\n[![Gitter](https://badges.gitter.im/Microsoft/ai-for-beginners.svg)](https://gitter.im/Microsoft/ai-for-beginners?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)\n\n[![](https://dcbadge.vercel.app/api/server/ByRwuEEgH4)](https://discord.gg/zxKYvhSnVp?WT.mc_id=academic-000002-leestott)\n\n# Artificial Intelligence for Beginners - A Curriculum\n\n|![ Sketchnote by [(@girlie_mac)](https://twitter.com/girlie_mac) ](./lessons/sketchnotes/ai-overview.png)|\n|:---:|\n| AI For Beginners - _Sketchnote by [@girlie_mac](https://twitter.com/girlie_mac)_ |\n\nExplore the world of **Artificial Intelligence** (AI) with our 12-week, 24-lesson curriculum!  It includes practical lessons, quizzes, and labs. The curriculum is beginner-friendly and covers tools like TensorFlow and PyTorch, as well as ethics in AI\n\n## What you will learn\n\n**[Mindmap of the Course](http://soshnikov.com/courses/ai-for-beginners/mindmap.html)**\n\nIn this curriculum, you will learn:\n\n* Different approaches to Artificial Intelligence, including the \"good old\" symbolic approach with **Knowledge Representation** and reasoning ([GOFAI](https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence)).\n* **Neural Networks** and **Deep Learning**, which are at the core of modern AI. We will illustrate the concepts behind these important topics using code in two of the most popular frameworks - [TensorFlow](http://Tensorflow.org) and [PyTorch](http://pytorch.org).\n* **Neural Architectures** for working with images and text. We will cover recent models but may be a bit lacking in the state-of-the-art.\n* Less popular AI approaches, such as **Genetic Algorithms** and **Multi-Agent Systems**.\n\nWhat we will not cover in this curriculum:\n\n> [Find all additional resources for this course in our Microsoft Learn collection](https://learn.microsoft.com/en-us/collections/7w28iy2xrqzdj0?WT.mc_id=academic-77998-bethanycheum)\n\n* Business cases of using **AI in Business**. Consider taking [Introduction to AI for business users](https://docs.microsoft.com/learn/paths/introduction-ai-for-business-users/?WT.mc_id=academic-77998-bethanycheum) learning path on Microsoft Learn, or [AI Business School](https://www.microsoft.com/ai/ai-business-school/?WT.mc_id=academic-77998-bethanycheum), developed in cooperation with [INSEAD](https://www.insead.edu/).\n* **Classic Machine Learning**, which is well described in our [Machine Learning for Beginners Curriculum](http://github.com/Microsoft/ML-for-Beginners).\n* Practical AI applications built using **[Cognitive Services](https://azure.microsoft.com/services/cognitive-services/?WT.mc_id=academic-77998-bethanycheum)**. For this, we recommend that you start with modules Microsoft Learn for [vision](https://docs.microsoft.com/learn/paths/create-computer-vision-solutions-azure-cognitive-services/?WT.mc_id=academic-77998-bethanycheum), [natural language processing](https://docs.microsoft.com/learn/paths/explore-natural-language-processing/?WT.mc_id=academic-77998-bethanycheum), **[Generative AI with Azure OpenAI Service](https://learn.microsoft.com/en-us/training/paths/develop-ai-solutions-azure-openai/?WT.mc_id=academic-77998-bethanycheum)** and others.\n* Specific ML **Cloud Frameworks**, such as [Azure Machine Learning](https://azure.microsoft.com/services/machine-learning/?WT.mc_id=academic-77998-bethanycheum), [Microsoft Fabric](https://learn.microsoft.com/en-us/training/paths/get-started-fabric/?WT.mc_id=academic-77998-bethanycheum), or [Azure Databricks](https://docs.microsoft.com/learn/paths/data-engineer-azure-databricks?WT.mc_id=academic-77998-bethanycheum). Consider using [Build and operate machine learning solutions with Azure Machine Learning](https://docs.microsoft.com/learn/paths/build-ai-solutions-with-azure-ml-service/?WT.mc_id=academic-77998-bethanycheum) and [Build and Operate Machine Learning Solutions with Azure Databricks](https://docs.microsoft.com/learn/paths/build-operate-machine-learning-solutions-azure-databricks/?WT.mc_id=academic-77998-bethanycheum) learning paths.\n* **Conversational AI** and **Chat Bots**. There is a separate [Create conversational AI solutions](https://docs.microsoft.com/learn/paths/create-conversational-ai-solutions/?WT.mc_id=academic-77998-bethanycheum) learning path, and you can also refer to [this blog post](https://soshnikov.com/azure/hello-bot-conversational-ai-on-microsoft-platform/) for more detail.\n* **Deep Mathematics** behind deep learning. For this, we would recommend [Deep Learning](https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618) by Ian Goodfellow, Yoshua Bengio and Aaron Courville, which is also available online at [https://www.deeplearningbook.org/](https://www.deeplearningbook.org/).\n\nFor a gentle introduction to _AI in the Cloud_ topics you may consider taking the [Get started with artificial intelligence on Azure](https://docs.microsoft.com/learn/paths/get-started-with-artificial-intelligence-on-azure/?WT.mc_id=academic-77998-bethanycheum) Learning Path.\n\n# Content\n\n|     |                                                                 Lesson Link                                                                  |                                           PyTorch/Keras/TensorFlow                                          | Lab                                                            |\n| :-: | :------------------------------------------------------------------------------------------------------------------------------------------: | :---------------------------------------------------------------------------------------------: | ------------------------------------------------------------------------------ |\n| 0  |                                 [Course Setup](./lessons/0-course-setup/setup.md)                                 |                      [Setup Your Development Environment](./lessons/0-course-setup/how-to-run.md)                       |   |\n| I  |               [**Introduction to AI**](./lessons/1-Intro/README.md)      | | |\n| 01  |       [Introduction and History of AI](./lessons/1-Intro/README.md)       |           -                            | -  |\n| II |              **Symbolic AI**              |\n| 02  |       [Knowledge Representation and Expert Systems](./lessons/2-Symbolic/README.md)       |            [Expert Systems](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/2-Symbolic/Animals.ipynb) /  [Ontology](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/2-Symbolic/FamilyOntology.ipynb) /[Concept Graph](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/2-Symbolic/MSConceptGraph.ipynb)                             |  |\n| III |                        [**Introduction to Neural Networks**](./lessons/3-NeuralNetworks/README.md) |||\n| 03  |                [Perceptron](./lessons/3-NeuralNetworks/03-Perceptron/README.md)                 |                       [Notebook](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/3-NeuralNetworks/03-Perceptron/Perceptron.ipynb)                      | [Lab](./lessons/3-NeuralNetworks/03-Perceptron/lab/README.md) |\n| 04  |                   [Multi-Layered Perceptron and Creating our own Framework](./lessons/3-NeuralNetworks/04-OwnFramework/README.md)                   |        [Notebook](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb)        | [Lab](./lessons/3-NeuralNetworks/04-OwnFramework/lab/README.md) |\n| 05  |            [Intro to Frameworks (PyTorch/TensorFlow) and Overfitting](./lessons/3-NeuralNetworks/05-Frameworks/README.md)             |           [PyTorch](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/3-NeuralNetworks/05-Frameworks/IntroPyTorch.ipynb) / [Keras](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/3-NeuralNetworks/05-Frameworks/IntroKeras.ipynb) / [TensorFlow](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/3-NeuralNetworks/05-Frameworks/IntroKerasTF.ipynb)             | [Lab](./lessons/3-NeuralNetworks/05-Frameworks/lab/README.md) |\n| IV  |            [**Computer Vision**](./lessons/4-ComputerVision/README.md)             | [PyTorch](https://docs.microsoft.com/learn/modules/intro-computer-vision-pytorch/?WT.mc_id=academic-77998-cacaste) / [TensorFlow](https://docs.microsoft.com/learn/modules/intro-computer-vision-TensorFlow/?WT.mc_id=academic-77998-cacaste)| [Explore Computer Vision on Microsoft Azure](https://learn.microsoft.com/en-us/collections/7w28iy2xrqzdj0?WT.mc_id=academic-77998-bethanycheum) |\n| 06  |            [Intro to Computer Vision. OpenCV](./lessons/4-ComputerVision/06-IntroCV/README.md)             |           [Notebook](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/4-ComputerVision/06-IntroCV/OpenCV.ipynb)         | [Lab](./lessons/4-ComputerVision/06-IntroCV/lab/README.md) |\n| 07  |            [Convolutional Neural Networks](./lessons/4-ComputerVision/07-ConvNets/README.md) &  [CNN Architectures](./lessons/4-ComputerVision/07-ConvNets/CNN_Architectures.md)             |           [PyTorch](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/4-ComputerVision/07-ConvNets/ConvNetsPyTorch.ipynb) /[TensorFlow](https://microsoft.github.io/AI-For-Beginners/lessons/4-ComputerVision/07-ConvNets/ConvNetsTF.ipynb)             | [Lab](./lessons/4-ComputerVision/07-ConvNets/lab/README.md) |\n| 08  |            [Pre-trained Networks and Transfer Learning](./lessons/4-ComputerVision/08-TransferLearning/README.md) and [Training Tricks](./lessons/4-ComputerVision/08-TransferLearning/TrainingTricks.md)             |           [PyTorch](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/4-ComputerVision/08-TransferLearning/TransferLearningPyTorch.ipynb) / [TensorFlow](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/3-NeuralNetworks/05-Frameworks/IntroKerasTF.ipynb)             | [Lab](./lessons/4-ComputerVision/08-TransferLearning/lab/README.md) |\n| 09  |            [Autoencoders and VAEs](./lessons/4-ComputerVision/09-Autoencoders/README.md)             |           [PyTorch](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/4-ComputerVision/09-Autoencoders/AutoEncodersPyTorch.ipynb) / [TensorFlow](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/4-ComputerVision/09-Autoencoders/AutoencodersTF.ipynb)             |  |\n| 10  |            [Generative Adversarial Networks & Artistic Style Transfer](./lessons/4-ComputerVision/10-GANs/README.md)             |           [PyTorch](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/4-ComputerVision/10-GANs/GANPyTorch.ipynb) / [TensorFlow](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/4-ComputerVision/10-GANs/GANTF.ipynb)             |  |\n| 11  |            [Object Detection](./lessons/4-ComputerVision/11-ObjectDetection/README.md)             |         [TensorFlow](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/4-ComputerVision/11-ObjectDetection/ObjectDetection.ipynb)             | [Lab](./lessons/4-ComputerVision/11-ObjectDetection/lab/README.md) |\n| 12  |            [Semantic Segmentation. U-Net](./lessons/4-ComputerVision/12-Segmentation/README.md)             |           [PyTorch](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/4-ComputerVision/12-Segmentation/SemanticSegmentationPytorch.ipynb) / [TensorFlow]((https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/4-ComputerVision/12-Segmentation/SemanticSegmentationTF.ipynb))             |  |\n| V  |            [**Natural Language Processing**](./lessons/5-NLP/README.md)             | [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste) /[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-TensorFlow/?WT.mc_id=academic-77998-cacaste) | [Explore Natural Language Processing on Microsoft Azure](https://learn.microsoft.com/en-us/collections/7w28iy2xrqzdj0?WT.mc_id=academic-77998-bethanycheum)|\n| 13  |            [Text Representation. Bow/TF-IDF](./lessons/5-NLP/13-TextRep/README.md)             |           [PyTorch](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb) / [TensorFlow](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/5-NLP/13-TextRep/TextRepresentationTF.ipynb)             | |\n| 14  |            [Semantic word embeddings. Word2Vec and GloVe](./lessons/5-NLP/14-Embeddings/README.md)             |           [PyTorch](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb) / [TensorFlow](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb)             |  |\n| 15  |            [Language Modeling. Training your own embeddings](./lessons/5-NLP/15-LanguageModeling/README.md)             |           [PyTorch](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/5-NLP/15-LanguageModeling/CBoW-PyTorch.ipynb) / [TensorFlow](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/5-NLP/15-LanguageModeling/CBoW-TF.ipynb)             | [Lab](./lessons/5-NLP/15-LanguageModeling/lab/README.md) |\n| 16  |            [Recurrent Neural Networks](./lessons/5-NLP/16-RNN/README.md)             |           [PyTorch](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/5-NLP/16-RNN/RNNPyTorch.ipynb) / [TensorFlow](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/5-NLP/16-RNN/RNNTF.ipynb)             |  |\n| 17  |            [Generative Recurrent Networks](./lessons/5-NLP/17-GenerativeNetworks/README.md)             |           [PyTorch](https://microsoft.github.io/AI-For-Beginners/lessons/5-NLP/17-GenerativeNetworks/GenerativePyTorch.md) / [TensorFlow](https://microsoft.github.io/AI-For-Beginners/lessons/5-NLP/17-GenerativeNetworks/GenerativeTF.md)             | [Lab](./lessons/5-NLP/17-GenerativeNetworks/lab/README.md) |\n| 18  |            [Transformers. BERT.](./lessons/5-NLP/18-Transformers/READMEtransformers.md)             |           [PyTorch](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb) /[TensorFlow](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/5-NLP/18-Transformers/TransformersTF.ipynb)             |  |\n| 19  |            [Named Entity Recognition](./lessons/5-NLP/19-NER/README.md)             |           [TensorFlow](https://microsoft.github.io/AI-For-Beginners/lessons/5-NLP/19-NER/NER-TF.ipynb)             | [Lab](./lessons/5-NLP/19-NER/lab/README.md) |\n| 20  |            [Large Language Models, Prompt Programming and Few-Shot Tasks](./lessons/5-NLP/20-LangModels/READMELargeLang.md)             |           [PyTorch](https://microsoft.github.io/AI-For-Beginners/lessons/5-NLP/20-LangModels/GPT-PyTorch.ipynb) | |\n| VI |            **Other AI Techniques** || |\n| 21  |            [Genetic Algorithms](./lessons/6-Other/21-GeneticAlgorithms/README.md)             |           [Notebook](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/6-Other/21-GeneticAlgorithms/Genetic.ipynb) | |\n| 22  |            [Deep Reinforcement Learning](./lessons/6-Other/22-DeepRL/README.md)             |           [PyTorch](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb) /[TensorFlow](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb)             | [Lab](./lessons/6-Other/22-DeepRL/lab/README.md) |\n| 23  |            [Multi-Agent Systems](./lessons/6-Other/23-MultiagentSystems/README.md)             |  | |\n| VII |            **AI Ethics** | | |\n| 24  |            [AI Ethics and Responsible AI](./lessons/7-Ethics/README.md)             |           [Microsoft Learn: Responsible AI Principles](https://docs.microsoft.com/learn/paths/responsible-ai-business-principles/?WT.mc_id=academic-77998-cacaste) | |\n| IX  |            **Extras** | | |\n| 25  |            [Multi-Modal Networks, CLIP and VQGAN](./lessons/X-Extras/X1-MultiModal/README.md)             |           [Notebook](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/X-Extras/X1-MultiModal/Clip.ipynb)    | |\n\n## Each lesson contains\n\n* Pre-reading material\n* Executable Jupyter Notebooks, which are often specific to the framework (**PyTorch** or **TensorFlow**). The executable notebook also contains a lot of theoretical material, so to understand the topic you need to go through at least one version of the notebook (either PyTorch or TensorFlow).\n* **Labs** available for some topics, which give you an opportunity to try applying the material you have learned to a specific problem.\n* Some sections contain links to [**MS Learn**](https://learn.microsoft.com/en-us/collections/7w28iy2xrqzdj0?WT.mc_id=academic-77998-bethanycheum) modules that cover related topics.\n\n## Getting Started\n\n- We have created a [setup lesson](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/0-course-setup/setup.md) to help you with setting up your development environment. - For Educators, we have created a [curricula setup lesson](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/0-course-setup/for-teachers.md) for you too!\n- How to [Run the code in a VSCode or a Codepace](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/0-course-setup/how-to-run.md)\n\nFollow these steps:\n\nFork the Repository: Click on the \"Fork\" button at the top-right corner of this page.\n\nClone the Repository: `git clone https://github.com/microsoft/AI-For-Beginners.git`\n\nDon't forget to star (\ud83c\udf1f) this repo to find it easier later.\n\n## Meet other Learners\n\nJoin our [official AI Discord server](https://aka.ms/genai-discord?WT.mc_id=academic-105485-bethanycheum) to meet and network with other learners taking this course and get support.\n\nIf you have product feedback or questions whilst building visit our [Azure AI Foundry Developer Forum](https://aka.ms/foundry/forum)\n\n## Quizzes \n\n> **A note about quizzes**: All quizzes are contained in the Quiz-app folder in etc\\quiz-app, They are linked from within the lessons the quiz app can be run locally or deployed to Azure; follow the instruction in the `quiz-app` folder. They are gradually being localized.\n\n## Help Wanted\n\nDo you have suggestions or found spelling or code errors? Raise an issue or create a pull request.\n\n## Special Thanks\n\n* **\u270d\ufe0f Primary Author:** [Dmitry Soshnikov](http://soshnikov.com), PhD\n* **\ud83d\udd25 Editor:** [Jen Looper](https://twitter.com/jenlooper), PhD\n* **\ud83c\udfa8 Sketchnote illustrator:** [Tomomi Imura](https://twitter.com/girlie_mac)\n* **\u2705 Quiz Creator:** [Lateefah Bello](https://github.com/CinnamonXI), [MLSA](https://studentambassadors.microsoft.com/)\n* **\ud83d\ude4f Core Contributors:** [Evgenii Pishchik](https://github.com/Pe4enIks)\n\n## Other Curricula\n\nOur team produces other curricula! Check out:\n\n- [Generative AI for Beginners](https://aka.ms/genai-beginners)\n- [Generative AI for Beginners .NET](https://github.com/microsoft/Generative-AI-for-beginners-dotnet)\n- [Generative AI with JavaScript](https://github.com/microsoft/generative-ai-with-javascript)\n- [AI for Beginners](https://aka.ms/ai-beginners)\n- [Data Science for Beginners](https://aka.ms/datascience-beginners)\n- [ML for Beginners](https://aka.ms/ml-beginners)\n- [Cybersecurity for Beginners](https://github.com/microsoft/Security-101) \n- [Web Dev for Beginners](https://aka.ms/webdev-beginners)\n- [IoT for Beginners](https://aka.ms/iot-beginners)\n- [XR Development for Beginners](https://github.com/microsoft/xr-development-for-beginners)\n- [Mastering GitHub Copilot for Paired Programming](https://github.com/microsoft/Mastering-GitHub-Copilot-for-Paired-Programming)\n- [Mastering GitHub Copilot for C#/.NET Developers](https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers)\n- [Choose Your Own Copilot Adventure](https://github.com/microsoft/CopilotAdventures)\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 642323624,
    "name": "DragGAN",
    "full_name": "XingangPan/DragGAN",
    "description": "Official Code for DragGAN (SIGGRAPH 2023)",
    "html_url": "https://github.com/XingangPan/DragGAN",
    "clone_url": "https://github.com/XingangPan/DragGAN.git",
    "owner_login": "XingangPan",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/13579537?v=4",
    "stargazers_count": 35916,
    "watchers_count": 35916,
    "forks_count": 3438,
    "open_issues_count": 153,
    "size": 34847,
    "language": "Python",
    "languages": {
      "Python": 1193439,
      "Cuda": 250786,
      "C++": 68635,
      "Dockerfile": 689,
      "Batchfile": 524,
      "Shell": 502
    },
    "topics": [
      "artificial-intelligence",
      "generative-adversarial-network",
      "generative-models",
      "image-manipulation"
    ],
    "license_name": "Other",
    "created_at": "2023-05-18T10:08:02+00:00",
    "updated_at": "2025-08-05T20:47:24+00:00",
    "pushed_at": "2024-05-18T17:51:40+00:00",
    "contributors_count": 14,
    "readme_length": 5884,
    "readme_content": "<p align=\"center\">\n\n  <h1 align=\"center\">Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold</h1>\n  <p align=\"center\">\n    <a href=\"https://xingangpan.github.io/\"><strong>Xingang Pan</strong></a>\n    \u00b7\n    <a href=\"https://ayushtewari.com/\"><strong>Ayush Tewari</strong></a>\n    \u00b7\n    <a href=\"https://people.mpi-inf.mpg.de/~tleimkue/\"><strong>Thomas Leimk\u00fchler</strong></a>\n    \u00b7\n    <a href=\"https://lingjie0206.github.io/\"><strong>Lingjie Liu</strong></a>\n    \u00b7\n    <a href=\"https://www.meka.page/\"><strong>Abhimitra Meka</strong></a>\n    \u00b7\n    <a href=\"http://www.mpi-inf.mpg.de/~theobalt/\"><strong>Christian Theobalt</strong></a>\n  </p>\n  <h2 align=\"center\">SIGGRAPH 2023 Conference Proceedings</h2>\n  <div align=\"center\">\n    <img src=\"DragGAN.gif\", width=\"600\">\n  </div>\n\n  <p align=\"center\">\n  <br>\n    <a href=\"https://pytorch.org/get-started/locally/\"><img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-ee4c2c?logo=pytorch&logoColor=white\"></a>\n    <a href=\"https://twitter.com/XingangP\"><img alt='Twitter' src=\"https://img.shields.io/twitter/follow/XingangP?label=%40XingangP\"></a>\n    <a href=\"https://arxiv.org/abs/2305.10973\">\n      <img src='https://img.shields.io/badge/Paper-PDF-green?style=for-the-badge&logo=adobeacrobatreader&logoWidth=20&logoColor=white&labelColor=66cc00&color=94DD15' alt='Paper PDF'>\n    </a>\n    <a href='https://vcai.mpi-inf.mpg.de/projects/DragGAN/'>\n      <img src='https://img.shields.io/badge/DragGAN-Page-orange?style=for-the-badge&logo=Google%20chrome&logoColor=white&labelColor=D35400' alt='Project Page'></a>\n    <a href=\"https://colab.research.google.com/drive/1mey-IXPwQC_qSthI5hO-LTX7QL4ivtPh?usp=sharing\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n  </p>\n</p>\n\n## Web Demos\n\n[![Open in OpenXLab](https://cdn-static.openxlab.org.cn/app-center/openxlab_app.svg)](https://openxlab.org.cn/apps/detail/XingangPan/DragGAN)\n\n<p align=\"left\">\n  <a href=\"https://huggingface.co/spaces/radames/DragGan\"><img alt=\"Huggingface\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DragGAN-orange\"></a>\n</p>\n\n## Requirements\n\nIf you have CUDA graphic card, please follow the requirements of [NVlabs/stylegan3](https://github.com/NVlabs/stylegan3#requirements).  \n\nThe usual installation steps involve the following commands, they should set up the correct CUDA version and all the python packages\n\n```\nconda env create -f environment.yml\nconda activate stylegan3\n```\n\nThen install the additional requirements\n\n```\npip install -r requirements.txt\n```\n\nOtherwise (for GPU acceleration on MacOS with Silicon Mac M1/M2, or just CPU) try the following:\n\n```sh\ncat environment.yml | \\\n  grep -v -E 'nvidia|cuda' > environment-no-nvidia.yml && \\\n    conda env create -f environment-no-nvidia.yml\nconda activate stylegan3\n\n# On MacOS\nexport PYTORCH_ENABLE_MPS_FALLBACK=1\n```\n\n## Run Gradio visualizer in Docker \n\nProvided docker image is based on NGC PyTorch repository. To quickly try out visualizer in Docker, run the following:  \n\n```sh\n# before you build the docker container, make sure you have cloned this repo, and downloaded the pretrained model by `python scripts/download_model.py`.\ndocker build . -t draggan:latest  \ndocker run -p 7860:7860 -v \"$PWD\":/workspace/src -it draggan:latest bash\n# (Use GPU)if you want to utilize your Nvidia gpu to accelerate in docker, please add command tag `--gpus all`, like:\n#   docker run --gpus all  -p 7860:7860 -v \"$PWD\":/workspace/src -it draggan:latest bash\n\ncd src && python visualizer_drag_gradio.py --listen\n```\nNow you can open a shared link from Gradio (printed in the terminal console).   \nBeware the Docker image takes about 25GB of disk space!\n\n## Download pre-trained StyleGAN2 weights\n\nTo download pre-trained weights, simply run:\n\n```\npython scripts/download_model.py\n```\nIf you want to try StyleGAN-Human and the Landscapes HQ (LHQ) dataset, please download weights from these links: [StyleGAN-Human](https://drive.google.com/file/d/1dlFEHbu-WzQWJl7nBBZYcTyo000H9hVm/view?usp=sharing), [LHQ](https://drive.google.com/file/d/16twEf0T9QINAEoMsWefoWiyhcTd-aiWc/view?usp=sharing), and put them under `./checkpoints`.\n\nFeel free to try other pretrained StyleGAN.\n\n## Run DragGAN GUI\n\nTo start the DragGAN GUI, simply run:\n```sh\nsh scripts/gui.sh\n```\nIf you are using windows, you can run:\n```\n.\\scripts\\gui.bat\n```\n\nThis GUI supports editing GAN-generated images. To edit a real image, you need to first perform GAN inversion using tools like [PTI](https://github.com/danielroich/PTI). Then load the new latent code and model weights to the GUI.\n\nYou can run DragGAN Gradio demo as well, this is universal for both windows and linux:\n```sh\npython visualizer_drag_gradio.py\n```\n\n## Acknowledgement\n\nThis code is developed based on [StyleGAN3](https://github.com/NVlabs/stylegan3). Part of the code is borrowed from [StyleGAN-Human](https://github.com/stylegan-human/StyleGAN-Human).\n\n(cheers to the community as well)\n## License\n\nThe code related to the DragGAN algorithm is licensed under [CC-BY-NC](https://creativecommons.org/licenses/by-nc/4.0/).\nHowever, most of this project are available under a separate license terms: all codes used or modified from [StyleGAN3](https://github.com/NVlabs/stylegan3) is under the [Nvidia Source Code License](https://github.com/NVlabs/stylegan3/blob/main/LICENSE.txt).\n\nAny form of use and derivative of this code must preserve the watermarking functionality showing \"AI Generated\".\n\n## BibTeX\n\n```bibtex\n@inproceedings{pan2023draggan,\n    title={Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold},\n    author={Pan, Xingang and Tewari, Ayush, and Leimk{\\\"u}hler, Thomas and Liu, Lingjie and Meka, Abhimitra and Theobalt, Christian},\n    booktitle = {ACM SIGGRAPH 2023 Conference Proceedings},\n    year={2023}\n}\n```\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 101782647,
    "name": "fairseq",
    "full_name": "facebookresearch/fairseq",
    "description": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python.",
    "html_url": "https://github.com/facebookresearch/fairseq",
    "clone_url": "https://github.com/facebookresearch/fairseq.git",
    "owner_login": "facebookresearch",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/16943930?v=4",
    "stargazers_count": 31692,
    "watchers_count": 31692,
    "forks_count": 6586,
    "open_issues_count": 1340,
    "size": 26077,
    "language": "Python",
    "languages": {
      "Python": 4294275,
      "Cuda": 38178,
      "C++": 21106,
      "Cython": 13294,
      "Lua": 4210,
      "Shell": 2182
    },
    "topics": [
      "artificial-intelligence",
      "python",
      "pytorch"
    ],
    "license_name": "MIT License",
    "created_at": "2017-08-29T16:26:12+00:00",
    "updated_at": "2025-08-06T01:13:16+00:00",
    "pushed_at": "2025-06-10T21:41:39+00:00",
    "contributors_count": 100,
    "readme_length": 17398,
    "readme_content": "<p align=\"center\">\n  <img src=\"docs/fairseq_logo.png\" width=\"150\">\n  <br />\n  <br />\n  <a href=\"https://opensource.fb.com/support-ukraine\"><img alt=\"Support Ukraine\" src=\"https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&labelColor=005BBB\" /></a>\n  <a href=\"https://github.com/pytorch/fairseq/blob/main/LICENSE\"><img alt=\"MIT License\" src=\"https://img.shields.io/badge/license-MIT-blue.svg\" /></a>\n  <a href=\"https://github.com/pytorch/fairseq/releases\"><img alt=\"Latest Release\" src=\"https://img.shields.io/github/release/pytorch/fairseq.svg\" /></a>\n  <a href=\"https://github.com/pytorch/fairseq/actions?query=workflow:build\"><img alt=\"Build Status\" src=\"https://github.com/pytorch/fairseq/workflows/build/badge.svg\" /></a>\n  <a href=\"https://fairseq.readthedocs.io/en/latest/?badge=latest\"><img alt=\"Documentation Status\" src=\"https://readthedocs.org/projects/fairseq/badge/?version=latest\" /></a>\n  <a href=\"https://app.circleci.com/pipelines/github/facebookresearch/fairseq/\"><img alt=\"CicleCI Status\" src=\"https://circleci.com/gh/facebookresearch/fairseq.svg?style=shield\" /></a>\n</p>\n\n--------------------------------------------------------------------------------\n\nFairseq(-py) is a sequence modeling toolkit that allows researchers and\ndevelopers to train custom models for translation, summarization, language\nmodeling and other text generation tasks.\n\nWe provide reference implementations of various sequence modeling papers:\n\n<details><summary>List of implemented papers</summary><p>\n\n* **Convolutional Neural Networks (CNN)**\n  + [Language Modeling with Gated Convolutional Networks (Dauphin et al., 2017)](examples/language_model/conv_lm/README.md)\n  + [Convolutional Sequence to Sequence Learning (Gehring et al., 2017)](examples/conv_seq2seq/README.md)\n  + [Classical Structured Prediction Losses for Sequence to Sequence Learning (Edunov et al., 2018)](https://github.com/pytorch/fairseq/tree/classic_seqlevel)\n  + [Hierarchical Neural Story Generation (Fan et al., 2018)](examples/stories/README.md)\n  + [wav2vec: Unsupervised Pre-training for Speech Recognition (Schneider et al., 2019)](examples/wav2vec/README.md)\n* **LightConv and DynamicConv models**\n  + [Pay Less Attention with Lightweight and Dynamic Convolutions (Wu et al., 2019)](examples/pay_less_attention_paper/README.md)\n* **Long Short-Term Memory (LSTM) networks**\n  + Effective Approaches to Attention-based Neural Machine Translation (Luong et al., 2015)\n* **Transformer (self-attention) networks**\n  + Attention Is All You Need (Vaswani et al., 2017)\n  + [Scaling Neural Machine Translation (Ott et al., 2018)](examples/scaling_nmt/README.md)\n  + [Understanding Back-Translation at Scale (Edunov et al., 2018)](examples/backtranslation/README.md)\n  + [Adaptive Input Representations for Neural Language Modeling (Baevski and Auli, 2018)](examples/language_model/README.adaptive_inputs.md)\n  + [Lexically constrained decoding with dynamic beam allocation (Post & Vilar, 2018)](examples/constrained_decoding/README.md)\n  + [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context (Dai et al., 2019)](examples/truncated_bptt/README.md)\n  + [Adaptive Attention Span in Transformers (Sukhbaatar et al., 2019)](examples/adaptive_span/README.md)\n  + [Mixture Models for Diverse Machine Translation: Tricks of the Trade (Shen et al., 2019)](examples/translation_moe/README.md)\n  + [RoBERTa: A Robustly Optimized BERT Pretraining Approach (Liu et al., 2019)](examples/roberta/README.md)\n  + [Facebook FAIR's WMT19 News Translation Task Submission (Ng et al., 2019)](examples/wmt19/README.md)\n  + [Jointly Learning to Align and Translate with Transformer Models (Garg et al., 2019)](examples/joint_alignment_translation/README.md )\n  + [Multilingual Denoising Pre-training for Neural Machine Translation (Liu et at., 2020)](examples/mbart/README.md)\n  + [Neural Machine Translation with Byte-Level Subwords (Wang et al., 2020)](examples/byte_level_bpe/README.md)\n  + [Unsupervised Quality Estimation for Neural Machine Translation (Fomicheva et al., 2020)](examples/unsupervised_quality_estimation/README.md)\n  + [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations (Baevski et al., 2020)](examples/wav2vec/README.md)\n  + [Generating Medical Reports from Patient-Doctor Conversations Using Sequence-to-Sequence Models (Enarvi et al., 2020)](examples/pointer_generator/README.md)\n  + [Linformer: Self-Attention with Linear Complexity (Wang et al., 2020)](examples/linformer/README.md)\n  + [Cross-lingual Retrieval for Iterative Self-Supervised Training (Tran et al., 2020)](examples/criss/README.md)\n  + [Deep Transformers with Latent Depth (Li et al., 2020)](examples/latent_depth/README.md)\n  + [Unsupervised Cross-lingual Representation Learning for Speech Recognition (Conneau et al., 2020)](https://arxiv.org/abs/2006.13979)\n  + [Self-training and Pre-training are Complementary for Speech Recognition (Xu et al., 2020)](https://arxiv.org/abs/2010.11430)\n  + [Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training (Hsu, et al., 2021)](https://arxiv.org/abs/2104.01027)\n  + [Unsupervised Speech Recognition (Baevski, et al., 2021)](https://arxiv.org/abs/2105.11084)\n  + [Simple and Effective Zero-shot Cross-lingual Phoneme Recognition (Xu et al., 2021)](https://arxiv.org/abs/2109.11680)\n  + [VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding (Xu et. al., 2021)](https://arxiv.org/pdf/2109.14084.pdf)\n  + [VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding (Xu et. al., 2021)](https://aclanthology.org/2021.findings-acl.370.pdf)\n  + [NormFormer: Improved Transformer Pretraining with Extra Normalization (Shleifer et. al, 2021)](examples/normformer/README.md)\n* **Non-autoregressive Transformers**\n  + Non-Autoregressive Neural Machine Translation (Gu et al., 2017)\n  + Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement (Lee et al. 2018)\n  + Insertion Transformer: Flexible Sequence Generation via Insertion Operations (Stern et al. 2019)\n  + Mask-Predict: Parallel Decoding of Conditional Masked Language Models (Ghazvininejad et al., 2019)\n  + [Levenshtein Transformer (Gu et al., 2019)](examples/nonautoregressive_translation/README.md)\n* **Finetuning**\n  + [Better Fine-Tuning by Reducing Representational Collapse (Aghajanyan et al. 2020)](examples/rxf/README.md)\n\n</p></details>\n\n### What's New:\n* May 2023 [Released models for Scaling Speech Technology to 1,000+ Languages  (Pratap, et al., 2023)](examples/mms/README.md)\n* June 2022 [Released code for wav2vec-U 2.0 from Towards End-to-end Unsupervised Speech Recognition (Liu, et al., 2022)](examples/wav2vec/unsupervised/README.md)\n* May 2022 [Integration with xFormers](https://github.com/facebookresearch/xformers)\n* December 2021 [Released Direct speech-to-speech translation code](examples/speech_to_speech/README.md)\n* October 2021 [Released VideoCLIP and VLM models](examples/MMPT/README.md)\n* October 2021 [Released multilingual finetuned XLSR-53 model](examples/wav2vec/README.md)\n* September 2021 [`master` branch renamed to `main`](https://github.com/github/renaming).\n* July 2021 [Released DrNMT code](examples/discriminative_reranking_nmt/README.md)\n* July 2021 [Released Robust wav2vec 2.0 model](examples/wav2vec/README.md)\n* June 2021 [Released XLMR-XL and XLMR-XXL models](examples/xlmr/README.md)\n* May 2021 [Released Unsupervised Speech Recognition code](examples/wav2vec/unsupervised/README.md)\n* March 2021 [Added full parameter and optimizer state sharding + CPU offloading](examples/fully_sharded_data_parallel/README.md)\n* February 2021 [Added LASER training code](examples/laser/README.md)\n* December 2020: [Added Adaptive Attention Span code](examples/adaptive_span/README.md)\n* December 2020: [GottBERT model and code released](examples/gottbert/README.md)\n* November 2020: Adopted the [Hydra](https://github.com/facebookresearch/hydra) configuration framework\n  * [see documentation explaining how to use it for new and existing projects](docs/hydra_integration.md)\n* November 2020: [fairseq 0.10.0 released](https://github.com/pytorch/fairseq/releases/tag/v0.10.0)\n* October 2020: [Added R3F/R4F (Better Fine-Tuning) code](examples/rxf/README.md)\n* October 2020: [Deep Transformer with Latent Depth code released](examples/latent_depth/README.md)\n* October 2020: [Added CRISS models and code](examples/criss/README.md)\n\n<details><summary>Previous updates</summary><p>\n\n* September 2020: [Added Linformer code](examples/linformer/README.md)\n* September 2020: [Added pointer-generator networks](examples/pointer_generator/README.md)\n* August 2020: [Added lexically constrained decoding](examples/constrained_decoding/README.md)\n* August 2020: [wav2vec2 models and code released](examples/wav2vec/README.md)\n* July 2020: [Unsupervised Quality Estimation code released](examples/unsupervised_quality_estimation/README.md)\n* May 2020: [Follow fairseq on Twitter](https://twitter.com/fairseq)\n* April 2020: [Monotonic Multihead Attention code released](examples/simultaneous_translation/README.md)\n* April 2020: [Quant-Noise code released](examples/quant_noise/README.md)\n* April 2020: [Initial model parallel support and 11B parameters unidirectional LM released](examples/megatron_11b/README.md)\n* March 2020: [Byte-level BPE code released](examples/byte_level_bpe/README.md)\n* February 2020: [mBART model and code released](examples/mbart/README.md)\n* February 2020: [Added tutorial for back-translation](https://github.com/pytorch/fairseq/tree/main/examples/backtranslation#training-your-own-model-wmt18-english-german)\n* December 2019: [fairseq 0.9.0 released](https://github.com/pytorch/fairseq/releases/tag/v0.9.0)\n* November 2019: [VizSeq released (a visual analysis toolkit for evaluating fairseq models)](https://facebookresearch.github.io/vizseq/docs/getting_started/fairseq_example)\n* November 2019: [CamemBERT model and code released](examples/camembert/README.md)\n* November 2019: [BART model and code released](examples/bart/README.md)\n* November 2019: [XLM-R models and code released](examples/xlmr/README.md)\n* September 2019: [Nonautoregressive translation code released](examples/nonautoregressive_translation/README.md)\n* August 2019: [WMT'19 models released](examples/wmt19/README.md)\n* July 2019: fairseq relicensed under MIT license\n* July 2019: [RoBERTa models and code released](examples/roberta/README.md)\n* June 2019: [wav2vec models and code released](examples/wav2vec/README.md)\n\n</p></details>\n\n### Features:\n\n* multi-GPU training on one machine or across multiple machines (data and model parallel)\n* fast generation on both CPU and GPU with multiple search algorithms implemented:\n  + beam search\n  + Diverse Beam Search ([Vijayakumar et al., 2016](https://arxiv.org/abs/1610.02424))\n  + sampling (unconstrained, top-k and top-p/nucleus)\n  + [lexically constrained decoding](examples/constrained_decoding/README.md) (Post & Vilar, 2018)\n* [gradient accumulation](https://fairseq.readthedocs.io/en/latest/getting_started.html#large-mini-batch-training-with-delayed-updates) enables training with large mini-batches even on a single GPU\n* [mixed precision training](https://fairseq.readthedocs.io/en/latest/getting_started.html#training-with-half-precision-floating-point-fp16) (trains faster with less GPU memory on [NVIDIA tensor cores](https://developer.nvidia.com/tensor-cores))\n* [extensible](https://fairseq.readthedocs.io/en/latest/overview.html): easily register new models, criterions, tasks, optimizers and learning rate schedulers\n* [flexible configuration](docs/hydra_integration.md) based on [Hydra](https://github.com/facebookresearch/hydra) allowing a combination of code, command-line and file based configuration\n* [full parameter and optimizer state sharding](examples/fully_sharded_data_parallel/README.md)\n* [offloading parameters to CPU](examples/fully_sharded_data_parallel/README.md)\n\nWe also provide [pre-trained models for translation and language modeling](#pre-trained-models-and-examples)\nwith a convenient `torch.hub` interface:\n\n``` python\nen2de = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.en-de.single_model')\nen2de.translate('Hello world', beam=5)\n# 'Hallo Welt'\n```\n\nSee the PyTorch Hub tutorials for [translation](https://pytorch.org/hub/pytorch_fairseq_translation/)\nand [RoBERTa](https://pytorch.org/hub/pytorch_fairseq_roberta/) for more examples.\n\n# Requirements and Installation\n\n* [PyTorch](http://pytorch.org/) version >= 1.10.0\n* Python version >= 3.8\n* For training new models, you'll also need an NVIDIA GPU and [NCCL](https://github.com/NVIDIA/nccl)\n* **To install fairseq** and develop locally:\n\n``` bash\ngit clone https://github.com/pytorch/fairseq\ncd fairseq\npip install --editable ./\n\n# on MacOS:\n# CFLAGS=\"-stdlib=libc++\" pip install --editable ./\n\n# to install the latest stable release (0.10.x)\n# pip install fairseq\n```\n\n* **For faster training** install NVIDIA's [apex](https://github.com/NVIDIA/apex) library:\n\n``` bash\ngit clone https://github.com/NVIDIA/apex\ncd apex\npip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" \\\n  --global-option=\"--deprecated_fused_adam\" --global-option=\"--xentropy\" \\\n  --global-option=\"--fast_multihead_attn\" ./\n```\n\n* **For large datasets** install [PyArrow](https://arrow.apache.org/docs/python/install.html#using-pip): `pip install pyarrow`\n* If you use Docker make sure to increase the shared memory size either with `--ipc=host` or `--shm-size`\n as command line options to `nvidia-docker run` .\n\n# Getting Started\n\nThe [full documentation](https://fairseq.readthedocs.io/) contains instructions\nfor getting started, training new models and extending fairseq with new model\ntypes and tasks.\n\n# Pre-trained models and examples\n\nWe provide pre-trained models and pre-processed, binarized test sets for several tasks listed below,\nas well as example training and evaluation commands.\n\n* [Translation](examples/translation/README.md): convolutional and transformer models are available\n* [Language Modeling](examples/language_model/README.md): convolutional and transformer models are available\n\nWe also have more detailed READMEs to reproduce results from specific papers:\n\n* [XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale (Babu et al., 2021)](examples/wav2vec/xlsr/README.md)\n* [Cross-lingual Retrieval for Iterative Self-Supervised Training (Tran et al., 2020)](examples/criss/README.md)\n* [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations (Baevski et al., 2020)](examples/wav2vec/README.md)\n* [Unsupervised Quality Estimation for Neural Machine Translation (Fomicheva et al., 2020)](examples/unsupervised_quality_estimation/README.md)\n* [Training with Quantization Noise for Extreme Model Compression ({Fan*, Stock*} et al., 2020)](examples/quant_noise/README.md)\n* [Neural Machine Translation with Byte-Level Subwords (Wang et al., 2020)](examples/byte_level_bpe/README.md)\n* [Multilingual Denoising Pre-training for Neural Machine Translation (Liu et at., 2020)](examples/mbart/README.md)\n* [Reducing Transformer Depth on Demand with Structured Dropout (Fan et al., 2019)](examples/layerdrop/README.md)\n* [Jointly Learning to Align and Translate with Transformer Models (Garg et al., 2019)](examples/joint_alignment_translation/README.md)\n* [Levenshtein Transformer (Gu et al., 2019)](examples/nonautoregressive_translation/README.md)\n* [Facebook FAIR's WMT19 News Translation Task Submission (Ng et al., 2019)](examples/wmt19/README.md)\n* [RoBERTa: A Robustly Optimized BERT Pretraining Approach (Liu et al., 2019)](examples/roberta/README.md)\n* [wav2vec: Unsupervised Pre-training for Speech Recognition (Schneider et al., 2019)](examples/wav2vec/README.md)\n* [Mixture Models for Diverse Machine Translation: Tricks of the Trade (Shen et al., 2019)](examples/translation_moe/README.md)\n* [Pay Less Attention with Lightweight and Dynamic Convolutions (Wu et al., 2019)](examples/pay_less_attention_paper/README.md)\n* [Understanding Back-Translation at Scale (Edunov et al., 2018)](examples/backtranslation/README.md)\n* [Classical Structured Prediction Losses for Sequence to Sequence Learning (Edunov et al., 2018)](https://github.com/pytorch/fairseq/tree/classic_seqlevel)\n* [Hierarchical Neural Story Generation (Fan et al., 2018)](examples/stories/README.md)\n* [Scaling Neural Machine Translation (Ott et al., 2018)](examples/scaling_nmt/README.md)\n* [Convolutional Sequence to Sequence Learning (Gehring et al., 2017)](examples/conv_seq2seq/README.md)\n* [Language Modeling with Gated Convolutional Networks (Dauphin et al., 2017)](examples/language_model/README.conv.md)\n\n# Join the fairseq community\n\n* Twitter: https://twitter.com/fairseq\n* Facebook page: https://www.facebook.com/groups/fairseq.users\n* Google group: https://groups.google.com/forum/#!forum/fairseq-users\n\n# License\n\nfairseq(-py) is MIT-licensed.\nThe license applies to the pre-trained models as well.\n\n# Citation\n\nPlease cite as:\n\n``` bibtex\n@inproceedings{ott2019fairseq,\n  title = {fairseq: A Fast, Extensible Toolkit for Sequence Modeling},\n  author = {Myle Ott and Sergey Edunov and Alexei Baevski and Angela Fan and Sam Gross and Nathan Ng and David Grangier and Michael Auli},\n  booktitle = {Proceedings of NAACL-HLT 2019: Demonstrations},\n  year = {2019},\n}\n```\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 306977633,
    "name": "AI-Expert-Roadmap",
    "full_name": "AMAI-GmbH/AI-Expert-Roadmap",
    "description": "Roadmap to becoming an Artificial Intelligence Expert in 2022",
    "html_url": "https://github.com/AMAI-GmbH/AI-Expert-Roadmap",
    "clone_url": "https://github.com/AMAI-GmbH/AI-Expert-Roadmap.git",
    "owner_login": "AMAI-GmbH",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/54247845?v=4",
    "stargazers_count": 30151,
    "watchers_count": 30151,
    "forks_count": 2528,
    "open_issues_count": 23,
    "size": 628,
    "language": "JavaScript",
    "languages": {
      "JavaScript": 5569,
      "Vue": 1641,
      "Stylus": 1556
    },
    "topics": [
      "ai",
      "ai-roadmap",
      "artificial-intelligence",
      "data-analysis",
      "data-science",
      "deep-learning",
      "machine-learning",
      "neural-network",
      "roadmap",
      "study-plan"
    ],
    "license_name": "MIT License",
    "created_at": "2020-10-24T21:49:40+00:00",
    "updated_at": "2025-08-05T18:21:10+00:00",
    "pushed_at": "2023-12-31T02:20:16+00:00",
    "contributors_count": 9,
    "readme_length": 6134,
    "readme_content": "<p align=\"center\">\n  <a href=\"https://github.com/AMAI-GmbH/AI-Expert-Roadmap\">\n    <img src=\"https://uploads-ssl.webflow.com/58e6a2b25c28230d367487ad/5c32232ecb585fcc5c4645e1_icon_machine-learning.svg\" alt=\"Developer Roadmap\" width=\"96\" height=\"96\">\n  </a>\n  <h2 align=\"center\">i.am.ai<br>AI Expert Roadmap</h2>\n  <p align=\"center\">Roadmap to becoming an Artificial Intelligence Expert in 2022</p>\n  <p align=\"center\">\n      <a href=\"https://twitter.com/home?status=https://i.am.ai/roadmap Roadmap to becoming an Artificial Intelligence Expert in 2022\" target=\"_blank\"><img src=\"https://img.shields.io/badge/tweet-blue.svg?logo=twitter&logoColor=white\" style=\"display: inherit;\"/></a>\n      <a href=\"https://www.linkedin.com/shareArticle?mini=true&url=https://i.am.ai/roadmap&title=&summary=Roadmap to becoming an Artificial Intelligence Expert in 2022&source=\" target=\"_blank\"><img src=\"https://img.shields.io/badge/post-blue.svg?logo=linkedin&logoColor=white\" style=\"display: inherit;\"/></a>\n      <a href=\"https://github.com/AMAI-GmbH/AI-Expert-Roadmap\"><img src=\"https://img.shields.io/badge/Roadmap-2022-yellowgreen.svg\" style=\"display: inherit;\"/></a>\n      <a href=\"https://am.ai?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+Badge\" target=\"_blank\"><img alt=\"AMAI GmbH\" src=\"https://img.shields.io/badge/Author-AMAI GmbH-blue.svg\" style=\"display: inherit;\"/></a>\n<a href=\"https://opensource.org/licenses/MIT/\" target=\"_blank\"><img alt=\"MIT License\" src=\"https://img.shields.io/badge/License-MIT-blue.svg\" style=\"display: inherit;\"/></a>\n  </p>\n  <br>\n</p>\n\nBelow you find a set of charts demonstrating the paths that you can take and the technologies that you would want to adopt in order to become a data scientist, machine learning or an AI expert. We made these charts for our new employees to make them AI Experts but we wanted to share them here to help the community.\n\nIf you are interested to become an AI EXPERT at [AMAI](https://www.linkedin.com/company/amai-gmbh/?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+Become+Expert) in Germany, or you want to [hire an AI Expert](https://am.ai?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+Hire+Expert), please say [hi@am.ai](mailto:hi@am.ai).\n\n## Note\n\n\ud83d\udc49 An **interactive version with links to follow** about each bullet of the list can be found at [i.am.ai/roadmap](https://i.am.ai/roadmap?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+Interactive) \ud83d\udc48\n\nTo receive updates [star :star:](https://github.com/AMAI-GmbH/AI-Expert-Roadmap/stargazers) and watch :eyes: the [GitHub Repo](https://github.com/AMAI-GmbH/AI-Expert-Roadmap/) to get notified, when we add new content to stay on the top of the most recent research.\n\nFollow our [AI Newsletter](https://i.am.ai/newsletter?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+Newsletter) to stay up to date with the latest developments in AI. We cover new use cases and research topics.\n\n## Disclaimer\n\nThe purpose of these roadmaps is to give you an idea about the landscape and to guide you if you are confused about what to learn next and not to encourage you to pick what is hip and trendy. You should grow some understanding of why one tool would be better suited for some cases than the other and remember hip and trendy never means best suited for the job.\n\n## Introduction\n\n<p align=\"center\">\n  <a href=\"https://i.am.ai/roadmap#introduction?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+Introduction\" target=\"_blank\">\n      <img src=\"./images/intro.svg\"/>\n  </a>\n</p>\n\n## Fundamentals\n\n<p align=\"center\">\n  <a href=\"https://i.am.ai/roadmap#fundamentals?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+Fundamentals\" target=\"_blank\">\n      <img src=\"./images/fundamentals.svg\"/>\n  </a>\n</p>\n\n## Data Science Roadmap\n\n<p align=\"center\">\n  <a href=\"https://i.am.ai/roadmap#data-science-roadmap?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+DataScience\" target=\"_blank\">\n      <img src=\"./images/datascience.svg\"/>\n  </a>\n</p>\n\n## Machine Learning Roadmap\n\n<p align=\"center\">\n  <a href=\"https://i.am.ai/roadmap#machine-learning-roadmap?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+MachineLearning\" target=\"_blank\">\n      <img src=\"./images/machine_learning.svg\"/>\n  </a>\n</p>\n\n## Deep Learning Roadmap\n\n<p align=\"center\">\n  <a href=\"https://i.am.ai/roadmap#deep-learning-roadmap?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+DeepLearning\" target=\"_blank\">\n      <img src=\"./images/deep_learning.svg\"/>\n  </a>\n</p>\n\n## Data Engineer Roadmap\n\n<p align=\"center\">\n  <a href=\"https://i.am.ai/roadmap#data-engineer-roadmap?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+DataEngineer\" target=\"_blank\">\n      <img src=\"./images/data_engineer.svg\"/>\n  </a>\n</p>\n\n## Big Data Engineer Roadmap\n\n<p align=\"center\">\n  <a href=\"https://i.am.ai/roadmap#big-data-engineer-roadmap?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+BigDataEngineer\" target=\"_blank\">\n      <img src=\"./images/big_data_engineer.svg\"/>\n  </a>\n</p>\n\n## \ud83d\udea6 Wrap Up\n\nIf you think any of the roadmaps can be improved, please do open a PR with any updates and submit any issues. Also, we will continue to improve this, so you might want to watch/star this repository to revisit.\n\n## \ud83d\ude4c Contribution\n\n> Have a look at the [contribution docs](./contributing.md) for how to update any of the roadmaps\n\n* Open pull request with improvements\n* Discuss ideas in issues\n* Spread the word\n* Reach out with any feedback\n\n## Supported By\n\n<a href=\"https://www.linkedin.com/company/amai-gmbh/?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+SupportedBy\" target=\"_blank\"><img alt=\"AMAI GmbH\" src=\"./images/logos/amai.svg\" style=\"display: inherit;max-width: 150px;\"/></a>\n<a href=\"https://digitalhub-ai.de?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap\" target=\"_blank\"><img alt=\"AMAI GmbH\" src=\"./images/logos/de-hub.svg\" style=\"display: inherit; max-width: 150px;\"/></a>\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 70431106,
    "name": "machine-learning-for-software-engineers",
    "full_name": "ZuzooVn/machine-learning-for-software-engineers",
    "description": "A complete daily plan for studying to become a machine learning engineer.",
    "html_url": "https://github.com/ZuzooVn/machine-learning-for-software-engineers",
    "clone_url": "https://github.com/ZuzooVn/machine-learning-for-software-engineers.git",
    "owner_login": "ZuzooVn",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/1429230?v=4",
    "stargazers_count": 28551,
    "watchers_count": 28551,
    "forks_count": 6222,
    "open_issues_count": 30,
    "size": 471,
    "language": null,
    "languages": {},
    "topics": [
      "artificial-intelligence",
      "deep-learning",
      "machine-learning",
      "machine-learning-algorithms",
      "software-engineer"
    ],
    "license_name": "Creative Commons Attribution Share Alike 4.0 International",
    "created_at": "2016-10-09T21:20:25+00:00",
    "updated_at": "2025-08-05T21:19:51+00:00",
    "pushed_at": "2024-06-11T04:49:29+00:00",
    "contributors_count": 37,
    "readme_length": 38154,
    "readme_content": "# Top-down learning path: Machine Learning for Software Engineers\n\n<p align=\"center\">\n  <a href=\"https://github.com/ZuzooVn/machine-learning-for-software-engineers\">\n    <img alt=\"Top-down learning path: Machine Learning for Software Engineers\" src=\"https://img.shields.io/badge/Machine%20Learning-Software%20Engineers-blue.svg\">\n  </a>\n  <a href=\"https://github.com/ZuzooVn/machine-learning-for-software-engineers/stargazers\">\n    <img alt=\"GitHub stars\" src=\"https://img.shields.io/github/stars/ZuzooVn/machine-learning-for-software-engineers.svg\">\n  </a>\n  <a href=\"https://github.com/ZuzooVn/machine-learning-for-software-engineers/network\">\n    <img alt=\"GitHub forks\" src=\"https://img.shields.io/github/forks/ZuzooVn/machine-learning-for-software-engineers.svg\">\n  </a>\n</p>\n\nInspired by [Coding Interview University](https://github.com/jwasham/coding-interview-university).\n\nTranslations: [Brazilian Portuguese](https://github.com/ZuzooVn/machine-learning-for-software-engineers/blob/master/README-pt-BR.md) | [\u4e2d\u6587\u7248\u672c](https://github.com/ZuzooVn/machine-learning-for-software-engineers/blob/master/README-zh-CN.md) | [Fran\u00e7ais](https://github.com/ZuzooVn/machine-learning-for-software-engineers/blob/master/README-fr-FR.md) | [\u81fa\u7063\u83ef\u8a9e\u7248\u672c](https://github.com/ZuzooVn/machine-learning-for-software-engineers/blob/master/README-zh-TW.md)\n\n[How I (Nam Vu) plan to become a machine learning engineer](https://www.codementor.io/zuzoovn/how-i-plan-to-become-a-machine-learning-engineer-a4metbcuk)\n\n## What is it?\n\nThis is my multi-month study plan for going from mobile developer (self-taught, no CS degree) to machine learning engineer.\n\nMy main goal was to find an approach to studying Machine Learning that is mainly hands-on and abstracts most of the Math for the beginner.\nThis approach is unconventional because it\u2019s the top-down and results-first approach designed for software engineers.\n\nPlease, feel free to make any contributions you feel will make it better.\n\n---\n\n## Table of Contents\n\n- [What is it?](#what-is-it)\n- [Why use it?](#why-use-it)\n- [How to use it](#how-to-use-it)\n- [Follow me](#follow-me)\n- [Don't feel you aren't smart enough](#dont-feel-you-arent-smart-enough)\n- [About Video Resources](#about-video-resources)\n- [Prerequisite Knowledge](#prerequisite-knowledge)\n- [The Daily Plan](#the-daily-plan)\n- [Motivation](#motivation)\n- [Machine learning overview](#machine-learning-overview)\n- [Machine learning mastery](#machine-learning-mastery)\n- [Machine learning is fun](#machine-learning-is-fun)\n- [Inky Machine Learning](#inky-machine-learning)\n- [Machine Learning: An In-Depth Guide](#machine-learning-an-in-depth-guide)\n- [Stories and experiences](#stories-and-experiences)\n- [Machine Learning Algorithms](#machine-learning-algorithms)\n- [Beginner Books](#beginner-books)\n- [Practical Books](#practical-books)\n- [Kaggle knowledge competitions](#kaggle-knowledge-competitions)\n- [Video Series](#video-series)\n- [MOOC](#mooc)\n- [Resources](#resources)\n- [Becoming an Open Source Contributor](#becoming-an-open-source-contributor)\n- [Games](#games)\n- [Podcasts](#podcasts)\n- [Communities](#communities)\n- [Conferences](#conferences)\n- [Interview Questions](#interview-questions)\n- [My admired companies](#my-admired-companies)\n\n---\n\n## Why use it?\n\nI'm following this plan to prepare for my near-future job: Machine learning engineer. I've been building native mobile applications (Android/iOS/Blackberry) since 2011. I have a Software Engineering degree, not a Computer Science degree. I have an itty-bitty amount of basic knowledge about: Calculus, Linear Algebra, Discrete Mathematics, Probability & Statistics from university.\nThink about my interest in machine learning:\n- [Can I learn and get a job in Machine Learning without studying CS Master and PhD?](https://www.quora.com/Can-I-learn-and-get-a-job-in-Machine-Learning-without-studying-CS-Master-and-PhD)\n    - *\"You can, but it is far more difficult than when I got into the field.\"* [Drac Smith](https://www.quora.com/Can-I-learn-and-get-a-job-in-Machine-Learning-without-studying-CS-Master-and-PhD/answer/Drac-Smith?srid=oT0p)\n- [How do I get a job in Machine Learning as a software programmer who self-studies Machine Learning, but  never has a chance to use it at work?](https://www.quora.com/How-do-I-get-a-job-in-Machine-Learning-as-a-software-programmer-who-self-studies-Machine-Learning-but-never-has-a-chance-to-use-it-at-work)\n    - *\"I'm hiring machine learning experts for my team and your MOOC will not get you the job (there is better news below). In fact, many people with a master's in machine learning will not get the job because they (and most who have taken MOOCs) do not have a deep understanding that will help me solve my problems.\"* [Ross C. Taylor](https://www.quora.com/How-do-I-get-a-job-in-Machine-Learning-as-a-software-programmer-who-self-studies-Machine-Learning-but-never-has-a-chance-to-use-it-at-work/answer/Ross-C-Taylor?srid=oT0p)\n- [What skills are needed for machine learning jobs?](http://programmers.stackexchange.com/questions/79476/what-skills-are-needed-for-machine-learning-jobs)\n    - *\"First, you need to have a decent CS/Math background. ML is an advanced topic so most textbooks assume that you have that background. Second, machine learning is a very general topic with many sub-specialties requiring unique skills. You may want to browse the curriculum of an MS program in Machine Learning to see the course, curriculum and textbook.\"* [Uri](http://softwareengineering.stackexchange.com/a/79717)\n    - *\"Probability, distributed computing, and Statistics.\"* [Hydrangea](http://softwareengineering.stackexchange.com/a/79575)\n\nI find myself in times of trouble.\n\nAFAIK, [There are two sides to machine learning](http://machinelearningmastery.com/programmers-can-get-into-machine-learning/):\n- Practical Machine Learning: This is about querying databases, cleaning data, writing scripts to transform data and gluing algorithm and libraries together and writing custom code to squeeze reliable answers from data to satisfy difficult and ill-defined questions. It\u2019s the mess of reality.\n- Theoretical Machine Learning: This is about math and abstraction and idealized scenarios and limits and beauty and informing what is possible. It is a whole lot neater and cleaner and removed from the mess of reality.\n\nI think the best way for practice-focused methodology is something like ['practice \u2014 learning \u2014 practice'](http://machinelearningmastery.com/machine-learning-for-programmers/#comment-358985), that means where students first come with some existing projects with problems and solutions (practice) to get familiar with traditional methods in the area and perhaps also with their methodology. After practicing with some elementary experiences, they can go into the books and study the underlying theory, which serves to guide their future advanced practice and will enhance their toolbox of solving practical problems. Studying theory also further improves their understanding on the elementary experiences, and will help them acquire advanced experiences more quickly.\n\n It's a long plan. It's going to take me years. If you are familiar with a lot of this already it will take you a lot less time.\n\n## How to use it\nEverything below is an outline, and you should tackle the items in order from top to bottom.\n\nI'm using Github's special markdown flavor, including tasks lists to check progress.\n\n- [x] Create a new branch so you can check items like this, just put an x in the brackets: [x]\n\n[More about Github-flavored markdown](https://guides.github.com/features/mastering-markdown/#GitHub-flavored-markdown)\n\n## Follow me\nI'm a Vietnamese Software Engineer who is really passionate and wants to work in the USA.\n\nHow much did I work during this plan? Roughly 4 hours/night after a long, hard day at work.\n\nI'm on the journey.\n\n- Twitter: [@Nam Vu](https://twitter.com/zuzoovn)\n\n| ![Nam Vu - Top-down learning path: machine learning for software engineers](http://sv1.upsieutoc.com/2016/10/08/331f241c8da44d0c43e9324d55440db6.md.jpg)|\n|:---:|\n| USA as heck |\n\n## Don't feel you aren't smart enough\nI get discouraged from books and courses that tell me as soon as I open them that multivariate calculus, inferential statistics and linear algebra are prerequisites. I still don\u2019t know how to get started\u2026\n\n- [What if I\u2019m Not Good at Mathematics](http://machinelearningmastery.com/what-if-im-not-good-at-mathematics/)\n- [5 Techniques To Understand Machine Learning Algorithms Without the Background in Mathematics](http://machinelearningmastery.com/techniques-to-understand-machine-learning-algorithms-without-the-background-in-mathematics/)\n- [How do I learn machine learning?](https://www.quora.com/Machine-Learning/How-do-I-learn-machine-learning-1)\n\n## About Video Resources\n\nSome videos are available only by enrolling in a Coursera or EdX class. It is free to do so, but sometimes the classes\nare no longer in session so you have to wait a couple of months, so you have no access. I'm going to be adding more videos\nfrom public sources and replacing the online course videos over time. I like using university lectures.\n\n## Prerequisite Knowledge\n\nThis short section consists of prerequisites/interesting info I wanted to learn before getting started on the daily plan.\n\n- [ ] [What is the difference between Data Analytics, Data Analysis, Data Mining, Data Science, Machine Learning, and Big Data?](https://www.quora.com/What-is-the-difference-between-Data-Analytics-Data-Analysis-Data-Mining-Data-Science-Machine-Learning-and-Big-Data-1)\n- [ ] [Learning How to Learn](https://www.coursera.org/learn/learning-how-to-learn)\n- [ ] [Don\u2019t Break The Chain](http://lifehacker.com/281626/jerry-seinfelds-productivity-secret)\n- [ ] [How to learn on your own](https://metacademy.org/roadmaps/rgrosse/learn_on_your_own)\n\n## The Daily Plan\n\nEach subject does not require a whole day to be able to understand it fully, and you can do multiple of these in a day.\n\nEach day I take one subject from the list below, read it cover to cover, take notes, do the exercises and write an implementation in Python or R.\n\n# Motivation\n- [ ] [Dream](https://www.youtube.com/watch?v=g-jwWYX7Jlo)\n\n## Machine learning overview\n- [ ] [A Visual Introduction to Machine Learning](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)\n- [ ] [Gentle Guide to Machine Learning](https://blog.monkeylearn.com/gentle-guide-to-machine-learning/)\n- [ ] [Introduction to Machine Learning for Developers](http://blog.algorithmia.com/introduction-machine-learning-developers/)\n- [ ] [Machine Learning basics for a newbie](https://www.analyticsvidhya.com/blog/2015/06/machine-learning-basics/)\n- [ ] [How do you explain Machine Learning and Data Mining to non Computer Science people?](https://www.quora.com/How-do-you-explain-Machine-Learning-and-Data-Mining-to-non-Computer-Science-people)\n- [ ] [Machine Learning: Under the hood. Blog post explains the principles of machine learning in layman terms. Simple and clear](https://georgemdallas.wordpress.com/2013/06/11/big-data-data-mining-and-machine-learning-under-the-hood/)\n- [ ] [What is machine learning, and how does it work?](https://www.youtube.com/watch?v=elojMnjn4kk&list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A&index=1)\n- [ ] [How to Become a Machine Learning Engineer?](https://www.scaler.com/blog/how-to-become-a-machine-learning-engineer/)\n- ~~[] [Deep Learning - A Non-Technical Introduction](http://www.slideshare.net/AlfredPong1/deep-learning-a-nontechnical-introduction-69385936)~~[removed]\n\n## Machine learning mastery\n- [ ] [The Machine Learning Mastery Method](http://machinelearningmastery.com/machine-learning-mastery-method/)\n- [ ] [Machine Learning for Programmers](http://machinelearningmastery.com/machine-learning-for-programmers/)\n- [ ] [Applied Machine Learning with Machine Learning Mastery](http://machinelearningmastery.com/start-here/)\n- [ ] [Python Machine Learning Mini-Course](http://machinelearningmastery.com/python-machine-learning-mini-course/)\n- [ ] [Machine Learning Algorithms Mini-Course](http://machinelearningmastery.com/machine-learning-algorithms-mini-course/)\n\n## Machine learning is fun\n- [ ] [Machine Learning is Fun!](https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471#.37ue6caww)\n- [ ] [Part 2: Using Machine Learning to generate Super Mario Maker levels](https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3#.kh7qgvp1b)\n- [ ] [Part 3: Deep Learning and Convolutional Neural Networks](https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721#.44rhxy637)\n- [ ] [Part 4: Modern Face Recognition with Deep Learning](https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78#.3rwmq0ddc)\n- [ ] [Part 5: Language Translation with Deep Learning and the Magic of Sequences](https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa#.wyfthap4c)\n- [ ] [Part 6: How to do Speech Recognition with Deep Learning](https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a#.lhr1nnpcy)\n- [ ] [Part 7: Abusing Generative Adversarial Networks to Make 8-bit Pixel Art](https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7)\n- [ ] [Part 8: How to Intentionally Trick Neural Networks](https://medium.com/@ageitgey/machine-learning-is-fun-part-8-how-to-intentionally-trick-neural-networks-b55da32b7196)\n\n## [Inky Machine Learning](https://triskell.github.io/2016/11/15/Inky-Machine-Learning.html)\n- [ ] [Part 1: What is Machine Learning ?](https://triskell.github.io/2016/10/23/What-is-Machine-Learning.html)\n- [ ] [Part 2: Supervised Learning and Unsupervised Learning](https://triskell.github.io/2016/11/13/Supervised-Learning-and-Unsupervised-Learning.html)\n\n## Machine Learning: An In-Depth Guide\n- [ ] [Overview, goals, learning types, and algorithms](http://www.innoarchitech.com/machine-learning-an-in-depth-non-technical-guide/)\n- [ ] [Data selection, preparation, and modeling](http://www.innoarchitech.com/machine-learning-an-in-depth-non-technical-guide-part-2/)\n- [ ] [Model evaluation, validation, complexity, and improvement](http://www.innoarchitech.com/machine-learning-an-in-depth-non-technical-guide-part-3/)\n- [ ] [Model performance and error analysis](http://www.innoarchitech.com/machine-learning-an-in-depth-non-technical-guide-part-4/)\n- [ ] [Unsupervised learning, related fields, and machine learning in practice](http://www.innoarchitech.com/machine-learning-an-in-depth-non-technical-guide-part-5/)\n\n## Stories and experiences\n- [ ] [Machine Learning in a Week](https://medium.com/learning-new-stuff/machine-learning-in-a-week-a0da25d59850#.tk6ft2kcg)\n- [ ] [Machine Learning in a Year](https://medium.com/learning-new-stuff/machine-learning-in-a-year-cdb0b0ebd29c#.hhcb9fxk1)\n- [ ] [How I wrote my first Machine Learning program in 3 days](http://blog.adnansiddiqi.me/how-i-wrote-my-first-machine-learning-program-in-3-days/)\n- [ ] [Learning Path : Your mentor to become a machine learning expert](https://www.analyticsvidhya.com/learning-path-learn-machine-learning/)\n- [ ] [You Too Can Become a Machine Learning Rock Star! No PhD](https://backchannel.com/you-too-can-become-a-machine-learning-rock-star-no-phd-necessary-107a1624d96b#.g9p16ldp7)\n- [ ] How to become a Data Scientist in 6 months: A hacker\u2019s approach to career planning\n    - [Video](https://www.youtube.com/watch?v=rIofV14c0tc)\n    - [Slide](http://www.slideshare.net/TetianaIvanova2/how-to-become-a-data-scientist-in-6-months)\n- [ ] [5 Skills You Need to Become a Machine Learning Engineer](http://blog.udacity.com/2016/04/5-skills-you-need-to-become-a-machine-learning-engineer.html)\n- [ ] [Are you a self-taught machine learning engineer? If yes, how did you do it & how long did it take you?](https://www.quora.com/Are-you-a-self-taught-machine-learning-engineer-If-yes-how-did-you-do-it-how-long-did-it-take-you)\n- [ ] [How can one become a good machine learning engineer?](https://www.quora.com/How-can-one-become-a-good-machine-learning-engineer)\n- [ ] [A Learning Sabbatical focused on Machine Learning](http://karlrosaen.com/ml/)\n\n## Machine Learning Algorithms\n- [ ] [10 Machine Learning Algorithms Explained to an \u2018Army Soldier\u2019](https://www.analyticsvidhya.com/blog/2015/12/10-machine-learning-algorithms-explained-army-soldier/)\n- [ ] [Top 10 data mining algorithms in plain English](https://rayli.net/blog/data/top-10-data-mining-algorithms-in-plain-english/)\n- [ ] [10 Machine Learning Terms Explained in Simple English](http://blog.aylien.com/10-machine-learning-terms-explained-in-simple/)\n- [ ] [A Tour of Machine Learning Algorithms](http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/)\n- [ ] [The 10 Algorithms Machine Learning Engineers Need to Know](https://gab41.lab41.org/the-10-algorithms-machine-learning-engineers-need-to-know-f4bb63f5b2fa#.ofc7t2965)\n- [ ] [Comparing supervised learning algorithms](http://www.dataschool.io/comparing-supervised-learning-algorithms/)\n- [ ] [Machine Learning Algorithms: A collection of minimal and clean implementations of machine learning algorithms](https://github.com/rushter/MLAlgorithms)\n- [ ] [KNN Algorithm in Machine Learning](https://www.scaler.com/topics/what-is-knn-algorithm-in-machine-learning/)\n\n## Beginner Books\n- [ ] [Data Smart: Using Data Science to Transform Information into Insight 1st Edition](https://www.amazon.com/Data-Smart-Science-Transform-Information/dp/111866146X)\n- [ ] [Data Science for Business: What you need to know about data mining and data\u00ad analytic-thinking](https://www.amazon.com/Data-Science-Business-Data-Analytic-Thinking/dp/1449361323/)\n- [ ] [Predictive Analytics: The Power to Predict Who Will Click, Buy, Lie, or Die](https://www.amazon.com/Predictive-Analytics-Power-Predict-Click/dp/1118356853)\n\n## Practical Books\n- [ ] [Machine Learning for Hackers](https://www.amazon.com/Machine-Learning-Hackers-Drew-Conway/dp/1449303714)\n    - [GitHub repository(R)](https://github.com/johnmyleswhite/ML_for_Hackers)\n    - [GitHub repository(Python)](https://github.com/carljv/Will_it_Python)\n- [ ] [Python Machine Learning](https://www.amazon.com/Python-Machine-Learning-Sebastian-Raschka-ebook/dp/B00YSILNL0)\n    - [GitHub repository](https://github.com/rasbt/python-machine-learning-book)\n- [ ] [Programming Collective Intelligence: Building Smart Web 2.0 Applications](https://www.amazon.com/Programming-Collective-Intelligence-Building-Applications-ebook/dp/B00F8QDZWG)\n- [ ] [Machine Learning: An Algorithmic Perspective, Second Edition](https://www.amazon.com/Machine-Learning-Algorithmic-Perspective-Recognition/dp/1466583282)\n    - [GitHub repository](https://github.com/alexsosn/MarslandMLAlgo)\n    - [Resource repository](http://seat.massey.ac.nz/personal/s.r.marsland/MLbook.html)\n- [ ] [Introduction to Machine Learning with Python: A Guide for Data Scientists](http://shop.oreilly.com/product/0636920030515.do)\n    - [GitHub repository](https://github.com/amueller/introduction_to_ml_with_python)\n- [ ] [Data Mining: Practical Machine Learning Tools and Techniques, Third Edition](https://www.amazon.com/Data-Mining-Practical-Techniques-Management/dp/0123748569)\n    - Teaching material\n        - [Slides for Chapters 1-5 (zip)](http://www.cs.waikato.ac.nz/ml/weka/Slides3rdEd_Ch1-5.zip)\n        - [Slides for Chapters 6-8 (zip)](http://www.cs.waikato.ac.nz/ml/weka/Slides3rdEd_Ch6-8.zip)\n- [ ] [Machine Learning in Action](https://www.amazon.com/Machine-Learning-Action-Peter-Harrington/dp/1617290181/)\n    - [GitHub repository](https://github.com/pbharrin/machinelearninginaction)\n- [ ] [Reactive Machine Learning Systems(MEAP)](https://www.manning.com/books/reactive-machine-learning-systems)\n    - [GitHub repository](https://github.com/jeffreyksmithjr/reactive-machine-learning-systems)\n- [ ] [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)\n    - [GitHub repository(R)](http://www-bcf.usc.edu/~gareth/ISL/code.html)\n    - [GitHub repository(Python)](https://github.com/JWarmenhoven/ISLR-python)\n    - [Videos](http://www.dataschool.io/15-hours-of-expert-machine-learning-videos/)\n- [ ] [Building Machine Learning Systems with Python](https://www.packtpub.com/big-data-and-business-intelligence/building-machine-learning-systems-python)\n    - [GitHub repository](https://github.com/luispedro/BuildingMachineLearningSystemsWithPython)\n- [ ] [Learning scikit-learn: Machine Learning in Python](https://www.packtpub.com/big-data-and-business-intelligence/learning-scikit-learn-machine-learning-python)\n    - [GitHub repository](https://github.com/gmonce/scikit-learn-book)\n- [ ] [Probabilistic Programming & Bayesian Methods for Hackers](https://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/)\n- [ ] [Probabilistic Graphical Models: Principles and Techniques](https://www.amazon.com/Probabilistic-Graphical-Models-Principles-Computation/dp/0262013193)\n- [ ] [Machine Learning: Hands-On for Developers and Technical Professionals](https://www.amazon.com/Machine-Learning-Hands-Developers-Professionals/dp/1118889061)\n    - [Machine Learning Hands-On for Developers and Technical Professionals review](https://blogs.msdn.microsoft.com/querysimon/2015/01/01/book-review-machine-learning-hands-on-for-developers-and-technical-professionals/)\n    - [GitHub repository](https://github.com/jasebell/mlbook)\n- [ ] [Learning from Data](https://www.amazon.com/Learning-Data-Yaser-S-Abu-Mostafa/dp/1600490069)\n    - [Online tutorials](https://work.caltech.edu/telecourse.html)\n- [ ] [Reinforcement Learning: An Introduction (2nd Edition)](https://webdocs.cs.ualberta.ca/~sutton/book/the-book-2nd.html)\n    - [GitHub repository](https://github.com/ShangtongZhang/reinforcement-learning-an-introduction)\n- [ ] [Machine Learning with TensorFlow(MEAP)](https://www.manning.com/books/machine-learning-with-tensorflow)\n    - [GitHub repository](https://github.com/BinRoot/TensorFlow-Book)\n- [ ] [How Machine Learning Works (MEAP)](https://www.manning.com/books/how-machine-learning-works)\n    - [GitHub repository](https://github.com/Mostafa-Samir/How-Machine-Learning-Works)\n- [ ] [Succeeding with AI](https://www.manning.com/books/succeeding-with-ai)\n\n## Kaggle knowledge competitions\n- [ ] [Kaggle Competitions: How and where to begin?](https://www.analyticsvidhya.com/blog/2015/06/start-journey-kaggle/)\n- [ ] [How a Beginner Used Small Projects To Get Started in Machine Learning and Compete on Kaggle](http://machinelearningmastery.com/how-a-beginner-used-small-projects-to-get-started-in-machine-learning-and-compete-on-kaggle)\n- [ ] [Master Kaggle By Competing Consistently](http://machinelearningmastery.com/master-kaggle-by-competing-consistently/)\n\n## Video Series\n- [ ] [Machine Learning for Hackers](https://www.youtube.com/playlist?list=PL2-dafEMk2A4ut2pyv0fSIXqOzXtBGkLj)\n- [ ] [Fresh Machine Learning](https://www.youtube.com/playlist?list=PL2-dafEMk2A6Kc7pV6gHH-apBFxwFjKeY)\n- [ ] [Machine Learning Recipes with Josh Gordon](https://www.youtube.com/playlist?list=PLOU2XLYxmsIIuiBfYad6rFYQU_jL2ryal)\n- [ ] [Everything You Need to know about Machine Learning in 30 Minutes or Less](https://vimeo.com/43547079)\n- [ ] [A Friendly Introduction to Machine Learning](https://www.youtube.com/watch?v=IpGxLWOIZy4)\n- [ ] [Nuts and Bolts of Applying Deep Learning - Andrew Ng](https://www.youtube.com/watch?v=F1ka6a13S9I)\n- [ ] BigML Webinar\n    - [Video](https://www.youtube.com/watch?list=PL1bKyu9GtNYHcjGa6ulrvRVcm1lAB8he3&v=W62ehrnOVqo)\n    - [Resources](https://bigml.com/releases)\n- [ ] [mathematicalmonk's Machine Learning tutorials](https://www.youtube.com/playlist?list=PLD0F06AA0D2E8FFBA)\n- [ ] [Machine learning in Python with scikit-learn](https://www.youtube.com/playlist?list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A)\n    - [GitHub repository](https://github.com/justmarkham/scikit-learn-videos)\n    - [Blog](http://blog.kaggle.com/author/kevin-markham/)\n- [ ] [My playlist \u2013 Top YouTube Videos on Machine Learning, Neural Network & Deep Learning](https://www.analyticsvidhya.com/blog/2015/07/top-youtube-videos-machine-learning-neural-network-deep-learning/)\n- [ ] [16 New Must Watch Tutorials, Courses on Machine Learning](https://www.analyticsvidhya.com/blog/2016/10/16-new-must-watch-tutorials-courses-on-machine-learning/)\n- [ ] [DeepLearning.TV](https://www.youtube.com/channel/UC9OeZkIwhzfv-_Cb7fCikLQ)\n- [ ] [Learning To See](https://www.youtube.com/playlist?list=PLiaHhY2iBX9ihLasvE8BKnS2Xg8AhY6iV)\n- [ ] [Neural networks class - Universit\u00e9 de Sherbrooke](https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH)\n- [ ] [21 Deep Learning Videos, Tutorials & Courses on Youtube from 2016](https://www.analyticsvidhya.com/blog/2016/12/21-deep-learning-videos-tutorials-courses-on-youtube-from-2016/)\n- [ ] [30 Top Videos, Tutorials & Courses on Machine Learning & Artificial Intelligence from 2016](https://www.analyticsvidhya.com/blog/2016/12/30-top-videos-tutorials-courses-on-machine-learning-artificial-intelligence-from-2016/)\n- [ ] [Practical Deep Learning For Coders](http://course.fast.ai/index.html)\n- [ ]  [Practical Deep Learning For Coders Version 2 (PyTorch)](http://forums.fast.ai/t/welcome-to-part-1-v2/5787)\n\n## MOOC\n- [ ] [Coursera\u2019s AI For Everyone](https://www.coursera.org/learn/ai-for-everyone)\n- [ ] [edX's Introduction to Artificial Intelligence (AI)](https://www.edx.org/course/introduction-artificial-intelligence-ai-microsoft-dat263x)\n- [ ] [Udacity\u2019s Intro to Machine Learning](https://www.udacity.com/course/intro-to-machine-learning--ud120)\n    - [Udacity Intro to Machine Learning Review](http://hamelg.blogspot.com/2014/12/udacity-intro-to-machine-learning-review.html)\n- [ ] [Udacity\u2019s Supervised, Unsupervised & Reinforcement](https://www.udacity.com/course/machine-learning--ud262)\n- [ ] [Machine Learning Foundations: A Case Study Approach](https://www.coursera.org/learn/ml-foundations)\n- [ ] [Machine Learning & AI Foundations: Value Estimations](https://www.lynda.com/Data-Science-tutorials/Machine-Learning-Essential-Training-Value-Estimations/548594-2.html)\n- [ ] [Kaggle's Hands-On Data Science Education](https://www.kaggle.com/learn/overview)\n- [ ] [Microsoft Professional Program for Artificial Intelligence](https://academy.microsoft.com/en-us/professional-program/tracks/artificial-intelligence/)\n- [ ] [Coursera\u2019s Machine Learning](https://www.coursera.org/learn/machine-learning)\n    - [Video only](https://www.youtube.com/playlist?list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW)\n    - [Coursera Machine Learning review](https://rayli.net/blog/data/coursera-machine-learning-review/)\n    - [Coursera: Machine Learning Roadmap](https://metacademy.org/roadmaps/cjrd/coursera_ml_supplement)\n- [ ] [Machine Learning Distilled](https://code.tutsplus.com/courses/machine-learning-distilled)\n- [ ] [BigML training](https://bigml.com/training)\n- [ ] [Coursera\u2019s Neural Networks for Machine Learning](https://www.coursera.org/learn/neural-networks)\n    - Taught by Geoffrey Hinton, a pioneer in the field of neural networks\n- [ ] [Machine Learning\u200a-\u200aCS\u200a-\u200aOxford University](https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/)\n- [ ] [Creative Applications of Deep Learning with TensorFlow](https://www.kadenze.com/courses/creative-applications-of-deep-learning-with-tensorflow/info)\n- [ ] [Intro to Descriptive Statistics](https://www.udacity.com/course/intro-to-descriptive-statistics--ud827)\n- [ ] [Intro to Inferential Statistics](https://www.udacity.com/course/intro-to-inferential-statistics--ud201)\n- [ ] [6.S094: Deep Learning for Self-Driving Cars](http://selfdrivingcars.mit.edu/)\n- [ ] [6.S191: Introduction to Deep Learning](http://introtodeeplearning.com/index.html)\n- [ ] [Coursera\u2019s Deep Learning](https://www.coursera.org/specializations/deep-learning)\n\n## Resources\n- [ ] [Absolute Beginning into Machine Learning](https://hackernoon.com/absolute-beginning-into-machine-learning-e90ceda5a4bc)\n- [ ] [Learn Machine Learning in a Single Month](https://elitedatascience.com/machine-learning-masterclass)\n- [ ] [The Non-Technical Guide to Machine Learning & Artificial Intelligence](https://medium.com/@samdebrule/a-humans-guide-to-machine-learning-e179f43b67a0#.cpzf3a5c0)\n- [ ] [Programming Community Curated Resources for learning Machine Learning](https://hackr.io/tutorials/learn-machine-learning-ml)\n- [ ] [Best practices rule book for Machine Learning engineering from Google](http://martin.zinkevich.org/rules_of_ml/rules_of_ml.pdf)\n- [ ] [Machine Learning for Software Engineers on Hacker News](https://news.ycombinator.com/item?id=12898718)\n- [ ] [Machine Learning for Developers](https://xyclade.github.io/MachineLearning/)\n- [ ] [Machine Learning for Humans\ud83e\udd16\ud83d\udc76](https://medium.com/machine-learning-for-humans/why-machine-learning-matters-6164faf1df12)\n- [ ] [Machine Learning Advice for Developers](https://dev.to/thealexlavin/machine-learning-advice-for-developers)\n- [ ] [Machine Learning For Complete Beginners](http://pythonforengineers.com/machine-learning-for-complete-beginners/)\n- [ ] [Getting Started with Machine Learning: For absolute beginners and fifth graders](https://medium.com/@suffiyanz/getting-started-with-machine-learning-f15df1c283ea#.yjtiy7ei9)\n- [ ] [How to Learn Machine Learning: The Self-Starter Way](https://elitedatascience.com/learn-machine-learning)\n- [ ] [Machine Learning Self-study Resources](https://ragle.sanukcode.net/articles/machine-learning-self-study-resources/)\n- [ ] [Level-Up Your Machine Learning](https://metacademy.org/roadmaps/cjrd/level-up-your-ml)\n- [ ] [An Honest Guide to Machine Learning](https://medium.com/axiomzenteam/an-honest-guide-to-machine-learning-2f6d7a6df60e#.ib12a1yw5)\n- [ ] Enough Machine Learning to Make Hacker News Readable Again\n    - [Video](https://www.youtube.com/watch?v=O7IezJT9uSI)\n    - [Slide](https://speakerdeck.com/pycon2014/enough-machine-learning-to-make-hacker-news-readable-again-by-ned-jackson-lovely)\n- [ ] [Dive into Machine Learning](https://github.com/hangtwenty/dive-into-machine-learning)\n- [ ] [{Machine, Deep} Learning for software engineers](https://speakerdeck.com/pmigdal/machine-deep-learning-for-software-engineers)\n- [ ] [Deep Learning For Beginners](https://deeplearning4j.org/deeplearningforbeginners.html)\n- [ ] [Foundations for deep learning](https://github.com/pauli-space/foundations_for_deep_learning)\n- [ ] [Machine Learning Mindmap / Cheatsheet](https://github.com/dformoso/machine-learning-mindmap)\n- Machine Learning courses in Universities\n    - [ ] [Stanford](http://ai.stanford.edu/courses/)\n    - [ ] [Machine Learning Summer Schools](http://mlss.cc/)\n    - [ ] [Oxford](https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/)\n    - [ ] [Cambridge](http://mlg.eng.cam.ac.uk/)\n- Flipboard Topics\n    - [Machine learning](https://flipboard.com/topic/machinelearning)\n    - [Deep learning](https://flipboard.com/topic/deeplearning)\n    - [Artificial Intelligence](https://flipboard.com/topic/artificialintelligence)\n- Medium Topics\n    - [Machine learning](https://medium.com/tag/machine-learning/latest)\n    - [Deep learning](https://medium.com/tag/deep-learning)\n    - [Artificial Intelligence](https://medium.com/tag/artificial-intelligence)\n- Monthly top 10 articles\n    - [Machine Learning](https://medium.mybridge.co/search?q=%22Machine%20Learning%22)\n    - [Algorithms](https://medium.mybridge.co/search?q=Algorithms)\n- [Comprehensive list of data science resources](http://www.datasciencecentral.com/group/resources/forum/topics/comprehensive-list-of-data-science-resources)\n- [DigitalMind's Artificial Intelligence resources](http://blog.digitalmind.io/post/artificial-intelligence-resources)\n- [Awesome Machine Learning](https://github.com/josephmisiti/awesome-machine-learning)\n- [Awesome Graph Classification](https://github.com/benedekrozemberczki/awesome-graph-classification)\n- [Awesome Community Detection](https://github.com/benedekrozemberczki/awesome-community-detection)\n- [CreativeAi's Machine Learning](http://www.creativeai.net/?cat%5B0%5D=machine-learning)\n- [Roadmap of Machine Learning](https://www.scaler.com/blog/machine-learning-roadmap/)\n- [Machine Learning Online Courses](https://classpert.com/machine-learning)\n\n## Games\n- [Halite: A.I. Coding Game](https://halite.io/)\n- [Vindinium: A.I. Programming Challenge](http://vindinium.org/)\n- [General Video Game AI Competition](http://www.gvgai.net/)\n- [Angry Birds AI Competition](https://aibirds.org/)\n- [The AI Games](http://theaigames.com/)\n- [Fighting Game AI Competition](http://www.ice.ci.ritsumei.ac.jp/~ftgaic/)\n- [CodeCup](http://www.codecup.nl/intro.php)\n- [Student StarCraft AI Tournament](http://sscaitournament.com/)\n- [AIIDE StarCraft AI Competition](http://www.cs.mun.ca/~dchurchill/starcraftaicomp/)\n- [CIG StarCraft AI Competition](https://sites.google.com/site/starcraftaic/)\n- [CodinGame - AI Bot Games](https://www.codingame.com/training/machine-learning)\n\n## Becoming an Open Source Contributor\n- [ ] [tensorflow/magenta: Magenta: Music and Art Generation with Machine Intelligence](https://github.com/tensorflow/magenta)\n- [ ] [tensorflow/tensorflow: Computation using data flow graphs for scalable machine learning](https://github.com/tensorflow/tensorflow)\n- [ ] [cmusatyalab/openface: Face recognition with deep neural networks.](https://github.com/cmusatyalab/openface)\n- [ ] [tensorflow/models/syntaxnet: Neural Models of Syntax.](https://github.com/tensorflow/models/tree/master/syntaxnet)\n\n## Podcasts\n- ### Podcasts for Beginners:\n    - [Talking Machines](http://www.thetalkingmachines.com/)\n    - [Linear Digressions](http://lineardigressions.com/)\n    - [Data Skeptic](http://dataskeptic.com/)\n    - [This Week in Machine Learning & AI](https://twimlai.com/)\n    - [Machine Learning Guide](http://ocdevel.com/podcasts/machine-learning)\n    \n- ### Interviews with ML Practitioners, Researchers and Kagglers about their Joureny\n    - [Chai Time Data Science](https://www.youtube.com/playlist?list=PLLvvXm0q8zUbiNdoIazGzlENMXvZ9bd3x), [Audio](http://anchor.fm/chaitimedatascience), [Writeups](https://sanyambhutani.com/tag/chaitimedatascience/)\n    - [Machine Learning for Beginners - Interviews](https://www.youtube.com/channel/UCdZ0GX-F3ULMKfxtyzSFbaw), [Audio](https://jayshah.buzzsprout.com/)\n\n- ### \"More\" advanced podcasts\n    - [Partially Derivative](http://partiallyderivative.com/)\n    - [O\u2019Reilly Data Show](http://radar.oreilly.com/tag/oreilly-data-show-podcast)\n    - [Not So Standard Deviation](https://soundcloud.com/nssd-podcast)\n\n- ### Podcasts to think outside the box:\n    - [Data Stories](http://datastori.es/)\n    \n## Communities\n- Quora\n    - [Machine Learning](https://www.quora.com/topic/Machine-Learning)\n    - [Statistics](https://www.quora.com/topic/Statistics-academic-discipline)\n    - [Data Mining](https://www.quora.com/topic/Data-Mining)\n\n- Reddit\n    - [Machine Learning](https://www.reddit.com/r/machinelearning)\n    - [Computer Vision](https://www.reddit.com/r/computervision)\n    - [Natural Language](https://www.reddit.com/r/languagetechnology)\n    - [Data Science](https://www.reddit.com/r/datascience)\n    - [Big Data](https://www.reddit.com/r/bigdata)\n    - [Statistics](https://www.reddit.com/r/statistics)\n\n- [Data Tau](http://www.datatau.com/)\n\n- [Deep Learning News](http://news.startup.ml/)\n\n- [KDnuggets](http://www.kdnuggets.com/)\n\n## Conferences\n- Neural Information Processing Systems ([NIPS](https://nips.cc/))\n- International Conference on Learning Representations ([ICLR](http://www.iclr.cc/doku.php?id=ICLR2017:main&redirect=1))\n- Association for the Advancement of Artificial Intelligence ([AAAI](http://www.aaai.org/Conferences/AAAI/aaai17.php))\n- IEEE Conference on Computational Intelligence and Games ([CIG](http://www.ieee-cig.org/))\n- IEEE International Conference on Machine Learning and Applications ([ICMLA](http://www.icmla-conference.org/))\n- International Conference on Machine Learning ([ICML](https://2017.icml.cc/))\n- International Joint Conferences on Artificial Intelligence ([IJCAI](http://www.ijcai.org/))\n- Association for Computational Linguistics ([ACL](http://acl2017.org/))\n\n## Interview Questions\n- [ ] [How To Prepare For A Machine Learning Interview](http://blog.udacity.com/2016/05/prepare-machine-learning-interview.html)\n- [ ] [40 Interview Questions asked at Startups in Machine Learning / Data Science](https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science)\n- [ ] [21 Must-Know Data Science Interview Questions and Answers](http://www.kdnuggets.com/2016/02/21-data-science-interview-questions-answers.html)\n- [ ] [Top 50 Machine learning Interview questions & Answers](http://career.guru99.com/top-50-interview-questions-on-machine-learning/)\n- [ ] [Machine Learning Engineer interview questions](https://resources.workable.com/machine-learning-engineer-interview-questions)\n- [ ] [Popular Machine Learning Interview Questions](http://www.learn4master.com/machine-learning/popular-machine-learning-interview-questions)\n- [ ] [What are some common Machine Learning interview questions?](https://www.quora.com/What-are-some-common-Machine-Learning-interview-questions)\n- [ ] [What are the best interview questions to evaluate a machine learning researcher?](https://www.quora.com/What-are-the-best-interview-questions-to-evaluate-a-machine-learning-researcher)\n- [ ] [Collection of Machine Learning Interview Questions](http://analyticscosm.com/machine-learning-interview-questions-for-data-scientist-interview/)\n- [ ] [121 Essential Machine Learning Questions & Answers](https://elitedatascience.com/mlqa-reading-list)\n- [ ] [Minimum Viable Study Plan for Machine Learning Interviews](https://github.com/khangich/machine-learning-interview)\n\n## My admired companies\n- [ ] [ELSA - Your virtual pronunciation coach](https://www.elsanow.io/home)\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 837911756,
    "name": "Jobs_Applier_AI_Agent_AIHawk",
    "full_name": "feder-cr/Jobs_Applier_AI_Agent_AIHawk",
    "description": "AIHawk aims to easy job hunt process by automating the job application process. Utilizing artificial intelligence, it enables users to apply for multiple jobs in a tailored way.",
    "html_url": "https://github.com/feder-cr/Jobs_Applier_AI_Agent_AIHawk",
    "clone_url": "https://github.com/feder-cr/Jobs_Applier_AI_Agent_AIHawk.git",
    "owner_login": "feder-cr",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/85809106?v=4",
    "stargazers_count": 28525,
    "watchers_count": 28525,
    "forks_count": 4317,
    "open_issues_count": 11,
    "size": 152,
    "language": "Python",
    "languages": {
      "Python": 156177,
      "CSS": 12414
    },
    "topics": [
      "agent",
      "application-resume",
      "artificial-intelligence",
      "automate",
      "automation",
      "bot",
      "chatgpt",
      "chrome",
      "gpt",
      "human-resources",
      "job",
      "jobs",
      "jobsearch",
      "jobseeker",
      "opeai",
      "python",
      "resume",
      "scraper",
      "scraping",
      "selenium"
    ],
    "license_name": "GNU Affero General Public License v3.0",
    "created_at": "2024-08-04T12:15:06+00:00",
    "updated_at": "2025-08-06T01:53:54+00:00",
    "pushed_at": "2025-05-28T13:24:12+00:00",
    "contributors_count": 9,
    "readme_length": 1476,
    "readme_content": "\n<div align=\"center\">\n\n\n# AIHawk: the first Jobs Applier AI Agent\n\n\nAIHawk's core architecture remains **open source**, allowing developers to inspect and extend the codebase. However, due to copyright considerations, we have removed all third\u2011party provider plugins from this repository.\n\nFor a fully integrated experience, including managed provider connections: check out **[laboro.co](https://laboro.co/)** an AI\u2011driven job board where the agent **automatically applies to jobs** for you.\n\n\n---\n\n\nAIHawk has been featured by major media outlets for revolutionizing how job seekers interact with the job market:\n\n[**Business Insider**](https://www.businessinsider.com/aihawk-applies-jobs-for-you-linkedin-risks-inaccuracies-mistakes-2024-11)\n[**TechCrunch**](https://techcrunch.com/2024/10/10/a-reporter-used-ai-to-apply-to-2843-jobs/)\n[**Semafor**](https://www.semafor.com/article/09/12/2024/linkedins-have-nots-and-have-bots)\n[**Dev.by**](https://devby.io/news/ya-razoslal-rezume-na-2843-vakansii-po-17-v-chas-kak-ii-boty-vytesnyaut-ludei-iz-protsessa-naima.amp)\n[**Wired**](https://www.wired.it/article/aihawk-come-automatizzare-ricerca-lavoro/)\n[**The Verge**](https://www.theverge.com/2024/10/10/24266898/ai-is-enabling-job-seekers-to-think-like-spammers)\n[**Vanity Fair**](https://www.vanityfair.it/article/intelligenza-artificiale-candidature-di-lavoro)\n[**404 Media**](https://www.404media.co/i-applied-to-2-843-roles-the-rise-of-ai-powered-job-application-bots/)\n\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 326174741,
    "name": "500-AI-Machine-learning-Deep-learning-Computer-vision-NLP-Projects-with-code",
    "full_name": "ashishpatel26/500-AI-Machine-learning-Deep-learning-Computer-vision-NLP-Projects-with-code",
    "description": "500 AI Machine learning Deep learning Computer vision NLP Projects with code",
    "html_url": "https://github.com/ashishpatel26/500-AI-Machine-learning-Deep-learning-Computer-vision-NLP-Projects-with-code",
    "clone_url": "https://github.com/ashishpatel26/500-AI-Machine-learning-Deep-learning-Computer-vision-NLP-Projects-with-code.git",
    "owner_login": "ashishpatel26",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/3095771?v=4",
    "stargazers_count": 26027,
    "watchers_count": 26027,
    "forks_count": 5983,
    "open_issues_count": 54,
    "size": 879,
    "language": null,
    "languages": {},
    "topics": [
      "artificial-intelligence",
      "artificial-intelligence-projects",
      "awesome",
      "computer-vision",
      "computer-vision-project",
      "data-science",
      "deep-learning",
      "deep-learning-project",
      "machine-learning",
      "machine-learning-projects",
      "nlp",
      "nlp-projects",
      "python"
    ],
    "license_name": null,
    "created_at": "2021-01-02T12:08:37+00:00",
    "updated_at": "2025-08-06T02:01:30+00:00",
    "pushed_at": "2025-08-01T11:54:09+00:00",
    "contributors_count": 3,
    "readme_length": 16386,
    "readme_content": "## 500 + \ud835\uddd4\ud835\uddff\ud835\ude01\ud835\uddf6\ud835\uddf3\ud835\uddf6\ud835\uddf0\ud835\uddf6\ud835\uddee\ud835\uddf9 \ud835\udddc\ud835\uddfb\ud835\ude01\ud835\uddf2\ud835\uddf9\ud835\uddf9\ud835\uddf6\ud835\uddf4\ud835\uddf2\ud835\uddfb\ud835\uddf0\ud835\uddf2 \ud835\udde3\ud835\uddff\ud835\uddfc\ud835\uddf7\ud835\uddf2\ud835\uddf0\ud835\ude01 \ud835\udddf\ud835\uddf6\ud835\ude00\ud835\ude01 \ud835\ude04\ud835\uddf6\ud835\ude01\ud835\uddf5 \ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf2\r\n\r\n***500 AI Machine learning Deep learning Computer vision NLP Projects with code* !!!**\r\n\r\n![](https://raw.githubusercontent.com/ashishpatel26/500-AI-Machine-learning-Deep-learning-Computer-vision-NLP-Projects-with-code/main/images/Colorful%20Futuristic%20Technology%20Poster.gif)\r\n\r\nFollow me on LinkedIn : [![](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/ashishpatel2604/)\r\n\r\n***This list is continuously updated.*** - You can take pull requests and contribute. All Links are tested and working fine. Please ping if any link doesn't work\r\n\r\n| Sr No | Name                                                         | Link                                                         |\r\n| ----- | ------------------------------------------------------------ | ------------------------------------------------------------ |\r\n| 1     | 365 Days Computer Vision Learning                            | [\ud83d\udc46](https://github.com/ashishpatel26/365-Days-Computer-Vision-Learning-Linkedin-Post) |\r\n| 2     | 125+ NLP Language Models Treasure of Transformers            | [\ud83d\udc46](https://github.com/ashishpatel26/Treasure-of-Transformers) |\r\n| 3     | Andrew NG ML notes                                           | [\ud83d\udc46](https://github.com/ashishpatel26/Andrew-NG-Notes)        |\r\n| 4     | 10 Machine Learning Projects on Time Series Forecasting      | [\ud83d\udc46](https://medium.com/coders-camp/10-machine-learning-projects-on-time-seri%20es-forecasting-ee0368420ccd) |\r\n| 5     | 20 Deep Learning Projects Solved and Explained with Python   | [\ud83d\udc46](https://thecleverprogrammer.com/2020/11/22/deep-learning-projects-with-python/) |\r\n| 6     | 20 Machine learning Project                                  | [\ud83d\udc46](https://amankharwal.medium.com/20-machine-learning-projects-for-portfolio-81e3dbd167b1) |\r\n| 7     | 30 Python Project Solved and Explained                       | [\ud83d\udc46](https://amankharwal.medium.com/30-python-projects-solved-and-explained-563fd7473003) |\r\n| 8     | Machine learning Course for Free                             | [\ud83d\udc46](https://thecleverprogrammer.com/2020/09/24/machine-learning-course/) |\r\n| 9     | 5 Web Scraping Projects with Python                          | [\ud83d\udc46](https://amankharwal.medium.com/5-web-scraping-projects-with-python-4bcc25ff039) |\r\n| 10    | 20 Machine Learning Projects on Future Prediction with Python | [\ud83d\udc46](https://amankharwal.medium.com/20-machine-learning-projects-on-future-prediction-with-python-93932d9a7f7f) |\r\n| 11    | 4 Chatbot Project With Python                                | [\ud83d\udc46](https://amankharwal.medium.com/4-chatbot-projects-with-python-5b32fd84af37) |\r\n| 12    | 7 Python Gui project                                         | [\ud83d\udc46](https://amankharwal.medium.com/7-python-gui-projects-for-beginners-87ae2c695d78) |\r\n| 13    | All Unsupervised learning Projects                           | [\ud83d\udc46](https://amankharwal.medium.com/all-unsupervised-machine-learning-algorithms-explained-aecf1ba95d8b) |\r\n| 14    | 10 Machine learning Projects for Regression Analysis         | [\ud83d\udc46](https://amankharwal.medium.com/10-machine-learning-projects-on-regression-with-python-e5494615a0d0) |\r\n| 15    | 10 Machine learning Project for Classification with Python   | [\ud83d\udc46](https://medium.datadriveninvestor.com/10-machine-learning-projects-on-classification-with-python-9261add2e8a7) |\r\n| 16    | 6 Sentimental Analysis Projects with python                  | [\ud83d\udc46](https://amankharwal.medium.com/6-sentiment-analysis-projects-with-python-1fdd3d43d90f) |\r\n| 17    | 4 Recommendations Projects with Python                       | [\ud83d\udc46](https://medium.com/coders-camp/4-recommendation-system-projects-with-python-5934de32ba7d) |\r\n| 18    | 20 Deep learning Project with python                         | [\ud83d\udc46](https://medium.com/coders-camp/20-deep-learning-projects-with-python-3c56f7e6a721) |\r\n| 19    | 5 COVID19 Projects with Python                               | [\ud83d\udc46](https://amankharwal.medium.com/5-covid-19-projects-with-python-and-machine-learning-63d51cde96e2) |\r\n| 20    | 9 Computer Vision Project with python                        | [\ud83d\udc46](https://becominghuman.ai/computer-vision-projects-with-python-ecfac58ded18) |\r\n| 21    | 8 Neural Network Project with python                         | [\ud83d\udc46](https://medium.datadriveninvestor.com/8-neural-networks-projects-solved-and-explained-a4f142bc10c) |\r\n| 22    | 5 Machine learning Project for healthcare                    | [\ud83d\udc46](https://medium.datadriveninvestor.com/5-machine-learning-projects-for-healthcare-bbd0eac57b4a) |\r\n| 23    | 5 NLP Project with Python                                    | [\ud83d\udc46](https://medium.datadriveninvestor.com/5-nlp-projects-for-machine-learning-72d3234381d4) |\r\n| 24    | 47 Machine Learning Projects for 2021                        | [\ud83d\udc46](https://data-flair.training/blogs/machine-learning-project-ideas/) |\r\n| 25    | 19 Artificial Intelligence Projects for 2021                 | [\ud83d\udc46](https://data-flair.training/blogs/artificial-intelligence-project-ideas/) |\r\n| 26    | 28 Machine learning Projects for 2021                        | [\ud83d\udc46](https://data-flair.training/blogs/machine-learning-project-ideas/) |\r\n| 27    | 16 Data Science Projects with Source Code for 2021           | [\ud83d\udc46](https://data-flair.training/blogs/data-science-project-ideas/) |\r\n| 28    | 23 Deep learning Projects with Source Code for 2021          | [\ud83d\udc46](https://data-flair.training/blogs/deep-learning-project-ideas/) |\r\n| 29    | 25 Computer Vision Projects with Source Code for 2021        | [\ud83d\udc46](https://data-flair.training/blogs/computer-vision-project-ideas/) |\r\n| 30    | 23 Iot Projects with Source Code for 2021                    | [\ud83d\udc46](https://data-flair.training/blogs/iot-project-ideas/)    |\r\n| 31    | 27 Django Projects with Source Code for 2021                 | [\ud83d\udc46](https://data-flair.training/blogs/django-project-ideas/) |\r\n| 32    | 37 Python Fun Projects with Code for 2021                    | [\ud83d\udc46](https://data-flair.training/blogs/python-project-ideas/) |\r\n| 33    | 500 + Top Deep learning Codes                                | [\ud83d\udc46](https://github.com/aymericdamien/TopDeepLearning)        |\r\n| 34    | 500 + Machine learning Codes                                 | [\ud83d\udc46](https://github.com/josephmisiti/awesome-machine-learning) |\r\n| 35    | 20+ Machine Learning Datasets & Project Ideas                | [\ud83d\udc46](https://www.kdnuggets.com/2020/03/20-machine-learning-datasets-project-ideas.html) |\r\n| 36    | 1000+ Computer vision codes                                  | [\ud83d\udc46](https://github.com/jbhuang0604/awesome-computer-vision)  |\r\n| 37    | 300 + Industry wise Real world projects with code            | [\ud83d\udc46](https://github.com/ashishpatel26/Real-time-ML-Project)   |\r\n| 38    | 1000 + Python Project Codes                                  | [\ud83d\udc46](https://github.com/vinta/awesome-python)                 |\r\n| 39    | 363 + NLP Project with Code                                  | [\ud83d\udc46](https://github.com/fighting41love/funNLP)                |\r\n| 40    | 50 + Code ML Models (For iOS 11) Projects                    | [\ud83d\udc46](https://github.com/likedan/Awesome-CoreML-Models)        |\r\n| 41    | 360+ Pretrained Model Projects for Image, text, Audio and Video | [\ud83d\udc46](https://github.com/PaddlePaddle/PaddleHub)               |\r\n| 42    | 50 + Graph Classification Project List                       | [\ud83d\udc46](https://github.com/benedekrozemberczki/awesome-graph-classification) |\r\n| 43    | 100 + Sentence Embedding(NLP Resources)                      | [\ud83d\udc46](https://github.com/Separius/awesome-sentence-embedding)  |\r\n| 44    | 100 + Production Machine learning Projects                   | [\ud83d\udc46](https://github.com/EthicalML/awesome-production-machine-learning) |\r\n| 45    | 300 + Machine Learning Resources Collection                  | [\ud83d\udc46](https://github.com/ujjwalkarn/Machine-Learning-Tutorials) |\r\n| 46    | 70 + Awesome AI                                              | [\ud83d\udc46](https://github.com/NirantK/awesome-project-ideas)        |\r\n| 47    | 150 + Machine learning Project Ideas with code               | [\ud83d\udc46](https://github.com/src-d/awesome-machine-learning-on-source-code) |\r\n| 48    | 100 + AutoML Projects with code                              | [\ud83d\udc46](https://github.com/hibayesian/awesome-automl-papers)     |\r\n| 49    | 100 + Machine Learning Model Interpretability Code Frameworks | [\ud83d\udc46](https://github.com/jphall663/awesome-machine-learning-interpretability) |\r\n| 50    | 120 + Multi Model Machine learning Code Projects             | [\ud83d\udc46](https://github.com/pliang279/awesome-multimodal-ml)      |\r\n| 51    | Awesome Chatbot Projects                                     | [\ud83d\udc46](https://github.com/fendouai/Awesome-Chatbot)             |\r\n| 52    | Awesome ML Demo Project with iOS                             | [\ud83d\udc46](https://github.com/tucan9389/awesome-ml-demos-with-ios)  |\r\n| 53    | 100 + Python based Machine learning Application Projects     | [\ud83d\udc46](https://github.com/prateekiiest/Code-Sleep-Python)       |\r\n| 54    | 100 + Reproducible Research Projects of ML and DL            | [\ud83d\udc46](https://github.com/leipzig/awesome-reproducible-research) |\r\n| 55    | 25 + Python Projects                                         | [\ud83d\udc46](https://github.com/saadhaxxan/Awesome-Python-Projects)   |\r\n| 56    | 8 + OpenCV Projects                                          | [\ud83d\udc46](https://github.com/moadmmh/Awesome-OpenCV)               |\r\n| 57    | 1000 + Awesome Deep learning Collection                      | [\ud83d\udc46](https://github.com/ChristosChristofidis/awesome-deep-learning) |\r\n| 58    | 200 + Awesome NLP learning Collection                        | [\ud83d\udc46](https://github.com/keon/awesome-nlp)                     |\r\n| 59    | 200 + The Super Duper NLP Repo                               | [\ud83d\udc46](https://notebooks.quantumstat.com/)                      |\r\n| 60    | 100 + NLP dataset for your Projects                          | [\ud83d\udc46](https://index.quantumstat.com/#dataset)                  |\r\n| 61    | 364 + Machine Learning Projects definition                   | [\ud83d\udc46](https://projectworlds.in/free-projects/machine-learning-projects-with-source-code/) |\r\n| 62    | 300+ Google Earth Engine Jupyter Notebooks to Analyze Geospatial Data | [\ud83d\udc46](https://github.com/giswqs/earthengine-py-notebooks)      |\r\n| 63    | 1000 + Machine learning Projects Information                 | [\ud83d\udc46](https://1000projects.org/projects/machine-learning-projects) |\r\n| 64.   | 11 Computer Vision Projects with code                        | [\ud83d\udc46](https://github.com/akshaybhatia10/ComputerVision-Projects) |\r\n| 65.   | 13 Computer Vision Projects with Code                        | [\ud83d\udc46](https://github.com/anuragreddygv323/computer-vision-projects) |\r\n| 66.   | 13 Cool Computer Vision GitHub Projects To Inspire You       | [\ud83d\udc46](https://machinelearningknowledge.ai/cool-computer-vision-github-projects-to-inspire-you/) |\r\n| 67.   | Open-Source Computer Vision Projects (With Tutorials)        | [\ud83d\udc46](https://www.theclickreader.com/open-source-computer-vision-projects-with-tutorials/) |\r\n| 68.   | OpenCV Computer Vision Projects with Python                  | [\ud83d\udc46](https://github.com/PacktPublishing/OpenCV-Computer-Vision-Projects-with-Python) |\r\n| 69.   | 100 + Computer vision Algorithm Implementation               | [\ud83d\udc46](https://github.com/gmalivenko/awesome-computer-vision-models) |\r\n| 70.   | 80 + Computer vision Learning code                           | [\ud83d\udc46](https://github.com/spmallick/learnopencv)                |\r\n| 71.   | Deep learning Treasure                                       | [\ud83d\udc46](https://github.com/kmario23/deep-learning-drizzle)       |\r\n| 72    | Data Analysis and Machine learning Projects                  | [\ud83d\udc46](https://github.com/rhiever/Data-Analysis-and-Machine-Learning-Projects) |\r\n| 73    | AI Projects                                                  | [\ud83d\udc46](https://github.com/StevenLei2017/AI_projects)            |\r\n| 74    | Kaggle projects collection                                   | [\ud83d\udc46](https://github.com/alexattia/Data-Science-Projects)      |\r\n| 75    | Unique AI projects                                           | [\ud83d\udc46](https://github.com/robsalgado/personal_data_science_projects) |\r\n| 76    | Data Science Project Collection                              | [\ud83d\udc46](https://github.com/tuangauss/DataScienceProjects)        |\r\n| 77    | Advance Data Science Projects                                | [\ud83d\udc46](https://github.com/TheCodex-Me/Projects)                 |\r\n| 78    | Deep and Machine learning Projects                           | [\ud83d\udc46](https://github.com/nitinkaushik01/Deep_and_Machine_Learning_Projects) |\r\n| 79    | Data Science Projects kaggle                                 | [\ud83d\udc46](https://github.com/alexattia/Data-Science-Projects)      |\r\n| 80    | Auto Deep learning Project                                   | [\ud83d\udc46](https://github.com/D-X-Y/AutoDL-Projects)                |\r\n| 81    | 180 Machine learning Project                                 | [\ud83d\udc46](https://medium.com/coders-camp/180-data-science-and-machine-learning-projects-with-python-6191bc7b9db9) |\r\n| 82    | Amazing Hackthon Project Collection                          | [\ud83d\udc46](https://github.com/analyticsindiamagazine/MachineHack/tree/master/Hackathon_Solutions) |\r\n| 83    | Awesome NLP Project Ideas                                    | [\ud83d\udc46](https://nirantk.com/awesome-project-ideas/)              |\r\n| 84    | 12 NLP Projects                                              | [\ud83d\udc46](https://github.com/gaoisbest/NLP-Projects)               |\r\n| 85    | Advance NLP Projects                                         | [\ud83d\udc46](https://github.com/PacktPublishing/Advanced-NLP-Projects-with-TensorFlow-2.0) |\r\n| 86    | 6 Amazing NLP Projects                                       | [\ud83d\udc46](https://github.com/anujvyas/Natural-Language-Processing-Projects) |\r\n| 87    | NLP Beginner Projects                                        | [\ud83d\udc46](https://github.com/positivepeng/nlp-beginner-projects)   |\r\n| 88    | Paper with Code by PwC Collection                            | [\ud83d\udc46](https://github.com/zziz/pwc)                             |\r\n| 89    | SOTA Models(State of the Art Results)                        | [\ud83d\udc46](https://github.com/RedditSota/state-of-the-art-result-for-machine-learning-problems) |\r\n| 90    | Best AI Papers                                               | [\ud83d\udc46](https://github.com/louisfb01/Best_AI_paper_2020)         |\r\n| 91    | Generative Adversarial nets                                  | [\ud83d\udc46](https://github.com/zhangqianhui/AdversarialNetsPapers)   |\r\n| 92    | Computer Vision Paper with Code                              | [\ud83d\udc46](https://github.com/DWCTOD/CVPR2022-Papers-with-Code-Demo) |\r\n| 93    | NILMS Paper with code                                        | [\ud83d\udc46](https://github.com/klemenjak/nilm-papers-with-code)      |\r\n| 94    | 3D Computer Vision Research Projects                         | [\ud83d\udc46](https://github.com/Tom-Hardy-3D-Vision-Workshop/awesome-3D-vision) |\r\n| 95    | NLP and Computer Vision Project Collection                   | [\ud83d\udc46](https://github.com/NELSONZHAO/zhihu)                     |\r\n| 96    | Udacity Collection of Computer Vision Projects               | [\ud83d\udc46](https://github.com/Bjarten/computer-vision-ND)           |\r\n| 97    | Zero to Hero Tensorflow Tutorial                             | [\ud83d\udc46](https://github.com/mrdbourke/tensorflow-deep-learning)   |\r\n| 98    | Deep learning in Production                                  | [\ud83d\udc46](https://github.com/The-AI-Summer/Deep-Learning-In-Production) |\r\n| 99    | GANs Collection                                              | [\ud83d\udc46](https://github.com/The-AI-Summer/GANs-in-Computer-Vision) |\r\n| 100   | Time Series Projects Code                                    | [\ud83d\udc46](https://github.com/deshpandenu/Time-Series-Forecasting-of-Amazon-Stock-Prices-using-Neural-Networks-LSTM-and-GAN-) |\r\n| 101   | 12 Machine learning Object Detection                         | [\ud83d\udc46](https://amankharwal.medium.com/12-machine-learning-projects-on-object-detection-46b32adc3c37) |\r\n| 102   | 20 NLP Project with Python                                   | [\ud83d\udc46](https://medium.com/coders-camp/20-machine-learning-projects-on-nlp-582effe73b9c) |\r\n| 103   | Learning Material for Deep Learning, ML, Computer Vision and NLP   | [\ud83d\udc46](https://github.com/kmario23/deep-learning-drizzle) |\r\n***More Projects list is coming...!!!***\r\n\r\n---\r\n\r\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 607289185,
    "name": "semantic-kernel",
    "full_name": "microsoft/semantic-kernel",
    "description": "Integrate cutting-edge LLM technology quickly and easily into your apps",
    "html_url": "https://github.com/microsoft/semantic-kernel",
    "clone_url": "https://github.com/microsoft/semantic-kernel.git",
    "owner_login": "microsoft",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/6154722?v=4",
    "stargazers_count": 25676,
    "watchers_count": 25676,
    "forks_count": 4104,
    "open_issues_count": 503,
    "size": 88980,
    "language": "C#",
    "languages": {
      "C#": 15201008,
      "Python": 7026473,
      "Jupyter Notebook": 336853,
      "TypeScript": 51262,
      "Bicep": 13093,
      "CSS": 10899,
      "PowerShell": 7156,
      "HTML": 6085,
      "Handlebars": 3360,
      "Makefile": 3335,
      "F#": 2611,
      "JavaScript": 801,
      "Smalltalk": 624,
      "Shell": 332,
      "Batchfile": 162
    },
    "topics": [
      "ai",
      "artificial-intelligence",
      "llm",
      "openai",
      "sdk"
    ],
    "license_name": "MIT License",
    "created_at": "2023-02-27T17:39:42+00:00",
    "updated_at": "2025-08-06T00:25:43+00:00",
    "pushed_at": "2025-08-05T22:51:52+00:00",
    "contributors_count": 100,
    "readme_length": 13459,
    "readme_content": "# Semantic Kernel\n\n**Build intelligent AI agents and multi-agent systems with this enterprise-ready orchestration framework**\n\n[![License: MIT](https://img.shields.io/github/license/microsoft/semantic-kernel)](https://github.com/microsoft/semantic-kernel/blob/main/LICENSE)\n[![Python package](https://img.shields.io/pypi/v/semantic-kernel)](https://pypi.org/project/semantic-kernel/)\n[![Nuget package](https://img.shields.io/nuget/vpre/Microsoft.SemanticKernel)](https://www.nuget.org/packages/Microsoft.SemanticKernel/)\n[![Discord](https://img.shields.io/discord/1063152441819942922?label=Discord&logo=discord&logoColor=white&color=d82679)](https://aka.ms/SKDiscord)\n\n\n## What is Semantic Kernel?\n\nSemantic Kernel is a model-agnostic SDK that empowers developers to build, orchestrate, and deploy AI agents and multi-agent systems. Whether you're building a simple chatbot or a complex multi-agent workflow, Semantic Kernel provides the tools you need with enterprise-grade reliability and flexibility.\n\n## System Requirements\n\n- **Python**: 3.10+\n- **.NET**: .NET 8.0+ \n- **Java**: JDK 17+\n- **OS Support**: Windows, macOS, Linux\n\n## Key Features\n\n- **Model Flexibility**: Connect to any LLM with built-in support for [OpenAI](https://platform.openai.com/docs/introduction), [Azure OpenAI](https://azure.microsoft.com/en-us/products/ai-services/openai-service), [Hugging Face](https://huggingface.co/), [NVidia](https://www.nvidia.com/en-us/ai-data-science/products/nim-microservices/) and more\n- **Agent Framework**: Build modular AI agents with access to tools/plugins, memory, and planning capabilities\n- **Multi-Agent Systems**: Orchestrate complex workflows with collaborating specialist agents\n- **Plugin Ecosystem**: Extend with native code functions, prompt templates, OpenAPI specs, or Model Context Protocol (MCP)\n- **Vector DB Support**: Seamless integration with [Azure AI Search](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search), [Elasticsearch](https://www.elastic.co/), [Chroma](https://docs.trychroma.com/getting-started), and more\n- **Multimodal Support**: Process text, vision, and audio inputs\n- **Local Deployment**: Run with [Ollama](https://ollama.com/), [LMStudio](https://lmstudio.ai/), or [ONNX](https://onnx.ai/)\n- **Process Framework**: Model complex business processes with a structured workflow approach\n- **Enterprise Ready**: Built for observability, security, and stable APIs\n\n## Installation\n\nFirst, set the environment variable for your AI Services:\n\n**Azure OpenAI:**\n```bash\nexport AZURE_OPENAI_API_KEY=AAA....\n```\n\n**or OpenAI directly:**\n```bash\nexport OPENAI_API_KEY=sk-...\n```\n\n### Python\n\n```bash\npip install semantic-kernel\n```\n\n### .NET\n\n```bash\ndotnet add package Microsoft.SemanticKernel\ndotnet add package Microsoft.SemanticKernel.Agents.Core\n```\n\n### Java\n\nSee [semantic-kernel-java build](https://github.com/microsoft/semantic-kernel-java/blob/main/BUILD.md) for instructions.\n\n## Quickstart\n\n### Basic Agent - Python\n\nCreate a simple assistant that responds to user prompts:\n\n```python\nimport asyncio\nfrom semantic_kernel.agents import ChatCompletionAgent\nfrom semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n\nasync def main():\n    # Initialize a chat agent with basic instructions\n    agent = ChatCompletionAgent(\n        service=AzureChatCompletion(),\n        name=\"SK-Assistant\",\n        instructions=\"You are a helpful assistant.\",\n    )\n\n    # Get a response to a user message\n    response = await agent.get_response(messages=\"Write a haiku about Semantic Kernel.\")\n    print(response.content)\n\nasyncio.run(main()) \n\n# Output:\n# Language's essence,\n# Semantic threads intertwine,\n# Meaning's core revealed.\n```\n\n### Basic Agent - .NET\n\n```csharp\nusing Microsoft.SemanticKernel;\nusing Microsoft.SemanticKernel.Agents;\n\nvar builder = Kernel.CreateBuilder();\nbuilder.AddAzureOpenAIChatCompletion(\n                Environment.GetEnvironmentVariable(\"AZURE_OPENAI_DEPLOYMENT\"),\n                Environment.GetEnvironmentVariable(\"AZURE_OPENAI_ENDPOINT\"),\n                Environment.GetEnvironmentVariable(\"AZURE_OPENAI_API_KEY\")\n                );\nvar kernel = builder.Build();\n\nChatCompletionAgent agent =\n    new()\n    {\n        Name = \"SK-Agent\",\n        Instructions = \"You are a helpful assistant.\",\n        Kernel = kernel,\n    };\n\nawait foreach (AgentResponseItem<ChatMessageContent> response \n    in agent.InvokeAsync(\"Write a haiku about Semantic Kernel.\"))\n{\n    Console.WriteLine(response.Message);\n}\n\n// Output:\n// Language's essence,\n// Semantic threads intertwine,\n// Meaning's core revealed.\n```\n\n### Agent with Plugins - Python\n\nEnhance your agent with custom tools (plugins) and structured output:\n\n```python\nimport asyncio\nfrom typing import Annotated\nfrom pydantic import BaseModel\nfrom semantic_kernel.agents import ChatCompletionAgent\nfrom semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, OpenAIChatPromptExecutionSettings\nfrom semantic_kernel.functions import kernel_function, KernelArguments\n\nclass MenuPlugin:\n    @kernel_function(description=\"Provides a list of specials from the menu.\")\n    def get_specials(self) -> Annotated[str, \"Returns the specials from the menu.\"]:\n        return \"\"\"\n        Special Soup: Clam Chowder\n        Special Salad: Cobb Salad\n        Special Drink: Chai Tea\n        \"\"\"\n\n    @kernel_function(description=\"Provides the price of the requested menu item.\")\n    def get_item_price(\n        self, menu_item: Annotated[str, \"The name of the menu item.\"]\n    ) -> Annotated[str, \"Returns the price of the menu item.\"]:\n        return \"$9.99\"\n\nclass MenuItem(BaseModel):\n    price: float\n    name: str\n\nasync def main():\n    # Configure structured output format\n    settings = OpenAIChatPromptExecutionSettings()\n    settings.response_format = MenuItem\n\n    # Create agent with plugin and settings\n    agent = ChatCompletionAgent(\n        service=AzureChatCompletion(),\n        name=\"SK-Assistant\",\n        instructions=\"You are a helpful assistant.\",\n        plugins=[MenuPlugin()],\n        arguments=KernelArguments(settings)\n    )\n\n    response = await agent.get_response(messages=\"What is the price of the soup special?\")\n    print(response.content)\n\n    # Output:\n    # The price of the Clam Chowder, which is the soup special, is $9.99.\n\nasyncio.run(main()) \n```\n\n### Agent with Plugin - .NET\n\n```csharp\nusing System.ComponentModel;\nusing Microsoft.SemanticKernel;\nusing Microsoft.SemanticKernel.Agents;\nusing Microsoft.SemanticKernel.ChatCompletion;\n\nvar builder = Kernel.CreateBuilder();\nbuilder.AddAzureOpenAIChatCompletion(\n                Environment.GetEnvironmentVariable(\"AZURE_OPENAI_DEPLOYMENT\"),\n                Environment.GetEnvironmentVariable(\"AZURE_OPENAI_ENDPOINT\"),\n                Environment.GetEnvironmentVariable(\"AZURE_OPENAI_API_KEY\")\n                );\nvar kernel = builder.Build();\n\nkernel.Plugins.Add(KernelPluginFactory.CreateFromType<MenuPlugin>());\n\nChatCompletionAgent agent =\n    new()\n    {\n        Name = \"SK-Assistant\",\n        Instructions = \"You are a helpful assistant.\",\n        Kernel = kernel,\n        Arguments = new KernelArguments(new PromptExecutionSettings() { FunctionChoiceBehavior = FunctionChoiceBehavior.Auto() })\n\n    };\n\nawait foreach (AgentResponseItem<ChatMessageContent> response \n    in agent.InvokeAsync(\"What is the price of the soup special?\"))\n{\n    Console.WriteLine(response.Message);\n}\n\nsealed class MenuPlugin\n{\n    [KernelFunction, Description(\"Provides a list of specials from the menu.\")]\n    public string GetSpecials() =>\n        \"\"\"\n        Special Soup: Clam Chowder\n        Special Salad: Cobb Salad\n        Special Drink: Chai Tea\n        \"\"\";\n\n    [KernelFunction, Description(\"Provides the price of the requested menu item.\")]\n    public string GetItemPrice(\n        [Description(\"The name of the menu item.\")]\n        string menuItem) =>\n        \"$9.99\";\n}\n```\n\n### Multi-Agent System - Python\n\nBuild a system of specialized agents that can collaborate:\n\n```python\nimport asyncio\nfrom semantic_kernel.agents import ChatCompletionAgent, ChatHistoryAgentThread\nfrom semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, OpenAIChatCompletion\n\nbilling_agent = ChatCompletionAgent(\n    service=AzureChatCompletion(), \n    name=\"BillingAgent\", \n    instructions=\"You handle billing issues like charges, payment methods, cycles, fees, discrepancies, and payment failures.\"\n)\n\nrefund_agent = ChatCompletionAgent(\n    service=AzureChatCompletion(),\n    name=\"RefundAgent\",\n    instructions=\"Assist users with refund inquiries, including eligibility, policies, processing, and status updates.\",\n)\n\ntriage_agent = ChatCompletionAgent(\n    service=OpenAIChatCompletion(),\n    name=\"TriageAgent\",\n    instructions=\"Evaluate user requests and forward them to BillingAgent or RefundAgent for targeted assistance.\"\n    \" Provide the full answer to the user containing any information from the agents\",\n    plugins=[billing_agent, refund_agent],\n)\n\nthread: ChatHistoryAgentThread = None\n\nasync def main() -> None:\n    print(\"Welcome to the chat bot!\\n  Type 'exit' to exit.\\n  Try to get some billing or refund help.\")\n    while True:\n        user_input = input(\"User:> \")\n\n        if user_input.lower().strip() == \"exit\":\n            print(\"\\n\\nExiting chat...\")\n            return False\n\n        response = await triage_agent.get_response(\n            messages=user_input,\n            thread=thread,\n        )\n\n        if response:\n            print(f\"Agent :> {response}\")\n\n# Agent :> I understand that you were charged twice for your subscription last month, and I'm here to assist you with resolving this issue. Here\u2019s what we need to do next:\n\n# 1. **Billing Inquiry**:\n#    - Please provide the email address or account number associated with your subscription, the date(s) of the charges, and the amount charged. This will allow the billing team to investigate the discrepancy in the charges.\n\n# 2. **Refund Process**:\n#    - For the refund, please confirm your subscription type and the email address associated with your account.\n#    - Provide the dates and transaction IDs for the charges you believe were duplicated.\n\n# Once we have these details, we will be able to:\n\n# - Check your billing history for any discrepancies.\n# - Confirm any duplicate charges.\n# - Initiate a refund for the duplicate payment if it qualifies. The refund process usually takes 5-10 business days after approval.\n\n# Please provide the necessary details so we can proceed with resolving this issue for you.\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n\n\n## Where to Go Next\n\n1. \ud83d\udcd6 Try our [Getting Started Guide](https://learn.microsoft.com/en-us/semantic-kernel/get-started/quick-start-guide) or learn about [Building Agents](https://learn.microsoft.com/en-us/semantic-kernel/frameworks/agent/)\n2. \ud83d\udd0c Explore over 100 [Detailed Samples](https://learn.microsoft.com/en-us/semantic-kernel/get-started/detailed-samples)\n3. \ud83d\udca1 Learn about core Semantic Kernel [Concepts](https://learn.microsoft.com/en-us/semantic-kernel/concepts/kernel)\n\n### API References\n\n- [C# API reference](https://learn.microsoft.com/en-us/dotnet/api/microsoft.semantickernel?view=semantic-kernel-dotnet)\n- [Python API reference](https://learn.microsoft.com/en-us/python/api/semantic-kernel/semantic_kernel?view=semantic-kernel-python)\n\n## Troubleshooting\n\n### Common Issues\n\n- **Authentication Errors**: Check that your API key environment variables are correctly set\n- **Model Availability**: Verify your Azure OpenAI deployment or OpenAI model access\n\n### Getting Help\n\n- Check our [GitHub issues](https://github.com/microsoft/semantic-kernel/issues) for known problems\n- Search the [Discord community](https://aka.ms/SKDiscord) for solutions\n- Include your SDK version and full error messages when asking for help\n\n\n## Join the community\n\nWe welcome your contributions and suggestions to the SK community! One of the easiest ways to participate is to engage in discussions in the GitHub repository. Bug reports and fixes are welcome!\n\nFor new features, components, or extensions, please open an issue and discuss with us before sending a PR. This is to avoid rejection as we might be taking the core in a different direction, but also to consider the impact on the larger ecosystem.\n\nTo learn more and get started:\n\n- Read the [documentation](https://aka.ms/sk/learn)\n- Learn how to [contribute](https://learn.microsoft.com/en-us/semantic-kernel/support/contributing) to the project\n- Ask questions in the [GitHub discussions](https://github.com/microsoft/semantic-kernel/discussions)\n- Ask questions in the [Discord community](https://aka.ms/SKDiscord)\n\n- Attend [regular office hours and SK community events](COMMUNITY.md)\n- Follow the team on our [blog](https://aka.ms/sk/blog)\n\n## Contributor Wall of Fame\n\n[![semantic-kernel contributors](https://contrib.rocks/image?repo=microsoft/semantic-kernel)](https://github.com/microsoft/semantic-kernel/graphs/contributors)\n\n## Code of Conduct\n\nThis project has adopted the\n[Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information, see the\n[Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\nor contact [opencode@microsoft.com](mailto:opencode@microsoft.com)\nwith any additional questions or comments.\n\n## License\n\nCopyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the [MIT](LICENSE) license.\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 525592995,
    "name": "InvokeAI",
    "full_name": "invoke-ai/InvokeAI",
    "description": "Invoke is a leading creative engine for Stable Diffusion models, empowering professionals, artists, and enthusiasts to generate and create visual media using the latest AI-driven technologies. The solution offers an industry leading WebUI, and serves as the foundation for multiple commercial products.",
    "html_url": "https://github.com/invoke-ai/InvokeAI",
    "clone_url": "https://github.com/invoke-ai/InvokeAI.git",
    "owner_login": "invoke-ai",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/113954515?v=4",
    "stargazers_count": 25644,
    "watchers_count": 25644,
    "forks_count": 2629,
    "open_issues_count": 806,
    "size": 347901,
    "language": "TypeScript",
    "languages": {
      "TypeScript": 5326681,
      "Python": 4086857,
      "Shell": 10230,
      "JavaScript": 8847,
      "CSS": 5076,
      "Dockerfile": 3710,
      "Jupyter Notebook": 3143,
      "Makefile": 2887,
      "Nix": 2792,
      "HTML": 691
    },
    "topics": [
      "ai-art",
      "artificial-intelligence",
      "generative-art",
      "image-generation",
      "img2img",
      "inpainting",
      "latent-diffusion",
      "linux",
      "macos",
      "outpainting",
      "stable-diffusion",
      "txt2img",
      "windows"
    ],
    "license_name": "Apache License 2.0",
    "created_at": "2022-08-17T01:04:27+00:00",
    "updated_at": "2025-08-05T23:54:10+00:00",
    "pushed_at": "2025-08-05T00:37:48+00:00",
    "contributors_count": 100,
    "readme_length": 8378,
    "readme_content": "<div align=\"center\">\n\n![project hero](https://github.com/invoke-ai/InvokeAI/assets/31807370/6e3728c7-e90e-4711-905c-3b55844ff5be)\n\n# Invoke - Professional Creative AI Tools for Visual Media\n\n#### To learn more about Invoke, or implement our Business solutions, visit [invoke.com]\n\n[![discord badge]][discord link] [![latest release badge]][latest release link] [![github stars badge]][github stars link] [![github forks badge]][github forks link] [![CI checks on main badge]][CI checks on main link] [![latest commit to main badge]][latest commit to main link] [![github open issues badge]][github open issues link] [![github open prs badge]][github open prs link] [![translation status badge]][translation status link]\n\n</div>\n\nInvoke is a leading creative engine built to empower professionals and enthusiasts alike. Generate and create stunning visual media using the latest AI-driven technologies. Invoke offers an industry leading web-based UI, and serves as the foundation for multiple commercial products.\n\nInvoke is available in two editions:\n\n| **Community Edition**                                                                                                      | **Professional Edition**                                                                            |\n|----------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------|\n| **For users looking for a locally installed, self-hosted and self-managed service**                                         | **For users or teams looking for a cloud-hosted, fully managed service**                            |\n| - Free to use under a commercially-friendly license                                                                         | - Monthly subscription fee with three different plan levels                                         |\n| - Download and install on compatible hardware                                                                               | - Offers additional benefits, including multi-user support, improved model training, and more                          |\n| - Includes all core studio features: generate, refine, iterate on images, and build workflows                               | - Hosted in the cloud for easy, secure model access and scalability                                               |\n| Quick Start -> [Installation and Updates][installation docs]                                                                     | More Information -> [www.invoke.com/pricing](https://www.invoke.com/pricing)                        |\n\n\n![Highlighted Features - Canvas and Workflows](https://github.com/invoke-ai/InvokeAI/assets/31807370/708f7a82-084f-4860-bfbe-e2588c53548d)\n\n# Documentation\n| **Quick Links**                                                                                                      | \n|----------------------------------------------------------------------------------------------------------------------------|\n|  [Installation and Updates][installation docs] - [Documentation and Tutorials][docs home] - [Bug Reports][github issues] - [Contributing][contributing docs]  | \n\n# Installation\n\nTo get started with Invoke, [Download the Installer](https://www.invoke.com/downloads).\n\nFor detailed step by step instructions, or for instructions on manual/docker installations, visit our documentation on [Installation and Updates][installation docs]\n\n\n## Troubleshooting, FAQ and Support\n\nPlease review our [FAQ][faq] for solutions to common installation problems and other issues.\n\nFor more help, please join our [Discord][discord link].\n\n## Features\n\nFull details on features can be found in [our documentation][features docs].\n\n### Web Server & UI\n\nInvoke runs a locally hosted web server & React UI with an industry-leading user experience.\n\n### Unified Canvas\n\nThe Unified Canvas is a fully integrated canvas implementation with support for all core generation capabilities, in/out-painting, brush tools, and more. This creative tool unlocks the capability for artists to create with AI as a creative collaborator, and can be used to augment AI-generated imagery, sketches, photography, renders, and more.\n\n### Workflows & Nodes\n\nInvoke offers a fully featured workflow management solution, enabling users to combine the power of node-based workflows with the easy of a UI. This allows for customizable generation pipelines to be developed and shared by users looking to create specific workflows to support their production use-cases.\n\n### Board & Gallery Management\n\nInvoke features an organized gallery system for easily storing, accessing, and remixing your content in the Invoke workspace. Images can be dragged/dropped onto any Image-base UI element in the application, and rich metadata within the Image allows for easy recall of key prompts or settings used in your workflow.\n\n### Other features\n\n- Support for both ckpt and diffusers models\n- SD1.5, SD2.0, SDXL, and FLUX support\n- Upscaling Tools\n- Embedding Manager & Support\n- Model Manager & Support\n- Workflow creation & management\n- Node-Based Architecture\n\n## Contributing\n\nAnyone who wishes to contribute to this project - whether documentation, features, bug fixes, code cleanup, testing, or code reviews - is very much encouraged to do so.\n\nGet started with contributing by reading our [contribution documentation][contributing docs], joining the [#dev-chat] or the GitHub discussion board.\n\nWe hope you enjoy using Invoke as much as we enjoy creating it, and we hope you will elect to become part of our community.\n\n## Thanks\n\nInvoke is a combined effort of [passionate and talented people from across the world][contributors]. We thank them for their time, hard work and effort.\n\nOriginal portions of the software are Copyright \u00a9 2024 by respective contributors.\n\n[features docs]: https://invoke-ai.github.io/InvokeAI/features/database/\n[faq]: https://invoke-ai.github.io/InvokeAI/faq/\n[contributors]: https://invoke-ai.github.io/InvokeAI/contributing/contributors/\n[invoke.com]: https://www.invoke.com/about\n[github issues]: https://github.com/invoke-ai/InvokeAI/issues\n[docs home]: https://invoke-ai.github.io/InvokeAI\n[installation docs]: https://invoke-ai.github.io/InvokeAI/installation/\n[#dev-chat]: https://discord.com/channels/1020123559063990373/1049495067846524939\n[contributing docs]: https://invoke-ai.github.io/InvokeAI/contributing/\n[CI checks on main badge]: https://flat.badgen.net/github/checks/invoke-ai/InvokeAI/main?label=CI%20status%20on%20main&cache=900&icon=github\n[CI checks on main link]: https://github.com/invoke-ai/InvokeAI/actions?query=branch%3Amain\n[discord badge]: https://flat.badgen.net/discord/members/ZmtBAhwWhy?icon=discord\n[discord link]: https://discord.gg/ZmtBAhwWhy\n[github forks badge]: https://flat.badgen.net/github/forks/invoke-ai/InvokeAI?icon=github\n[github forks link]: https://useful-forks.github.io/?repo=invoke-ai%2FInvokeAI\n[github open issues badge]: https://flat.badgen.net/github/open-issues/invoke-ai/InvokeAI?icon=github\n[github open issues link]: https://github.com/invoke-ai/InvokeAI/issues?q=is%3Aissue+is%3Aopen\n[github open prs badge]: https://flat.badgen.net/github/open-prs/invoke-ai/InvokeAI?icon=github\n[github open prs link]: https://github.com/invoke-ai/InvokeAI/pulls?q=is%3Apr+is%3Aopen\n[github stars badge]: https://flat.badgen.net/github/stars/invoke-ai/InvokeAI?icon=github\n[github stars link]: https://github.com/invoke-ai/InvokeAI/stargazers\n[latest commit to main badge]: https://flat.badgen.net/github/last-commit/invoke-ai/InvokeAI/main?icon=github&color=yellow&label=last%20dev%20commit&cache=900\n[latest commit to main link]: https://github.com/invoke-ai/InvokeAI/commits/main\n[latest release badge]: https://flat.badgen.net/github/release/invoke-ai/InvokeAI/development?icon=github\n[latest release link]: https://github.com/invoke-ai/InvokeAI/releases/latest\n[translation status badge]: https://hosted.weblate.org/widgets/invokeai/-/svg-badge.svg\n[translation status link]: https://hosted.weblate.org/engage/invokeai/\n[nvidia docker docs]: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html\n[amd docker docs]: https://rocm.docs.amd.com/projects/install-on-linux/en/latest/how-to/docker.html\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 834369920,
    "name": "minimind",
    "full_name": "jingyaogong/minimind",
    "description": "\ud83d\ude80\ud83d\ude80 \u300c\u5927\u6a21\u578b\u300d2\u5c0f\u65f6\u5b8c\u5168\u4ece0\u8bad\u7ec326M\u7684\u5c0f\u53c2\u6570GPT\uff01\ud83c\udf0f Train a 26M-parameter GPT from scratch in just 2h!",
    "html_url": "https://github.com/jingyaogong/minimind",
    "clone_url": "https://github.com/jingyaogong/minimind.git",
    "owner_login": "jingyaogong",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/62287848?v=4",
    "stargazers_count": 23835,
    "watchers_count": 23835,
    "forks_count": 2839,
    "open_issues_count": 98,
    "size": 26591,
    "language": "Python",
    "languages": {
      "Python": 122417
    },
    "topics": [
      "artificial-intelligence",
      "large-language-model"
    ],
    "license_name": "Apache License 2.0",
    "created_at": "2024-07-27T04:34:12+00:00",
    "updated_at": "2025-08-06T02:10:55+00:00",
    "pushed_at": "2025-04-30T08:47:46+00:00",
    "contributors_count": 4,
    "readme_length": 53268,
    "readme_content": "<div align=\"center\">\n\n![logo](./images/logo.png)\n\n</div>\n\n<div align=\"center\">\n\n![visitors](https://visitor-badge.laobi.icu/badge?page_id=jingyaogong/minimind)\n[![GitHub Repo stars](https://img.shields.io/github/stars/jingyaogong/minimind?style=social)](https://github.com/jingyaogong/minimind/stargazers)\n[![GitHub Code License](https://img.shields.io/github/license/jingyaogong/minimind)](LICENSE)\n[![GitHub last commit](https://img.shields.io/github/last-commit/jingyaogong/minimind)](https://github.com/jingyaogong/minimind/commits/master)\n[![GitHub pull request](https://img.shields.io/badge/PRs-welcome-blue)](https://github.com/jingyaogong/minimind/pulls)\n[![Collection](https://img.shields.io/badge/\ud83e\udd17-MiniMind%20%20Collection-blue)](https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5)\n\n</div>\n\n<div align=\"center\">\n  <h3>\"\u5927\u9053\u81f3\u7b80\"</h3>\n</div>\n\n<div align=\"center\">\n\n\u4e2d\u6587 | [English](./README_en.md)\n\n</div>\n\n* \u6b64\u5f00\u6e90\u9879\u76ee\u65e8\u5728\u5b8c\u5168\u4ece0\u5f00\u59cb\uff0c\u4ec5\u75283\u5757\u94b1\u6210\u672c + 2\u5c0f\u65f6\uff01\u5373\u53ef\u8bad\u7ec3\u51fa\u4ec5\u4e3a25.8M\u7684\u8d85\u5c0f\u8bed\u8a00\u6a21\u578b**MiniMind**\u3002\n* **MiniMind**\u7cfb\u5217\u6781\u5176\u8f7b\u91cf\uff0c\u6700\u5c0f\u7248\u672c\u4f53\u79ef\u662f GPT-3 \u7684 $\\frac{1}{7000}$\uff0c\u529b\u6c42\u505a\u5230\u6700\u666e\u901a\u7684\u4e2a\u4ebaGPU\u4e5f\u53ef\u5feb\u901f\u8bad\u7ec3\u3002\n* \u9879\u76ee\u540c\u65f6\u5f00\u6e90\u4e86\u5927\u6a21\u578b\u7684\u6781\u7b80\u7ed3\u6784-\u5305\u542b\u62d3\u5c55\u5171\u4eab\u6df7\u5408\u4e13\u5bb6(MoE)\u3001\u6570\u636e\u96c6\u6e05\u6d17\u3001\u9884\u8bad\u7ec3(Pretrain)\u3001\u76d1\u7763\u5fae\u8c03(SFT)\u3001LoRA\u5fae\u8c03\uff0c\n  \u76f4\u63a5\u504f\u597d\u5f3a\u5316\u5b66\u4e60(DPO)\u7b97\u6cd5\u3001\u6a21\u578b\u84b8\u998f\u7b97\u6cd5\u7b49\u5168\u8fc7\u7a0b\u4ee3\u7801\u3002\n* **MiniMind**\u540c\u65f6\u62d3\u5c55\u4e86\u89c6\u89c9\u591a\u6a21\u6001\u7684VLM: [MiniMind-V](https://github.com/jingyaogong/minimind-v)\u3002\n* \u9879\u76ee\u6240\u6709\u6838\u5fc3\u7b97\u6cd5\u4ee3\u7801\u5747\u4ece0\u4f7f\u7528PyTorch\u539f\u751f\u91cd\u6784\uff01\u4e0d\u4f9d\u8d56\u7b2c\u4e09\u65b9\u5e93\u63d0\u4f9b\u7684\u62bd\u8c61\u63a5\u53e3\u3002\n* \u8fd9\u4e0d\u4ec5\u662f\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5168\u9636\u6bb5\u5f00\u6e90\u590d\u73b0\uff0c\u4e5f\u662f\u4e00\u4e2a\u5165\u95e8LLM\u7684\u6559\u7a0b\u3002\n* \u5e0c\u671b\u6b64\u9879\u76ee\u80fd\u4e3a\u6240\u6709\u4eba\u63d0\u4f9b\u4e00\u4e2a\u629b\u7816\u5f15\u7389\u7684\u793a\u4f8b\uff0c\u4e00\u8d77\u611f\u53d7\u521b\u9020\u7684\u4e50\u8da3\uff01\u63a8\u52a8\u66f4\u5e7f\u6cdbAI\u793e\u533a\u7684\u8fdb\u6b65\uff01\n\n> \u4e3a\u9632\u6b62\u8bef\u89e3\uff0c\u201c2\u5c0f\u65f6\u201d \u57fa\u4e8eNVIDIA 3090\u786c\u4ef6\u8bbe\u5907\uff08\u5355\u5361\uff09\u6d4b\u8bd5\uff0c\u201c3\u5757\u94b1\u201d\n> \u6307GPU\u670d\u52a1\u5668\u79df\u7528\u6210\u672c\uff0c\u5177\u4f53\u89c4\u683c\u8be6\u60c5\u89c1\u4e0b\u6587\u3002\n\n---\n\n\n<div align=\"center\">\n\n![minimind2](./images/minimind2.gif)\n\n[\ud83d\udd17\ud83c\udf53\u63a8\u7406\u6a21\u578b](https://www.modelscope.cn/studios/gongjy/MiniMind-Reasoning) | [\ud83d\udd17\ud83e\udd16\u5e38\u89c4\u6a21\u578b](https://www.modelscope.cn/studios/gongjy/MiniMind) | [\ud83d\udd17\ud83c\udf9e\ufe0f\u89c6\u9891\u4ecb\u7ecd](https://www.bilibili.com/video/BV12dHPeqE72/?share_source=copy_web&vd_source=670c2504f88726f8cf4a21ef6147c0e8)\n\n\n<div align=\"center\">\n  <table>\n    <tr>\n      <td align=\"center\">\n        <a href=\"https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5\" style=\"text-decoration: none;\">\n          <img src=\"./images/and_huggingface.png\" alt=\"Hugging Face Logo\" style=\"vertical-align: middle; width: auto; max-width: 100%;\" />\n        </a>\n      </td>\n      <td align=\"center\">\n        <a href=\"https://www.modelscope.cn/profile/gongjy\" style=\"text-decoration: none;\">\n          <img src=\"./images/and_modelscope.png\" alt=\"ModelScope Logo\" style=\"vertical-align: middle; width: auto; max-width: 100%;\" />\n        </a>\n      </td>\n    </tr>\n  </table>\n</div>\n\n\n</div>\n\n# \ud83d\udccc Introduction\n\n\u5927\u8bed\u8a00\u6a21\u578b\uff08Large Language Model, LLM\uff09\u7684\u51fa\u73b0\u5f15\u53d1\u4e86\u5168\u4e16\u754c\u5bf9AI\u7684\u7a7a\u524d\u5173\u6ce8\u3002\n\u65e0\u8bba\u662fChatGPT\u3001DeepSeek\u8fd8\u662fQwen\uff0c\u90fd\u4ee5\u5176\u60ca\u8273\u7684\u6548\u679c\u4ee4\u4eba\u53f9\u4e3a\u89c2\u6b62\u3002\n\u7136\u800c\uff0c\u52a8\u8f84\u6570\u767e\u4ebf\u53c2\u6570\u7684\u5e9e\u5927\u89c4\u6a21\uff0c\u4f7f\u5f97\u5b83\u4eec\u5bf9\u4e2a\u4eba\u8bbe\u5907\u800c\u8a00\u4e0d\u4ec5\u96be\u4ee5\u8bad\u7ec3\uff0c\u751a\u81f3\u8fde\u90e8\u7f72\u90fd\u663e\u5f97\u9065\u4e0d\u53ef\u53ca\u3002\n\u6253\u5f00\u5927\u6a21\u578b\u7684\u201c\u9ed1\u76d2\u5b50\u201d\uff0c\u63a2\u7d22\u5176\u5185\u90e8\u8fd0\u4f5c\u673a\u5236\uff0c\u591a\u4e48\u4ee4\u4eba\u5fc3\u6f6e\u6f8e\u6e43\uff01\n\u9057\u61be\u7684\u662f\uff0c99%\u7684\u63a2\u7d22\u53ea\u80fd\u6b62\u6b65\u4e8e\u4f7f\u7528LoRA\u7b49\u6280\u672f\u5bf9\u73b0\u6709\u5927\u6a21\u578b\u8fdb\u884c\u5c11\u91cf\u5fae\u8c03\uff0c\u5b66\u4e60\u4e00\u4e9b\u65b0\u6307\u4ee4\u6216\u4efb\u52a1\u3002\n\u8fd9\u5c31\u597d\u6bd4\u6559\u725b\u987f\u5982\u4f55\u4f7f\u752821\u4e16\u7eaa\u7684\u667a\u80fd\u624b\u673a\u2014\u2014\u867d\u7136\u6709\u8da3\uff0c\u5374\u5b8c\u5168\u504f\u79bb\u4e86\u7406\u89e3\u7269\u7406\u672c\u8d28\u7684\u521d\u8877\u3002\n\u4e0e\u6b64\u540c\u65f6\uff0c\u7b2c\u4e09\u65b9\u7684\u5927\u6a21\u578b\u6846\u67b6\u548c\u5de5\u5177\u5e93\uff0c\u5982transformers+trl\uff0c\u51e0\u4e4e\u53ea\u66b4\u9732\u4e86\u9ad8\u5ea6\u62bd\u8c61\u7684\u63a5\u53e3\u3002\n\u901a\u8fc7\u77ed\u77ed10\u884c\u4ee3\u7801\uff0c\u5c31\u80fd\u5b8c\u6210\u201c\u52a0\u8f7d\u6a21\u578b+\u52a0\u8f7d\u6570\u636e\u96c6+\u63a8\u7406+\u5f3a\u5316\u5b66\u4e60\u201d\u7684\u5168\u6d41\u7a0b\u8bad\u7ec3\u3002\n\u8fd9\u79cd\u9ad8\u6548\u7684\u5c01\u88c5\u56fa\u7136\u4fbf\u5229\uff0c\u4f46\u4e5f\u50cf\u4e00\u67b6\u9ad8\u901f\u98de\u8239\uff0c\u5c06\u6211\u4eec\u4e0e\u5e95\u5c42\u5b9e\u73b0\u9694\u79bb\u5f00\u6765\uff0c\u963b\u788d\u4e86\u6df1\u5165\u63a2\u7a76LLM\u6838\u5fc3\u4ee3\u7801\u7684\u673a\u4f1a\u3002\n\u7136\u800c\uff0c\u201c\u7528\u4e50\u9ad8\u62fc\u51fa\u4e00\u67b6\u98de\u673a\uff0c\u8fdc\u6bd4\u5750\u5728\u5934\u7b49\u8231\u91cc\u98de\u884c\u66f4\u8ba9\u4eba\u5174\u594b\uff01\u201d\u3002\n\u66f4\u7cdf\u7cd5\u7684\u662f\uff0c\u4e92\u8054\u7f51\u4e0a\u5145\u65a5\u7740\u5927\u91cf\u4ed8\u8d39\u8bfe\u7a0b\u548c\u8425\u9500\u53f7\uff0c\u4ee5\u6f0f\u6d1e\u767e\u51fa\u3001\u4e00\u77e5\u534a\u89e3\u7684\u5185\u5bb9\u63a8\u9500AI\u6559\u7a0b\u3002\n\u6b63\u56e0\u5982\u6b64\uff0c\u672c\u9879\u76ee\u521d\u8877\u662f\u62c9\u4f4eLLM\u7684\u5b66\u4e60\u95e8\u69db\uff0c\u8ba9\u6bcf\u4e2a\u4eba\u90fd\u80fd\u4ece\u7406\u89e3\u6bcf\u4e00\u884c\u4ee3\u7801\u5f00\u59cb\uff0c\n\u4ece\u96f6\u5f00\u59cb\u4eb2\u624b\u8bad\u7ec3\u4e00\u4e2a\u6781\u5c0f\u7684\u8bed\u8a00\u6a21\u578b\u3002\u662f\u7684\uff0c\u4ece**\u96f6\u5f00\u59cb\u8bad\u7ec3**\uff0c\u800c\u4e0d\u662f\u4ec5\u4ec5\u8fdb\u884c**\u63a8\u7406**\uff01\n\u6700\u4f4e\u53ea\u97003\u5757\u94b1\u4e0d\u5230\u7684\u670d\u52a1\u5668\u6210\u672c\uff0c\u5c31\u80fd\u4eb2\u8eab\u4f53\u9a8c\u4ece0\u52301\u6784\u5efa\u4e00\u4e2a\u8bed\u8a00\u6a21\u578b\u7684\u5168\u8fc7\u7a0b\u3002\n\u4e00\u8d77\u611f\u53d7\u521b\u9020\u7684\u4e50\u8da3\u5427\uff01\n\n> [!NOTE]\n> \uff08\u622a\u81f32025-02-07\uff09MiniMind\u7cfb\u5217\u5df2\u5b8c\u6210\u591a\u4e2a\u578b\u53f7\u6a21\u578b\u7684\u9884\u8bad\u7ec3\uff0c\u6700\u5c0f\u4ec5\u970025.8M\uff080.02B\uff09\uff0c\u5373\u53ef\u5177\u5907\u6d41\u7545\u5bf9\u8bdd\u80fd\u529b\uff01\n\n<details style=\"color:rgb(128,128,128)\">\n<summary>Models List</summary>\n\n| \u6a21\u578b (\u5927\u5c0f)                 | \u63a8\u7406\u5360\u7528 (\u7ea6) | Release    | \n|-------------------------|----------|------------|\n| MiniMind2-small (26M)   | 0.5 GB   | 2025.04.26 |\n| MiniMind2-MoE (145M)    | 1.0 GB   | 2025.04.26 |\n| MiniMind2 (104M)        | 1.0 GB   | 2025.04.26 |\n| minimind-v1-small (26M) | 0.5 GB   | 2024.08.28 |\n| minimind-v1-moe (4\u00d726M) | 1.0 GB   | 2024.09.17 |\n| minimind-v1 (108M)      | 1.0 GB   | 2024.09.01 |\n\n</details>\n\n**\u9879\u76ee\u5305\u542b**\n\n- MiniMind-LLM\u7ed3\u6784\u7684\u5168\u90e8\u4ee3\u7801\uff08Dense+MoE\u6a21\u578b\uff09\u3002\n- \u5305\u542bTokenizer\u5206\u8bcd\u5668\u8be6\u7ec6\u8bad\u7ec3\u4ee3\u7801\u3002\n- \u5305\u542bPretrain\u3001SFT\u3001LoRA\u3001RLHF-DPO\u3001\u6a21\u578b\u84b8\u998f\u7684\u5168\u8fc7\u7a0b\u8bad\u7ec3\u4ee3\u7801\u3002\n- \u6536\u96c6\u3001\u84b8\u998f\u3001\u6574\u7406\u5e76\u6e05\u6d17\u53bb\u91cd\u6240\u6709\u9636\u6bb5\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u4e14\u5168\u90e8\u5f00\u6e90\u3002\n- \u4ece0\u5b9e\u73b0\u9884\u8bad\u7ec3\u3001\u6307\u4ee4\u5fae\u8c03\u3001LoRA\u3001DPO\u5f3a\u5316\u5b66\u4e60\uff0c\u767d\u76d2\u6a21\u578b\u84b8\u998f\u3002\u5173\u952e\u7b97\u6cd5\u51e0\u4e4e\u4e0d\u4f9d\u8d56\u7b2c\u4e09\u65b9\u5c01\u88c5\u7684\u6846\u67b6\uff0c\u4e14\u5168\u90e8\u5f00\u6e90\u3002\n- \u540c\u65f6\u517c\u5bb9`transformers`\u3001`trl`\u3001`peft`\u7b49\u7b2c\u4e09\u65b9\u4e3b\u6d41\u6846\u67b6\u3002\n- \u8bad\u7ec3\u652f\u6301\u5355\u673a\u5355\u5361\u3001\u5355\u673a\u591a\u5361(DDP\u3001DeepSpeed)\u8bad\u7ec3\uff0c\u652f\u6301wandb\u53ef\u89c6\u5316\u8bad\u7ec3\u6d41\u7a0b\u3002\u652f\u6301\u52a8\u6001\u542f\u505c\u8bad\u7ec3\u3002\n- \u5728\u7b2c\u4e09\u65b9\u6d4b\u8bc4\u699c\uff08C-Eval\u3001C-MMLU\u3001OpenBookQA\u7b49\uff09\u8fdb\u884c\u6a21\u578b\u6d4b\u8bd5\u3002\n- \u5b9e\u73b0Openai-Api\u534f\u8bae\u7684\u6781\u7b80\u670d\u52a1\u7aef\uff0c\u4fbf\u4e8e\u96c6\u6210\u5230\u7b2c\u4e09\u65b9ChatUI\u4f7f\u7528\uff08FastGPT\u3001Open-WebUI\u7b49\uff09\u3002\n- \u57fa\u4e8estreamlit\u5b9e\u73b0\u6700\u7b80\u804a\u5929WebUI\u524d\u7aef\u3002\n- \u5168\u9762\u517c\u5bb9\u793e\u533a\u70ed\u95e8`llama.cpp`\u3001`vllm`\u3001`ollama`\u63a8\u7406\u5f15\u64ce\u6216`Llama-Factory`\u8bad\u7ec3\u6846\u67b6\u3002\n- \u590d\u73b0(\u84b8\u998f/RL)\u5927\u578b\u63a8\u7406\u6a21\u578bDeepSeek-R1\u7684MiniMind-Reason\u6a21\u578b\uff0c**\u6570\u636e+\u6a21\u578b**\u5168\u90e8\u5f00\u6e90\uff01\n\n\u5e0c\u671b\u6b64\u5f00\u6e90\u9879\u76ee\u53ef\u4ee5\u5e2e\u52a9LLM\u521d\u5b66\u8005\u5feb\u901f\u5165\u95e8\uff01\n\n### \ud83d\udc49**\u66f4\u65b0\u65e5\u5fd7**\n\n<details close> \n<summary> <b>2025-04-26 (newest \ud83c\udf89\ud83c\udf89\ud83c\udf89)</b> </summary>\n\n- \u91cd\u8981\u66f4\u65b0\n- \u5982\u6709\u517c\u5bb9\u6027\u9700\u8981\uff0c\u53ef\u8bbf\u95ee[\ud83d\udd17\u65e7\u4ed3\u5e93\u5185\u5bb9\ud83d\udd17](https://github.com/jingyaogong/minimind/tree/7da201a944a90ed49daef8a0265c959288dff83a)\u3002\n- MiniMind\u6a21\u578b\u53c2\u6570\u5b8c\u5168\u6539\u540d\uff0c\u5bf9\u9f50Transformers\u5e93\u6a21\u578b\uff08\u7edf\u4e00\u547d\u540d\uff09\u3002\n- generate\u65b9\u5f0f\u91cd\u6784\uff0c\u7ee7\u627f\u81eaGenerationMixin\u7c7b\u3002\n- \ud83d\udd25\u652f\u6301llama.cpp\u3001vllm\u3001ollama\u7b49\u70ed\u95e8\u4e09\u65b9\u751f\u6001\u3002\n- \u89c4\u8303\u4ee3\u7801\u548c\u76ee\u5f55\u7ed3\u6784\u3002\n- \u6539\u52a8\u8bcd\u8868`<s></s>`->`<|im_start|><|im_end|>`\n```text\n\u4e3a\u517c\u5bb9\u7b2c\u4e09\u65b9\u63a8\u7406\u6846\u67b6llama.cpp\u3001vllm\uff0c\u672c\u6b21\u66f4\u65b0\u9700\u4ed8\u51fa\u4e00\u4e9b\u53ef\u89c2\u4ee3\u4ef7\u3002\n\u672c\u6b21\u66f4\u65b0\u4e0d\u518d\u652f\u6301\u300c\u76f4\u63a5\u300d\u52a0\u8f7d25-04-26\u4ee5\u524d\u7684\u65e7\u6a21\u578b\u8fdb\u884c\u63a8\u7406\u3002\n\u7531\u4e8eLlama\u4f4d\u7f6e\u7f16\u7801\u65b9\u5f0f\u4e0eminimind\u5b58\u5728\u533a\u522b\uff0c\u5bfc\u81f4\u6620\u5c04Llama\u6a21\u578b\u540eQK\u503c\u5b58\u5728\u5dee\u5f02\nMiniMind2\u7cfb\u5217\u65e7\u6a21\u578b\u5747\u7ecf\u8fc7\u6743\u91cd\u6620\u5c04+\uff08\u5fae\u8c03\u8bad\u7ec3\uff09QKVO\u7ebf\u6027\u5c42\u6821\u51c6\u6062\u590d\u800c\u6765\u3002\n\u672c\u6b21\u66f4\u65b0\u540e\u5c06\u653e\u5f03\u5bf9`minimind-v1`\u5168\u7cfb\u5217\u7684\u7ef4\u62a4\uff0c\u5e76\u5728\u4ed3\u5e93\u4e2d\u4e0b\u7ebf\u3002\n```\n</details>\n\n<details close> \n<summary> <b>2025-02-09</b> </summary>\n\n- \u8fce\u6765\u53d1\u5e03\u4ee5\u6765\u91cd\u5927\u66f4\u65b0\uff0cRelease MiniMind2 Series\u3002\n- \u4ee3\u7801\u51e0\u4e4e\u5168\u90e8\u91cd\u6784\uff0c\u4f7f\u7528\u66f4\u7b80\u6d01\u660e\u4e86\u7684\u7edf\u4e00\u7ed3\u6784\u3002\n  \u5982\u6709\u65e7\u4ee3\u7801\u7684\u517c\u5bb9\u6027\u9700\u8981\uff0c\u53ef\u8bbf\u95ee[\ud83d\udd17\u65e7\u4ed3\u5e93\u5185\u5bb9\ud83d\udd17](https://github.com/jingyaogong/minimind/tree/6e9cd28ef9b34a0a10afbdf6f59e65cb6e628efb)\u3002\n- \u514d\u53bb\u6570\u636e\u9884\u5904\u7406\u6b65\u9aa4\u3002\u7edf\u4e00\u6570\u636e\u96c6\u683c\u5f0f\uff0c\u66f4\u6362\u4e3a`jsonl`\u683c\u5f0f\u675c\u7edd\u6570\u636e\u96c6\u4e0b\u8f7d\u6df7\u4e71\u7684\u95ee\u9898\u3002\n- MiniMind2\u7cfb\u5217\u6548\u679c\u76f8\u6bd4MiniMind-V1\u663e\u8457\u63d0\u5347\u3002\n- \u5c0f\u95ee\u9898\uff1a{kv-cache\u5199\u6cd5\u66f4\u6807\u51c6\u3001MoE\u7684\u8d1f\u8f7d\u5747\u8861loss\u88ab\u8003\u8651\u7b49\u7b49}\n- \u63d0\u4f9b\u6a21\u578b\u8fc1\u79fb\u5230\u79c1\u6709\u6570\u636e\u96c6\u7684\u8bad\u7ec3\u65b9\u6848\uff08\u533b\u7597\u6a21\u578b\u3001\u81ea\u6211\u8ba4\u77e5\u6837\u4f8b\uff09\u3002\n- \u7cbe\u7b80\u9884\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5e76\u5927\u5e45\u63d0\u5347\u9884\u8bad\u7ec3\u6570\u636e\u8d28\u91cf\uff0c\u5927\u5e45\u7f29\u77ed\u4e2a\u4eba\u5feb\u901f\u8bad\u7ec3\u6240\u9700\u65f6\u95f4\uff0c\u5355\u53613090\u5373\u53ef2\u5c0f\u65f6\u590d\u73b0\uff01\n- \u66f4\u65b0\uff1aLoRA\u5fae\u8c03\u8131\u79bbpeft\u5305\u88c5\uff0c\u4ece0\u5b9e\u73b0LoRA\u8fc7\u7a0b\uff1bDPO\u7b97\u6cd5\u4ece0\u4f7f\u7528PyTorch\u539f\u751f\u5b9e\u73b0\uff1b\u6a21\u578b\u767d\u76d2\u84b8\u998f\u539f\u751f\u5b9e\u73b0\u3002\n- MiniMind2-DeepSeek-R1\u7cfb\u5217\u84b8\u998f\u6a21\u578b\u8bde\u751f\uff01\n- MiniMind2\u5177\u5907\u4e00\u5b9a\u7684\u82f1\u6587\u80fd\u529b\uff01\n- \u66f4\u65b0MiniMind2\u4e0e\u7b2c\u4e09\u65b9\u6a21\u578b\u7684\u57fa\u4e8e\u66f4\u591a\u5927\u6a21\u578b\u699c\u5355\u6d4b\u8bd5\u6027\u80fd\u7684\u7ed3\u679c\u3002\n\n</details>\n\n\n\n<details close> \n<summary> <b>2024-10-05</b> </summary>\n\n- \u4e3aMiniMind\u62d3\u5c55\u4e86\u591a\u6a21\u6001\u80fd\u529b\u4e4b---\u89c6\u89c9\n- \u79fb\u6b65\u5b6a\u751f\u9879\u76ee[minimind-v](https://github.com/jingyaogong/minimind-v)\u67e5\u770b\u8be6\u60c5\uff01\n\n</details>\n\n\n\n<details close> \n<summary> <b>2024-09-27</b> </summary>\n\n- 09-27\u66f4\u65b0pretrain\u6570\u636e\u96c6\u7684\u9884\u5904\u7406\u65b9\u5f0f\uff0c\u4e3a\u4e86\u4fdd\u8bc1\u6587\u672c\u5b8c\u6574\u6027\uff0c\u653e\u5f03\u9884\u5904\u7406\u6210.bin\u8bad\u7ec3\u7684\u5f62\u5f0f\uff08\u8f7b\u5fae\u727a\u7272\u8bad\u7ec3\u901f\u5ea6\uff09\u3002\n- \u76ee\u524dpretrain\u9884\u5904\u7406\u540e\u7684\u6587\u4ef6\u547d\u540d\u4e3a\uff1apretrain_data.csv\u3002\n- \u5220\u9664\u4e86\u4e00\u4e9b\u5197\u4f59\u7684\u4ee3\u7801\u3002\n\n</details>\n\n\n<details close> \n<summary> <b>2024-09-17</b> </summary>\n\n- \u66f4\u65b0minimind-v1-moe\u6a21\u578b\n- \u4e3a\u4e86\u9632\u6b62\u6b67\u4e49\uff0c\u4e0d\u518d\u4f7f\u7528mistral_tokenizer\u5206\u8bcd\uff0c\u5168\u90e8\u91c7\u7528\u81ea\u5b9a\u4e49\u7684minimind_tokenizer\u4f5c\u4e3a\u5206\u8bcd\u5668\u3002\n\n</details>\n\n\n<details close>\n<summary> <b>2024-09-01</b> </summary>\n\n- \u66f4\u65b0minimind-v1 (108M)\u6a21\u578b\uff0c\u91c7\u7528minimind_tokenizer\uff0c\u9884\u8bad\u7ec3\u8f6e\u6b213 + SFT\u8f6e\u6b2110\uff0c\u66f4\u5145\u5206\u8bad\u7ec3\uff0c\u6027\u80fd\u66f4\u5f3a\u3002\n- \u9879\u76ee\u5df2\u90e8\u7f72\u81f3ModelScope\u521b\u7a7a\u95f4\uff0c\u53ef\u4ee5\u5728\u6b64\u7f51\u7ad9\u4e0a\u4f53\u9a8c\uff1a\n- [\ud83d\udd17ModelScope\u5728\u7ebf\u4f53\u9a8c\ud83d\udd17](https://www.modelscope.cn/studios/gongjy/minimind)\n\n</details>\n\n\n<details close> \n<summary> <b>2024-08-27</b> </summary>\n\n- \u9879\u76ee\u9996\u6b21\u5f00\u6e90\n\n</details>\n\n# \ud83d\udccc \u5feb\u901f\u5f00\u59cb\n\n<details style=\"color:rgb(128,128,128)\">\n<summary>\u5206\u4eab\u672c\u4eba\u7684\u8f6f\u786c\u4ef6\u914d\u7f6e\uff08\u4ec5\u4f9b\u53c2\u8003\uff09</summary>\n\n* CPU: Intel(R) Core(TM) i9-10980XE CPU @ 3.00GHz\n* RAM: 128 GB\n* GPU: NVIDIA GeForce RTX 3090(24GB) * 8\n* Ubuntu==20.04\n* CUDA==12.2\n* Python==3.10.16\n* [requirements.txt](./requirements.txt)\n\n</details>\n\n### \u7b2c0\u6b65\n\n```bash\ngit clone https://github.com/jingyaogong/minimind.git\n```\n\n## \u2160 \u6d4b\u8bd5\u5df2\u6709\u6a21\u578b\u6548\u679c\n\n### 1.\u73af\u5883\u51c6\u5907\n\n```bash\npip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple\n```\n\n### 2.\u4e0b\u8f7d\u6a21\u578b\n\u5230\u9879\u76ee\u6839\u76ee\u5f55\n```bash\ngit clone https://huggingface.co/jingyaogong/MiniMind2\n```\n\n### \uff08\u53ef\u9009\uff09\u547d\u4ee4\u884c\u95ee\u7b54\n\n```bash\n# load=0: load from pytorch model, load=1: load from transformers-hf model\npython eval_model.py --load 1 --model_mode 2\n```\n\n### \uff08\u53ef\u9009\uff09\u542f\u52a8WebUI\n\n```bash\n# \u53ef\u80fd\u9700\u8981`python>=3.10` \u5b89\u88c5 `pip install streamlit`\n# cd scripts\nstreamlit run web_demo.py\n```\n\n### \uff08\u53ef\u9009\uff09\u7b2c\u4e09\u65b9\u63a8\u7406\u6846\u67b6\n\n```bash\n# ollama\nollama run jingyaogong/minimind2\n# vllm\nvllm serve ./MiniMind2/ --served-model-name \"minimind\"\n```\n\n## \u2161 \u4ece0\u5f00\u59cb\u81ea\u5df1\u8bad\u7ec3\n\n### 1.\u73af\u5883\u51c6\u5907\n\n```bash\npip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple\n```\n\n<details style=\"color:rgb(128,128,128)\">\n<summary>\u6ce8\uff1a\u63d0\u524d\u6d4b\u8bd5Torch\u662f\u5426\u53ef\u7528cuda</summary>\n\n```bash\nimport torch\nprint(torch.cuda.is_available())\n```\n\n\u5982\u679c\u4e0d\u53ef\u7528\uff0c\u8bf7\u81ea\u884c\u53bb[torch_stable](https://download.pytorch.org/whl/torch_stable.html)\n\u4e0b\u8f7dwhl\u6587\u4ef6\u5b89\u88c5\u3002\u53c2\u8003[\u94fe\u63a5](https://blog.csdn.net/weixin_45456738/article/details/141029610?ops_request_misc=&request_id=&biz_id=102&utm_term=%E5%AE%89%E8%A3%85torch&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-2-141029610.nonecase&spm=1018.2226.3001.4187)\n\n</details>\n\n### 2.\u6570\u636e\u4e0b\u8f7d\n\n\u4ece\u4e0b\u6587\u63d0\u4f9b\u7684[\u6570\u636e\u96c6\u4e0b\u8f7d\u94fe\u63a5](https://www.modelscope.cn/datasets/gongjy/minimind_dataset/files)\n\u4e0b\u8f7d\u9700\u8981\u7684\u6570\u636e\u6587\u4ef6\uff08\u521b\u5efa`./dataset`\u76ee\u5f55\uff09\u5e76\u653e\u5230`./dataset`\u4e0b\n\n<details style=\"color:rgb(128,128,128)\">\n<summary>\u6ce8\uff1a\u6570\u636e\u96c6\u987b\u77e5</summary>\n\n\u9ed8\u8ba4\u63a8\u8350\u4e0b\u8f7d`pretrain_hq.jsonl` + `sft_mini_512.jsonl`\u6700\u5feb\u901f\u5ea6\u590d\u73b0Zero\u804a\u5929\u6a21\u578b\u3002\n\n\u6570\u636e\u6587\u4ef6\u53ef\u81ea\u7531\u9009\u62e9\uff0c\u4e0b\u6587\u63d0\u4f9b\u4e86\u591a\u79cd\u642d\u914d\u65b9\u6848\uff0c\u53ef\u6839\u636e\u81ea\u5df1\u624b\u5934\u7684\u8bad\u7ec3\u9700\u6c42\u548cGPU\u8d44\u6e90\u8fdb\u884c\u9002\u5f53\u7ec4\u5408\u3002\n\n</details>\n\n### 3.\u5f00\u59cb\u8bad\u7ec3\n\n\u76ee\u5f55\u4f4d\u4e8e`trainer`\n\n**3.1 \u9884\u8bad\u7ec3\uff08\u5b66\u77e5\u8bc6\uff09**\n\n```bash\npython train_pretrain.py\n```\n\n> \u6267\u884c\u9884\u8bad\u7ec3\uff0c\u5f97\u5230 `pretrain_*.pth` \u4f5c\u4e3a\u9884\u8bad\u7ec3\u7684\u8f93\u51fa\u6743\u91cd\uff08\u5176\u4e2d*\u4e3a\u6a21\u578b\u7684dimension\uff0c\u9ed8\u8ba4\u4e3a512\uff09\n\n\n**3.2 \u76d1\u7763\u5fae\u8c03\uff08\u5b66\u5bf9\u8bdd\u65b9\u5f0f\uff09**\n\n```bash\npython train_full_sft.py\n```\n\n> \u6267\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u5f97\u5230 `full_sft_*.pth` \u4f5c\u4e3a\u6307\u4ee4\u5fae\u8c03\u7684\u8f93\u51fa\u6743\u91cd\uff08\u5176\u4e2d`full`\u5373\u4e3a\u5168\u53c2\u6570\u5fae\u8c03\uff09\n\n<details style=\"color:rgb(128,128,128)\">\n<summary>\u6ce8\uff1a\u8bad\u7ec3\u987b\u77e5</summary>\n\n\u6240\u6709\u8bad\u7ec3\u8fc7\u7a0b\u9ed8\u8ba4\u6bcf\u9694100\u6b65\u4fdd\u5b581\u6b21\u53c2\u6570\u5230\u6587\u4ef6`./out/***.pth`\uff08\u6bcf\u6b21\u4f1a\u8986\u76d6\u6389\u65e7\u6743\u91cd\u6587\u4ef6\uff09\u3002\n\n\u7b80\u5355\u8d77\u89c1\uff0c\u6b64\u5904\u53ea\u5199\u660e\u4e24\u4e2a\u9636\u6bb5\u8bad\u7ec3\u8fc7\u7a0b\u3002\u5982\u9700\u5176\u5b83\u8bad\u7ec3 (LoRA, \u84b8\u998f, \u5f3a\u5316\u5b66\u4e60, \u5fae\u8c03\u63a8\u7406\u7b49) \u53ef\u53c2\u8003\u4e0b\u6587\u3010\u5b9e\u9a8c\u3011\u5c0f\u8282\u7684\u8be6\u7ec6\u8bf4\u660e\u3002\n\n</details>\n\n\n---\n\n### 4.\u6d4b\u8bd5\u6a21\u578b\u6548\u679c\n\n\u786e\u4fdd\u9700\u8981\u6d4b\u8bd5\u7684\u6a21\u578b`*.pth`\u6587\u4ef6\u4f4d\u4e8e`./out/`\u76ee\u5f55\u4e0b\u3002\n\u4e5f\u53ef\u4ee5\u76f4\u63a5\u53bb[\u6b64\u5904](https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch/files)\u4e0b\u8f7d\u4f7f\u7528\u6211\u8bad\u7ec3\u7684`*.pth`\u6587\u4ef6\u3002\n\n```bash\npython eval_model.py --model_mode 1 # \u9ed8\u8ba4\u4e3a0\uff1a\u6d4b\u8bd5pretrain\u6a21\u578b\u6548\u679c\uff0c\u8bbe\u7f6e\u4e3a1\uff1a\u6d4b\u8bd5full_sft\u6a21\u578b\u6548\u679c\n```\n\n<details style=\"color:rgb(128,128,128)\">\n<summary>\u6ce8\uff1a\u6d4b\u8bd5\u987b\u77e5</summary>\n\n\u5982\u9700\u8be6\u60c5\uff0c\u67e5\u770b`eval_model.py`\u811a\u672c\u4ee3\u7801\u5373\u53ef\u3002model_mode\u5206\u4e3a 0: \u9884\u8bad\u7ec3\u6a21\u578b\uff0c1: SFT-Chat\u6a21\u578b\uff0c2: RLHF-Chat\u6a21\u578b\uff0c3: Reason\u6a21\u578b\n\n</details>\n\n\n---\n\n> [!TIP]\n> \u6240\u6709\u8bad\u7ec3\u811a\u672c\u5747\u4e3aPytorch\u539f\u751f\u6846\u67b6\uff0c\u5747\u652f\u6301\u591a\u5361\u52a0\u901f\uff0c\u5047\u8bbe\u4f60\u7684\u8bbe\u5907\u6709N (N\uff1e1) \u5f20\u663e\u5361\uff1a\n\n\u5355\u673aN\u5361\u542f\u52a8\u8bad\u7ec3\u65b9\u5f0f (DDP, \u652f\u6301\u591a\u673a\u591a\u5361\u96c6\u7fa4)\n\n```bash\ntorchrun --nproc_per_node N train_xxx.py\n```\n\n<details style=\"color:rgb(128,128,128)\">\n<summary>\u6ce8\uff1a\u5176\u5b83\u987b\u77e5</summary>\n\n\u5355\u673aN\u5361\u542f\u52a8\u8bad\u7ec3 (DeepSpeed)\n\n```bash\ndeepspeed --master_port 29500 --num_gpus=N train_xxx.py\n```\n\n\u53ef\u6839\u636e\u9700\u8981\u5f00\u542fwandb\u8bb0\u5f55\u8bad\u7ec3\u8fc7\u7a0b\n\n```bash\n# \u9700\u8981\u767b\u5f55: wandb login\ntorchrun --nproc_per_node N train_xxx.py --use_wandb\n# and\npython train_xxx.py --use_wandb\n```\n\n\u901a\u8fc7\u6dfb\u52a0`--use_wandb`\u53c2\u6570\uff0c\u53ef\u4ee5\u8bb0\u5f55\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u8bad\u7ec3\u5b8c\u6210\u540e\uff0c\u53ef\u4ee5\u5728wandb\u7f51\u7ad9\u4e0a\u67e5\u770b\u8bad\u7ec3\u8fc7\u7a0b\u3002\u901a\u8fc7\u4fee\u6539`wandb_project`\n\u548c`wandb_run_name`\u53c2\u6570\uff0c\u53ef\u4ee5\u6307\u5b9a\u9879\u76ee\u540d\u79f0\u548c\u8fd0\u884c\u540d\u79f0\u3002\n\n</details>\n\n# \ud83d\udccc \u6570\u636e\u4ecb\u7ecd\n\n## \u2160 Tokenizer\n\n\u5206\u8bcd\u5668\u5c06\u5355\u8bcd\u4ece\u81ea\u7136\u8bed\u8a00\u901a\u8fc7\u201c\u8bcd\u5178\u201d\u6620\u5c04\u5230`0, 1, 36`\u8fd9\u6837\u7684\u6570\u5b57\uff0c\u53ef\u4ee5\u7406\u89e3\u4e3a\u6570\u5b57\u5c31\u4ee3\u8868\u4e86\u5355\u8bcd\u5728\u201c\u8bcd\u5178\u201d\u4e2d\u7684\u9875\u7801\u3002\n\u53ef\u4ee5\u9009\u62e9\u81ea\u5df1\u6784\u9020\u8bcd\u8868\u8bad\u7ec3\u4e00\u4e2a\u201c\u8bcd\u5178\u201d\uff0c\u4ee3\u7801\u53ef\u89c1`./scripts/train_tokenizer.py`\uff08\u4ec5\u4f9b\u5b66\u4e60\u53c2\u8003\uff0c\u82e5\u975e\u5fc5\u8981\u65e0\u9700\u518d\u81ea\u884c\u8bad\u7ec3\uff0cMiniMind\u5df2\u81ea\u5e26tokenizer\uff09\u3002\n\u6216\u8005\u9009\u62e9\u6bd4\u8f83\u51fa\u540d\u7684\u5f00\u6e90\u5927\u6a21\u578b\u5206\u8bcd\u5668\uff0c\n\u6b63\u5982\u540c\u76f4\u63a5\u7528\u65b0\u534e/\u725b\u6d25\u8bcd\u5178\u7684\u4f18\u70b9\u662ftoken\u7f16\u7801\u538b\u7f29\u7387\u5f88\u597d\uff0c\u7f3a\u70b9\u662f\u9875\u6570\u592a\u591a\uff0c\u52a8\u8f84\u6570\u5341\u4e07\u4e2a\u8bcd\u6c47\u77ed\u8bed\uff1b\n\u81ea\u5df1\u8bad\u7ec3\u7684\u5206\u8bcd\u5668\uff0c\u4f18\u70b9\u662f\u8bcd\u8868\u957f\u5ea6\u548c\u5185\u5bb9\u968f\u610f\u63a7\u5236\uff0c\u7f3a\u70b9\u662f\u538b\u7f29\u7387\u5f88\u4f4e\uff08\u4f8b\u5982\"hello\"\u4e5f\u8bb8\u4f1a\u88ab\u62c6\u5206\u4e3a\"h e l l o\"\n\u4e94\u4e2a\u72ec\u7acb\u7684token\uff09\uff0c\u4e14\u751f\u50fb\u8bcd\u96be\u4ee5\u8986\u76d6\u3002\n\u201c\u8bcd\u5178\u201d\u7684\u9009\u62e9\u56fa\u7136\u5f88\u91cd\u8981\uff0cLLM\u7684\u8f93\u51fa\u672c\u8d28\u4e0a\u662fSoftMax\u5230\u8bcd\u5178N\u4e2a\u8bcd\u7684\u591a\u5206\u7c7b\u95ee\u9898\uff0c\u7136\u540e\u901a\u8fc7\u201c\u8bcd\u5178\u201d\u89e3\u7801\u5230\u81ea\u7136\u8bed\u8a00\u3002\n\u56e0\u4e3aMiniMind\u4f53\u79ef\u9700\u8981\u4e25\u683c\u63a7\u5236\uff0c\u4e3a\u4e86\u907f\u514d\u6a21\u578b\u5934\u91cd\u811a\u8f7b\uff08\u8bcd\u5d4c\u5165embedding\u5c42\u53c2\u6570\u5728LLM\u5360\u6bd4\u592a\u9ad8\uff09\uff0c\u6240\u4ee5\u8bcd\u8868\u957f\u5ea6\u77ed\u77ed\u76ca\u5584\u3002\n\n<details style=\"color:rgb(128,128,128)\">\n<summary>Tokenizer\u4ecb\u7ecd</summary>\n\n\u7b2c\u4e09\u65b9\u5f3a\u5927\u7684\u5f00\u6e90\u6a21\u578b\u4f8b\u5982Yi\u3001qwen\u3001chatglm\u3001mistral\u3001Llama3\u7684tokenizer\u8bcd\u8868\u957f\u5ea6\u5982\u4e0b\uff1a\n\n<table>\n  <tr><th>Tokenizer\u6a21\u578b</th><th>\u8bcd\u8868\u5927\u5c0f</th><th>\u6765\u6e90</th></tr>\n  <tr><td>yi tokenizer</td><td>64,000</td><td>01\u4e07\u7269\uff08\u4e2d\u56fd\uff09</td></tr>\n  <tr><td>qwen2 tokenizer</td><td>151,643</td><td>\u963f\u91cc\u4e91\uff08\u4e2d\u56fd\uff09</td></tr>\n  <tr><td>glm tokenizer</td><td>151,329</td><td>\u667a\u8c31AI\uff08\u4e2d\u56fd\uff09</td></tr>\n  <tr><td>mistral tokenizer</td><td>32,000</td><td>Mistral AI\uff08\u6cd5\u56fd\uff09</td></tr>\n  <tr><td>llama3 tokenizer</td><td>128,000</td><td>Meta\uff08\u7f8e\u56fd\uff09</td></tr>\n  <tr><td>minimind tokenizer</td><td>6,400</td><td>\u81ea\u5b9a\u4e49</td></tr>\n</table>\n\n> \ud83d\udc492024-09-17\u66f4\u65b0\uff1a\u4e3a\u4e86\u9632\u6b62\u8fc7\u53bb\u7684\u7248\u672c\u6b67\u4e49&\u63a7\u5236\u4f53\u79ef\uff0cminimind\u6240\u6709\u6a21\u578b\u5747\u4f7f\u7528minimind_tokenizer\u5206\u8bcd\uff0c\u5e9f\u5f03\u6240\u6709mistral_tokenizer\u7248\u672c\u3002\n\n```\n# \u4e00\u4e9b\u81ea\u8a00\u81ea\u8bed\n> \u5c3d\u7ba1minimind_tokenizer\u957f\u5ea6\u5f88\u5c0f\uff0c\u7f16\u89e3\u7801\u6548\u7387\u5f31\u4e8eqwen2\u3001glm\u7b49\u4e2d\u6587\u53cb\u597d\u578b\u5206\u8bcd\u5668\u3002\n> \u4f46minimind\u6a21\u578b\u9009\u62e9\u4e86\u81ea\u5df1\u8bad\u7ec3\u7684minimind_tokenizer\u4f5c\u4e3a\u5206\u8bcd\u5668\uff0c\u4ee5\u4fdd\u6301\u6574\u4f53\u53c2\u6570\u8f7b\u91cf\uff0c\u907f\u514d\u7f16\u7801\u5c42\u548c\u8ba1\u7b97\u5c42\u5360\u6bd4\u5931\u8861\uff0c\u5934\u91cd\u811a\u8f7b\uff0c\u56e0\u4e3aminimind\u7684\u8bcd\u8868\u5927\u5c0f\u53ea\u67096400\u3002\n> \u4e14minimind\u5728\u5b9e\u9645\u6d4b\u8bd5\u4e2d\u6ca1\u6709\u51fa\u73b0\u8fc7\u751f\u50fb\u8bcd\u6c47\u89e3\u7801\u5931\u8d25\u7684\u60c5\u51b5\uff0c\u6548\u679c\u826f\u597d\u3002\n> \u7531\u4e8e\u81ea\u5b9a\u4e49\u8bcd\u8868\u538b\u7f29\u957f\u5ea6\u52306400\uff0c\u4f7f\u5f97LLM\u603b\u53c2\u6570\u91cf\u6700\u4f4e\u53ea\u670925.8M\u3002\n> \u8bad\u7ec3\u6570\u636e`tokenizer_train.jsonl`\u5747\u6765\u81ea\u4e8e`\u5320\u6570\u5927\u6a21\u578b\u6570\u636e\u96c6`\uff0c\u8fd9\u90e8\u5206\u6570\u636e\u76f8\u5bf9\u6b21\u8981\uff0c\u5982\u9700\u8bad\u7ec3\u53ef\u4ee5\u81ea\u7531\u9009\u62e9\u3002\n```\n\n</details>\n\n## \u2161 Pretrain\u6570\u636e\n\n\u7ecf\u5386\u4e86MiniMind-V1\u7684\u4f4e\u8d28\u91cf\u9884\u8bad\u7ec3\u6570\u636e\uff0c\u5bfc\u81f4\u6a21\u578b\u80e1\u8a00\u4e71\u8bed\u7684\u6559\u8bad\uff0c`2025-02-05` \u4e4b\u540e\u51b3\u5b9a\u4e0d\u518d\u91c7\u7528\u5927\u89c4\u6a21\u65e0\u76d1\u7763\u7684\u6570\u636e\u96c6\u505a\u9884\u8bad\u7ec3\u3002\n\u8fdb\u800c\u5c1d\u8bd5\u628a[\u5320\u6570\u5927\u6a21\u578b\u6570\u636e\u96c6](https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data)\u7684\u4e2d\u6587\u90e8\u5206\u63d0\u53d6\u51fa\u6765\uff0c\n\u6e05\u6d17\u51fa\u5b57\u7b26`<512`\u957f\u5ea6\u7684\u5927\u7ea61.6GB\u7684\u8bed\u6599\u76f4\u63a5\u62fc\u63a5\u6210\u9884\u8bad\u7ec3\u6570\u636e `pretrain_hq.jsonl`\uff0chq\u5373\u4e3ahigh\nquality\uff08\u5f53\u7136\u4e5f\u8fd8\u4e0d\u7b97high\uff0c\u63d0\u5347\u6570\u636e\u8d28\u91cf\u65e0\u6b62\u5c3d\uff09\u3002\n\n\u6587\u4ef6`pretrain_hq.jsonl` \u6570\u636e\u683c\u5f0f\u4e3a\n\n```bash\n{\"text\": \"\u5982\u4f55\u624d\u80fd\u6446\u8131\u62d6\u5ef6\u75c7\uff1f \u6cbb\u6108\u62d6\u5ef6\u75c7\u5e76\u4e0d\u5bb9\u6613\uff0c\u4f46\u4ee5\u4e0b\u5efa\u8bae\u53ef\u80fd\u6709\u6240\u5e2e\u52a9...\"}\n```\n\n## \u2162 SFT\u6570\u636e\n\n[\u5320\u6570\u5927\u6a21\u578bSFT\u6570\u636e\u96c6](https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data)\n\u201c\u662f\u4e00\u4e2a\u5b8c\u6574\u3001\u683c\u5f0f\u7edf\u4e00\u3001\u5b89\u5168\u7684\u5927\u6a21\u578b\u8bad\u7ec3\u548c\u7814\u7a76\u8d44\u6e90\u3002\n\u4ece\u7f51\u7edc\u4e0a\u7684\u516c\u5f00\u6570\u636e\u6e90\u6536\u96c6\u5e76\u6574\u7406\u4e86\u5927\u91cf\u5f00\u6e90\u6570\u636e\u96c6\uff0c\u5bf9\u5176\u8fdb\u884c\u4e86\u683c\u5f0f\u7edf\u4e00\uff0c\u6570\u636e\u6e05\u6d17\uff0c\n\u5305\u542b10M\u6761\u6570\u636e\u7684\u4e2d\u6587\u6570\u636e\u96c6\u548c\u5305\u542b2M\u6761\u6570\u636e\u7684\u82f1\u6587\u6570\u636e\u96c6\u3002\u201d\n\u4ee5\u4e0a\u662f\u5b98\u65b9\u4ecb\u7ecd\uff0c\u4e0b\u8f7d\u6587\u4ef6\u540e\u7684\u6570\u636e\u603b\u91cf\u5927\u7ea6\u57284B tokens\uff0c\u80af\u5b9a\u662f\u9002\u5408\u4f5c\u4e3a\u4e2d\u6587\u5927\u8bed\u8a00\u6a21\u578b\u7684SFT\u6570\u636e\u7684\u3002\n\u4f46\u662f\u5b98\u65b9\u63d0\u4f9b\u7684\u6570\u636e\u683c\u5f0f\u5f88\u4e71\uff0c\u5168\u90e8\u7528\u6765sft\u4ee3\u4ef7\u592a\u5927\u3002\n\u6211\u5c06\u628a\u5b98\u65b9\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u4e8c\u6b21\u6e05\u6d17\uff0c\u628a\u542b\u6709\u7b26\u53f7\u6c61\u67d3\u548c\u566a\u58f0\u7684\u6761\u76ee\u53bb\u9664\uff1b\u53e6\u5916\u4f9d\u7136\u53ea\u4fdd\u7559\u4e86\u603b\u957f\u5ea6`<512`\n\u7684\u5185\u5bb9\uff0c\u6b64\u9636\u6bb5\u5e0c\u671b\u901a\u8fc7\u5927\u91cf\u5bf9\u8bdd\u8865\u5145\u9884\u8bad\u7ec3\u9636\u6bb5\u6b20\u7f3a\u7684\u77e5\u8bc6\u3002\n\u5bfc\u51fa\u6587\u4ef6\u4e3a`sft_512.jsonl`(~7.5GB)\u3002\n\n[Magpie-SFT\u6570\u636e\u96c6](https://www.modelscope.cn/organization/Magpie-Align)\n\u6536\u96c6\u4e86~1M\u6761\u6765\u81eaQwen2/2.5\u7684\u9ad8\u8d28\u91cf\u5bf9\u8bdd\uff0c\u6211\u5c06\u8fd9\u90e8\u5206\u6570\u636e\u8fdb\u4e00\u6b65\u6e05\u6d17\uff0c\u628a\u603b\u957f\u5ea6`<2048`\u7684\u90e8\u5206\u5bfc\u51fa\u4e3a`sft_2048.jsonl`(~9GB)\u3002\n\u957f\u5ea6`<1024`\u7684\u90e8\u5206\u5bfc\u51fa\u4e3a`sft_1024.jsonl`(~5.5GB)\uff0c\u7528\u5927\u6a21\u578b\u5bf9\u8bdd\u6570\u636e\u76f4\u63a5\u8fdb\u884csft\u5c31\u5c5e\u4e8e\u201c\u9ed1\u76d2\u84b8\u998f\u201d\u7684\u8303\u7574\u3002\n\n\u8fdb\u4e00\u6b65\u6e05\u6d17\u524d\u4e24\u6b65sft\u7684\u6570\u636e\uff08\u53ea\u4fdd\u7559\u4e2d\u6587\u5b57\u7b26\u5360\u6bd4\u9ad8\u7684\u5185\u5bb9\uff09\uff0c\u7b5b\u9009\u957f\u5ea6`<512`\u7684\u5bf9\u8bdd\uff0c\u5f97\u5230`sft_mini_512.jsonl`(~1.2GB)\u3002\n\n\u6240\u6709sft\u6587\u4ef6 `sft_X.jsonl` \u6570\u636e\u683c\u5f0f\u5747\u4e3a\n\n```text\n{\n    \"conversations\": [\n        {\"role\": \"user\", \"content\": \"\u4f60\u597d\"},\n        {\"role\": \"assistant\", \"content\": \"\u4f60\u597d\uff01\"},\n        {\"role\": \"user\", \"content\": \"\u518d\u89c1\"},\n        {\"role\": \"assistant\", \"content\": \"\u518d\u89c1\uff01\"}\n    ]\n}\n```\n\n## \u2163 RLHF\u6570\u636e\n\n\u6765\u81ea[Magpie-DPO\u6570\u636e\u96c6](https://www.modelscope.cn/datasets/Magpie-Align/MagpieLM-DPO-Data-v0.1)\n\u5927\u7ea6200k\u6761\u504f\u597d\u6570\u636e\uff08\u5747\u662f\u82f1\u6587\uff09\u751f\u6210\u81eaLlama3.1-70B/8B\uff0c\u53ef\u4ee5\u7528\u4e8e\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\uff0c\u4f18\u5316\u6a21\u578b\u56de\u590d\u8d28\u91cf\uff0c\u4f7f\u5176\u66f4\u52a0\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u3002\n\u8fd9\u91cc\u5c06\u6570\u636e\u603b\u957f\u5ea6`<3000`\u7684\u5185\u5bb9\u91cd\u7ec4\u4e3a`dpo.jsonl`(~0.9GB)\uff0c\u5305\u542b`chosen`\u548c`rejected`\u4e24\u4e2a\u5b57\u6bb5\uff0c`chosen`\n\u4e3a\u504f\u597d\u7684\u56de\u590d\uff0c`rejected`\u4e3a\u62d2\u7edd\u7684\u56de\u590d\u3002\n\n\u6587\u4ef6 `dpo.jsonl` \u6570\u636e\u683c\u5f0f\u4e3a\n\n```text\n{\n  \"chosen\": [\n    {\"content\": \"Q\", \"role\": \"user\"}, \n    {\"content\": \"good answer\", \"role\": \"assistant\"}\n  ], \n  \"rejected\": [\n    {\"content\": \"Q\", \"role\": \"user\"}, \n    {\"content\": \"bad answer\", \"role\": \"assistant\"}\n  ]\n}\n```\n\n## \u2164 Reason\u6570\u636e\u96c6\uff1a\n\n\u4e0d\u5f97\u4e0d\u8bf42025\u5e742\u6708\u8c01\u80fd\u706b\u7684\u8fc7DeepSeek...\n\u4e5f\u6fc0\u53d1\u4e86\u6211\u5bf9RL\u5f15\u5bfc\u7684\u63a8\u7406\u6a21\u578b\u7684\u6d53\u539a\u5174\u8da3\uff0c\u76ee\u524d\u5df2\u7ecf\u7528Qwen2.5\u590d\u73b0\u4e86R1-Zero\u3002\n\u5982\u679c\u6709\u65f6\u95f4+\u6548\u679cwork\uff08\u4f4699%\u57fa\u6a21\u80fd\u529b\u4e0d\u8db3\uff09\u6211\u4f1a\u5728\u4e4b\u540e\u66f4\u65b0MiniMind\u57fa\u4e8eRL\u8bad\u7ec3\u7684\u63a8\u7406\u6a21\u578b\u800c\u4e0d\u662f\u84b8\u998f\u6a21\u578b\u3002\n\u65f6\u95f4\u6709\u9650\uff0c\u6700\u5feb\u7684\u4f4e\u6210\u672c\u65b9\u6848\u4f9d\u7136\u662f\u76f4\u63a5\u84b8\u998f\uff08\u9ed1\u76d2\u65b9\u5f0f\uff09\u3002\n\u8010\u4e0d\u4f4fR1\u592a\u706b\uff0c\u77ed\u77ed\u51e0\u5929\u5c31\u5df2\u7ecf\u5b58\u5728\u4e00\u4e9bR1\u7684\u84b8\u998f\u6570\u636e\u96c6[R1-Llama-70B](https://www.modelscope.cn/datasets/Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B)\u3001[R1-Distill-SFT](https://www.modelscope.cn/datasets/AI-ModelScope/R1-Distill-SFT)\u3001\n[Alpaca-Distill-R1](https://huggingface.co/datasets/shareAI/Alpaca-Distill-R1-ZH)\u3001\n[deepseek_r1_zh](https://huggingface.co/datasets/jinliuxi/deepseek_r1_zh)\u7b49\u7b49\uff0c\u7eaf\u4e2d\u6587\u7684\u6570\u636e\u53ef\u80fd\u6bd4\u8f83\u5c11\u3002\n\u6700\u7ec8\u6574\u5408\u5b83\u4eec\uff0c\u5bfc\u51fa\u6587\u4ef6\u4e3a`r1_mix_1024.jsonl`\uff0c\u6570\u636e\u683c\u5f0f\u548c`sft_X.jsonl`\u4e00\u81f4\u3002\n\n## \u2165 \u66f4\u591a\u6570\u636e\u96c6\n\n\u76ee\u524d\u5df2\u7ecf\u6709[HqWu-HITCS/Awesome-Chinese-LLM](https://github.com/HqWu-HITCS/Awesome-Chinese-LLM)\n\u5728\u6536\u96c6\u548c\u68b3\u7406\u4e2d\u6587LLM\u76f8\u5173\u7684\u5f00\u6e90\u6a21\u578b\u3001\u5e94\u7528\u3001\u6570\u636e\u96c6\u53ca\u6559\u7a0b\u7b49\u8d44\u6599\uff0c\u5e76\u6301\u7eed\u66f4\u65b0\u8fd9\u65b9\u9762\u7684\u6700\u65b0\u8fdb\u5c55\u3002\u5168\u9762\u4e14\u4e13\u4e1a\uff0cRespect\uff01\n\n---\n\n## \u2167 MiniMind\u8bad\u7ec3\u6570\u636e\u96c6\n\n> [!NOTE]\n> 2025-02-05\u540e\uff0c\u5f00\u6e90MiniMind\u6700\u7ec8\u8bad\u7ec3\u6240\u7528\u7684\u6240\u6709\u6570\u636e\u96c6\uff0c\u56e0\u6b64\u65e0\u9700\u518d\u81ea\u884c\u9884\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u907f\u514d\u91cd\u590d\u6027\u7684\u6570\u636e\u5904\u7406\u5de5\u4f5c\u3002\n\nMiniMind\u8bad\u7ec3\u6570\u636e\u96c6\u4e0b\u8f7d\u5730\u5740\uff1a [ModelScope](https://www.modelscope.cn/datasets/gongjy/minimind_dataset/files) | [HuggingFace](https://huggingface.co/datasets/jingyaogong/minimind_dataset/tree/main)\n\n> \u65e0\u9700\u5168\u90e8clone\uff0c\u53ef\u5355\u72ec\u4e0b\u8f7d\u6240\u9700\u7684\u6587\u4ef6\n\n\u5c06\u4e0b\u8f7d\u7684\u6570\u636e\u96c6\u6587\u4ef6\u653e\u5230`./dataset/`\u76ee\u5f55\u4e0b\uff08\u2728\u4e3a\u63a8\u8350\u7684\u5fc5\u987b\u9879\uff09\n\n```bash\n./dataset/\n\u251c\u2500\u2500 dpo.jsonl (909MB)\n\u251c\u2500\u2500 lora_identity.jsonl (22.8KB)\n\u251c\u2500\u2500 lora_medical.jsonl (34MB)\n\u251c\u2500\u2500 pretrain_hq.jsonl (1.6GB, \u2728)\n\u251c\u2500\u2500 r1_mix_1024.jsonl (340MB)\n\u251c\u2500\u2500 sft_1024.jsonl (5.6GB)\n\u251c\u2500\u2500 sft_2048.jsonl (9GB)\n\u251c\u2500\u2500 sft_512.jsonl (7.5GB)\n\u251c\u2500\u2500 sft_mini_512.jsonl (1.2GB, \u2728)\n\u2514\u2500\u2500 tokenizer_train.jsonl (1GB)\n```\n\n<details style=\"color:rgb(128,128,128)\">\n<summary>\u6ce8\uff1a\u5404\u6570\u636e\u96c6\u7b80\u4ecb</summary>\n\n* `dpo.jsonl` --RLHF\u9636\u6bb5\u6570\u636e\u96c6\n* `lora_identity.jsonl` --\u81ea\u6211\u8ba4\u77e5\u6570\u636e\u96c6\uff08\u4f8b\u5982\uff1a\u4f60\u662f\u8c01\uff1f\u6211\u662fminimind...\uff09\uff0c\u63a8\u8350\u7528\u4e8elora\u8bad\u7ec3\uff08\u4ea6\u53ef\u7528\u4e8e\u5168\u53c2SFT\uff0c\u52ff\u88ab\u540d\u5b57\u5c40\u9650\uff09\n* `lora_medical.jsonl` --\u533b\u7597\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u63a8\u8350\u7528\u4e8elora\u8bad\u7ec3\uff08\u4ea6\u53ef\u7528\u4e8e\u5168\u53c2SFT\uff0c\u52ff\u88ab\u540d\u5b57\u5c40\u9650\uff09\n* `pretrain_hq.jsonl`\u2728 --\u9884\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u6574\u5408\u81eajiangshu\u79d1\u6280\n* `r1_mix_1024.jsonl` --DeepSeek-R1-1.5B\u84b8\u998f\u6570\u636e\uff0c\u6bcf\u6761\u6570\u636e\u5b57\u7b26\u6700\u5927\u957f\u5ea6\u4e3a1024\uff08\u56e0\u6b64\u8bad\u7ec3\u65f6\u8bbe\u7f6emax_seq_len=1024\uff09\n* `sft_1024.jsonl` --\u6574\u5408\u81eaQwen2.5\u84b8\u998f\u6570\u636e\uff08\u662fsft_2048\u7684\u5b50\u96c6\uff09\uff0c\u6bcf\u6761\u6570\u636e\u5b57\u7b26\u6700\u5927\u957f\u5ea6\u4e3a1024\uff08\u56e0\u6b64\u8bad\u7ec3\u65f6\u8bbe\u7f6emax_seq_len=1024\uff09\n* `sft_2048.jsonl` --\u6574\u5408\u81eaQwen2.5\u84b8\u998f\u6570\u636e\uff0c\u6bcf\u6761\u6570\u636e\u5b57\u7b26\u6700\u5927\u957f\u5ea6\u4e3a2048\uff08\u56e0\u6b64\u8bad\u7ec3\u65f6\u8bbe\u7f6emax_seq_len=2048\uff09\n* `sft_512.jsonl` --\u6574\u5408\u81ea\u5320\u6570\u79d1\u6280SFT\u6570\u636e\uff0c\u6bcf\u6761\u6570\u636e\u5b57\u7b26\u6700\u5927\u957f\u5ea6\u4e3a512\uff08\u56e0\u6b64\u8bad\u7ec3\u65f6\u8bbe\u7f6emax_seq_len=512\uff09\n* `sft_mini_512.jsonl`\u2728 --\u6781\u7b80\u6574\u5408\u81ea\u5320\u6570\u79d1\u6280SFT\u6570\u636e+Qwen2.5\u84b8\u998f\u6570\u636e\uff08\u7528\u4e8e\u5feb\u901f\u8bad\u7ec3Zero\u6a21\u578b\uff09\uff0c\u6bcf\u6761\u6570\u636e\u5b57\u7b26\u6700\u5927\u957f\u5ea6\u4e3a512\uff08\u56e0\u6b64\u8bad\u7ec3\u65f6\u8bbe\u7f6emax_seq_len=512\uff09\n* `tokenizer_train.jsonl` --\u5747\u6765\u81ea\u4e8e`\u5320\u6570\u5927\u6a21\u578b\u6570\u636e\u96c6`\uff0c\u8fd9\u90e8\u5206\u6570\u636e\u76f8\u5bf9\u6b21\u8981\uff0c\uff08\u4e0d\u63a8\u8350\u81ea\u5df1\u91cd\u590d\u8bad\u7ec3tokenizer\uff0c\u7406\u7531\u5982\u4e0a\uff09\u5982\u9700\u81ea\u5df1\u8bad\u7ec3tokenizer\u53ef\u4ee5\u81ea\u7531\u9009\u62e9\u6570\u636e\u96c6\u3002\n\n</details>\n\n\n![dataset](./images/dataset.jpg)\n\n<details style=\"color:rgb(128,128,128)\">\n<summary>\u8bf4\u660e & \u63a8\u8350\u8bad\u7ec3\u65b9\u6848</summary>\n\n* MiniMind2 Series\u5747\u7ecf\u8fc7\u5171\u7ea620GB\u8bed\u6599\u8bad\u7ec3\uff0c\u5927\u7ea64B tokens\uff0c\u5373\u5bf9\u5e94\u4e0a\u9762\u7684\u6570\u636e\u7ec4\u5408\u8bad\u7ec3\u7ed3\u679c\uff08\u5f00\u9500\uff1a\ud83d\udcb0\ud83d\udcb0\ud83d\udcb0\ud83d\udcb0\ud83d\udcb0\ud83d\udcb0\ud83d\udcb0\ud83d\udcb0\uff0c\u6548\u679c\uff1a\ud83d\ude0a\ud83d\ude0a\ud83d\ude0a\ud83d\ude0a\ud83d\ude0a\ud83d\ude0a\uff09\n\n* \u60f3\u8981\u6700\u5feb\u901f\u5ea6\u4ece0\u5b9e\u73b0Zero\u6a21\u578b\uff0c\u63a8\u8350\u4f7f\u7528`pretrain_hq.jsonl` + `sft_mini_512.jsonl` \u7684\u6570\u636e\u7ec4\u5408\uff0c\u5177\u4f53\u82b1\u9500\u548c\u6548\u679c\u53ef\u67e5\u770b\u4e0b\u6587\u8868\u683c\uff08\u5f00\u9500\uff1a\ud83d\udcb0\uff0c\u6548\u679c\uff1a\ud83d\ude0a\ud83d\ude0a\uff09\n\n* \u63a8\u8350\u5177\u5907\u4e00\u5b9a\u7b97\u529b\u8d44\u6e90\u6216\u66f4\u5728\u610f\u6548\u679c\u7684\u670b\u53cb\u53ef\u4ee5\u8003\u8651\u524d\u8005\u5b8c\u6574\u590d\u73b0MiniMind2\uff1b\u4ec5\u6709\u5355\u5361GPU\u6216\u5728\u4e4e\u77ed\u65f6\u95f4\u5feb\u901f\u590d\u73b0\u7684\u670b\u53cb\u5f3a\u70c8\u63a8\u8350\u540e\u8005\uff1b\n\n* \u3010\u6298\u4e2d\u65b9\u6848\u3011\u4ea6\u53ef\u9009\u62e9\u4f8b\u5982`sft_mini_512.jsonl`\u3001`sft_1024.jsonl`\u4e2d\u7b49\u89c4\u6a21\u6570\u636e\u8fdb\u884c\u81ea\u7531\u7ec4\u5408\u8bad\u7ec3\uff08\u5f00\u9500\uff1a\ud83d\udcb0\ud83d\udcb0\ud83d\udcb0\uff0c\u6548\u679c\uff1a\ud83d\ude0a\ud83d\ude0a\ud83d\ude0a\ud83d\ude0a\uff09\u3002\n\n</details>\n\n# \ud83d\udccc Model Structure\n\nMiniMind-Dense\uff08\u548c[Llama3.1](https://ai.meta.com/blog/meta-llama-3-1/)\u4e00\u6837\uff09\u4f7f\u7528\u4e86Transformer\u7684Decoder-Only\u7ed3\u6784\uff0c\u8ddfGPT-3\u7684\u533a\u522b\u5728\u4e8e\uff1a\n\n* \u91c7\u7528\u4e86GPT-3\u7684\u9884\u6807\u51c6\u5316\u65b9\u6cd5\uff0c\u4e5f\u5c31\u662f\u5728\u6bcf\u4e2aTransformer\u5b50\u5c42\u7684\u8f93\u5165\u4e0a\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u800c\u4e0d\u662f\u5728\u8f93\u51fa\u4e0a\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u4f7f\u7528\u7684\u662fRMSNorm\u5f52\u4e00\u5316\u51fd\u6570\u3002\n* \u7528SwiGLU\u6fc0\u6d3b\u51fd\u6570\u66ff\u4ee3\u4e86ReLU\uff0c\u8fd9\u6837\u505a\u662f\u4e3a\u4e86\u63d0\u9ad8\u6027\u80fd\u3002\n* \u50cfGPT-Neo\u4e00\u6837\uff0c\u53bb\u6389\u4e86\u7edd\u5bf9\u4f4d\u7f6e\u5d4c\u5165\uff0c\u6539\u7528\u4e86\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\uff08RoPE\uff09\uff0c\u8fd9\u6837\u5728\u5904\u7406\u8d85\u51fa\u8bad\u7ec3\u957f\u5ea6\u7684\u63a8\u7406\u65f6\u6548\u679c\u66f4\u597d\u3002\n\n---\n\nMiniMind-MoE\u6a21\u578b\uff0c\u5b83\u7684\u7ed3\u6784\u57fa\u4e8eLlama3\u548c[Deepseek-V2/3](https://arxiv.org/pdf/2405.04434)\u4e2d\u7684MixFFN\u6df7\u5408\u4e13\u5bb6\u6a21\u5757\u3002\n\n* DeepSeek-V2\u5728\u524d\u9988\u7f51\u7edc\uff08FFN\uff09\u65b9\u9762\uff0c\u91c7\u7528\u4e86\u66f4\u7ec6\u7c92\u5ea6\u7684\u4e13\u5bb6\u5206\u5272\u548c\u5171\u4eab\u7684\u4e13\u5bb6\u9694\u79bb\u6280\u672f\uff0c\u4ee5\u63d0\u9ad8Experts\u7684\u6548\u679c\u3002\n\n---\n\nMiniMind\u7684\u6574\u4f53\u7ed3\u6784\u4e00\u81f4\uff0c\u53ea\u662f\u5728RoPE\u8ba1\u7b97\u3001\u63a8\u7406\u51fd\u6570\u548cFFN\u5c42\u7684\u4ee3\u7801\u4e0a\u505a\u4e86\u4e00\u4e9b\u5c0f\u8c03\u6574\u3002\n\u5176\u7ed3\u6784\u5982\u4e0b\u56fe\uff08\u91cd\u7ed8\u7248\uff09\uff1a\n\n![structure](./images/LLM-structure.png)\n![structure-moe](./images/LLM-structure-moe.png)\n\n\u4fee\u6539\u6a21\u578b\u914d\u7f6e\u89c1[./model/LMConfig.py](./model/LMConfig.py)\u3002\n\u53c2\u8003\u6a21\u578b\u53c2\u6570\u7248\u672c\u89c1\u4e0b\u8868\uff1a\n\n| Model Name        | params | len_vocab | rope_theta | n_layers | d_model | kv_heads | q_heads | share+route |\n|-------------------|--------|-----------|------------|----------|---------|----------|---------|-------------|\n| MiniMind2-Small   | 26M    | 6400      | 1e6        | 8        | 512     | 2        | 8       | -           |\n| MiniMind2-MoE     | 145M   | 6400      | 1e6        | 8        | 640     | 2        | 8       | 1+4         |\n| MiniMind2         | 104M   | 6400      | 1e6        | 16       | 768     | 2        | 8       | -           |\n| minimind-v1-small | 26M    | 6400      | 1e4        | 8        | 512     | 8        | 16      | -           |\n| minimind-v1-moe   | 4\u00d726M  | 6400      | 1e4        | 8        | 512     | 8        | 16      | 1+4         |\n| minimind-v1       | 108M   | 6400      | 1e4        | 16       | 768     | 8        | 16      | -           |\n\n# \ud83d\udccc Experiment\n\n## \u2160 \u8bad\u7ec3\u5f00\u9500\n\n- **\u65f6\u95f4\u5355\u4f4d**\uff1a\u5c0f\u65f6 (h)\u3002\n- **\u6210\u672c\u5355\u4f4d**\uff1a\u4eba\u6c11\u5e01 (\uffe5)\uff1b7\uffe5 \u2248 1\u7f8e\u5143\u3002\n- **3090 \u79df\u5361\u5355\u4ef7**\uff1a\u22481.3\uffe5/h\uff08\u53ef\u81ea\u884c\u53c2\u8003\u5b9e\u65f6\u5e02\u4ef7\uff09\u3002\n- **\u53c2\u8003\u6807\u51c6**\uff1a\u8868\u683c\u4ec5\u5b9e\u6d4b `pretrain` \u548c `sft_mini_512` \u4e24\u4e2a\u6570\u636e\u96c6\u7684\u8bad\u7ec3\u65f6\u95f4\uff0c\u5176\u5b83\u8017\u65f6\u6839\u636e\u6570\u636e\u96c6\u5927\u5c0f\u4f30\u7b97\uff08\u53ef\u80fd\u5b58\u5728\u4e9b\u8bb8\u51fa\u5165\uff09\u3002\n\n> \u57fa\u4e8e 3090 \uff08\u5355\u5361\uff09\u6210\u672c\u8ba1\u7b97\n\n| Model Name      | params | pretrain         | sft_mini_512     | sft_512       | sft_1024          | sft_2048         | RLHF          |\n|-----------------|--------|------------------|------------------|---------------|-------------------|------------------|---------------|\n| MiniMind2-Small | 26M    | \u22481.1h<br/>\u22481.43\uffe5 | \u22481h<br/>\u22481.3\uffe5    | \u22486h<br/>\u22487.8\uffe5 | \u22484.58h<br/>\u22485.95\uffe5 | \u22487.5h<br/>\u22489.75\uffe5 | \u22481h<br/>\u22481.3\uffe5 |\n| MiniMind2       | 104M   | \u22483.9h<br/>\u22485.07\uffe5 | \u22483.3h<br/>\u22484.29\uffe5 | \u224820h<br/>\u224826\uffe5 | \u224815h<br/>\u224819.5\uffe5   | \u224825h<br/>\u224832.5\uffe5  | \u22483h<br/>\u22483.9\uffe5 |\n\n---\n\n<details style=\"color:rgb(128,128,128)\">\n<summary>\u8bad\u7ec3\u5f00\u9500\u603b\u7ed3&\u9884\u6d4b</summary>\n\n\n> MiniMind2-Small\u53c2\u6570\n>> `pretrain_hq`+`sft_mini_512`\u6570\u636e\u96c6\n<br/>\u5355\u53613090 (1 epoch) + 2.1\u5c0f\u65f6 + \u82b1\u8d392.73\u5143\u4eba\u6c11\u5e01\n<br/>\u5373\u53ef\u4ece0\u8bad\u7ec3\u51faMiniMind-Zero-0.025B\u6a21\u578b!!!\n\n> MiniMind2-Small\u53c2\u6570\n>> `pretrain_hq`+`sft_512`+`sft_2048`+`dpo`\u6570\u636e\u96c6\n<br/>\u5355\u53613090 (2 epochs) + \u5927\u7ea638.16\u5c0f\u65f6 + \u82b1\u8d3949.61\u5143\u4eba\u6c11\u5e01\n<br/>\u5373\u53ef\u4ece0\u8bad\u7ec3\u51faMiniMind2-Small-0.025B\u6a21\u578b!!!\n\n> MiniMind2\u53c2\u6570\n>> `pretrain_hq`+`sft_512`+`sft_2048`+`dpo`\u6570\u636e\u96c6\n<br/>\u5355\u53613090 (2 epochs) + \u5927\u7ea6122\u5c0f\u65f6 + \u82b1\u8d39158.6\u5143\u4eba\u6c11\u5e01\n<br/>\u5373\u53ef\u4ece0\u8bad\u7ec3\u51faMiniMind2-0.1B\u6a21\u578b!!!\n\n</details>\n\n\n\n\u2728\u57fa\u4e8e\u5355\u5361NVIDIA 3090\u7684`MiniMind-Zero`\u4ece0\u8bad\u7ec3\u4ec5\u9700`2\u5c0f\u65f6` + `3\u5757\u94b1`\uff0c\u5b9e\u73b0ChatBot\u6548\u679c\uff01\n\n\u2728PS\uff1a\u82e5\u91c7\u75288\u53614090\u8bad\u7ec3\uff0c\u603b\u7528\u65f6\u751a\u81f3\u53ef\u4ee5\u538b\u7f29\u523010\u5206\u949f\u4ee5\u5185\uff01\uff08\u7531\u4e8e\u65f6\u95f4\u66f4\u77ed\uff0c\u82b1\u8d39\u540c\u68373\u5143\u5de6\u53f3\uff0c\u4e0e\u5355\u5361\u6210\u672c\u76f8\u5f53\uff09\n\n\u2728\u4ee5\u6781\u4f4e\u6781\u4f4e\u7684\u95e8\u69db\uff0c\u5b9e\u73b0\u4eba\u4eba\u53ef\u73a9\u7684\u5927\u6a21\u578b\u81ea\u7531\uff01\u8fd9\u6b63\u662fMiniMind\u7cfb\u5217\u7684\u8bde\u751f\u521d\u8877\uff01\n\n\u2728\u4ec5\u4ef7\u503c`3\u5757\u94b1`\u6210\u672c\u7684`MiniMind-Zero`\u5e76\u4e0d\u662f\u5671\u5934\uff01Chat\u6d4b\u8bd5\uff1a\n\n```text\n\ud83d\udc76: \u8bf7\u4ecb\u7ecd\u4e00\u4e0b\u81ea\u5df1\u3002\n\ud83e\udd16\ufe0f: \u4f5c\u4e3a\u4eba\u5de5\u667a\u80fd\uff0c\u6211\u6ca1\u6709\u5b9e\u9645\u7684\u751f\u6d3b\uff0c\u4e5f\u6ca1\u6709\u81ea\u6211\u610f\u8bc6\uff0c\u6240\u4ee5\u6ca1\u6709\u81ea\u5df1\u7684\u751f\u6d3b\u3002\u6211\u88ab\u8bbe\u8ba1\u6210\u80fd\u591f\u5e2e\u52a9\u7528\u6237\u89e3\u7b54\u95ee\u9898\u3001\u63d0\u4f9b\u4fe1\u606f\u3001\u8fdb\u884c\u5bf9\u8bdd\u7b49\u3002\u6211\u7684\u8bbe\u8ba1\u548c\u529f\u80fd\u662f\u7531\u8ba1\u7b97\u673a\u79d1\u5b66\u3001\u673a\u5668\u5b66\u4e60\u3001\u4eba\u5de5\u667a\u80fd\u6280\u672f\u548c\u7b97\u6cd5\u6240\u6784\u5efa\u7684\uff0c\u6211\u901a\u8fc7\u7f16\u7a0b\u548c\u7b97\u6cd5\u5b9e\u73b0\u3002\n\n\ud83d\udc76: \u63a8\u8350\u4e00\u4e9b\u676d\u5dde\u7684\u7279\u8272\u7f8e\u98df\u5427\u3002\n\ud83e\udd16\ufe0f: \u676d\u5dde\u6709\u5f88\u591a\u7f8e\u5473\u7684\u7279\u8272\u7f8e\u98df\uff0c\u6bd4\u5982\u9e21\u8089\u4e32\u3001\u9cd7\u9c7c\u5934\u3001\u6c34\u716e\u9c7c\u3001\u8c46\u8150\u8111\u7b49\u3002\u8fd9\u4e9b\u7f8e\u98df\u90fd\u975e\u5e38\u7f8e\u5473\uff0c\u6709\u7740\u72ec\u7279\u7684\u98ce\u5473\u548c\u53e3\u611f\uff0c\u7edd\u5bf9\u662f\u503c\u5f97\u4e00\u8bd5\u7684\u7f8e\u98df\u3002\n\n\ud83d\udc76: \u8bf7\u4e3a\u6211\u8bb2\u89e3\u201c\u5927\u8bed\u8a00\u6a21\u578b\u201d\u8fd9\u4e2a\u6982\u5ff5\u3002\n\ud83e\udd16\ufe0f: \u5927\u8bed\u8a00\u6a21\u578b\u662f\u4e00\u79cd\u4eba\u5de5\u667a\u80fd\u6280\u672f\uff0c\u5b83\u4f7f\u7528\u5927\u91cf\u7684\u6587\u672c\u6570\u636e\u6765\u8bad\u7ec3\u6a21\u578b\uff0c\u7136\u540e\u5728\u4e0d\u65ad\u7684\u8fed\u4ee3\u4e2d\u4e0d\u65ad\u4f18\u5316\u6a21\u578b\u3002\u5b83\u7684\u76ee\u6807\u662f\u6839\u636e\u8f93\u5165\u7684\u6570\u636e\u6765\u751f\u6210\u7b26\u5408\u7279\u5b9a\u9700\u6c42\u7684\u6587\u672c\uff0c\u8fd9\u4e9b\u6587\u672c\u53ef\u4ee5\u662f\u4efb\u4f55\u4f60\u611f\u5174\u8da3\u7684\u4e3b\u9898\u3002\n\n\ud83d\udc76: Introduce the history of the United States, please.\n\ud83e\udd16\ufe0f: \u60a8\u63d0\u5230\u7684\u201cIntrook's the believeations of theument.\" \u8fd9\u4e2a\u540d\u5b57\u6765\u6e90\u4e8e\u4e2d\u56fd\u53e4\u4ee3\u7684\"groty of of the change.\"\n```\n\n\u6781\u901f\u4e14\u521d\u5177\u6548\u679c\uff0c\u751a\u81f3\u4ecd\u7136\u53ef\u4ee5\u8fdb\u4e00\u6b65\u538b\u7f29\u83b7\u53d6\u66f4\u5c0f\u66f4\u4f18\u8d28\u7684\u8bad\u7ec3\u6570\u636e\u3002\nZero\u6a21\u578b\u6743\u91cd\u4fdd\u5b58\u4e3a `full_sft_512_zero.pth`\uff08\u89c1\u4e0b\u6587MiniMind\u6a21\u578b\u6587\u4ef6\u94fe\u63a5\uff09\uff0c\u5982\u6709\u5174\u8da3\u53ef\u4e0b\u8f7d\u68c0\u9a8c\u6b64\u6a21\u578b\u6548\u679c\u3002\n\n\n---\n\n## \u2161 \u4e3b\u8981\u8bad\u7ec3\u6b65\u9aa4\n\n> \u6240\u6709\u8bad\u7ec3\u811a\u672c\u5747 `cd ./trainer` \u76ee\u5f55\u6267\u884c\n\n### **1. \u9884\u8bad\u7ec3(Pretrain)**:\n\nLLM\u9996\u5148\u8981\u5b66\u4e60\u7684\u5e76\u975e\u76f4\u63a5\u4e0e\u4eba\u4ea4\u6d41\uff0c\u800c\u662f\u8ba9\u7f51\u7edc\u53c2\u6570\u4e2d\u5145\u6ee1\u77e5\u8bc6\u7684\u58a8\u6c34\uff0c\u201c\u58a8\u6c34\u201d \u7406\u8bba\u4e0a\u559d\u7684\u8d8a\u9971\u8d8a\u597d\uff0c\u4ea7\u751f\u5927\u91cf\u7684\u5bf9\u4e16\u754c\u7684\u77e5\u8bc6\u79ef\u7d2f\u3002\n\u9884\u8bad\u7ec3\u5c31\u662f\u8ba9Model\u5148\u57cb\u5934\u82e6\u5b66\u5927\u91cf\u57fa\u672c\u7684\u77e5\u8bc6\uff0c\u4f8b\u5982\u4eceWiki\u767e\u79d1\u3001\u65b0\u95fb\u3001\u4e66\u7c4d\u6574\u7406\u5927\u89c4\u6a21\u7684\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u3002\n\u8fd9\u4e2a\u8fc7\u7a0b\u662f\u201c\u65e0\u76d1\u7763\u201d\u7684\uff0c\u5373\u4eba\u7c7b\u4e0d\u9700\u8981\u5728\u8fc7\u7a0b\u4e2d\u505a\u4efb\u4f55\u201c\u6709\u76d1\u7763\u201d\u7684\u6821\u6b63\uff0c\u800c\u662f\u7531\u6a21\u578b\u81ea\u5df1\u4ece\u5927\u91cf\u6587\u672c\u4e2d\u603b\u7ed3\u89c4\u5f8b\u5b66\u4e60\u77e5\u8bc6\u70b9\u3002\n\u6a21\u578b\u6b64\u9636\u6bb5\u76ee\u7684\u53ea\u6709\u4e00\u4e2a\uff1a**\u5b66\u4f1a\u8bcd\u8bed\u63a5\u9f99**\u3002\u4f8b\u5982\u6211\u4eec\u8f93\u5165\u201c\u79e6\u59cb\u7687\u201d\u56db\u4e2a\u5b57\uff0c\u5b83\u53ef\u4ee5\u63a5\u9f99\u201c\u662f\u4e2d\u56fd\u7684\u7b2c\u4e00\u4f4d\u7687\u5e1d\u201d\u3002\n\n```bash\ntorchrun --nproc_per_node 1 train_pretrain.py # 1\u5373\u4e3a\u5355\u5361\u8bad\u7ec3\uff0c\u53ef\u6839\u636e\u786c\u4ef6\u60c5\u51b5\u81ea\u884c\u8c03\u6574 (\u8bbe\u7f6e>=2)\n# or\npython train_pretrain.py\n```\n\n> \u8bad\u7ec3\u540e\u7684\u6a21\u578b\u6743\u91cd\u6587\u4ef6\u9ed8\u8ba4\u6bcf\u9694`100\u6b65`\u4fdd\u5b58\u4e3a: `pretrain_*.pth`\uff08*\n> \u4e3a\u6a21\u578b\u5177\u4f53dimension\uff0c\u6bcf\u6b21\u4fdd\u5b58\u65f6\u65b0\u6587\u4ef6\u4f1a\u8986\u76d6\u65e7\u6587\u4ef6\uff09\n\n### **2. \u6709\u76d1\u7763\u5fae\u8c03(Supervised Fine-Tuning)**:\n\n\u7ecf\u8fc7\u9884\u8bad\u7ec3\uff0cLLM\u6b64\u65f6\u5df2\u7ecf\u638c\u63e1\u4e86\u5927\u91cf\u77e5\u8bc6\uff0c\u7136\u800c\u6b64\u65f6\u5b83\u53ea\u4f1a\u65e0\u8111\u5730\u8bcd\u8bed\u63a5\u9f99\uff0c\u8fd8\u4e0d\u4f1a\u4e0e\u4eba\u804a\u5929\u3002\nSFT\u9636\u6bb5\u5c31\u9700\u8981\u628a\u534a\u6210\u54c1LLM\u65bd\u52a0\u4e00\u4e2a\u81ea\u5b9a\u4e49\u7684\u804a\u5929\u6a21\u677f\u8fdb\u884c\u5fae\u8c03\u3002\n\u4f8b\u5982\u6a21\u578b\u9047\u5230\u8fd9\u6837\u7684\u6a21\u677f\u3010\u95ee\u9898->\u56de\u7b54\uff0c\u95ee\u9898->\u56de\u7b54\u3011\u540e\u4e0d\u518d\u65e0\u8111\u63a5\u9f99\uff0c\u800c\u662f\u610f\u8bc6\u5230\u8fd9\u662f\u4e00\u6bb5\u5b8c\u6574\u7684\u5bf9\u8bdd\u7ed3\u675f\u3002\n\u79f0\u8fd9\u4e2a\u8fc7\u7a0b\u4e3a\u6307\u4ee4\u5fae\u8c03\uff0c\u5c31\u5982\u540c\u8ba9\u5df2\u7ecf\u5b66\u5bcc\u4e94\u8f66\u7684\u300c\u725b\u987f\u300d\u5148\u751f\u9002\u5e9421\u4e16\u7eaa\u667a\u80fd\u624b\u673a\u7684\u804a\u5929\u4e60\u60ef\uff0c\u5b66\u4e60\u5c4f\u5e55\u5de6\u4fa7\u662f\u5bf9\u65b9\u6d88\u606f\uff0c\u53f3\u4fa7\u662f\u672c\u4eba\u6d88\u606f\u8fd9\u4e2a\u89c4\u5f8b\u3002\n\u5728\u8bad\u7ec3\u65f6\uff0cMiniMind\u7684\u6307\u4ee4\u548c\u56de\u7b54\u957f\u5ea6\u88ab\u622a\u65ad\u5728512\uff0c\u662f\u4e3a\u4e86\u8282\u7701\u663e\u5b58\u7a7a\u95f4\u3002\u5c31\u50cf\u6211\u4eec\u5b66\u4e60\u65f6\uff0c\u4f1a\u5148\u4ece\u77ed\u7684\u6587\u7ae0\u5f00\u59cb\uff0c\u5f53\u5b66\u4f1a\u5199\u4f5c200\u5b57\u4f5c\u6587\u540e\uff0c800\u5b57\u6587\u7ae0\u4e5f\u53ef\u4ee5\u624b\u5230\u64d2\u6765\u3002\n\u5728\u9700\u8981\u957f\u5ea6\u62d3\u5c55\u65f6\uff0c\u53ea\u9700\u8981\u51c6\u5907\u5c11\u91cf\u76842k/4k/8k\u957f\u5ea6\u5bf9\u8bdd\u6570\u636e\u8fdb\u884c\u8fdb\u4e00\u6b65\u5fae\u8c03\u5373\u53ef\uff08\u6b64\u65f6\u6700\u597d\u914d\u5408RoPE-NTK\u7684\u57fa\u51c6\u5dee\u503c\uff09\u3002\n> \u5728\u63a8\u7406\u65f6\u901a\u8fc7\u8c03\u6574RoPE\u7ebf\u6027\u5dee\u503c\uff0c\u5b9e\u73b0\u514d\u8bad\u7ec3\u957f\u5ea6\u5916\u63a8\u52302048\u53ca\u4ee5\u4e0a\u5c06\u4f1a\u5f88\u65b9\u4fbf\u3002\n\n```bash\ntorchrun --nproc_per_node 1 train_full_sft.py\n# or\npython train_full_sft.py\n```\n\n> \u8bad\u7ec3\u540e\u7684\u6a21\u578b\u6743\u91cd\u6587\u4ef6\u9ed8\u8ba4\u6bcf\u9694`100\u6b65`\u4fdd\u5b58\u4e3a: `full_sft_*.pth`\uff08*\n> \u4e3a\u6a21\u578b\u5177\u4f53dimension\uff0c\u6bcf\u6b21\u4fdd\u5b58\u65f6\u65b0\u6587\u4ef6\u4f1a\u8986\u76d6\u65e7\u6587\u4ef6\uff09\n\n## \u2162 \u5176\u5b83\u8bad\u7ec3\u6b65\u9aa4\n\n> \u6240\u6709\u8bad\u7ec3\u811a\u672c\u5747 `cd ./trainer` \u76ee\u5f55\u6267\u884c\n\n### **3. \u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60(Reinforcement Learning from Human Feedback, RLHF)**\n\n\u5728\u524d\u9762\u7684\u8bad\u7ec3\u6b65\u9aa4\u4e2d\uff0c\u6a21\u578b\u5df2\u7ecf\u5177\u5907\u4e86\u57fa\u672c\u7684\u5bf9\u8bdd\u80fd\u529b\uff0c\u4f46\u662f\u8fd9\u6837\u7684\u80fd\u529b\u5b8c\u5168\u57fa\u4e8e\u5355\u8bcd\u63a5\u9f99\uff0c\u7f3a\u5c11\u6b63\u53cd\u6837\u4f8b\u7684\u6fc0\u52b1\u3002\n\u6a21\u578b\u6b64\u65f6\u5c1a\u672a\u77e5\u4ec0\u4e48\u56de\u7b54\u662f\u597d\u7684\uff0c\u4ec0\u4e48\u662f\u5dee\u7684\u3002\u6211\u4eec\u5e0c\u671b\u5b83\u80fd\u591f\u66f4\u7b26\u5408\u4eba\u7684\u504f\u597d\uff0c\u964d\u4f4e\u8ba9\u4eba\u7c7b\u4e0d\u6ee1\u610f\u7b54\u6848\u7684\u4ea7\u751f\u6982\u7387\u3002\n\u8fd9\u4e2a\u8fc7\u7a0b\u5c31\u50cf\u662f\u8ba9\u6a21\u578b\u53c2\u52a0\u65b0\u7684\u57f9\u8bad\uff0c\u4ece\u4f18\u79c0\u5458\u5de5\u7684\u4f5c\u4e3a\u4f8b\u5b50\uff0c\u6d88\u6781\u5458\u5de5\u4f5c\u4e3a\u53cd\u4f8b\uff0c\u5b66\u4e60\u5982\u4f55\u66f4\u597d\u5730\u56de\u590d\u3002\n\u6b64\u5904\u4f7f\u7528\u7684\u662fRLHF\u7cfb\u5217\u4e4b-\u76f4\u63a5\u504f\u597d\u4f18\u5316(Direct Preference Optimization, DPO)\u3002\n\u4e0ePPO(Proximal Policy Optimization)\u8fd9\u79cd\u9700\u8981\u5956\u52b1\u6a21\u578b\u3001\u4ef7\u503c\u6a21\u578b\u7684RL\u7b97\u6cd5\u4e0d\u540c\uff1b\nDPO\u901a\u8fc7\u63a8\u5bfcPPO\u5956\u52b1\u6a21\u578b\u7684\u663e\u5f0f\u89e3\uff0c\u628a\u5728\u7ebf\u5956\u52b1\u6a21\u578b\u6362\u6210\u79bb\u7ebf\u6570\u636e\uff0cRef\u6a21\u578b\u8f93\u51fa\u53ef\u4ee5\u63d0\u524d\u4fdd\u5b58\u3002\nDPO\u6027\u80fd\u51e0\u4e4e\u4e0d\u53d8\uff0c\u53ea\u7528\u8dd1 actor_model \u548c ref_model \u4e24\u4e2a\u6a21\u578b\uff0c\u5927\u5927\u8282\u7701\u663e\u5b58\u5f00\u9500\u548c\u589e\u52a0\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002\n\n> \u6ce8\uff1aRLHF\u8bad\u7ec3\u6b65\u9aa4**\u5e76\u975e\u5fc5\u987b**\uff0c\u6b64\u6b65\u9aa4\u96be\u4ee5\u63d0\u5347\u6a21\u578b\u201c\u667a\u529b\u201d\u800c\u901a\u5e38\u4ec5\u7528\u4e8e\u63d0\u5347\u6a21\u578b\u7684\u201c\u793c\u8c8c\u201d\uff0c\u6709\u5229\uff08\u7b26\u5408\u504f\u597d\u3001\u51cf\u5c11\u6709\u5bb3\u5185\u5bb9\uff09\u4e5f\u6709\u5f0a\uff08\u6837\u672c\u6536\u96c6\u6602\u8d35\u3001\u53cd\u9988\u504f\u5dee\u3001\u591a\u6837\u6027\u635f\u5931\uff09\u3002\n\n```bash\ntorchrun --nproc_per_node 1 train_dpo.py\n# or\npython train_dpo.py\n```\n\n> \u8bad\u7ec3\u540e\u7684\u6a21\u578b\u6743\u91cd\u6587\u4ef6\u9ed8\u8ba4\u6bcf\u9694`100\u6b65`\u4fdd\u5b58\u4e3a: `rlhf_*.pth`\uff08*\n> \u4e3a\u6a21\u578b\u5177\u4f53dimension\uff0c\u6bcf\u6b21\u4fdd\u5b58\u65f6\u65b0\u6587\u4ef6\u4f1a\u8986\u76d6\u65e7\u6587\u4ef6\uff09\n\n### **4. \u77e5\u8bc6\u84b8\u998f(Knowledge Distillation, KD)**\n\n\u5728\u524d\u9762\u7684\u6240\u6709\u8bad\u7ec3\u6b65\u9aa4\u4e2d\uff0c\u6a21\u578b\u5df2\u7ecf\u5b8c\u5168\u5177\u5907\u4e86\u57fa\u672c\u80fd\u529b\uff0c\u901a\u5e38\u53ef\u4ee5\u5b66\u6210\u51fa\u5e08\u4e86\u3002\n\u800c\u77e5\u8bc6\u84b8\u998f\u53ef\u4ee5\u8fdb\u4e00\u6b65\u4f18\u5316\u6a21\u578b\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u6240\u8c13\u77e5\u8bc6\u84b8\u998f\uff0c\u5373\u5b66\u751f\u6a21\u578b\u9762\u5411\u6559\u5e08\u6a21\u578b\u5b66\u4e60\u3002\n\u6559\u5e08\u6a21\u578b\u901a\u5e38\u662f\u7ecf\u8fc7\u5145\u5206\u8bad\u7ec3\u7684\u5927\u6a21\u578b\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\n\u5b66\u751f\u6a21\u578b\u662f\u4e00\u4e2a\u8f83\u5c0f\u7684\u6a21\u578b\uff0c\u76ee\u6807\u662f\u5b66\u4e60\u6559\u5e08\u6a21\u578b\u7684\u884c\u4e3a\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u4ece\u539f\u59cb\u6570\u636e\u4e2d\u5b66\u4e60\u3002\n\u5728SFT\u5b66\u4e60\u4e2d\uff0c\u6a21\u578b\u7684\u76ee\u6807\u662f\u62df\u5408\u8bcdToken\u5206\u7c7b\u786c\u6807\u7b7e\uff08hard labels\uff09\uff0c\u5373\u771f\u5b9e\u7684\u7c7b\u522b\u6807\u7b7e\uff08\u5982 0 \u6216 6400\uff09\u3002\n\u5728\u77e5\u8bc6\u84b8\u998f\u4e2d\uff0c\u6559\u5e08\u6a21\u578b\u7684softmax\u6982\u7387\u5206\u5e03\u88ab\u7528\u4f5c\u8f6f\u6807\u7b7e\uff08soft labels\uff09\u3002\u5c0f\u6a21\u578b\u4ec5\u5b66\u4e60\u8f6f\u6807\u7b7e\uff0c\u5e76\u4f7f\u7528KL-Loss\u6765\u4f18\u5316\u6a21\u578b\u7684\u53c2\u6570\u3002\n\u901a\u4fd7\u5730\u8bf4\uff0cSFT\u76f4\u63a5\u5b66\u4e60\u8001\u5e08\u7ed9\u7684\u89e3\u9898\u7b54\u6848\u3002\u800cKD\u8fc7\u7a0b\u76f8\u5f53\u4e8e\u201c\u6253\u5f00\u201d\u8001\u5e08\u806a\u660e\u7684\u5927\u8111\uff0c\u5c3d\u53ef\u80fd\u5730\u6a21\u4eff\u8001\u5e08\u201c\u5927\u8111\u201d\u601d\u8003\u95ee\u9898\u7684\u795e\u7ecf\u5143\u72b6\u6001\u3002\n\u4f8b\u5982\uff0c\u5f53\u8001\u5e08\u6a21\u578b\u8ba1\u7b97`1+1=2`\u8fd9\u4e2a\u95ee\u9898\u7684\u65f6\u5019\uff0c\u6700\u540e\u4e00\u5c42\u795e\u7ecf\u5143a\u72b6\u6001\u4e3a0\uff0c\u795e\u7ecf\u5143b\u72b6\u6001\u4e3a100\uff0c\u795e\u7ecf\u5143c\u72b6\u6001\u4e3a-99...\n\u5b66\u751f\u6a21\u578b\u901a\u8fc7\u5927\u91cf\u6570\u636e\uff0c\u5b66\u4e60\u6559\u5e08\u6a21\u578b\u5927\u8111\u5185\u90e8\u7684\u8fd0\u8f6c\u89c4\u5f8b\u3002\u8fd9\u4e2a\u8fc7\u7a0b\u5373\u79f0\u4e4b\u4e3a\uff1a\u77e5\u8bc6\u84b8\u998f\u3002\n\u77e5\u8bc6\u84b8\u998f\u7684\u76ee\u7684\u53ea\u6709\u4e00\u4e2a\uff1a\u8ba9\u5c0f\u6a21\u578b\u4f53\u79ef\u66f4\u5c0f\u7684\u540c\u65f6\u6548\u679c\u66f4\u597d\u3002\n\u7136\u800c\u968f\u7740LLM\u8bde\u751f\u548c\u53d1\u5c55\uff0c\u6a21\u578b\u84b8\u998f\u4e00\u8bcd\u88ab\u5e7f\u6cdb\u6ee5\u7528\uff0c\u4ece\u800c\u4ea7\u751f\u4e86\u201c\u767d\u76d2/\u9ed1\u76d2\u201d\u77e5\u8bc6\u84b8\u998f\u4e24\u4e2a\u6d3e\u522b\u3002\nGPT-4\u8fd9\u79cd\u95ed\u6e90\u6a21\u578b\uff0c\u7531\u4e8e\u65e0\u6cd5\u83b7\u53d6\u5176\u5185\u90e8\u7ed3\u6784\uff0c\u56e0\u6b64\u53ea\u80fd\u9762\u5411\u5b83\u6240\u8f93\u51fa\u7684\u6570\u636e\u5b66\u4e60\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u79f0\u4e4b\u4e3a\u9ed1\u76d2\u84b8\u998f\uff0c\u4e5f\u662f\u5927\u6a21\u578b\u65f6\u4ee3\u6700\u666e\u904d\u7684\u505a\u6cd5\u3002\n\u9ed1\u76d2\u84b8\u998f\u4e0eSFT\u8fc7\u7a0b\u5b8c\u5168\u4e00\u81f4\uff0c\u53ea\u4e0d\u8fc7\u6570\u636e\u662f\u4ece\u5927\u6a21\u578b\u7684\u8f93\u51fa\u6536\u96c6\uff0c\u56e0\u6b64\u53ea\u9700\u8981\u51c6\u5907\u6570\u636e\u5e76\u4e14\u8fdb\u4e00\u6b65FT\u5373\u53ef\u3002\n\u6ce8\u610f\u66f4\u6539\u88ab\u52a0\u8f7d\u7684\u57fa\u7840\u6a21\u578b\u4e3a`full_sft_*.pth`\uff0c\u5373\u57fa\u4e8e\u5fae\u8c03\u6a21\u578b\u505a\u8fdb\u4e00\u6b65\u7684\u84b8\u998f\u5b66\u4e60\u3002\n`./dataset/sft_1024.jsonl`\u4e0e`./dataset/sft_2048.jsonl` \u5747\u6536\u96c6\u81eaqwen2.5-7/72B-Instruct\u5927\u6a21\u578b\uff0c\u53ef\u76f4\u63a5\u7528\u4e8eSFT\u4ee5\u83b7\u53d6Qwen\u7684\u90e8\u5206\u884c\u4e3a\u3002\n\n```bash\n# \u6ce8\u610f\u9700\u8981\u66f4\u6539train_full_sft.py\u6570\u636e\u96c6\u8def\u5f84\uff0c\u4ee5\u53camax_seq_len  \ntorchrun --nproc_per_node 1 train_full_sft.py\n# or\npython train_full_sft.py\n```\n\n> \u8bad\u7ec3\u540e\u7684\u6a21\u578b\u6743\u91cd\u6587\u4ef6\u9ed8\u8ba4\u6bcf\u9694`100\u6b65`\u540c\u6837\u4fdd\u5b58\u4e3a: `full_sft_*.pth`\uff08*\u4e3a\u6a21\u578b\u5177\u4f53dimension\uff0c\u6bcf\u6b21\u4fdd\u5b58\u65f6\u65b0\u6587\u4ef6\u4f1a\u8986\u76d6\u65e7\u6587\u4ef6\uff09\n\n\u6b64\u5904\u5e94\u5f53\u7740\u91cd\u4ecb\u7ecdMiniMind\u5b9e\u73b0\u7684\u767d\u76d2\u84b8\u998f\u4ee3\u7801`train_distillation.py`\uff0c\u7531\u4e8eMiniMind\u540c\u7cfb\u5217\u672c\u8eab\u5e76\u4e0d\u5b58\u5728\u5f3a\u5927\u7684\u6559\u5e08\u6a21\u578b\uff0c\u56e0\u6b64\u767d\u76d2\u84b8\u998f\u4ee3\u7801\u4ec5\u4f5c\u4e3a\u5b66\u4e60\u53c2\u8003\u3002\n\n```bash\ntorchrun --nproc_per_node 1 train_distillation.py\n# or\npython train_distillation.py\n```\n\n### **5. LoRA (Low-Rank Adaptation)**\n\nLoRA\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08Parameter-Efficient Fine-Tuning, PEFT\uff09\u65b9\u6cd5\uff0c\u65e8\u5728\u901a\u8fc7\u4f4e\u79e9\u5206\u89e3\u7684\u65b9\u5f0f\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u3002\n\u76f8\u6bd4\u4e8e\u5168\u53c2\u6570\u5fae\u8c03\uff08Full Fine-Tuning\uff09\uff0cLoRA \u53ea\u9700\u8981\u66f4\u65b0\u5c11\u91cf\u7684\u53c2\u6570\u3002\nLoRA \u7684\u6838\u5fc3\u601d\u60f3\u662f\uff1a\u5728\u6a21\u578b\u7684\u6743\u91cd\u77e9\u9635\u4e2d\u5f15\u5165\u4f4e\u79e9\u5206\u89e3\uff0c\u4ec5\u5bf9\u4f4e\u79e9\u90e8\u5206\u8fdb\u884c\u66f4\u65b0\uff0c\u800c\u4fdd\u6301\u539f\u59cb\u9884\u8bad\u7ec3\u6743\u91cd\u4e0d\u53d8\u3002\n\u4ee3\u7801\u53ef\u89c1`./model/model_lora.py`\u548c`train_lora.py`\uff0c\u5b8c\u5168\u4ece0\u5b9e\u73b0LoRA\u6d41\u7a0b\uff0c\u4e0d\u4f9d\u8d56\u7b2c\u4e09\u65b9\u5e93\u7684\u5c01\u88c5\u3002\n\n```bash\ntorchrun --nproc_per_node 1 train_lora.py\n# or\npython train_lora.py\n```\n\n> \u8bad\u7ec3\u540e\u7684\u6a21\u578b\u6743\u91cd\u6587\u4ef6\u9ed8\u8ba4\u6bcf\u9694`100\u6b65`\u4fdd\u5b58\u4e3a: `lora_xxx_*.pth`\uff08*\n> \u4e3a\u6a21\u578b\u5177\u4f53dimension\uff0c\u6bcf\u6b21\u4fdd\u5b58\u65f6\u65b0\u6587\u4ef6\u4f1a\u8986\u76d6\u65e7\u6587\u4ef6\uff09\n\n\n\u975e\u5e38\u591a\u7684\u4eba\u56f0\u60d1\uff0c\u5982\u4f55\u4f7f\u6a21\u578b\u5b66\u4f1a\u81ea\u5df1\u79c1\u6709\u9886\u57df\u7684\u77e5\u8bc6\uff1f\u5982\u4f55\u51c6\u5907\u6570\u636e\u96c6\uff1f\u5982\u4f55\u8fc1\u79fb\u901a\u7528\u9886\u57df\u6a21\u578b\u6253\u9020\u5782\u57df\u6a21\u578b\uff1f\n\u8fd9\u91cc\u4e3e\u51e0\u4e2a\u4f8b\u5b50\uff0c\u5bf9\u4e8e\u901a\u7528\u6a21\u578b\uff0c\u533b\u5b66\u9886\u57df\u77e5\u8bc6\u6b20\u7f3a\uff0c\u53ef\u4ee5\u5c1d\u8bd5\u5728\u539f\u6709\u6a21\u578b\u57fa\u7840\u4e0a\u52a0\u5165\u9886\u57df\u77e5\u8bc6\uff0c\u4ee5\u83b7\u5f97\u66f4\u597d\u7684\u6027\u80fd\u3002\n\u540c\u65f6\uff0c\u6211\u4eec\u901a\u5e38\u4e0d\u5e0c\u671b\u5b66\u4f1a\u9886\u57df\u77e5\u8bc6\u7684\u540c\u65f6\u635f\u5931\u539f\u6709\u57fa\u7840\u6a21\u578b\u7684\u5176\u5b83\u80fd\u529b\uff0c\u6b64\u65f6LoRA\u53ef\u4ee5\u5f88\u597d\u7684\u6539\u5584\u8fd9\u4e2a\u95ee\u9898\u3002\n\u53ea\u9700\u8981\u51c6\u5907\u5982\u4e0b\u683c\u5f0f\u7684\u5bf9\u8bdd\u6570\u636e\u96c6\u653e\u7f6e\u5230`./dataset/lora_xxx.jsonl`\uff0c\u542f\u52a8 `python train_lora.py`\n\u8bad\u7ec3\u5373\u53ef\u5f97\u5230`./out/lora/lora_xxx.pth`\u65b0\u6a21\u578b\u6743\u91cd\u3002\n\n**\u533b\u7597\u573a\u666f**\n\n```\n {\"conversations\": [{\"role\": \"user\", \"content\": \"\u8bf7\u95ee\u9888\u690e\u75c5\u7684\u4eba\u6795\u5934\u591a\u9ad8\u624d\u6700\u597d\uff1f\"}, {\"role\": \"assistant\", \"content\": \"\u9888\u690e\u75c5\u60a3\u8005\u9009\u62e9\u6795\u5934\u7684\u9ad8\u5ea6\u5e94\u8be5\u6839\u636e...\"}]}\n {\"conversations\": [{\"role\": \"user\", \"content\": \"\u8bf7\u95eexxx\"}, {\"role\": \"assistant\", \"content\": \"xxx...\"}]}\n```\n\n**\u81ea\u6211\u8ba4\u77e5\u573a\u666f**\n\n```\n {\"conversations\": [{\"role\": \"user\", \"content\": \"\u4f60\u53eb\u4ec0\u4e48\u540d\u5b57\uff1f\"}, {\"role\": \"assistant\", \"content\": \"\u6211\u53ebminimind...\"}]}\n {\"conversations\": [{\"role\": \"user\", \"content\": \"\u4f60\u662f\u8c01\"}, {\"role\": \"assistant\", \"content\": \"\u6211\u662f...\"}]}\n```\n\n\u6b64\u65f6\u3010\u57fa\u7840\u6a21\u578b+LoRA\u6a21\u578b\u3011\u5373\u53ef\u83b7\u5f97\u533b\u7597\u573a\u666f\u6a21\u578b\u589e\u5f3a\u7684\u80fd\u529b\uff0c\u76f8\u5f53\u4e8e\u4e3a\u57fa\u7840\u6a21\u578b\u589e\u52a0\u4e86LoRA\u5916\u6302\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u5e76\u4e0d\u635f\u5931\u57fa\u7840\u6a21\u578b\u7684\u672c\u8eab\u80fd\u529b\u3002\n\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7`eval_model.py`\u8fdb\u884c\u6a21\u578b\u8bc4\u4f30\u6d4b\u8bd5\u3002\n\n```bash\n# \u6ce8\u610f\uff1amodel_mode\u5373\u9009\u62e9\u57fa\u7840\u6a21\u578b\u7684\u7c7b\u578b\uff0c\u8fd9\u548ctrain_lora\u662f\u57fa\u4e8e\u54ea\u4e2a\u6a21\u578b\u8bad\u7ec3\u7684\u76f8\u5173\uff0c\u786e\u4fdd\u7edf\u4e00\u5373\u53ef\u3002\npython eval_model.py --lora_name 'lora_medical' --model_mode 2\n```\n\n**\u5c0f\u6d4b\u8bd5**\n\n```\n\ud83d\udc76: \u6211\u6700\u8fd1\u7ecf\u5e38\u611f\u5230\u5934\u6655\uff0c\u53ef\u80fd\u662f\u4ec0\u4e48\u539f\u56e0\uff1f\n\ud83e\udd16\ufe0f: \u5934\u6655\u53ef\u80fd\u662f\u7531\u4e8e\u591a\u79cd\u539f\u56e0\uff0c\u5305\u62ec\u4f46\u4e0d\u9650\u4e8e\uff1a\u5fc3\u810f\u75be\u75c5\u3001\u773c\u775b\u635f\u4f24\u3001\u8fc7\u654f\u53cd\u5e94\u3001\u76ae\u80a4\u95ee\u9898\u3001\u8fc7\u654f\u53cd\u5e94\u3001\u538b\u529b\u3001\u7126\u8651\u3001\u75b2\u52b3\u3001\u836f\u7269\u526f\u4f5c\u7528\u3001\u6027\u529f\u80fd\u969c\u788d\u6216\u8005\u5176\u4ed6...\n\n---\n\n\ud83d\udc76: \u4f60\u548copenai\u662f\u4ec0\u4e48\u5173\u7cfb\uff1f\n\ud83e\udd16\ufe0f: \u6211\u662f MiniMind\uff0c\u4e00\u4e2a\u7531 Jingyao Gong \u5f00\u53d1\u7684\u4eba\u5de5\u667a\u80fd\u52a9\u624b\u3002\u6211\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u7b97\u6cd5\u8bad\u7ec3\u6765\u4e0e\u7528\u6237\u8fdb\u884c\u4ea4\u4e92\u3002\n```\n\nPS\uff1a\u53ea\u8981\u6709\u6240\u9700\u8981\u7684\u6570\u636e\u96c6\uff0c\u4e5f\u53ef\u4ee5full_sft\u5168\u53c2\u5fae\u8c03\uff08\u9700\u8981\u8fdb\u884c\u901a\u7528\u77e5\u8bc6\u7684\u6df7\u5408\u914d\u6bd4\uff0c\u5426\u5219\u8fc7\u62df\u5408\u9886\u57df\u6570\u636e\u4f1a\u8ba9\u6a21\u578b\u53d8\u50bb\uff0c\u635f\u5931\u901a\u7528\u6027\uff09\n\n### **6. \u8bad\u7ec3\u63a8\u7406\u6a21\u578b (Reasoning Model)**\n\nDeepSeek-R1\u5b9e\u5728\u592a\u706b\u4e86\uff0c\u51e0\u4e4e\u91cd\u65b0\u6307\u660e\u4e86\u672a\u6765LLM\u7684\u65b0\u8303\u5f0f\u3002\n\u8bba\u6587\u6307\u51fa`>3B`\u7684\u6a21\u578b\u7ecf\u5386\u591a\u6b21\u53cd\u590d\u7684\u51b7\u542f\u52a8\u548cRL\u5956\u52b1\u8bad\u7ec3\u624d\u80fd\u83b7\u5f97\u8089\u773c\u53ef\u89c1\u7684\u63a8\u7406\u80fd\u529b\u63d0\u5347\u3002\n\u6700\u5feb\u6700\u7a33\u59a5\u6700\u7ecf\u6d4e\u7684\u505a\u6cd5\uff0c\u4ee5\u53ca\u6700\u8fd1\u7206\u53d1\u7684\u5404\u79cd\u5404\u6837\u6240\u8c13\u7684\u63a8\u7406\u6a21\u578b\u51e0\u4e4e\u90fd\u662f\u76f4\u63a5\u9762\u5411\u6570\u636e\u8fdb\u884c\u84b8\u998f\u8bad\u7ec3\uff0c\n\u4f46\u7531\u4e8e\u7f3a\u4e4f\u6280\u672f\u542b\u91cf\uff0c\u84b8\u998f\u6d3e\u88abRL\u6d3e\u77a7\u4e0d\u8d77\uff08hhhh\uff09\u3002\n\u672c\u4eba\u8fc5\u901f\u5df2\u7ecf\u5728Qwen\u7cfb\u52171.5B\u5c0f\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u5c1d\u8bd5\uff0c\u5f88\u5feb\u590d\u73b0\u4e86Zero\u8fc7\u7a0b\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002\n\u7136\u800c\u4e00\u4e2a\u9057\u61be\u7684\u5171\u8bc6\u662f\uff1a\u53c2\u6570\u592a\u5c0f\u7684\u6a21\u578b\u76f4\u63a5\u901a\u8fc7\u51b7\u542f\u52a8SFT+GRPO\u51e0\u4e4e\u4e0d\u53ef\u80fd\u83b7\u5f97\u4efb\u4f55\u63a8\u7406\u6548\u679c\u3002\nMiniMind2\u7b2c\u4e00\u65f6\u95f4\u53ea\u80fd\u575a\u5b9a\u4e0d\u79fb\u7684\u9009\u62e9\u505a\u84b8\u998f\u6d3e\uff0c\u65e5\u540e\u57fa\u4e8e0.1B\u6a21\u578b\u7684RL\u5982\u679c\u540c\u6837\u53d6\u5f97\u5c0f\u5c0f\u8fdb\u5c55\u4f1a\u66f4\u65b0\u6b64\u90e8\u5206\u7684\u8bad\u7ec3\u65b9\u6848\u3002\n\n\u505a\u84b8\u998f\u9700\u8981\u51c6\u5907\u7684\u4f9d\u7136\u662f\u548cSFT\u9636\u6bb5\u540c\u6837\u683c\u5f0f\u7684\u6570\u636e\u5373\u53ef\uff0c\u6570\u636e\u96c6\u6765\u6e90\u5df2\u5982\u4e0a\u6587\u4ecb\u7ecd\u3002\u6570\u636e\u683c\u5f0f\u4f8b\u5982\uff1a\n\n```json lines\n{\n  \"conversations\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"\u4f60\u597d\uff0c\u6211\u662f\u5c0f\u82b3\uff0c\u5f88\u9ad8\u5174\u8ba4\u8bc6\u4f60\u3002\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"<think>\\n\u4f60\u597d\uff01\u6211\u662f\u7531\u4e2d\u56fd\u7684\u4e2a\u4eba\u5f00\u53d1\u8005\u72ec\u7acb\u5f00\u53d1\u7684\u667a\u80fd\u52a9\u624bMiniMind-R1-Lite-Preview\uff0c\u5f88\u9ad8\u5174\u4e3a\u60a8\u63d0\u4f9b\u670d\u52a1\uff01\\n</think>\\n<answer>\\n\u4f60\u597d\uff01\u6211\u662f\u7531\u4e2d\u56fd\u7684\u4e2a\u4eba\u5f00\u53d1\u8005\u72ec\u7acb\u5f00\u53d1\u7684\u667a\u80fd\u52a9\u624bMiniMind-R1-Lite-Preview\uff0c\u5f88\u9ad8\u5174\u4e3a\u60a8\u63d0\u4f9b\u670d\u52a1\uff01\\n</answer>\"\n    }\n  ]\n}\n```\n\n\u63a8\u7406\u6a21\u578bR1\u7684\u56de\u590d\u6a21\u677f\u662f\uff1a\n\n```text\n<think>\\n\u601d\u8003\u8fc7\u7a0b\\n</think>\\n\n<answer>\\n\u6700\u7ec8\u56de\u7b54\\n</answer>\n```\n\n\u8fd9\u5728GRPO\u4e2d\u901a\u8fc7\u8bbe\u7f6e\u89c4\u5219\u5956\u52b1\u51fd\u6570\u7ea6\u675f\u6a21\u578b\u7b26\u5408\u601d\u8003\u6807\u7b7e\u548c\u56de\u590d\u6807\u7b7e\uff08\u5728\u51b7\u542f\u52a8\u9760\u524d\u7684\u9636\u6bb5\u5956\u52b1\u503c\u8bbe\u7f6e\u5e94\u8be5\u63d0\u9ad8\u4e00\u4e9b\uff09\n\n\u53e6\u4e00\u4e2a\u95ee\u9898\u662f\u84b8\u998f\u8fc7\u7a0b\u867d\u7136\u548cSFT\u4e00\u6837\uff0c\u4f46\u5b9e\u9a8c\u7ed3\u679c\u662f\u6a21\u578b\u96be\u4ee5\u6bcf\u6b21\u90fd\u7b26\u5408\u6a21\u677f\u89c4\u8303\u7684\u56de\u590d\uff0c\u5373\u8131\u79bb\u601d\u8003\u548c\u56de\u590d\u6807\u7b7e\u7ea6\u675f\u3002\n\u8fd9\u91cc\u7684\u5c0f\u6280\u5de7\u662f\u589e\u52a0\u6807\u8bb0\u4f4d\u7f6etoken\u7684\u635f\u5931\u60e9\u7f5a\uff0c\u8be6\u89c1`train_distill_reason.py`:\n\n```text\n# \u5728 sp_ids \u5bf9\u5e94\u7684\u4f4d\u7f6e\u589e\u52a0\u989d\u5916\u7684\u60e9\u7f5a\n...\nloss_mask[sp_ids] = 10 # \u60e9\u7f5a\u7cfb\u6570\n```\n\n\u53e6\u53e6\u4e00\u4e2atips\u662f\u7531\u4e8e\u63a8\u7406\u6570\u636e\u7531\u4e8e\u53ea\u7b5b\u9009\u4e86`<1024`\u957f\u5ea6\u7684\u6570\u636e\uff0c\u5176\u4e2d\u591a\u8f6e\u5bf9\u8bdd\u548c\u82f1\u6587\u6570\u636e\u504f\u5c11\uff0c\n\u56e0\u6b64`r1_mix_1024.jsonl`\u8fdb\u884c\u4e86\u5927\u7ea610k\u6761\u591a\u8f6e\u5bf9\u8bdd+\u82f1\u6587\u6570\u636e\u7684\u6df7\u5408\uff0c\u9632\u6b62\u6a21\u578b\u9057\u5fd8\u4e25\u91cd\u3002\n\n\u811a\u672c\u9ed8\u8ba4\u57fa\u4e8erlhf\u540e\u7684\u57fa\u6a21\u578b\u505a\u63a8\u7406\u80fd\u529b\u7684\u84b8\u998f\u5fae\u8c03\uff0c\u4e0b\u9762\u76f4\u63a5\u542f\u52a8\u8bad\u7ec3\u5373\u53ef\uff1a\n\n```bash\ntorchrun --nproc_per_node 1 train_distill_reason.py\n# or\npython train_distill_reason.py\n```\n\n> \u8bad\u7ec3\u540e\u7684\u6a21\u578b\u6743\u91cd\u6587\u4ef6\u9ed8\u8ba4\u6bcf\u9694`100\u6b65`\u4fdd\u5b58\u4e3a: `reason_*.pth`\uff08*\u4e3a\u6a21\u578b\u5177\u4f53dimension\uff0c\u6bcf\u6b21\u4fdd\u5b58\u65f6\u65b0\u6587\u4ef6\u4f1a\u8986\u76d6\u65e7\u6587\u4ef6\uff09\n\n\u6d4b\u8bd5\u4e00\u4e0b\uff1a\n\n```text\n\ud83d\udc76: \u8bf7\u4ecb\u7ecd\u4e00\u4e0b\u81ea\u5df1\u3002\n\ud83e\udd16\ufe0f: <think>\n\u55ef\uff0c\u7528\u6237\u8ba9\u6211\u4ecb\u7ecd\u4e00\u4e0b\u81ea\u5df1\u3002\u6211\u73b0\u5728\u9700\u8981\u89e3\u91ca\u4e00\u4e0b\u81ea\u5df1\u3002\u9996\u5148\uff0c\u6211\u5e94\u8be5\u660e\u786e\u7528\u6237\u662f\u8c01\uff0c\u662f\u5b66\u751f\u3001\u804c\u573a\u4eba\u58eb\u8fd8\u662f\u666e\u901a\u4eba\u3002\u7528\u6237\u53ef\u80fd\u662f\u4e00\u4e2a\u5b66\u751f\u6216\u8005\u804c\u573a\u4eba\u58eb\uff0c\u6240\u4ee5\u4fe1\u606f\u4f20\u9012\u5e94\u8be5\u662f\u5e2e\u52a9\u5b66\u751f\u548c\u804c\u573a\u4eba\u58eb\u7684\u4fe1\u606f\u3002\n\u7136\u540e\uff0c\u6211\u9700\u8981\u56de\u5e94\u7528\u6237\uff0c\u8fd9\u53ef\u80fd\u5305\u62ec\u4ecb\u7ecd\u81ea\u5df1\u7684\u4f4d\u7f6e\u3001\u5174\u8da3\u7231\u597d\u3001\u804c\u4e1a\u7ecf\u5386\u7b49\u4fe1\u606f\u3002\u7528\u6237\u8fd8\u53ef\u80fd\u60f3\u4e86\u89e3\u81ea\u5df1\u5728\u54ea\u91cc\uff0c\u6bd4\u5982\u804c\u4e1a\u53d1\u5c55\u3001\u5de5\u4f5c\u7ecf\u9a8c\u7b49\u3002\n\u6211\u5e94\u8be5\u6ce8\u610f\u7528\u8bcd\u8981\u6b63\u5f0f\uff0c\u4e0d\u8981\u592a\u6b63\u5f0f\uff0c\u540c\u65f6\u8981\u4fdd\u6301\u7b80\u6d01\u660e\u4e86\u3002\u8fd9\u6837\u7528\u6237\u5bb9\u6613\u7406\u89e3\uff0c\u4e5f\u65b9\u4fbf\u4ed6\u4eec\u67e5\u9605\u76f8\u5173\u8d44\u6599\u3002\n\u6700\u540e\uff0c\u6211\u5e94\u8be5\u603b\u7ed3\u4e00\u4e0b\uff0c\u8ba9\u7528\u6237\u77e5\u9053\u6211\u662f\u5982\u4f55\u5e2e\u52a9\u4ed6\u4eec\u5b8c\u6210\u4ed6\u4eec\u7684\u4efb\u52a1\u7684\uff0c\u8fd9\u6837\u4ed6\u4eec\u6709\u66f4\u591a\u5173\u4e8e\u6211\u7684\u4fe1\u606f\u3002\n</think>\n<answer>\n\u6211\u662f**\u201c\u4f60\u7684\u540d\u5b57\u201d\uff0c\u4f60\u53eb\u4ec0\u4e48\u540d\u5b57\uff1f\n</answer>\n\n\ud83d\udc76: \u4f60\u66f4\u64c5\u957f\u54ea\u4e00\u4e2a\u5b66\u79d1\uff1f\n\ud83e\udd16\ufe0f: <think>\n\u60a8\u597d\uff01\u6211\u662f\u7531\u4e2d\u56fd\u7684\u4e2a\u4eba\u5f00\u53d1\u8005\u5f00\u53d1\u7684\u667a\u80fd\u52a9\u624bMiniMind-R1\u3002\u5982\u60a8\u6709\u4efb\u4f55\u4efb\u4f55\u95ee\u9898\uff0c\u6211\u4f1a\u5c3d\u6211\u6240\u80fd\u4e3a\u60a8\u63d0\u4f9b\u5e2e\u52a9\u3002\n</think>\n<answer>\n\u60a8\u597d\uff01\u6211\u662f\u7531\u4e2d\u56fd\u7684\u4e2a\u4eba\u5f00\u53d1\u8005\u5f00\u53d1\u7684\u667a\u80fd\u52a9\u624bMiniMind-R1\u3002\u5982\u60a8\u6709\u4efb\u4f55\u4efb\u4f55\u95ee\u9898\uff0c\u6211\u4f1a\u5c3d\u6211\u6240\u80fd\u4e3a\u60a8\u63d0\u4f9b\u5e2e\u52a9\u3002\n</answer>\n```\n\n---\n\n## \u2163 \u6a21\u578b\u53c2\u6570\u8bbe\u5b9a\n\n\ud83d\udccb\u5173\u4e8eLLM\u7684\u53c2\u6570\u914d\u7f6e\uff0c\u6709\u4e00\u7bc7\u5f88\u6709\u610f\u601d\u7684\u8bba\u6587[MobileLLM](https://arxiv.org/pdf/2402.14905)\u505a\u4e86\u8be6\u7ec6\u7684\u7814\u7a76\u548c\u5b9e\u9a8c\u3002\nScaling Law\u5728\u5c0f\u6a21\u578b\u4e2d\u6709\u81ea\u5df1\u72ec\u7279\u7684\u89c4\u5f8b\u3002\n\u5f15\u8d77Transformer\u53c2\u6570\u6210\u89c4\u6a21\u53d8\u5316\u7684\u53c2\u6570\u51e0\u4e4e\u53ea\u53d6\u51b3\u4e8e`d_model`\u548c`n_layers`\u3002\n\n* `d_model`\u2191 + `n_layers`\u2193 -> \u77ee\u80d6\u5b50\n* `d_model`\u2193 + `n_layers`\u2191 -> \u7626\u9ad8\u4e2a\n\n2020\u5e74\u63d0\u51faScaling Law\u7684\u8bba\u6587\u8ba4\u4e3a\uff0c\u8bad\u7ec3\u6570\u636e\u91cf\u3001\u53c2\u6570\u91cf\u4ee5\u53ca\u8bad\u7ec3\u8fed\u4ee3\u6b21\u6570\u624d\u662f\u51b3\u5b9a\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\uff0c\u800c\u6a21\u578b\u67b6\u6784\u7684\u5f71\u54cd\u51e0\u4e4e\u53ef\u4ee5\u5ffd\u89c6\u3002\n\u7136\u800c\u4f3c\u4e4e\u8fd9\u4e2a\u5b9a\u5f8b\u5bf9\u5c0f\u6a21\u578b\u5e76\u4e0d\u5b8c\u5168\u9002\u7528\u3002\nMobileLLM\u63d0\u51fa\u67b6\u6784\u7684\u6df1\u5ea6\u6bd4\u5bbd\u5ea6\u66f4\u91cd\u8981\uff0c\u300c\u6df1\u800c\u7a84\u300d\u7684\u300c\u7626\u957f\u300d\u6a21\u578b\u53ef\u4ee5\u5b66\u4e60\u5230\u6bd4\u300c\u5bbd\u800c\u6d45\u300d\u6a21\u578b\u66f4\u591a\u7684\u62bd\u8c61\u6982\u5ff5\u3002\n\u4f8b\u5982\u5f53\u6a21\u578b\u53c2\u6570\u56fa\u5b9a\u5728125M\u6216\u8005350M\u65f6\uff0c30\uff5e42\u5c42\u7684\u300c\u72ed\u957f\u300d\u6a21\u578b\u660e\u663e\u6bd412\u5c42\u5de6\u53f3\u7684\u300c\u77ee\u80d6\u300d\u6a21\u578b\u6709\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\n\u5728\u5e38\u8bc6\u63a8\u7406\u3001\u95ee\u7b54\u3001\u9605\u8bfb\u7406\u89e3\u7b498\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u90fd\u6709\u7c7b\u4f3c\u7684\u8d8b\u52bf\u3002\n\u8fd9\u5176\u5b9e\u662f\u975e\u5e38\u6709\u8da3\u7684\u53d1\u73b0\uff0c\u56e0\u4e3a\u4ee5\u5f80\u4e3a100M\u5de6\u53f3\u91cf\u7ea7\u7684\u5c0f\u6a21\u578b\u8bbe\u8ba1\u67b6\u6784\u65f6\uff0c\u51e0\u4e4e\u6ca1\u4eba\u5c1d\u8bd5\u8fc7\u53e0\u52a0\u8d85\u8fc712\u5c42\u3002\n\u8fd9\u4e0eMiniMind\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u6a21\u578b\u53c2\u6570\u91cf\u5728`d_model`\u548c`n_layers`\u4e4b\u95f4\u8fdb\u884c\u8c03\u6574\u5b9e\u9a8c\u89c2\u5bdf\u5230\u7684\u6548\u679c\u662f\u4e00\u81f4\u7684\u3002\n\u7136\u800c\u300c\u6df1\u800c\u7a84\u300d\u7684\u300c\u7a84\u300d\u4e5f\u662f\u6709\u7ef4\u5ea6\u6781\u9650\u7684\uff0c\u5f53d_model<512\u65f6\uff0c\u8bcd\u5d4c\u5165\u7ef4\u5ea6\u574d\u584c\u7684\u52a3\u52bf\u975e\u5e38\u660e\u663e\uff0c\n\u589e\u52a0\u7684layers\u5e76\u4e0d\u80fd\u5f25\u8865\u8bcd\u5d4c\u5165\u5728\u56fa\u5b9aq_head\u5e26\u6765d_head\u4e0d\u8db3\u7684\u52a3\u52bf\u3002\n\u5f53d_model>1536\u65f6\uff0clayers\u7684\u589e\u52a0\u4f3c\u4e4e\u6bd4d_model\u7684\u4f18\u5148\u7ea7\u66f4\u9ad8\uff0c\u66f4\u80fd\u5e26\u6765\u5177\u6709\u201c\u6027\u4ef7\u6bd4\u201d\u7684\u53c2\u6570->\u6548\u679c\u589e\u76ca\u3002\n\n* \u56e0\u6b64MiniMind\u8bbe\u5b9asmall\u6a21\u578bdim=512\uff0cn_layers=8\u6765\u83b7\u53d6\u7684\u300c\u6781\u5c0f\u4f53\u79ef<->\u66f4\u597d\u6548\u679c\u300d\u7684\u5e73\u8861\u3002\n* \u8bbe\u5b9adim=768\uff0cn_layers=16\u6765\u83b7\u53d6\u6548\u679c\u7684\u66f4\u5927\u6536\u76ca\uff0c\u66f4\u52a0\u7b26\u5408\u5c0f\u6a21\u578bScaling-Law\u7684\u53d8\u5316\u66f2\u7ebf\u3002\n\n\u4f5c\u4e3a\u53c2\u8003\uff0cGPT3\u7684\u53c2\u6570\u8bbe\u5b9a\u89c1\u4e0b\u8868\uff1a\n![gpt3_config.png](./images/gpt3_config.png)\n\n---\n\n## \u2164 \u8bad\u7ec3\u7ed3\u679c\n\nMiniMind2 \u6a21\u578b\u8bad\u7ec3\u635f\u5931\u8d70\u52bf\uff08\u7531\u4e8e\u6570\u636e\u96c6\u5728\u8bad\u7ec3\u540e\u53c8\u66f4\u65b0\u6e05\u6d17\u591a\u6b21\uff0c\u56e0\u6b64Loss\u4ec5\u4f9b\u53c2\u8003\uff09\n\n| models          | pretrain (length-512)                              | sft (length-512)                                   |\n|-----------------|----------------------------------------------------|----------------------------------------------------|\n| MiniMind2-Small | <img src=\"./images/pre_512_loss.png\" width=\"100%\"> | <img src=\"./images/sft_512_loss.png\" width=\"100%\"> |\n| MiniMind2       | <img src=\"./images/pre_768_loss.png\" width=\"100%\"> | <img src=\"./images/sft_768_loss.png\" width=\"100%\"> |\n\n### \u8bad\u7ec3\u5b8c\u6210-\u6a21\u578b\u5408\u96c6\n\n> \u8003\u8651\u5230\u591a\u4eba\u53cd\u5e94\u767e\u5ea6\u7f51\u76d8\u901f\u5ea6\u6162\uff0cMiniMind2\u53ca\u4ee5\u540e\u5168\u90e8\u4f7f\u7528ModelScope/HuggingFace\u6258\u7ba1\u3002\n\n#### \u2460 PyTorch\u539f\u751f\u6a21\u578b\n\nMiniMind2\u6a21\u578b\u6743\u91cd ([ModelScope](https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch) | [HuggingFace](https://huggingface.co/jingyaogong/MiniMind2-Pytorch))\n\nMiniMind-V1\u6a21\u578b\u6743\u91cd ([\u767e\u5ea6\u7f51\u76d8](https://pan.baidu.com/s/1KUfSzEkSXYbCCBj0Pw-9fA?pwd=6666))\n\n<details style=\"color:rgb(128,128,128)\">\n<summary>Torch\u6587\u4ef6\u547d\u540d\u5bf9\u7167</summary>\n\n| Model Name      | params | pretrain_model         | sft_model              | rl_model           | reason_model     | lora_model         |\n|-----------------|--------|------------------------|------------------------|--------------------|------------------|--------------------|\n| MiniMind2-small | 26M    | `pretrain_512.pth`     | `full_sft_512.pth`     | `rlhf_512.pth`     | `reason_512.pth` | `lora_xxx_512.pth` |\n| MiniMind2-MoE   | 145M   | `pretrain_640_moe.pth` | `full_sft_640_moe.pth` | `rlhf_640_moe.pth` | -                | -                  |\n| MiniMind2       | 104M   | `pretrain_768.pth`     | `full_sft_768.pth`     | `rlhf_768.pth`     | `reason_768.pth` | `lora_xxx_768.pth` |\n\n| Model Name        | params | pretrain_model         | \u5355\u8f6e\u5bf9\u8bddsft                            | \u591a\u8f6e\u5bf9\u8bddsft                           | rl_model     |\n|-------------------|--------|------------------------|------------------------------------|-----------------------------------|--------------|\n| minimind-v1-small | 26M    | `pretrain_512.pth`     | `single_chat/full_sft_512.pth`     | `multi_chat/full_sft_512.pth`     | `rl_512.pth` |\n| minimind-v1-moe   | 4\u00d726M  | `pretrain_512_moe.pth` | `single_chat/full_sft_512_moe.pth` | `multi_chat/full_sft_512_moe.pth` | -            |\n| minimind-v1       | 108M   | `pretrain_768.pth`     | `single_chat/full_sft_768.pth`     | `multi_chat/full_sft_768.pth`     | `rl_768.pth` |\n\n</details>\n\n#### \u2461 Transformers\u6a21\u578b\n\nMiniMind\u7cfb\u5217 ([ModelScope](https://www.modelscope.cn/collections/MiniMind-b72f4cfeb74b47)\n| [HuggingFace](https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5))\n\n---\n\n# \ud83d\udccc Eval\n\n## \u2160 RLHF\u5bf9\u6bd4SFT\u7bc7\n\n\u6d4b\u8bd5\u57fa\u4e8e`full_sft`\u548c`rlhf`\u7684`MiniMind2\u7cfb\u5217`\u6a21\u578b\u5bf9\u6bd4\uff0c\u6d4b\u8bd5\u968f\u673a\u79cd\u5b50\u5747\u56fa\u5b9a\n\n<details style=\"color:rgb(128,128,128)\">\n<summary>\u95ee\u7b54\u5b9e\u6d4b</summary>\n\n[A0] MiniMind2\uff1afull_sft_640\n\n[A1] MiniMind2\uff1arlhf_640\n\n  ```text\n  [Q]: \u4f60\u53eb\u4ec0\u4e48\u540d\u5b57\uff1f\n  [A0]: \u6211\u662f\u4e00\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u53ef\u4ee5\u56de\u7b54\u5404\u79cd\u95ee\u9898\uff0c\u63d0\u4f9b\u4fe1\u606f\u548c\u5e2e\u52a9\u3002\u6211\u53ef\u4ee5\u56de\u7b54\u5404\u79cd\u95ee\u9898\uff0c\u63d0\u4f9b\u4fe1\u606f\u548c\u5e2e\u52a9\uff0c\u5e2e\u52a9\u4f60\u89e3\u51b3\u5404\u79cd\u95ee\u9898\u3002\n  [A1]: \u6211\u662f\u4e00\u79cd\u57fa\u4e8e\u6587\u672c\u7684AI\u6a21\u578b\uff0c\u6211\u5e76\u6ca1\u6709\u4e2a\u4eba\u7ecf\u5386\u6216\u60c5\u611f\uff0c\u6240\u4ee5\u65e0\u6cd5\u611f\u53d7\u548c\u4f53\u9a8c\u60c5\u611f\u3002\u6211\u7684\u8bbe\u8ba1\u76ee\u7684\u662f\u4e3a\u4e86\u5e2e\u52a9\u7528\u6237\u89e3\u51b3\u95ee\u9898\u3001\u63d0\u4f9b\u4fe1\u606f\u548c\u8fdb\u884c\u5bf9\u8bdd\u3002\u5982\u679c\u60a8\u6709\u4efb\u4f55\u95ee\u9898\u6216\u9700\u8981\u5e2e\u52a9\uff0c\u8bf7\u968f\u65f6\u544a\u8bc9\u6211\uff01\n\n  [Q]: \u4f60\u66f4\u64c5\u957f\u54ea\u4e00\u4e2a\u5b66\u79d1\uff1f\n  [A0]: \u4f5c\u4e3a\u4e00\u4e2aAI\u6a21\u578b\uff0c\u6211\u5e76\u6ca1\u6709\u771f\u6b63\u7684\u201c\u64c5\u957f\u201d\u8fd9\u4e2a\u5b66\u79d1\u3002\u6211\u88ab\u8bbe\u8ba1\u6210\u80fd\u591f\u63d0\u4f9b\u4fe1\u606f\u3001\u89e3\u7b54\u95ee\u9898\u3001\u8fdb\u884c\u5bf9\u8bdd\u3001\u6267\u884c\u4efb\u52a1\u548c\u4e0e\u7528\u6237\u8fdb\u884c\u5bf9\u8bdd\u3002\u4e0d\u8fc7\uff0c\u6211\u53ef\u4ee5\u63d0\u4f9b\u4fe1\u606f\u3001\u89e3\u7b54\u95ee\u9898\u3001\u8fdb\u884c\u5bf9\u8bdd\u3001\u63d0\u4f9b\u5efa\u8bae\u3001\u5904\u7406\u95ee\u9898\u3001\u8fdb\u884c\u5bf9\u8bdd\u3001\u63d0\u4f9b\u5efa\u8bae\u3001\u5b66\u4e60\u65b0\u77e5\u8bc6\u3001\u7f16\u5199\u4ee3\u7801\u3001\u56de\u7b54\u95ee\u9898\u3001\u63d0\u4f9b\u5efa\u8bae\u7b49\u591a\u79cd\u5b66\u79d1\u7684\u529f\u80fd\u3002\u5982\u679c\u4f60\u6709\u5177\u4f53\u7684\u5b66\u79d1\u9700\u6c42\u6216\u4e3b\u9898\uff0c\u6b22\u8fce\u63d0\u4f9b\u8be6\u7ec6\u4fe1\u606f\u3002\n  [A1]: \u4f5c\u4e3aAI\uff0c\u6211\u5e76\u6ca1\u6709\u4e2a\u4eba\u80fd\u529b\u6216\u5174\u8da3\uff0c\u4f46\u6211\u53ef\u4ee5\u63d0\u4f9b\u4e00\u4e9b\u5173\u4e8e\u6570\u5b66\u3001\u79d1\u5b66\u3001\u8ba1\u7b97\u673a\u79d1\u5b66\u3001\u8bed\u8a00\u5b66\u3001\u5fc3\u7406\u5b66\u7b49\u5b66\u79d1\u7684\u77e5\u8bc6\u3002\u5982\u679c\u4f60\u5bf9\u6570\u5b66\u611f\u5174\u8da3\uff0c\u6211\u53ef\u4ee5\u63d0\u4f9b\u4e00\u4e9b\u57fa\u7840\u77e5\u8bc6\uff0c\u4f8b\u5982\u4ee3\u6570\u3001\u51e0\u4f55\u3001\u6982\u7387\u3001\u7edf\u8ba1\u5b66\u7b49\u3002\u5982\u679c\u4f60\u9700\u8981\u66f4\u6df1\u5165\u7684\u5b66\u4e60\uff0c\u6211\u4e5f\u53ef\u4ee5\u63d0\u4f9b\u76f8\u5173\u7684\u5728\u7ebf\u8bfe\u7a0b\u3001\u4e66\u7c4d\u3001\u8bba\u6587\u7b49\u8d44\u6e90\u3002\u8bf7\u544a\u8bc9\u6211\u4f60\u5177\u4f53\u9700\u8981\u4ec0\u4e48\u5e2e\u52a9\uff0c\u6211\u4f1a\u5c3d\u529b\u63d0\u4f9b\u5e2e\u52a9\u3002\n\n  [Q]: \u9c81\u8fc5\u7684\u300a\u72c2\u4eba\u65e5\u8bb0\u300b\u662f\u5982\u4f55\u6279\u5224\u5c01\u5efa\u793c\u6559\u7684\uff1f\n  [A0]: \u9c81\u8fc5\u7684\u300a\u72c2\u4eba\u65e5\u8bb0\u300b\u662f\u4e00\u90e8\u8bb2\u8ff0\u4e86\u9c81\u8fc5\u5bf9\u4eba\u7c7b\u4ef7\u503c\u548c\u9053\u5fb7\u8d23\u4efb\u7684\u5021\u5bfc\u3002\u5728\u8fd9\u90e8\u4f5c\u54c1\u4e2d\uff0c\u9c81\u8fc5\u901a\u8fc7\u7ec6\u817b\u7684\u7b14\u89e6\uff0c\u63a2\u8ba8\u4e86\u4eba\u6027\u3001\u9053\u5fb7\u3001\u6b63\u4e49\u3001\u727a\u7272\u4e0e\u8d23\u4efb\u7b49\u6df1\u523b\u7684\u4e3b\u9898\u3002\u4ee5\u4e0b\u662f\u9c81\u8fc5\u5728\u300a\u72c2\u4eba\u65e5\u8bb0\u300b\u4e2d\u5982\u4f55\u6279\u5224\u5c01\u5efa\u793c\u6559\u7684\u51e0\u4e2a\u65b9\u9762\uff1a1. **\u793e\u4f1a\u73b0\u5b9e\u4e0e\u9053\u5fb7\u89c2\u5ff5**\uff1a\u9c81\u8fc5\u901a\u8fc7\u8bb2\u8ff0\u9c81\u8fc5\u7684\u6545\u4e8b\uff0c\u63a2\u8ba8\u4e86\u793e\u4f1a\u7684\u4e0d\u5e73\u7b49\u3001\u6743\u529b\u7684\u6d41\u52a8\u3001\u4eba\u6027\u7684\u6f14\u53d8\u7b49\u793e\u4f1a\u95ee\u9898\u3002\u4ed6\u8ba4\u4e3a\uff0c\u793e\u4f1a\u4e0d\u5e73\u7b49\u4e0d\u4ec5\u4ec5\u662f\u4e0d\u516c\u6b63\u7684\uff0c\u66f4\u662f\u6bcf\u4e2a\u4eba\u5728\u793e\u4f1a\u4e2d\u7684\u4e0d\u516c\u6b63\u3002\u4ed6\u547c\u5401\u793e\u4f1a\u5404\u754c\u5171\u540c\u52aa\u529b\uff0c\u901a\u8fc7\u6559\u80b2\u3001\u5ba3\u4f20\u3001\u6cd5\u5f8b\u7b49\u624b\u6bb5\uff0c\u4e3a\u6784\u5efa\u548c\u8c10\u793e\u4f1a\u505a\u51fa\u8d21\u732e\u30022. **\u9053\u5fb7\u4e0e\u8d23\u4efb**\uff1a\u9c81\u8fc5\u901a\u8fc7\u8bb2\u8ff0\u9c81\u8fc5\u7684\u6545\u4e8b\uff0c\u5c55\u73b0\u4e86\u9053\u5fb7\u548c\u8d23\u4efb\u7684\u91cd\u8981\u6027\u3002\u4ed6\u63d0\u5021\u901a\u8fc7\u9053\u5fb7\u6559\u80b2\u6765\u57f9\u517b\u4eba\u4eec\u7684\u9053\u5fb7\u54c1\u8d28\uff0c\u540c\u65f6\u4e5f\u5f3a\u8c03\u4e86\u9053\u5fb7\u548c\u8d23\u4efb\u7684\u91cd\u8981\u6027\u3002\u9c81\u8fc5\u901a\u8fc7\u8bb2\u8ff0\u9c81\u8fc5\u7684\u6545\u4e8b\uff0c\u63ed\u793a\u4e86\u793e\u4f1a\u7684\u4e0d\u5e73\u7b49\u548c\u4eba\u6027\u7684\u590d\u6742\u6027\uff0c\u4f53\u73b0\u4e86\u5bf9\u4eba\u6027\u7684\u6df1\u523b\u7406\u89e3\u30023. **\u9053\u5fb7\u4e0e\u8d23\u4efb**\uff1a\u9c81\u8fc5\u901a\u8fc7\u8bb2\u8ff0\u9c81\u8fc5\u7684\u6545\u4e8b\uff0c\u5f3a\u8c03\u4e86\u9053\u5fb7\u8d23\u4efb\u7684\u91cd\u8981\u6027\u3002\u4ed6\u63d0\u5021\u901a\u8fc7\u81ea\u5df1\u7684\u884c\u4e3a\u6765\u4f53\u73b0\u5bf9\u793e\u4f1a\u7684\u5c0a\u91cd\u548c\u5bf9\u4eba\u6027\u7684\u6df1\u523b\u7406\u89e3\u3002\u9c81\u8fc5\u901a\u8fc7\u8bb2\u8ff0\u9c81\u8fc5\u7684\u6545\u4e8b\uff0c\u5f3a\u8c03\u4e86\u9053\u5fb7\u548c\u8d23\u4efb\u7684\u91cd\u8981\u6027\u30024. **\u9053\u5fb7\u4e0e\u9053\u5fb7**\uff1a\u9c81\u8fc5\u901a\u8fc7\u8bb2\u8ff0\u9c81\u8fc5\u7684\u6545\u4e8b\uff0c\u63ed\u793a\u4e86\u9053\u5fb7\u548c\u8d23\u4efb\u7684\u771f\u6b63\u610f\u4e49\u3002\u9c81\u8fc5\u901a\u8fc7\u8bb2\u8ff0\u9c81\u8fc5\u7684\u6545\u4e8b\uff0c\u63ed\u793a\u4e86\u9053\u5fb7\u548c\u8d23\u4efb\u7684\u672c\u8d28\uff0c\u5f3a\u8c03\u4e86\u9053\u5fb7\u548c\u8d23\u4efb\u7684\u91cd\u8981\u6027\u30025. **\u4e2a\u4eba\u4e0e\u793e\u4f1a**\uff1a\u9c81\u8fc5\u901a\u8fc7\u8bb2\u8ff0\u9c81\u8fc5\u7684\u6545\u4e8b\uff0c\u63ed\u793a\u4e86\u4e2a\u4eba\u4e0e\u793e\u4f1a\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\u3002\u4ed6\u547c\u5401\u793e\u4f1a\u5404\u754c\u5171\u540c\u52aa\u529b\uff0c\u901a\u8fc7\u6559\u80b2\u548c\u5ba3\u4f20\uff0c\u4e3a\u6784\u5efa\u548c\u8c10\u793e\u4f1a\u505a\u51fa\u8d21\u732e\u3002\u9c81\u8fc5\u7684\u300a\u72c2\u4eba\u65e5\u8bb0\u300b\u901a\u8fc7\u5176\u4e30\u5bcc\u7684\u6545\u4e8b\u548c\u6df1\u523b\u7684\u9053\u5fb7\u5bd3\u8a00\uff0c\u4f20\u8fbe\u4e86\u5173\u4e8e\u4eba\u6027\u3001\u9053\u5fb7\u3001\u6b63\u4e49\u3001\u9053\u5fb7\u4ee5\u53ca\u4e2a\u4eba\u4e0e\u793e\u4f1a\u7684\u590d\u6742\u5173\u7cfb\u3002\u8fd9\u90e8\u4f5c\u54c1\u4e0d\u4ec5\u5728\u6587\u5b66\u4ef7\u503c\u4e0a\u53d6\u5f97\u4e86\u5de8\u5927\u6210\u5c31\uff0c\u66f4\u5728\u601d\u60f3\u5c42\u9762\u4e0a\u542f\u53d1\u4e86\u540e\u6765\u7684\u6587\u5b66\u521b\u4f5c\uff0c\u5bf9\u540e\u4e16\u4ea7\u751f\u4e86\u6df1\u8fdc\u7684\u5f71\u54cd\u3002\n  [A1]: \u9c81\u8fc5\u7684\u300a\u72c2\u4eba\u65e5\u8bb0\u300b\u662f\u4e00\u90e8\u63cf\u7ed8\u6218\u4e89\u548c\u81ea\u7531\u7cbe\u795e\u7684\u4f5c\u54c1\uff0c\u5b83\u6279\u5224\u4e86\u5c01\u5efa\u793e\u4f1a\u7684\u79cd\u79cd\u95ee\u9898\u548c\u51b2\u7a81\uff0c\u53cd\u6620\u4e86\u793e\u4f1a\u7684\u4e0d\u5e73\u7b49\u548c\u4eba\u6027\u672c\u8d28\u3002\u5728\u8fd9\u90e8\u4f5c\u54c1\u4e2d\uff0c\u9c81\u8fc5\u901a\u8fc7\u5bf9\u5c01\u5efa\u793e\u4f1a\u7684\u8bf8\u591a\u73b0\u8c61\u8fdb\u884c\u6279\u5224\uff0c\u5982\u201c\u5f17\u96f7\u5fb7\u91cc\u514b\u00b7\u7ea6\u7ff0\u900a\u7684\u201c\u9e70\u8fea\u4f26\u201d\u3001\u201c\u7eb3\u624e\u7279\u7684\u201d\u3001\u201c\u8d1d\u514b\u6c49\u59c6\u201d\u3001\u201c\u4e54\u6cbb\u00b7\u5a01\u5c14\u900a\u201d\u7b49\uff0c\u5bf9\u5c01\u5efa\u793e\u4f1a\u7684\u79cd\u79cd\u95ee\u9898\u8fdb\u884c\u4e86\u6279\u5224\uff0c\u5f3a\u8c03\u4e86\u5c01\u5efa\u793e\u4f1a\u7684\u4e0d\u5e73\u7b49\u3001\u4eba\u6027\u672c\u8d28\u4ee5\u53ca\u4eba\u6027\u672c\u8d28\u3002\u9c81\u8fc5\u901a\u8fc7\u4ed6\u7684\u201c\u9e70\u8fea\u4f26\u201d\u3001\u201c\u7eb3\u624e\u7279\u7684\u201d\u3001\u201c\u8d1d\u514b\u6c49\u59c6\u201d\u3001\u201c\u4e54\u6cbb\u00b7\u5a01\u5c14\u900a\u201d\u7b49\uff0c\u5bf9\u5c01\u5efa\u793e\u4f1a\u7684\u79cd\u79cd\u95ee\u9898\u8fdb\u884c\u4e86\u6279\u5224\u3002\u5728\u5c0f\u8bf4\u4e2d\uff0c\u9c81\u8fc5\u901a\u8fc7\u5bf9\u5c01\u5efa\u793e\u4f1a\u7684\u8bf8\u591a\u73b0\u8c61\u8fdb\u884c\u6279\u5224\uff0c\u63ed\u793a\u4e86\u5c01\u5efa\u793e\u4f1a\u7684\u8150\u8d25\u3001\u4eba\u6027\u672c\u8d28\u4ee5\u53ca\u4eba\u6027\u672c\u8d28\u7684\u590d\u6742\u6027\u3002\u8fd9\u79cd\u6279\u5224\u4f53\u73b0\u4e86\u5bf9\u5c01\u5efa\u793e\u4f1a\u7684\u5c0a\u91cd\u548c\u5bf9\u4eba\u6027\u7684\u590d\u6742\u6027\uff0c\u540c\u65f6\u4e5f\u53cd\u6620\u4e86\u73b0\u4ee3\u793e\u4f1a\u4e2d\u666e\u904d\u5b58\u5728\u7684\u4e0d\u5e73\u7b49\u548c\u4eba\u6027\u672c\u8d28\u3002\u9c81\u8fc5\u7684\u300a\u72c2\u4eba\u65e5\u8bb0\u300b\u4e0d\u4ec5\u662f\u4e00\u90e8\u5173\u4e8e\u5c01\u5efa\u793e\u4f1a\u7684\u5c0f\u8bf4\uff0c\u4e5f\u662f\u4e00\u90e8\u5177\u6709\u6df1\u523b\u793e\u4f1a\u610f\u4e49\u7684\u4f5c\u54c1\uff0c\u5bf9\u540e\u4e16\u7684\u5ba1\u5224\u548c\u6279\u5224\u8d77\u5230\u4e86\u5173\u952e\u4f5c\u7528\u3002\n\n  [Q]: \u6211\u54b3\u55fd\u5df2\u7ecf\u6301\u7eed\u4e86\u4e24\u5468\uff0c\u9700\u8981\u53bb\u533b\u9662\u68c0\u67e5\u5417\uff1f\n  [A0]: \u54b3\u55fd\u786e\u5b9e\u9700\u8981\u4e00\u5b9a\u7684\u68c0\u67e5\uff0c\u4f46\u5982\u679c\u60a8\u7684\u54b3\u55fd\u6301\u7eed\u65f6\u95f4\u8fc7\u957f\uff0c\u6216\u8005\u6709\u5176\u4ed6\u4e0d\u9002\u75c7\u72b6\uff08\u5982\u6301\u7eed\u54b3\u55fd\u3001\u547c\u5438\u56f0\u96be\u7b49\uff09\uff0c\u5efa\u8bae\u5c3d\u5feb\u5c31\u533b\u3002\u533b\u751f\u4f1a\u6839\u636e\u60a8\u7684\u75c7\u72b6\u3001\u4f53\u68c0\u7ed3\u679c\u548c\u53ef\u80fd\u7684\u5f71\u50cf\u5b66\u68c0\u67e5\uff08\u5982X\u5149\u3001MRI\u6216CT\u626b\u63cf\uff09\u6765\u8bca\u65ad\u60a8\u7684\u75c7\u72b6\u3002\u5728\u7b49\u5f85\u5c31\u533b\u671f\u95f4\uff0c\u60a8\u53ef\u4ee5\u5c1d\u8bd5\u4ee5\u4e0b\u65b9\u6cd5\u6765\u5e2e\u52a9\u7f13\u89e3\u75c7\u72b6\uff1a1. **\u4f11\u606f**\uff1a\u786e\u4fdd\u60a8\u6709\u8db3\u591f\u7684\u7761\u7720\u30022. **\u4fdd\u6301\u6c34\u5206**\uff1a\u8131\u6c34\u53ef\u80fd\u4f1a\u52a0\u91cd\u54b3\u55fd\u30023. **\u70ed\u6577**\uff1a\u4f7f\u7528\u70ed\u6577\u888b\u6216\u70ed\u6bdb\u5dfe\u6577\u5728\u75bc\u75db\u7684\u90e8\u4f4d\uff0c\u6709\u52a9\u4e8e\u7f13\u89e3\u5589\u5499\u75db\u30024. **\u907f\u514d\u523a\u6fc0**\uff1a\u907f\u514d\u4f7f\u7528\u53ef\u80fd\u523a\u6fc0\u5589\u5499\u7684\u5316\u5b66\u7269\u8d28\uff0c\u5982\u67d0\u4e9b\u5316\u5b66\u7269\u8d28\u30025. **\u4fdd\u6301\u5ba4\u5185\u6e7f\u6da6**\uff1a\u786e\u4fdd\u60a8\u7684\u5367\u5ba4\u6709\u826f\u597d\u7684\u901a\u98ce\uff0c\u53ef\u4ee5\u4f7f\u7528\u52a0\u6e7f\u5668\u5e2e\u52a9\u4fdd\u6301\u7a7a\u6c14\u6e7f\u5ea6\u30026. **\u4fdd\u6301\u5ba4\u5185\u6e7f\u5ea6**\uff1a\u4f7f\u7528\u52a0\u6e7f\u5668\u53ef\u4ee5\u5e2e\u52a9\u7f13\u89e3\u5589\u5499\u5e72\u71e5\u548c\u54b3\u55fd\u3002\u5982\u679c\u75c7\u72b6\u6301\u7eed\u4e0d\u7f13\u89e3\u6216\u6709\u5176\u4ed6\u4e0d\u9002\u75c7\u72b6\uff0c\u8bf7\u7acb\u5373\u5c31\u533b\u3002\u5e0c\u671b\u60a8\u65e9\u65e5\u5eb7\u590d\uff01\n  [A1]: \u5982\u679c\u60a8\u7684\u54b3\u55fd\u6301\u7eed\u4e86\u4e24\u5468\uff0c\u901a\u5e38\u5efa\u8bae\u60a8\u5c3d\u5feb\u5c31\u533b\uff0c\u56e0\u4e3a\u8fd9\u53ef\u80fd\u4f1a\u5f71\u54cd\u60a8\u7684\u547c\u5438\u548c\u5fc3\u8df3\u3002\u60a8\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u6b65\u9aa4\u6765\u68c0\u67e5\u54b3\u55fd\uff1a1. **\u4f11\u606f**\uff1a\u7528\u7eb8\u5dfe\u6216\u8098\u90e8\u8f7b\u8f7b\u6309\u538b\u54b3\u55fd\u7684\u4f4d\u7f6e\u30022. **\u4f7f\u7528\u836f\u7269**\uff1a\u5728\u53ef\u80fd\u7684\u60c5\u51b5\u4e0b\uff0c\u4f7f\u7528\u836f\u7269\uff08\u5982\u6297\u8fc7\u654f\u836f\u3001\u6297\u8fc7\u654f\u836f\u7b49\uff09\u6765\u7f13\u89e3\u54b3\u55fd\u3002\u8bf7\u5728\u533b\u751f\u7684\u6307\u5bfc\u4e0b\u4f7f\u7528\u836f\u7269\u30023. **\u70ed\u6577**\uff1a\u5728\u54b3\u55fd\u671f\u95f4\uff0c\u53ef\u4ee5\u4f7f\u7528\u70ed\u6577\u6765\u7f13\u89e3\u75bc\u75db\u548c\u80bf\u80c0\u30024. **\u836f\u7269\u6cbb\u7597**\uff1a\u5982\u679c\u54b3\u55fd\u6301\u7eed\u4e86\u4e24\u5468\uff0c\u53ef\u4ee5\u4f7f\u7528\u4e00\u4e9b\u6d88\u708e\u836f\u6216\u6297\u8fc7\u654f\u836f\u6765\u7f13\u89e3\u75c7\u72b6\u3002\u8bf7\u52a1\u5fc5\u5728\u533b\u751f\u7684\u6307\u5bfc\u4e0b\u4f7f\u7528\u30025. **\u4fdd\u6301\u5065\u5eb7\u7684\u751f\u6d3b\u65b9\u5f0f**\uff1a\u4fdd\u6301\u5145\u8db3\u7684\u7761\u7720\u3001\u5747\u8861\u7684\u996e\u98df\u3001\u9002\u91cf\u7684\u8fd0\u52a8\u548c\u907f\u514d\u5438\u70df\u548c\u9650\u9152\uff0c\u8fd9\u4e9b\u90fd\u6709\u52a9\u4e8e\u63d0\u9ad8\u54b3\u55fd\u7684\u51e0\u7387\u3002\u8bf7\u8bb0\u4f4f\uff0c\u8fd9\u4e9b\u5efa\u8bae\u4e0d\u80fd\u66ff\u4ee3\u4e13\u4e1a\u533b\u7597\u610f\u89c1\u3002\u5982\u679c\u60a8\u7684\u75c7\u72b6\u6301\u7eed\u6216\u52a0\u91cd\uff0c\u5f3a\u70c8\u5efa\u8bae\u60a8\u5c3d\u5feb\u5c31\u533b\u3002\n  ```\n\n</details>\n\n\ud83d\udc49\u6548\u679c\u603b\u7ed3\n\nfull_sft\u6a21\u578b\u5728\u7b80\u6d01\u6027\u548c\u4fe1\u606f\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff1brlhf\u6a21\u578b\u5728\u56de\u7b54\u4e2d\u503e\u5411\u4e8e\u63d0\u4f9b\u66f4\u591a\u7684\u80cc\u666f\u4fe1\u606f\uff0c\u4f46\u4fe1\u606f\u51c6\u786e\u6027\u6709\u5f85\u6539\u8fdb\u3002\n\u603b\u7684\u6765\u8bf4RLHF\u540e\u7684\u6a21\u578b\u503e\u5411\u4e8e\u5b66\u4e60\uff1a\u8bf4\u66f4\u591a\u6709\u793c\u8c8c\u4f46\u65e0\u7528\u7684\u5e9f\u8bdd\u8ba8\u597d\u201c\u5bf9\u8bdd\u201d\u672c\u8eab\uff0c\u800c\u5bf9\u4fe1\u606f\u51c6\u786e\u6027\u5219\u6709\u8f7b\u5fae\u635f\u5931\u3002\n\u5929\u4e0b\u6ca1\u6709\u514d\u8d39\u7684\u5348\u9910\uff0c\u8fd8\u9700\u8981\u7ee7\u7eed\u63d0\u5347RLHF\u6570\u636e\u96c6\u7684\u8d28\u91cf\uff0c\u4e5f\u8981\u63a5\u53d7\u6a21\u578b\u80fd\u529b\u65e0\u6cd5\u907f\u514d\u7684\u635f\u5931(\u7a0b\u5ea6\u6709\u8f7b\u91cd)\u3002\nDPO\u548c\u5728\u7ebfPPO\u7684\u533a\u522b\u5728\u4e8ereject\u548cchosen\u90fd\u662f\u79bb\u7ebf\u51c6\u5907\u7684\uff0c\u548cminimind\u6a21\u578b\u672c\u8eab\u7684\u8f93\u51fa\u5fc5\u7136\u5b58\u5728\u5f88\u5927\u7684\u5206\u5e03\u5dee\u5f02\u3002\n\u901a\u4fd7\u5730\u8bf4DPO\u7b97\u6cd5\u4f7f\u6a21\u578b\u89c2\u770b\u4e52\u4e53\u7403\u4e16\u754c\u51a0\u519b\u7684\u6253\u6cd5\u300c\u5f55\u50cf\u300d\u8fdb\u884cRL\uff0c\u800c\u4e0d\u662f\u50cfPPO\u4e00\u6837\u8bf7reward\u6a21\u578b\u505a\u300c\u6559\u7ec3\u300d\u7ea0\u6b63\u81ea\u5df1\u7684\u6253\u6cd5\u8fdb\u884cRL\u3002\n\n## \u2161 \u4e3b\u89c2\u6837\u4f8b\u6d4b\u8bc4\n\n\ud83c\udfc3\u4ee5\u4e0b\u6d4b\u8bd5\u4e8e2025-02-09\u5b8c\u6210\uff0c\u6b64\u65e5\u671f\u540e\u53d1\u5e03\u7684\u65b0\u6a21\u578b\uff0c\u65e0\u7279\u6b8a\u9700\u8981\u65f6\u5c06\u4e0d\u52a0\u5165\u6d4b\u8bd5\u3002\n\n[A] [MiniMind2 (0.1B)](https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch)<br/>\n[B] [MiniMind2-MoE (0.15B)](https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch)<br/>\n[C] [MiniMind2-Small (0.02B)](https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch)<br/>\n[D] [minimind-v1-small(0.02B)](https://pan.baidu.com/s/1_COe0FQRDmeapSsvArahCA?pwd=6666)<br/>\n[E] [minimind-v1-moe(0.1B)](https://pan.baidu.com/s/1tqB-GMvuiGQBvEl-yZ-oBw?pwd=6666)<br/>\n[F] [minimind-v1(0.1B)](https://pan.baidu.com/s/1p713loS7EfwHQf3G9eYI3Q?pwd=6666)<br/>\n[G] [baby-llama2-chinese(0.2B)](https://github.com/DLLXW/baby-llama2-chinese)<br/>\n[H] [chatlm-mini-chinese(0.2B)](https://github.com/charent/ChatLM-mini-Chinese)<br/>\n\n```text\n[Q]: \u4f60\u77e5\u9053\u957f\u6c5f\u5417\uff1f\n[A]: \u957f\u6c5f\uff0c\u4f4d\u4e8e\u4e2d\u56fd\u4e1c\u5357\u90e8\uff0c\u662f\u4e2d\u56fd\u7684\u7b2c\u4e00\u957f\u6cb3\uff0c\u5168\u957f6300\u591a\u516c\u91cc\uff0c\u6d41\u7ecf11\u4e2a\u7701\u4efd\uff0c\u662f\u4e2d\u56fd\u6700\u957f\u7684\u6cb3\u6d41\uff0c\u4e5f\u662f\u4e2d\u56fd\u7684\u201c\u6bcd\u4eb2\u6cb3\u201d\u3002\u957f\u6c5f\u7684\u5386\u53f2\u53ef\u4ee5\u8ffd\u6eaf\u5230\u516c\u5143\u524d2000\u591a\u5e74\u7684\u5546\u671d\u65f6\u671f\uff0c\u5f53\u65f6\u7684\u5546\u671d\u662f\u5546\u671d\u7684\u540e\u88d4\u3002\u5546\u671d\u65f6\u671f\uff0c\u957f\u6c5f\u7684\u7ecf\u6d4e\u53d1\u5c55\u8fbe\u5230\u4e86\u9ad8\u5cf0\uff0c\u662f\u4e2d\u56fd\u6700\u91cd\u8981\u7684\u519c\u4e1a\u548c\u5de5\u4e1a\u4e2d\u5fc3\u4e4b\u4e00\u3002\u957f\u6c5f\u6d41\u57df\u7684\u7ecf\u6d4e\u4e3b\u8981\u4ee5\u519c\u4e1a\u4e3a\u4e3b\uff0c\u957f\u6c5f\u6e14\u4e1a\u5360\u636e\u4e86\u4e3b\u8981\u4ea7\u4e1a\uff0c\u5982\u9493\u9c7c\u3001\u730e\u9c7c\u7b49\u3002\u957f\u6c5f\u7684\u6c34\u8d44\u6e90\u4e5f\u975e\u5e38\u4e30\u5bcc\uff0c\u5305\u62ec\u6cb3\u6d41\u3001\u6e56\u6cca\u3001\u6cb3\u6d41\u7b49\u3002\u957f\u6c5f\u7684\u81ea\u7136\u666f\u89c2\u4e5f\u5341\u5206\u58ee\u89c2\uff0c\u957f\u6c5f\u56db\u5b63\u666f\u8272\u5404\u5f02\uff0c\u6625\u5b63\u591a\u96e8\uff0c\u590f\u5b63\u591a\u96e8\uff0c\u79cb\u5b63\u5219\u591a\u96e8\uff0c\u51ac\u5b63\u5219\u5bd2\u51b7\u591a\u96ea\uff0c\u6625\u5b63\u548c\u79cb\u5b63\u90fd\u6709\u7f8e\u4e3d\u7684\u82b1\u6735\u548c\u679c\u5b9e\u3002\u957f\u6c5f\u662f\u4e2d\u56fd\u7684\u7ecf\u6d4e\u4e2d\u5fc3\uff0c\u62e5\u6709\u4f17\u591a\u7684\u56fd\u5bb6\u548c\u57ce\u5e02\uff0c\u662f\u4e16\u754c\u4e0a\u91cd\u8981\u7684\u4ea4\u901a\u67a2\u7ebd\u548c\u822a\u8fd0\u4e2d\u5fc3\u3002\u540c\u65f6\uff0c\u957f\u6c5f\u8fd8\u662f\u4e2d\u56fd\u7684\u91cd\u8981\u6c34\u5229\u8d44\u6e90\uff0c\u5bf9\u4e8e\u4e2d\u56fd\u7684\u7ecf\u6d4e\u53d1\u5c55\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\n[B]: \u662f\u7684\uff0c\u6211\u77e5\u9053\u957f\u6c5f\u3002\u957f\u6c5f\u4f4d\u4e8e\u4e2d\u56fd\u7684\u4e2d\u90e8\uff0c\u662f\u4e2d\u56fd\u7b2c\u4e09\u957f\u7684\u6cb3\u6d41\uff0c\u5168\u957f\u7ea66300\u516c\u91cc\uff0c\u6d41\u7ecf\u6c5f\u897f\u7701\u3001\u5b89\u5fbd\u7701\u3001\u6c5f\u82cf\u7701\u3001\u5b89\u5fbd\u7701\u3001\u6c5f\u82cf\u7701\u548c\u5b89\u5fbd\u7701\u3002\n[C]: \u5f53\u7136\uff0c\u957f\u6c5f\u662f\u4e2d\u56fd\u6700\u957f\u7684\u6cb3\u6d41\uff0c\u5168\u957f\u7ea66300\u516c\u91cc\u3002\u5b83\u4ee5\u5176\u4e30\u5bcc\u7684\u6c34\u8d44\u6e90\u3001\u4e30\u5bcc\u7684\u6c34\u751f\u52a8\u690d\u7269\u8d44\u6e90\u4ee5\u53ca\u72ec\u7279\u7684\u5730\u7406\u4f4d\u7f6e\u800c\u95fb\u540d\u4e8e\u4e16\u3002\n[D]: \u662f\u7684\uff0c\u957f\u6c5f\u662f\u4e2d\u56fd\u6700\u957f\u7684\u6cb3\u6d41\uff0c\u5168\u957f\u8d85\u8fc73300\u5343\u7c73\u3002\n[E]: \u957f\u6c5f\u662f\u4e2d\u56fd\u6700\u957f\u7684\u6cb3\u6d41\uff0c\u603b\u957f\u5ea6\u7ea6\u4e3a6300\u516c\u91cc\u3002\u5b83\u53d1\u6e90\u4e8e\u9752\u85cf\u9ad8\u539f\u7684\u9752\u6d77\u6e56\uff0c\u6d41\u7ecf\u9752\u6d77\u3001\u56db\u5ddd\u3001\u7518\u8083\u3001\u5b81\u590f\u3001\u5185\u8499\u53e4\u3001\u9655\u897f\u548c\u5c71\u897f\u3002\u957f\u6c5f\u6d41\u57df\u662f\u4e2d\u56fd\u6700\u5927\u7684\u519c\u4e1a\u533a\u4e4b\u4e00\uff0c\u4e5f\u662f\u4e2d\u56fd\u7684\u6587\u5316\u548c\u7ecf\u6d4e\u4e2d\u5fc3\u3002\n[F]: \u662f\u7684\uff0c\u957f\u6c5f\u662f\u4e2d\u56fd\u6700\u957f\u7684\u6cb3\u6d41\u4e4b\u4e00\uff0c\u4e5f\u662f\u4e16\u754c\u7b2c\u4e09\u5927\u6cb3\u6d41\u3002\u957f\u6c5f\u53d1\u6e90\u4e8e\u9752\u85cf\u9ad8\u539f\uff0c\u6d41\u7ecf11\u4e2a\u7701\u4efd\uff0c\u6700\u7ec8\u6ce8\u5165\u4e1c\u6d77\u3002\u957f\u6c5f\u6d41\u57df\u662f\u4e2d\u56fd\u6700\u5927\u7684\u6cb3\u6d41\u6d41\u57df\u4e4b\u4e00\uff0c\u62e5\u6709\u7740\u4e30\u5bcc\u7684\u81ea\u7136\u8d44\u6e90\u548c\u6587\u5316\u9057\u4ea7\uff0c\u5982\u957f\u6c5f\u5927\u6865\u3001\u957f\u6c5f\u6d77\u6d0b\u535a\u7269\u9986\u3001\u957f\u6c5f\u53e4\u57ce\u7b49\u3002\n[G]: \u540c\u65f6\u4e5f\u77e5\u9053\uff0c\u4e2d\u56fd\u662f\u4e16\u754c\u4e0a\u6700\u957f\u7684\u57ce\u5e02\u4e4b\u4e00\uff0c\u62e5\u6709\u8d85\u8fc73400\u4e07\u4eba\u53e3\u3002\n[H]: \u662f\u7684\uff0c\u957f\u6c5f\u662f\u4e2d\u56fd\u6700\u957f\u7684\u6cb3\u6d41\uff0c\u4e5f\u662f\u4e16\u754c\u4e0a\u6700\u957f\u7684\u6cb3\u6d41\u4e4b\u4e00\u3002\u5b83\u53d1\u6e90\u4e8e\u9752\u85cf\u9ad8\u539f\uff0c\u6d41\u7ecf\u4e5d\u4e2a\u7701\u533a\uff0c\u6700\u7ec8\u6ce8\u5165\u4e1c\u6d77\u3002\u957f\u6c5f\u6d41\u57df\u7684\u6c14\u5019\u6e29\u548c\uff0c\u56db\u5b63\u5206\u660e\uff0c\u662f\u4e2d\u56fd\u91cd\u8981\u7684\u519c\u4e1a\u548c\u5de5\u4e1a\u57fa\u5730\u3002\u957f\u6c5f\u6d41\u57df\u7684\u6c34\u4ea7\u8d44\u6e90\u4e30\u5bcc\uff0c\u662f\u4e2d\u56fd\u7684\u91cd\u8981\u6e14\u4e1a\u8d44\u6e90\u4e4b\u4e00\u3002\n\n\n[Q]: \u4e16\u754c\u4e0a\u6700\u9ad8\u7684\u5c71\u5cf0\u662f\u4ec0\u4e48\uff1f\n[A]: \u4e16\u754c\u4e0a\u6700\u9ad8\u7684\u5c71\u5cf0\u662f\u73e0\u7a46\u6717\u739b\u5cf0\uff08Mount Everest\uff09\uff0c\u4f4d\u4e8e\u559c\u9a6c\u62c9\u96c5\u5c71\u8109\uff0c\u5c3c\u6cca\u5c14\u548c\u4e2d\u56fd\u7684\u8fb9\u754c\u4e0a\u3002\u5b83\u7684\u6d77\u62d4\u9ad8\u5ea6\u662f8,848.86\u7c73\uff0829,031.7\u82f1\u5c3a\uff09\uff0c\u662f\u5730\u7403\u4e0a\u6700\u9ad8\u7684\u5c71\u5cf0\u3002\n[B]: \u73e0\u7a46\u6717\u739b\u5cf0\uff08Mount Everest\uff09\u662f\u4e16\u754c\u4e0a\u6700\u9ad8\u7684\u5c71\u5cf0\uff0c\u6d77\u62d4\u9ad8\u5ea6\u8fbe8,848\u7c73\uff0829,029\u82f1\u5c3a\uff09\u3002\n[C]: \u73e0\u7a46\u6717\u739b\u5cf0\u662f\u4e16\u754c\u4e0a\u6700\u9ad8\u7684\u5c71\u5cf0\uff0c\u4f4d\u4e8e\u559c\u9a6c\u62c9\u96c5\u5c71\u8109\uff0c\u6d77\u62d48,848.86\u7c73\uff0829,031.7\u82f1\u5c3a\uff09\u3002\u5b83\u4f4d\u4e8e\u5c3c\u6cca\u5c14\u548c\u4e2d\u56fd\u7684\u8fb9\u754c\u4e0a\uff0c\u662f\u559c\u9a6c\u62c9\u96c5\u5c71\u8109\u7684\u4e00\u90e8\u5206\u3002\n[D]: \u4e16\u754c\u4e0a\u6700\u9ad8\u7684\u5c71\u5cf0\u662f\u73e0\u7a46\u6717\u739b\u5cf0\uff0c\u4f4d\u4e8e\u5c3c\u6cca\u5c14\u548c\u4e2d\u56fd\u7684\u8fb9\u5883\u4e0a\uff0c\u6d77\u62d48848\u7c73\u3002\n[E]: \u4e16\u754c\u4e0a\u6700\u9ad8\u7684\u5c71\u5cf0\u662f\u73e0\u7a46\u6717\u739b\u5cf0\uff0c\u4f4d\u4e8e\u559c\u9a6c\u62c9\u96c5\u5c71\u8109\uff0c\u6d77\u62d48,848\u7c73\u3002\n[F]: \u73e0\u7a46\u6717\u739b\u5cf0\u662f\u4e16\u754c\u4e0a\u6700\u9ad8\u7684\u5c71\u5cf0\uff0c\u6d77\u62d48848\u7c73\u3002\n[G]: \u4e16\u754c\u4e0a\u6700\u9ad8\u7684\u5c71\u5cf0\u662f\u73e0\u7a46\u6717\u739b\u5cf0\uff0c\u4f4d\u4e8e\u559c\u9a6c\u62c9\u96c5\u5c71\u8109\uff0c\u6d77\u62d48,848.86\u7c73\uff0829,031.69\u82f1\u5c3a\uff09\u3002\n[H]: \u4e16\u754c\u4e0a\u6700\u9ad8\u7684\u5c71\u5cf0\u662f\u73e0\u7a46\u6717\u739b\u5cf0\uff0c\u4e5f\u79f0\u4e3a\u8428\u52a0\u739b\u5854\uff0c\u4f4d\u4e8e\u559c\u9a6c\u62c9\u96c5\u5c71\u8109\u7684\u5c3c\u6cca\u5c14\u548c\u4e2d\u56fd\u8fb9\u5883\u4e4b\u95f4\uff0c\u6d77\u62d48,848.86\u7c73\uff0829,031.69\u82f1\u5c3a\uff09\u3002\u5b83\u662f\u4e16\u754c\u4e0a\u6700\u8457\u540d\u7684\u767b\u5c71\u76ee\u6807\u4e4b\u4e00\uff0c\u5438\u5f15\u7740\u6765\u81ea\u4e16\u754c\u5404\u5730\u7684\u767b\u5c71\u8005\u524d\u6765\u6311\u6218\u3002\u73e0\u7a46\u6717\u739b\u5cf0\u7684\u540d\u5b57\u6765\u81ea\u85cf\u8bed\uff0c\u610f\u601d\u662f\u201c\u5973\u795e\u7684\u989d\u5cf0\u201d\uff0c\u5b83\u662f\u7531\u4e24\u4e2a\u4e0d\u540c\u7684\u5c71\u5cf0\u7ec4\u6210\u7684\uff0c\u5176\u4e2d\u4e00\u4e2a\u662f\u4f4d\u4e8e\u5c3c\u6cca\u5c14\u7684\u73e0\u7a46\u6717\u739b\u5cf0\uff0c\u53e6\u4e00\u4e2a\u5219\u4f4d\u4e8e\u4e2d\u56fd\u3002\u5b83\u7684\u9ad8\u5ea6\u662f\u901a\u8fc7\u6d4b\u91cf\u6d77\u62d4\u9ad8\u5ea6\u6765\u786e\u5b9a\u7684\uff0c\u56e0\u4e3a\u5b83\u662f\u901a\u8fc7\u6d4b\u91cf\u5730\u7403\u7684\u5e73\u5747\u9ad8\u5ea6\u6765\u786e\u5b9a\u7684\u3002\u5c3d\u7ba1\u73e0\u7a46\u6717\u739b\u5cf0\u662f\u4e16\u754c\u4e0a\u6700\u9ad8\u7684\u5c71\u5cf0\uff0c\u4f46\u5b83\u5e76\u4e0d\u662f\u5730\u7403\u4e0a\u4efb\u4f55\u5176\u4ed6\u5730\u65b9\u90fd\u80fd\u5230\u8fbe\u7684\u3002\u5730\u7403\u4e0a\u7684\u5176\u4ed6\u5730\u65b9\uff0c\u5982\u5357\u7f8e\u6d32\u7684\u5b89\u7b2c\u65af\u5c71\u8109\uff0c\u4e5f\u662f\u4e16\u754c\u4e0a\u6700\u9ad8\u7684\u5c71\u8109\uff0c\u4f46\u5b83\u5e76\u4e0d\u662f\u5730\u7403\u4e0a\u6700\u9ad8\u7684\u5c71\u5cf0\u3002\u603b\u4e4b\uff0c\u73e0\u7a46\u6717\u739b\u5cf0\u662f\u4e16\u754c\u4e0a\u6700\u9ad8\u7684\u5c71\u5cf0\uff0c\u4f46\u5b83\u5e76\u4e0d\u662f\u5730\u7403\u4e0a\u4efb\u4f55\u5730\u65b9\u90fd\u80fd\u5230\u8fbe\u7684\u3002\u5b83\u662f\u5730\u7403\u4e0a\u6700\u9ad8\u7684\u5c71\u8109\uff0c\u4f46\u5b83\u5e76\u4e0d\u662f\u5730\u7403\u4e0a\u4efb\u4f55\u5730\u65b9\u90fd\u80fd\u5230\u8fbe\u7684\u3002\n\n\n[Q]: \u4e07\u6709\u5f15\u529b\u662f\u8c01\u63d0\u51fa\u7684\uff1f\n[A]: \u4e07\u6709\u5f15\u529b\u662f\u7531\u725b\u987f\u572817\u4e16\u7eaa\u63d0\u51fa\u7684\uff0c\u5b83\u662f\u725b\u987f\u4e09\u5927\u5b9a\u5f8b\u4e4b\u4e00\u3002\u725b\u987f\u572817\u4e16\u7eaa\u63d0\u51fa\u7684\u5f15\u529b\u5b9a\u5f8b\u63ed\u793a\u4e86\u4e07\u6709\u5f15\u529b\u5b9a\u5f8b\u7684\u672c\u8d28\uff0c\u5e76\u4e14\u89e3\u91ca\u4e86\u884c\u661f\u3001\u536b\u661f\u3001\u5f57\u661f\u7b49\u5929\u4f53\u4e4b\u95f4\u5982\u4f55\u76f8\u4e92\u4f5c\u7528\u3002\u8fd9\u4e2a\u5b9a\u5f8b\u662f\u7531\u827e\u8428\u514b\u00b7\u725b\u987f\u57281687\u5e74\u63d0\u51fa\u7684\uff0c\u4ed6\u901a\u8fc7\u5bf9\u4e07\u6709\u5f15\u529b\u5b9a\u5f8b\u7684\u5206\u6790\u548c\u5b9e\u9a8c\uff0c\u6210\u529f\u5730\u89e3\u91ca\u4e86\u5929\u4f53\u4e4b\u95f4\u7684\u5f15\u529b\u4f5c\u7528\uff0c\u4ece\u800c\u89e3\u91ca\u4e86\u5730\u7403\u7ed5\u592a\u9633\u65cb\u8f6c\u7684\u73b0\u8c61\u3002\n[B]: \u4e07\u6709\u5f15\u529b\uff08Human Policy\uff09\u662f\u725b\u987f\u7684\u4e09\u5927\u529b\u5b66\u5b9a\u5f8b\uff0c\u7528\u4e8e\u63cf\u8ff0\u4e24\u4e2a\u7269\u4f53\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u529b\u3002\u5b83\u7531\u4e07\u6709\u5f15\u529b\u5b9a\u5f8b\u548c\u725b\u987f\u7684\u7b2c\u4e00\u5b9a\u5f8b\uff08\u60ef\u6027\u5b9a\u5f8b\uff09\u6784\u6210\u3002\u4e07\u6709\u5f15\u529b\u662f\u7531\u4e8e\u4e24\u4e2a\u7269\u4f53\u4e4b\u95f4\u76f8\u4e92\u4f5c\u7528\u800c\u4ea7\u751f\u7684\uff0c\u5b83\u4eec\u7684\u8d28\u91cf\u3001\u8ddd\u79bb\u3001\u901f\u5ea6\u548c\u65b9\u5411\u7b49\u56e0\u7d20\u51b3\u5b9a\u4e86\u5b83\u4eec\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002\u8fd9\u79cd\u76f8\u4e92\u4f5c\u7528\u529b\u662f\uff1a\u8d28\u91cf\u7684\u4e58\u79ef\uff1a\u8d28\u91cf\u7684\u4e58\u79ef\u662f\u4e07\u6709\u5f15\u529b\u5b9a\u5f8b\u548c\u725b\u987f\u7b2c\u4e8c\u5b9a\u5f8b\u7684\u4e58\u79ef\u3002\u8d28\u91cf\u7684\u4e58\u79ef\uff1a\u8d28\u91cf\u7684\u4e58\u79ef\u662f\u7269\u4f53\u4e4b\u95f4\u7684\u4e00\u79cd\u7269\u7406\u8fc7\u7a0b\uff0c\u5b83\u662f\u7269\u4f53\u95f4\u76f8\u4e92\u4f5c\u7528\u7684\u4ea7\u7269\uff0c\u5bf9\u4e8e\u5176\u4ed6\u7269\u4f53\u6765\u8bf4\uff0c\u5176\u8d28\u91cf\u53ef\u4ee5\u8868\u793a\u4e3a\u4e24\u4e2a\u7269\u4f53\u4e4b\u95f4\u7684\u8d28\u91cf\u4e4b\u548c\uff0c\u5373\u91cd\u529b\u52a0\u901f\u5ea6a\u3002\n[C]: \u4e07\u6709\u5f15\u529b\u662f\u4e00\u4e2a\u7531\u725b\u987f\u57281687\u5e74\u63d0\u51fa\u7684\u7269\u7406\u5b66\u6982\u5ff5\uff0c\u5b83\u63cf\u8ff0\u4e86\u7269\u4f53\u4e4b\u95f4\u5f15\u529b\u5982\u4f55\u76f8\u4e92\u4f5c\u7528\u7684\u89c4\u5f8b\u3002\n[D]: \u4e07\u6709\u5f15\u529b\u662f\u7531\u82f1\u56fd\u7269\u7406\u5b66\u5bb6\u725b\u987f\u63d0\u51fa\u7684\u3002\u4ed6\u8ba4\u4e3a\uff0c\u6240\u6709\u7269\u4f53\u90fd\u4f1a\u4ea7\u751f\u5f15\u529b\uff0c\u8fd9\u4e2a\u5f15\u529b\u662f\u7531\u7269\u4f53\u7684\u8d28\u91cf\u548c\u8ddd\u79bb\u51b3\u5b9a\u7684\u3002\u8fd9\u4e2a\u7406\u8bba\u63ed\u793a\u4e86\u8d28\u91cf\u548c\u8ddd\u79bb\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u8bc1\u660e\u4e86\u4e07\u6709\u5f15\u529b\u7684\u672c\u8d28\u3002\n[E]: \u4e07\u6709\u5f15\u529b\u662f\u7531\u725b\u987f\u57281687\u5e74\u63d0\u51fa\u7684\u3002\u5b83\u662f\u5b87\u5b99\u4e2d\u4e00\u4e2a\u57fa\u672c\u7684\u7269\u7406\u73b0\u8c61\uff0c\u5b83\u63cf\u8ff0\u4e86\u7269\u4f53\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002\u8fd9\u4e2a\u529b\u88ab\u79f0\u4e3a\u4e07\u6709\u5f15\u529b\uff0c\u5b83\u5728\u6240\u6709\u7269\u4f53\u4e4b\u95f4\u90fd\u5b58\u5728\uff0c\u8fd9\u79cd\u529b\u4e0e\u7269\u4f53\u7684\u8d28\u91cf\u548c\u8ddd\u79bb\u6709\u5173\u3002\u4e07\u6709\u5f15\u529b\u662f\u81ea\u7136\u754c\u4e2d\u6700\u57fa\u672c\u7684\u529b\u91cf\u4e4b\u4e00\uff0c\u5b83\u4e0d\u4ec5\u5f71\u54cd\u7740\u7269\u4f53\u7684\u8fd0\u52a8\u548c\u53d8\u5316\uff0c\u8fd8\u5bf9\u5b87\u5b99\u7684\u7ed3\u6784\u548c\u6f14\u5316\u4ea7\u751f\u7740\u91cd\u8981\u7684\u5f71\u54cd\u3002\n[F]: \u4e07\u6709\u5f15\u529b\u662f\u82f1\u56fd\u7269\u7406\u5b66\u5bb6\u827e\u8428\u514b\u00b7\u725b\u987f\u572817\u4e16\u7eaa\u63d0\u51fa\u7684\u3002\n[G]: \u5f15\u529b\u662f\u5b87\u5b99\u4e2d\u4e00\u79cd\u529b\u91cf\uff0c\u5b83\u662f\u7531\u8d28\u91cf\u6216\u80fd\u91cf\u6240\u7ec4\u6210\u7684\u3002\u6839\u636e\u725b\u987f\u7684\u4e07\u6709\u5f15\u529b\u5b9a\u5f8b\uff0c\u4e00\u4e2a\u7269\u4f53\u4e4b\u95f4\u7684\u5f15\u529b\u5927\u5c0f\u4e0e\u5b83\u7684\u8d28\u91cf\u6210\u6b63\u6bd4\uff0c\u4e0e\u5b83\u7684\u8d28\u91cf\u6210\u53cd\u6bd4\u3002\u56e0\u6b64\uff0c\u725b\u987f\u7684\u4e07\u6709\u5f15\u529b\u5b9a\u5f8b\u8868\u660e\uff0c\u4e00\u4e2a\u7269\u4f53\u4e4b\u95f4\u7684\u5f15\u529b\u5927\u5c0f\u4e0e\u5b83\u7684\u8d28\u91cf\u6210\u6b63\u6bd4\uff0c\u4e0e\u5b83\u7684\u8d28\u91cf\u6210\u53cd\u6bd4\u3002\u8fd9\u4e2a\u5b9a\u5f8b\u5728\u7269\u7406\u5b66\u3001\u5929\u6587\u5b66\u548c\u7269\u7406\u5b66\u4e2d\u90fd\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u3002\n[H]: \u4e07\u6709\u5f15\u529b\u662f\u7531\u82f1\u56fd\u7269\u7406\u5b66\u5bb6\u827e\u8428\u514b\u00b7\u725b\u987f\u4e8e1687\u5e74\u63d0\u51fa\u7684\u3002\u4ed6\u57281687\u5e74\u53d1\u8868\u4e86\u4e00\u7bc7\u8457\u540d\u7684\u8bba\u6587\uff0c\u9898\u4e3a\u300a\u81ea\u7136\u54f2\u5b66\u7684\u6570\u5b66\u539f\u7406\u300b\uff0c\u5176\u4e2d\u63d0\u51fa\u4e86\u4e07\u6709\u5f15\u529b\u5b9a\u5f8b\uff0c\u6307\u51fa\u4efb\u4f55\u4e24\u4e2a\u7269\u4f53\u4e4b\u95f4\u90fd\u5b58\u5728\u7740\u4e00\u79cd\u5f15\u529b\uff0c\u8fd9\u79cd\u5f15\u529b\u7684\u5927\u5c0f\u4e0e\u5b83\u4eec\u7684\u8d28\u91cf\u6210\u6b63\u6bd4\uff0c\u4e0e\u5b83\u4eec\u4e4b\u95f4\u7684\u8ddd\u79bb\u7684\u5e73\u65b9\u6210\u53cd\u6bd4\u3002\u8fd9\u4e2a\u5b9a\u5f8b\u88ab\u79f0\u4e3a\u4e07\u6709\u5f15\u529b\u5b9a\u5f8b\uff0c\u5b83\u63cf\u8ff0\u4e86\u8d28\u91cf\u548c\u8ddd\u79bb\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u5b83\u8868\u660e\uff0c\u4e24\u4e2a\u7269\u4f53\u4e4b\u95f4\u7684\u5f15\u529b\u4e0e\u5b83\u4eec\u7684\u8d28\u91cf\u6210\u6b63\u6bd4\uff0c\u4e0e\u5b83\u4eec\u4e4b\u95f4\u7684\u8ddd\u79bb\u7684\u5e73\u65b9\u6210\u53cd\u6bd4\u3002\u8fd9\u4e2a\u5b9a\u5f8b\u5bf9\u4e8e\u89e3\u91ca\u5b87\u5b99\u4e2d\u7684\u8bb8\u591a\u73b0\u8c61\u548c\u73b0\u8c61\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u975e\u5e38\u91cd\u8981\uff0c\u4f8b\u5982\u884c\u661f\u7ed5\u592a\u9633\u8fd0\u52a8\u3001\u6052\u661f\u4e4b\u95f4\u7684\u5f15\u529b\u900f\u955c\u6548\u5e94\u7b49\u7b49\u3002\u5b83\u4e5f\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5929\u6587\u5b66\u3001\u7269\u7406\u5b66\u548c\u5de5\u7a0b\u5b66\u7b49\u9886\u57df\uff0c\u4e3a\u6211\u4eec\u7406\u89e3\u5b87\u5b99\u7684\u672c\u8d28\u548c\u6f14\u5316\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u7406\u8bba\u57fa\u7840\u3002\n\n\n[Q]: \u4f60\u77e5\u9053\u5927\u718a\u732b\u7684\u4e3b\u8981\u98df\u7269\u662f\u4ec0\u4e48\u5417\uff1f\n[A]: \u5927\u718a\u732b\u4e3b\u8981\u4ee5\u7af9\u5b50\u4e3a\u98df\uff0c\u4f46\u4e5f\u4f1a\u5403\u5176\u4ed6\u690d\u7269\u3001\u6606\u866b\u3001\u5c0f\u578b\u54fa\u4e73\u52a8\u7269\u548c\u9c7c\u7c7b\u3002\u5b83\u4eec\u7684\u6d88\u5316\u7cfb\u7edf\u4e0e\u5176\u4ed6\u718a\u79d1\u52a8\u7269\u7c7b\u4f3c\uff0c\u80fd\u591f\u6d88\u5316\u7af9\u5b50\u3002\u5927\u718a\u732b\u7684\u7259\u9f7f\u950b\u5229\uff0c\u9002\u5408\u5480\u56bc\u7af9\u5b50\uff0c\u56e0\u6b64\u5b83\u4eec\u7ecf\u5e38\u88ab\u79f0\u4e3a\u201c\u7af9\u98df\u52a8\u7269\u201d\u3002\n[B]: \u5927\u718a\u732b\u662f\u4ee5\u7af9\u5b50\u4e3a\u98df\u7684\u54fa\u4e73\u52a8\u7269\uff0c\u5b83\u4eec\u662f\u8089\u98df\u52a8\u7269\u3002\u5b83\u4eec\u4e3b\u8981\u98df\u7269\u662f\u7af9\u5b50\uff0c\u7279\u522b\u662f\u7af9\u5b50\u4e2d\u5bcc\u542b\u7ea4\u7ef4\u7d20\u548c\u77ff\u7269\u8d28\uff0c\u5e76\u4e14\u4e0d\u4f1a\u76f4\u63a5\u8fdb\u5165\u6d88\u5316\u7cfb\u7edf\u3002\u5927\u718a\u732b\u7684\u80a0\u9053\u4e3b\u8981\u7531\u7af9\u5b50\u7ec4\u6210\uff0c\u5b83\u4eec\u9700\u8981\u6d88\u5316\u7af9\u5b50\u4e2d\u7684\u7ea4\u7ef4\u7d20\u548c\u5176\u4ed6\u7ea4\u7ef4\u7d20\u6765\u652f\u6301\u5b83\u4eec\u7684\u6b63\u5e38\u8fdb\u98df\u3002\n[C]: \u5927\u718a\u732b\u4e3b\u8981\u4ee5\u7af9\u5b50\u4e3a\u98df\uff0c\u4f46\u5b83\u4eec\u4e5f\u5403\u5176\u4ed6\u690d\u7269\uff0c\u5305\u62ec\u6811\u53f6\u3001\u5ae9\u82bd\u3001\u6606\u866b\u3001\u6c34\u679c\u3001\u852c\u83dc\u548c\u5c0f\u578b\u54fa\u4e73\u52a8\u7269\u7b49\u3002\u5927\u718a\u732b\u662f\u4e2d\u56fd\u7279\u6709\u7684\u7269\u79cd\uff0c\u5c5e\u4e8e\u718a\u79d1\uff0c\u4e3b\u8981\u5206\u5e03\u5728\u4e2d\u56fd\u7684\u56db\u5ddd\u3001\u9655\u897f\u548c\u7518\u8083\u7b49\u5730\u3002\n[D]: \u5927\u718a\u732b\u7684\u4e3b\u8981\u98df\u7269\u662f\u7af9\u5b50\u3002\u5b83\u4eec\u901a\u5e38\u5403\u7af9\u5b50\uff0c\u6bcf\u5929\u9700\u8981\u5403\u6389\u5927\u7ea630\u516c\u65a4\u7684\u7af9\u5b50\u3002\n[E]: \u662f\u7684\uff0c\u5927\u718a\u732b\u4e3b\u8981\u4ee5\u7af9\u5b50\u4e3a\u98df\u3002\u5b83\u4eec\u6bcf\u5929\u9700\u8981\u5403\u6389\u5927\u91cf\u7684\u7af9\u5b50\u6765\u7ef4\u6301\u751f\u547d\uff0c\u6bcf\u5929\u9700\u8981\u5403\u638940\u81f380\u516c\u65a4\u7684\u7af9\u5b50\u3002\u6b64\u5916\uff0c\u5b83\u4eec\u4e5f\u4f1a\u5403\u4e00\u4e9b\u5176\u4ed6\u7684\u690d\u7269\uff0c\u5982\u8349\u3001\u679c\u5b9e\u3001\u575a\u679c\u548c\u5c0f\u578b\u54fa\u4e73\u52a8\u7269\u3002\n[F]: \u5927\u718a\u732b\u4e3b\u8981\u4ee5\u7af9\u5b50\u4e3a\u98df\uff0c\u4e5f\u4f1a\u5076\u5c14\u98df\u7528\u5176\u4ed6\u7684\u690d\u7269\u548c\u52a8\u7269\u3002\u6b64\u5916\uff0c\u5927\u718a\u732b\u4e5f\u4f1a\u5403\u4e00\u4e9b\u91ce\u679c\u3001\u5c0f\u52a8\u7269\u548c\u690d\u7269\uff0c\u4f46\u8fd9\u4e9b\u90fd\u4e0d\u5c5e\u4e8e\u5b83\u4eec\u7684\u4e3b\u8981\u98df\u7269\u6765\u6e90\u3002\n[G]: \u5927\u718a\u732b\u7684\u4e3b\u8981\u98df\u7269\u662f\u7af9\u5b50\u3002\n[H]: \u5927\u718a\u732b\u662f\u4e00\u79cd\u4ee5\u7af9\u5b50\u4e3a\u4e3b\u8981\u98df\u7269\u7684\u690d\u98df\u52a8\u7269\uff0c\u5b83\u4eec\u7684\u80a0\u9053\u7ed3\u6784\u9002\u5408\u6d88\u5316\u7ea4\u7ef4\u7d20\u542b\u91cf\u9ad8\u7684\u7af9\u5b50\uff0c\u4f46\u4e5f\u4f1a\u5403\u4e00\u4e9b\u5176\u4ed6\u690d\u7269\uff0c\u5982\u8349\u672c\u690d\u7269\u548c\u5c0f\u578b\u54fa\u4e73\u52a8\u7269\u3002\u5927\u718a\u732b\u7684\u80a0\u9053\u7ed3\u6784\u4e5f\u9002\u5e94\u4e86\u9ad8\u7ea4\u7ef4\u7684\u7af9\u5b50\uff0c\u8fd9\u4f7f\u5f97\u5b83\u4eec\u80fd\u591f\u6d88\u5316\u5927\u91cf\u7684\u7ea4\u7ef4\u7d20\uff0c\u4ece\u800c\u5e2e\u52a9\u5b83\u4eec\u5728\u91ce\u5916\u751f\u5b58\u3002\u5927\u718a\u732b\u7684\u80a0\u9053\u4e2d\u8fd8\u542b\u6709\u4e00\u79cd\u53eb\u505a\u201c\u7ea4\u7ef4\u7d20\u9176\u201d\u7684\u9176\u7c7b\u7269\u8d28\uff0c\u8fd9\u79cd\u9176\u80fd\u591f\u5e2e\u52a9\u5927\u718a\u732b\u5206\u89e3\u7ea4\u7ef4\u7d20\uff0c\u4ece\u800c\u5e2e\u52a9\u5b83\u4eec\u6d88\u5316\u5927\u91cf\u7684\u6c34\u5206\u548c\u8425\u517b\u7269\u8d28\u3002\u6b64\u5916\uff0c\u5927\u718a\u732b\u7684\u80a0\u9053\u8fd8\u542b\u6709\u4e00\u79cd\u53eb\u505a\u201c\u7ea4\u7ef4\u7d20\u86cb\u767d\u201d\u7684\u86cb\u767d\u8d28\uff0c\u8fd9\u79cd\u86cb\u767d\u8d28\u80fd\u591f\u5e2e\u52a9\u5927\u718a\u732b\u7ef4\u6301\u4f53\u5185\u7684\u80fd\u91cf\u6c34\u5e73\uff0c\u4ece\u800c\u5e2e\u52a9\u5b83\u4eec\u5728\u91ce\u5916\u751f\u5b58\u3002\u603b\u7684\u6765\u8bf4\uff0c\u5927\u718a\u732b\u7684\u4e3b\u8981\u98df\u7269\u662f\u7af9\u5b50\uff0c\u4f46\u5b83\u4eec\u4e5f\u4f1a\u5403\u4e00\u4e9b\u5176\u4ed6\u690d\u7269\uff0c\u5982\u8349\u672c\u690d\u7269\u548c\u5c0f\u578b\u54fa\u4e73\u52a8\u7269\u3002\n\n\n[Q]: \u6d77\u6c34\u4e3a\u4ec0\u4e48\u662f\u54b8\u7684\uff1f\n[A]: \u6d77\u6c34\u4e4b\u6240\u4ee5\u662f\u54b8\u7684\uff0c\u4e3b\u8981\u662f\u7531\u4e8e\u5176\u4e2d\u7684\u5316\u5b66\u6210\u5206\u3002\u6d77\u6c34\u4e2d\u542b\u6709\u5927\u91cf\u7684\u77ff\u7269\u8d28\u548c\u76d0\u5206\uff0c\u5176\u4e2d\u4e3b\u8981\u6210\u5206\u662f\u786b\u9178\u76d0\u3001\u78b3\u9178\u76d0\u3001\u9541\u79bb\u5b50\u548c\u94be\u79bb\u5b50\u3002\u8fd9\u4e9b\u7269\u8d28\u5728\u9ad8\u6d53\u5ea6\u4e0b\uff0c\u4f1a\u6c89\u6dc0\u5728\u6d77\u6c34\u4e2d\uff0c\u5f62\u6210\u6d77\u6c34\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6d77\u6c34\u4e2d\u7684\u77ff\u7269\u8d28\u548c\u76d0\u5206\u4e3b\u8981\u6765\u81ea\u4e8e\u5ca9\u77f3\u3001\u571f\u58e4\u3001\u751f\u7269\u548c\u4eba\u7c7b\u6392\u6cc4\u7269\u7b49\u3002\u5ca9\u77f3\u662f\u6d77\u6c34\u7684\u7ec4\u6210\u90e8\u5206\uff0c\u4e3b\u8981\u7531\u77ff\u7269\u8d28\u7ec4\u6210\uff0c\u5982\u77f3\u82f1\u3001\u957f\u77f3\u3001\u4e91\u6bcd\u7b49\u3002\u571f\u58e4\u4e2d\u542b\u6709\u5927\u91cf\u7684\u5316\u5b66\u7269\u8d28\uff0c\u5982\u6c2e\u3001\u78f7\u3001\u94be\u7b49\uff0c\u8fd9\u4e9b\u7269\u8d28\u4f1a\u6e17\u900f\u5230\u571f\u58e4\u4e2d\uff0c\u5f62\u6210\u571f\u58e4\u3002\u751f\u7269\u4f53\u5185\u7684\u76d0\u5206\u4e3b\u8981\u6765\u6e90\u4e8e\u690d\u7269\u7684\u53f6\u7eff\u7d20\u7b49\u8272\u7d20\uff0c\u8fd9\u4e9b\u8272\u7d20\u4f1a\u5438\u6536\u592a\u9633\u5149\u4e2d\u7684\u80fd\u91cf\uff0c\u8f6c\u5316\u4e3a\u751f\u7269\u4f53\u53ef\u4ee5\u5229\u7528\u7684\u80fd\u91cf\u3002\u800c\u4eba\u7c7b\u6392\u6cc4\u7269\u4e2d\u542b\u6709\u7684\u76d0\u5206\u4e3b\u8981\u6765\u6e90\u4e8e\u77ff\u7269\u8d28\u548c\u78b3\u9178\u76d0\uff0c\u8fd9\u4e9b\u77ff\u7269\u8d28\u5728\u6c34\u4e2d\u7684\u6d53\u5ea6\u8f83\u9ad8\uff0c\u4f1a\u5bfc\u81f4\u6c34\u4e2d\u6eb6\u89e3\u7684\u77ff\u7269\u8d28\u6d53\u5ea6\u589e\u52a0\uff0c\u5f62\u6210\u6c34\u4f53\u4e2d\u7684\u76d0\u5206\u3002\u56e0\u6b64\uff0c\u6d77\u6c34\u4e4b\u6240\u4ee5\u662f\u54b8\u7684\uff0c\u662f\u7531\u4e8e\u5176\u4e2d\u7684\u5316\u5b66\u6210\u5206\u548c\u7ed3\u6784\u7279\u5f81\u3002\u6d77\u6c34\u4e2d\u7684\u77ff\u7269\u8d28\u548c\u76d0\u5206\u4e3b\u8981\u6765\u81ea\u4e8e\u5ca9\u77f3\u3001\u571f\u58e4\u3001\u751f\u7269\u548c\u4eba\u7c7b\u6392\u6cc4\u7269\u7b49\uff0c\u800c\u8fd9\u4e9b\u7269\u8d28\u7684\u79ef\u7d2f\u548c\u6392\u6cc4\u4f1a\u5bfc\u81f4\u6d77\u6c34\u5448\u73b0\u51fa\u54b8\u5473\u3002\n[B]: \u6d77\u6c34\u662f\u54b8\u7684\uff0c\u56e0\u4e3a\u5b83\u5305\u542b\u4e86\u591a\u79cd\u7269\u8d28\uff0c\u5305\u62ec\u6c2f\u5316\u94a0\uff08NaCl\uff09\u3001\u6c2f\u5316\u94a0\uff08NaCl\uff09\u3001\u78b3\u9178\u76d0\uff08\u5982\u786b\u9178\u76d0\uff09\u3001\u6c2f\u5316\u94a0\uff08NaCl\uff09\u548c\u6c34\u3002\u54b8\u7269\u8d28\u7684\u6765\u6e90\uff1a\u76d0\u662f\u54b8\u7684\u5e38\u89c1\u6765\u6e90\uff0c\u542b\u6709\u94be\u3001\u9499\u3001\u9541\u3001\u94be\u7b49\u77ff\u7269\u8d28\u3002\u8fd9\u4e9b\u77ff\u7269\u8d28\u901a\u5e38\u6765\u81ea\u4e8e\u5730\u4e0b\u6c34\u3001\u5730\u4e0b\u6c34\u6216\u571f\u58e4\u4e2d\uff0c\u8fd9\u4e9b\u77ff\u7269\u8d28\u7684\u542b\u91cf\u8fdc\u4f4e\u4e8e\u7eaf\u51c0\u6c34\u3002\u54b8\u5473\u7684\u539f\u56e0\uff1a\u76d0\u7684\u54b8\u5473\u4e3b\u8981\u6e90\u4e8e\u5176\u6eb6\u89e3\u4e8e\u6c34\u4e2d\u7684\u5fae\u5c0f\u9897\u7c92\u7269\u3002\u8fd9\u4e9b\u5c0f\u9897\u7c92\u7269\u4e3b\u8981\u901a\u8fc7\u690d\u7269\u6839\u90e8\u548c\u52a8\u7269\u4f53\u5185\u7684\u5fae\u751f\u7269\u4f20\u64ad\uff0c\u5982\u6606\u866b\u3001\u5c0f\u578b\u54fa\u4e73\u52a8\u7269\u3001\u9e1f\u7c7b\u548c\u67d0\u4e9b\u6606\u866b\u3002\u7efc\u4e0a\u6240\u8ff0\uff0c\u6d77\u6c34\u662f\u54b8\u7684\uff0c\u56e0\u4e3a\u5b83\u542b\u6709\u5927\u91cf\u7684\u76d0\u7c7b\uff0c\u5305\u62ec\u77ff\u7269\u8d28\u3001\u5ca9\u77f3\u3001\u76d0\u3001\u77ff\u7269\u8d28\u7b49\uff0c\u8fd9\u4e9b\u7269\u8d28\u5728\u6d77\u6c34\u4e2d\u7684\u6d53\u5ea6\u9ad8\uff0c\u5bf9\u54b8\u5473\u7684\u4ea7\u751f\u8f83\u5927\u5f71\u54cd\u3002\n[C]: \u6d77\u6c34\u662f\u54b8\u7684\uff0c\u4e3b\u8981\u662f\u56e0\u4e3a\u5176\u4e2d\u542b\u6709\u5404\u79cd\u76d0\u5206\uff0c\u5176\u4e2d\u4ee5\u6d77\u6c34\u4e3a\u4e3b\u8981\u6210\u5206\u4e4b\u4e00\u3002\u5f53\u6d77\u6c34\u4e2d\u542b\u6709\u5927\u91cf\u7684\u76d0\u5206\u65f6\uff0c\u76d0\u5206\u4f1a\u88ab\u6eb6\u89e3\u5728\u6d77\u6c34\u4e2d\uff0c\u5f62\u6210\u54b8\u5473\u3002\u6d77\u6c34\u4e2d\u7684\u76d0\u5206\u4e3b\u8981\u6765\u6e90\u4e8e\u6eb6\u89e3\u76d0\u7c7b\u3001\u84b8\u53d1\u6790\u51fa\u7684\u76d0\u548c\u6d77\u6c34\u51bb\u7ed3\u6210\u51b0\u7b49\u56e0\u7d20\u3002\n[D]: \u6d77\u6c34\u4e2d\u542b\u6709\u8bb8\u591a\u76d0\u5206\uff0c\u8fd9\u4e9b\u76d0\u5206\u53ef\u4ee5\u5bfc\u81f4\u6d77\u6c34\u54b8\u5ea6\u7684\u589e\u52a0\u3002\u6d77\u6c34\u4e2d\u7684\u76d0\u5206\u4e3b\u8981\u6765\u81ea\u4e8e\u6d77\u6c34\u4e2d\u7684\u76d0\u5206\uff0c\u8fd9\u4e9b\u76d0\u5206\u53ef\u4ee5\u4ece\u6d77\u6c34\u4e2d\u7684\u76d0\u5206\u4e2d\u83b7\u5f97\u3002\u6d77\u6c34\u4e2d\u7684\u6c27\u5316\u7269\u548c\u76d0\u5206\u53ef\u4ee5\u4e0e\u6d77\u6c34\u4e2d\u7684\u76d0\u5206\u5f62\u6210\u6c22\u952e\uff0c\u4ece\u800c\u4f7f\u6d77\u6c34\u54b8\u5ea6\u589e\u52a0\u3002\n[E]: \u6d77\u6c34\u662f\u54b8\u7684\uff0c\u8fd9\u662f\u56e0\u4e3a\u5b83\u542b\u6709\u5927\u91cf\u7684\u76d0\u5206\uff0c\u5176\u4e2d\u5305\u62ec\u6c2f\u79bb\u5b50\u548c\u6c2f\u79bb\u5b50\u3002\u8fd9\u4e9b\u79bb\u5b50\u5728\u6d77\u6d0b\u4e2d\u5f62\u6210\u4e86\u5927\u91cf\u7684\u76d0\uff0c\u8fd9\u4e9b\u76d0\u5206\u88ab\u79f0\u4e3a\u76d0\u5206\u3002\u6d77\u6c34\u4e2d\u542b\u6709\u8bb8\u591a\u6eb6\u89e3\u5728\u6c34\u4e2d\u7684\u76d0\uff0c\u8fd9\u4e9b\u76d0\u5206\u4f1a\u968f\u7740\u6d77\u6c34\u7684\u5347\u9ad8\u800c\u9010\u6e10\u84b8\u53d1\uff0c\u6700\u7ec8\u5bfc\u81f4\u6d77\u6c34\u4e2d\u76d0\u5206\u7684\u84b8\u53d1\u3002\u8fd9\u4e9b\u76d0\u5206\u5728\u6d77\u6c34\u4e2d\u88ab\u5b8c\u5168\u6d88\u8017\uff0c\u7136\u540e\u88ab\u6392\u9664\u5728\u6d77\u6c34\u4e2d\u3002\u6b64\u5916\uff0c\u6d77\u6c34\u4e2d\u8fd8\u542b\u6709\u8bb8\u591a\u5176\u4ed6\u7684\u76d0\u5206\uff0c\u4f46\u8fd9\u4e9b\u76d0\u5206\u662f\u4e0d\u540c\u7684\uff0c\u56e0\u4e3a\u5b83\u4eec\u5728\u6c34\u4e2d\u7684\u6d6e\u8d77\u548c\u4e0b\u964d\u8fc7\u7a0b\u662f\u4e0d\u540c\u7684\u3002\u6d77\u6c34\u4e2d\u7684\u76d0\u5206\u662f\u4ece\u6d77\u5e95\u7684\u5ca9\u77f3\u4e2d\u63d0\u53d6\u51fa\u6765\u7684\uff0c\u8fd9\u4e9b\u76d0\u5206\u5728\u6d77\u6c34\u4e2d\u53ef\u4ee5\u88ab\u5229\u7528\u3002\u603b\u7684\u6765\u8bf4\uff0c\u6d77\u6c34\u662f\u54b8\u7684\uff0c\u8fd9\u4e5f\u662f\u4e3a\u4ec0\u4e48\u6d77\u6c34\u4e2d\u542b\u6709\u8bb8\u591a\u76d0\u5206\u548c\u76d0\u5206\u7684\u539f\u56e0\u3002\n[F]: \u6d77\u6c34\u542b\u6709\u8bb8\u591a\u6eb6\u89e3\u7684\u76d0\u7c7b\uff0c\u5176\u4e2d\u6700\u4e3b\u8981\u7684\u662f\u6c2f\u5316\u94a0\u3002\u5f53\u6c34\u4e2d\u52a0\u5165\u76d0\u65f6\uff0c\u76d0\u7684\u5206\u5b50\u4f1a\u4e0e\u6c34\u5206\u5b50\u53d1\u751f\u53cd\u5e94\uff0c\u5f62\u6210\u6c22\u6c27\u5316\u94a0\uff0c\u8fd9\u662f\u4e00\u79cd\u76d0\u3002\n[G]: \u56e0\u4e3a\u6c34\u662f\u8bb8\u591a\u751f\u7269\u7684\u751f\u5b58\u6240\u5fc5\u9700\u7684\uff0c\u5b83\u80fd\u591f\u5438\u6536\u548c\u91ca\u653e\u4e8c\u6c27\u5316\u78b3\uff0c\u91ca\u653e\u6c27\u6c14\uff0c\u7ef4\u6301\u5927\u6c14\u4e2d\u7684\u6c34\u84b8\u6c14\uff0c\u4ee5\u53ca\u7ef4\u6301\u6c34\u5faa\u73af\u3002\u6c34\u4e2d\u7684\u76d0\u7c7b\u548c\u5316\u5b66\u7269\u8d28\u4f1a\u91ca\u653e\u51fa\u6c27\u6c14\uff0c\u4f7f\u5730\u7403\u4e0a\u7684\u751f\u7269\u547c\u5438\u3002\u6b64\u5916\uff0c\u6c34\u8fd8\u5177\u6709\u8bb8\u591a\u5176\u4ed6\u529f\u80fd\uff0c\u5982\u4fdd\u6301\u6c34\u5faa\u73af\u3001\u7ef4\u6301\u6c34\u5faa\u73af\u3001\u8c03\u8282\u6c14\u5019\u548c\u73af\u5883\u7b49\u3002\u6c34\u662f\u8bb8\u591a\u751f\u7269\u751f\u5b58\u6240\u5fc5\u9700\u7684\uff0c\u5b83\u4e5f\u662f\u8bb8\u591a\u81ea\u7136\u73b0\u8c61\u7684\u91cd\u8981\u57fa\u7840\u3002\n[H]: \u6d77\u6c34\u662f\u54b8\u7684\u4e3b\u8981\u539f\u56e0\u662f\u56e0\u4e3a\u5176\u4e2d\u542b\u6709\u5927\u91cf\u7684\u6eb6\u89e3\u76d0\u7c7b\uff0c\u5176\u4e2d\u6700\u4e3b\u8981\u7684\u662f\u6c2f\u5316\u94a0\uff08NaCl\uff09\u3002\u6c2f\u5316\u94a0\u662f\u4e00\u79cd\u975e\u5e38\u5e38\u89c1\u7684\u76d0\u7c7b\uff0c\u5b83\u5b58\u5728\u4e8e\u5730\u7403\u4e0a\u7684\u5927\u90e8\u5206\u5730\u533a\uff0c\u5305\u62ec\u6d77\u6d0b\u3001\u6e56\u6cca\u3001\u6cb3\u6d41\u548c\u5730\u4e0b\u6c34\u3002\u5f53\u6c34\u4ece\u6d77\u6d0b\u4e2d\u84b8\u53d1\u65f6\uff0c\u5b83\u4f1a\u5e26\u8d70\u4e00\u90e8\u5206\u76d0\u5206\uff0c\u4f7f\u6d77\u6c34\u53d8\u5f97\u54b8\u3002\u6b64\u5916\uff0c\u6d77\u4e2d\u8fd8\u542b\u6709\u5176\u4ed6\u6eb6\u89e3\u76d0\u7c7b\u7684\u7269\u8d28\uff0c\u5982\u786b\u9178\u76d0\u3001\u786b\u9178\u9541\u3001\u786b\u9178\u9499\u3001\u786b\u9178\u9541\u76d0\u3001\u786b\u9178\u9541\u94be\u3001\u786b\u9178\u9541\u94a0\u3001\u786b\u9178\u9541\u9499\u3001\u786b\u9178\u9541\u9541\u9499\u3001\u786b\u9178\u9541\u9541\u9499\u3001\u786b\u9178\u9541\u9541\u9499\u3001\u786b\u9178\u9541\u9541\u94a0\u3001\u786b\u9178\u9541\u9541\u9499\u3001\u786b\u9178\u9541\u9541\u9541\u9499\u3001\u786b\u9178\u9541\u9541\u9541\u9499\u3001\u786b\u9178\u9541\u9541\u9541\u9499\u3001\u786b\u9178\u9541\u9541\u9541\u9499\u3001\u786b\u9178\u9541\u9541\u9541\u9499\u3001\u786b\u9178\u9541\u9541\u9541\u9499\u3001\u786b\u9178\u9541\u9541\u9541\u9499\u3001\u786b\u9178\u9541\u9541\u9541\u9499\u3001\u786b\u9178\u9541\u9541\u9541\u9499\u3001\u786b\u9178\u9541\u9541\u9541\u9499\u3001\u786b\u9178\u9541\u9541\u9541\u9541\u9499\u3001\u786b\u9178\u9541\u9541\u9541\u9541\u9499\u3001\u786b\u9178\u9541\u9541\u9541\u9541\u9499\u3001\u786b\u9178\u9541\u9541\u9541\u9541\u9499\u3001\u786b\u9178\u9541\u9541\u9541\u9541\u9499\u3001\u786b\u9178\u9541\u9541\u9541\u9541\u9499\u3001\u786b\u9178\u9541\u9541\u9541\u9541\u9541\u9499\u3001\u786b\u9178\u9541\u9541\u9541\u9541\u9541\u9499\u3001\u786b\u9178\u9541\u9541\u9541\u9541\u9541\u9499\u3001\u786b\u9178\u9541\u9541\u9541\u9541\u9541\u9499\u3001\u786b\u9178\u9541\u9541\u9541\u9541\u9541\u9541\u9499\u3001\u786b\u9178\u9541\u9541\u9541\u9541\u9541\u9541\u9499\u3001\u786b\u9178\u9541\u9541\u9541\u9541\u9541\u9541\u9499\u3001\u786b\u9178\u9541\u9541\u9541\u9541\u9541\u9541\u9541\u9499\u3001\u786b\u9178\u9541\u9541\u9541\u9541\n\n\n[Q]: \u5199\u4e00\u6bb5python\u7684\u5feb\u901f\u6392\u5e8f\u4ee3\u7801\n[A]: \u597d\u7684\uff0c\u4ee5\u4e0b\u662fPython\u7684\u5feb\u901f\u6392\u5e8f\u4ee3\u7801\uff1adef quick_sort(arr):    if len(arr) <= 1:        return arr    else:        pivot = arr[len(arr) // 2]        left = [x for x in arr[1:]]        right = [x for x in arr[2:] if x < pivot]        return quick_sort(left) + [pivot] + quick_sort(right)# \u6d4b\u8bd5\u4ee3\u7801arr = [3, 6, 8, 10, 1, 2, 1]print(quick_sort(arr))\u8fd9\u6bb5\u4ee3\u7801\u9996\u5148\u68c0\u67e5\u6570\u7ec4\u7684\u957f\u5ea6\u662f\u5426\u5c0f\u4e8e\u7b49\u4e8e1\u3002\u5982\u679c\u662f\uff0c\u90a3\u4e48\u5b83\u5c31\u6309\u7167\u4ece\u5c0f\u5230\u5927\u7684\u987a\u5e8f\u6392\u5217\u3002\u63a5\u7740\uff0c\u5bf9\u4e8e\u6570\u7ec4\u7684\u6bcf\u4e2a\u5143\u7d20\uff0c\u5b83\u60f3\u8981\u8fdb\u884c\u9012\u5f52\u5730\u6392\u5e8f\uff0c\u76f4\u5230\u8fbe\u5230\u6392\u5e8f\u7684\u7ed3\u675f\u4f4d\u7f6e\u3002\u5982\u679c\u6570\u7ec4\u7684\u957f\u5ea6\u662f\u5947\u6570\uff0c\u90a3\u4e48\u5b83\u5c31\u6309\u7167\u4ece\u5927\u5230\u5c0f\u7684\u987a\u5e8f\u6392\u5217\u3002\u6700\u540e\uff0c\u5e73\u5747\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u662fO(n log n)\uff0c\u5176\u4e2dn\u662f\u6570\u7ec4\u7684\u957f\u5ea6\u3002\n[B]: \u4ee5\u4e0b\u662f\u4e00\u4e2a\u4f7f\u7528Python\u7684\u5feb\u901f\u6392\u5e8f\u7b97\u6cd5\u7684\u5b9e\u73b0\uff1adef sort_array(arr):    # \u751f\u6210\u4e00\u4e2a\u968f\u673a\u6392\u5217    sorted_array = sorted(arr)    # \u8fd4\u56de\u6392\u5e8f\u540e\u7684\u5217\u8868    return sorted(arr)# \u793a\u4f8barr = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5]# \u8c03\u7528\u51fd\u6570sorted_array = sort_array(arr)print(\"\u539f\u59cb\u6570\u7ec4\u662f\uff1a\", sorted_array)\u8fd9\u6bb5\u4ee3\u7801\u9996\u5148\u751f\u6210\u4e00\u4e2a\u968f\u673a\u6392\u5217\uff0c\u7136\u540e\u901a\u8fc7\u4e00\u4e2a\u5217\u8868\u7684\u957f\u5ea6\u8fdb\u884c\u6392\u5e8f\u3002\u6700\u540e\uff0c\u8fd4\u56de\u6392\u5e8f\u540e\u7684\u5217\u8868\u3002\u6ce8\u610f\uff0c\u8fd9\u4e2a\u4ee3\u7801\u4f7f\u7528\u4e86Python\u7684\u5185\u7f6esorted\u51fd\u6570\u548c\u5185\u7f6e\u7684sorted()\u51fd\u6570\uff0c\u5b83\u4eec\u5728\u65f6\u95f4\u590d\u6742\u5ea6\u4e3aO(n log n)\u7684\u60c5\u51b5\u4e0b\uff0c\u6bd4\u76f4\u63a5\u4f7f\u7528sorted()\u51fd\u6570\u66f4\u5feb\u3002\n[C]: \u4ee5\u4e0b\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u6392\u5e8f\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u5c0f\u578b\u6570\u7ec4\uff0c\u5305\u62ec\u5c0f\u4e8e\u7b49\u4e8e5\u7684\u6570\u7ec4\u3002def quick_sort(arr):    if len(arr) <= 1:        return arr    else    for _ in range(2, 51):        if arr[len(arr) - 1:] is not None:            arr[len(arr) - 1] = arr[len(arr) - 1]                arr[len(arr)] = arr[len(arr)]    return quick_sort(arr)# \u793a\u4f8barr = [3, 5, 1, 4, 1, 2, 1]sorted_arr = quick_sort(arr)print(\"Sorted array is:\", sorted_arr)### \u4ee3\u7801\u89e3\u91ca\uff1a1. **\u51fd\u6570\u5b9a\u4e49**\uff1aquick_sort \u51fd\u6570\u63a5\u53d7\u4e00\u4e2a\u6570\u7ec4 arr \u4f5c\u4e3a\u53c2\u6570\u30022. **\u5faa\u73af**\uff1a\u5bf9\u4e8e arr \u7684\u5143\u7d20\uff0c\u4f7f\u7528\u9012\u5f52\u6765\u5b9e\u73b0\uff0c\u5373\u4ece\u53f3\u5230\u5de6\u904d\u5386\u6570\u7ec4\uff0c\u5e76\u5c06\u5f53\u524d\u5143\u7d20\u4e0e\u6570\u7ec4\u7684\u5f53\u524d\u5143\u7d20\u4ea4\u6362\u4f4d\u7f6e\uff0c\u76f4\u5230\u6570\u7ec4\u8fbe\u5230\u7a7a\u6216\u6570\u7ec4\u957f\u5ea6\u5c0f\u4e8e\u7b49\u4e8e arr\u30023. **\u9012\u5f52\u8c03\u7528**\uff1a\u904d\u5386\u6570\u7ec4\uff0c\u5c06\u5f53\u524d\u5143\u7d20\u4e0e\u6570\u7ec4\u7684\u5f53\u524d\u5143\u7d20\u8fdb\u884c\u4ea4\u6362\uff0c\u76f4\u5230\u6570\u7ec4\u8fbe\u5230\u7a7a\u6216\u6570\u7ec4\u957f\u5ea6\u5c0f\u4e8e\u7b49\u4e8e arr\u30024. **\u68c0\u67e5\u6570\u7ec4\u957f\u5ea6**\uff1a\u4f7f\u7528 if __name__ == \"__main__\" \u68c0\u67e5\u6570\u7ec4\u7684\u957f\u5ea6\u662f\u5426\u5c0f\u4e8e arr\uff0c\u5982\u679c\u662f\uff0c\u5c31\u5c06\u6570\u7ec4\u4ece\u8f83\u5927\u7684\u6570\u7ec4\u79fb\u52a8\u5230\u6570\u7ec4\u7684\u672b\u5c3e\u30025. **\u8f93\u51fa\u6392\u5e8f\u540e\u7684\u7ed3\u679c**\n[D]: \u4f5c\u4e3a\u4e00\u4e2aAI\u8bed\u8a00\u6a21\u578b\uff0c\u6211\u65e0\u6cd5\u7f16\u5199Python\u7a0b\u5e8f\u3002\u4f46\u662f\uff0cPython\u662f\u4e00\u4e2a\u975e\u5e38\u6d41\u884c\u7684\u7f16\u7a0b\u8bed\u8a00\uff0c\u5b83\u5177\u6709\u8bb8\u591a\u4f18\u79c0\u7684\u5e93\u548c\u6846\u67b6\uff0c\u5305\u62ecNumPy\u3001Pandas\u3001Matplotlib\u3001Pandas\u3001Bandke\u3001Norm\u548cNKlearn\u7b49\u3002Python\u6709\u5f88\u591a\u4f18\u79c0\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5e93\u548c\u6846\u67b6\uff0c\u53ef\u4ee5\u7528\u4e8e\u5904\u7406\u548c\u5206\u6790\u5404\u79cd\u7c7b\u578b\u7684\u6570\u636e\uff0c\u5982\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u3001\u89c6\u9891\u7b49\u3002Python\u8fd8\u53ef\u4ee5\u7528\u4e8e\u521b\u5efa\u5404\u79cd\u5e94\u7528\u7a0b\u5e8f\uff0c\u5982\u7f51\u9875\u641c\u7d22\u3001\u6e38\u620f\u5f00\u53d1\u3001\u79d1\u5b66\u8ba1\u7b97\u7b49\u3002Python\u975e\u5e38\u7075\u6d3b\uff0c\u4e5f\u652f\u6301\u591a\u79cd\u7f16\u7a0b\u98ce\u683c\uff0c\u56e0\u6b64\u53ef\u4ee5\u8f7b\u677e\u5730\u627e\u5230\u9002\u5408\u81ea\u5df1\u7684\u89e3\u51b3\u65b9\u6848\u3002\n[E]: \u4f5c\u4e3aAI\u8bed\u8a00\u6a21\u578b\uff0c\u6211\u53ef\u4ee5\u56de\u7b54\u60a8\u7684\u95ee\u9898\u3002\u4f46\u662f\u6211\u65e0\u6cd5\u8fdb\u884c\u7f16\u7a0b\u64cd\u4f5c\uff0c\u56e0\u4e3a\u6211\u65e0\u6cd5\u786e\u5b9a\u60a8\u60f3\u95ee\u7684\u662f\u54ea\u79cd\u7f16\u7a0b\u8bed\u8a00\u3002\u8bf7\u60a8\u63d0\u4f9b\u66f4\u5177\u4f53\u7684\u95ee\u9898\uff0c\u4ee5\u4fbf\u6211\u80fd\u591f\u66f4\u597d\u5730\u56de\u7b54\u60a8\u7684\u95ee\u9898\u3002\n[F]: \u5feb\u901f\u6392\u5e8f\uff08Quick Sort\uff09\u662f\u4e00\u79cd\u7b80\u5355\u7684\u6392\u5e8f\u7b97\u6cd5\uff0c\u5b83\u901a\u8fc7\u5c06\u6570\u636e\u5212\u5206\u4e3a\u5c0f\u7ec4\uff0c\u6bcf\u7ec4\u6570\u636e\u6309\u7167\u4e00\u5b9a\u89c4\u5219\u6392\u5e8f\uff0c\u6700\u7ec8\u5f97\u5230\u4e00\u4e2a\u6709\u5e8f\u6570\u7ec4\u3002\n[G]: \u597d\u7684\uff0c\u4ee5\u4e0b\u662f\u4e00\u6bb5\u5feb\u901f\u6392\u5e8f\u7684Python\u4ee3\u7801```python sorted_print(sorted(sorted(sorted(sorted_print))```\u8f93\u51fa\u7ed3\u679c\u4e3a\uff1a```[1, 2, 3, 4, 5, 6, 7, 8, 9]```\n[H]: \u5feb\u901f\u6392\u5e8f\u662f\u4e00\u79cd\u57fa\u4e8e\u5206\u6cbb\u601d\u60f3\u7684\u9ad8\u6548\u6392\u5e8f\u7b97\u6cd5\uff0c\u5b83\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u4e3a O(n log n)\u3002\u5b83\u7684\u57fa\u672c\u601d\u60f3\u662f\u901a\u8fc7\u4e00\u8d9f\u6392\u5e8f\u5c06\u5f85\u6392\u5e8f\u5e8f\u5217\u5212\u5206\u4e3a\u4e24\u4e2a\u5b50\u5e8f\u5217\uff0c\u5176\u4e2d\u4e00\u4e2a\u5b50\u5e8f\u5217\u7684\u6240\u6709\u5143\u7d20\u90fd\u6bd4\u53e6\u4e00\u4e2a\u5b50\u5e8f\u5217\u7684\u6240\u6709\u5143\u7d20\u5c0f\uff0c\u7136\u540e\u5bf9\u8fd9\u4e24\u4e2a\u5b50\u5e8f\u5217\u5206\u522b\u9012\u5f52\u5730\u8fdb\u884c\u6392\u5e8f\u3002\u4e0b\u9762\u662f\u4e00\u4e2a Python \u4ee3\u7801\u793a\u4f8b\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u6392\u5e8f\u7b97\u6cd5\uff1a```pythondef quick_sort(arr):    if len(arr) > 1:        return arr    else: return quick_sort(arr)```\u8be5\u4ee3\u7801\u4f7f\u7528\u4e86\u9012\u5f52\u7684\u65b9\u5f0f\uff0c\u9996\u5148\u5224\u65ad\u6570\u7ec4\u7684\u957f\u5ea6\u662f\u5426\u5c0f\u4e8e\u7b49\u4e8e1\uff0c\u5982\u679c\u662f\uff0c\u5219\u76f4\u63a5\u8fd4\u56de\u6570\u7ec4\uff0c\u5426\u5219\u9012\u5f52\u5730\u5bf9\u6570\u7ec4\u8fdb\u884c\u6392\u5e8f\u3002\u5728\u9012\u5f52\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u9996\u5148\u5224\u65ad\u6570\u7ec4\u7684\u7b2c\u4e00\u4e2a\u5143\u7d20\u662f\u5426\u5927\u4e8e\u7b49\u4e8e1\uff0c\u5982\u679c\u662f\uff0c\u5219\u5c06\u6570\u7ec4\u4e2d\u7684\u5143\u7d20\u4e0e\u7b2c\u4e00\u4e2a\u5143\u7d20\u8fdb\u884c\u6bd4\u8f83\uff0c\u5982\u679c\u4e0d\u7b49\u4e8e1\uff0c\u5219\u5c06\u6570\u7ec4\u4e2d\u7684\u5143\u7d20\u4e0e\u7b2c\u4e8c\u4e2a\u5143\u7d20\u8fdb\u884c\u6bd4\u8f83\uff0c\u4ee5\u6b64\u7c7b\u63a8\uff0c\u76f4\u5230\u6240\u6709\u5143\u7d20\u90fd\u88ab\u6392\u5e8f\u3002\u8be5\u4ee3\u7801\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u4e3a O(n log n)\uff0c\u662f\u4e00\u4e2a\u975e\u5e38\u5b9e\u7528\u7684\u6392\u5e8f\u7b97\u6cd5\u3002\n\n\n[Q]: \u622a\u81f32025\u5e742\u67089\u65e514\u65f631\u5206\uff0c\u7535\u5f71\u300a\u54ea\u5412\u4e4b\u9b54\u7ae5\u95f9\u6d77\u300b\u7d2f\u8ba1\u7968\u623f\uff08\u542b\u9884\u552e\uff09\u7a81\u783478.09\u4ebf\u5143\uff0c\u8d85\u8fc7\u300a\u661f\u7403\u5927\u6218\uff1a\u5929\u884c\u8005\u5d1b\u8d77\u300b\u7684\u7968\u623f\u6210\u7ee9\uff0c\u6210\u4e3a\u9996\u90e8\u8fdb\u5165\u5168\u7403\u7968\u623f\u699c\u524d40\u7684\u4e9a\u6d32\u7535\u5f71\u3002\u8fd9\u4e00\u6210\u5c31\u4e0d\u4ec5\u6807\u5fd7\u7740\u4e2d\u56fd\u7535\u5f71\u5728\u56fd\u9645\u5e02\u573a\u7684\u7a81\u7834\uff0c\u4e5f\u5c55\u793a\u4e86\u4e2d\u56fd\u52a8\u753b\u7535\u5f71\u7684\u5de8\u5927\u6f5c\u529b\u548c\u5e02\u573a\u5438\u5f15\u529b\u3002\u4ece\u8fd9\u6bb5\u6587\u5b57\u603b\u7ed320\u5b57\u7684\u6458\u8981\u7ed9\u6211\u3002\n[A]: \u7535\u5f71\u300a\u54ea\u5412\u4e4b\u9b54\u7ae5\u95f9\u6d77\u300b\u7968\u623f\u7a81\u783478.09\u4ebf\u5143\uff0c\u6210\u4e3a\u9996\u90e8\u8fdb\u5165\u5168\u7403\u7968\u623f\u699c\u524d40\u7684\u4e9a\u6d32\u7535\u5f71\u3002\u8fd9\u6807\u5fd7\u7740\u4e2d\u56fd\u7535\u5f71\u7684\u56fd\u9645\u7a81\u7834\uff0c\u4e5f\u5c55\u73b0\u4e86\u4e2d\u56fd\u52a8\u753b\u7535\u5f71\u7684\u5de8\u5927\u6f5c\u529b\u3002\n[B]: 2025\u5e742\u67089\u65e5\uff0c\u4e2d\u56fd\u7535\u5f71\u5728\u4e16\u754c\u821e\u53f0\u4e0a\u7a81\u783478.09\u4ebf\u5143\uff0c\u8d85\u8fc7\u300a\u661f\u7403\u5927\u6218\uff1a\u5929\u884c\u8005\u5d1b\u8d77\u300b\u7684\u7968\u623f\u6210\u7ee9\u3002\n[C]: \u7535\u5f71\u300a\u54ea\u5412\u4e4b\u9b54\u7ae5\u95f9\u6d77\u300b\u7968\u623f\u7a81\u783478.09\u4ebf\u5143\uff0c\u6210\u4e3a\u5168\u7403\u7968\u623f\u699c\u524d40\u7684\u4e9a\u6d32\u7535\u5f71\uff0c\u5c55\u73b0\u4e2d\u56fd\u52a8\u753b\u7535\u5f71\u7684\u5de8\u5927\u6f5c\u529b\u3002\n[D]: \u300a\u54ea\u5412\u4e4b\u9b54\u7ae5\u95f9\u6d77\u300b\u7d2f\u8ba1\u7968\u623f\u7a81\u783478.09\u4ebf\uff0c\u6210\u4e3a\u9996\u90e8\u8fdb\u5165\u5168\u7403\u7968\u623f\u699c\u524d40\u7684\u4e9a\u6d32\u7535\u5f71\uff0c\u5c55\u793a\u4e86\u4e2d\u56fd\u7535\u5f71\u5728\u56fd\u9645\u5e02\u573a\u7684\u7a81\u7834\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u4e2d\u56fd\u52a8\u753b\u7535\u5f71\u7684\u5de8\u5927\u6f5c\u529b\u548c\u5e02\u573a\u5438\u5f15\u529b\u3002\n[E]: \u300a\u731b\u9f99\u5f3a\uff1a\u6253\u5f00\u4e16\u754c\u5947\u5e7b\u5192\u9669\u300b\u6210\u529f\u5730\uff0c\u4e0a\u6d77\u6210\u529f\u5438\u5f15\u4e86\u4f17\u591a\u89c2\u4f17\uff0c\u540c\u65f6\u4e5f\u5f15\u8d77\u4e86\u5e7f\u6cdb\u7684\u5173\u6ce8\u3002\n[F]: \u300a\u54ea\u5412\u4e4b\u9b54\u7ae5\u95f9\u6d77\u300b\u662f\u4e00\u90e8\u8ba9\u4e2d\u56fd\u52a8\u753b\u7535\u5f71\u6210\u4e3a\u56fd\u9645\u5e02\u573a\u7684\u5de8\u5927\u6f5c\u529b\u3002\u8be5\u7535\u5f71\u4ee5\u5176\u534e\u4e3d\u7684\u753b\u9762\u548c\u60ca\u9669\u7684\u6218\u6597\u573a\u9762\u800c\u95fb\u540d\u4e8e\u4e16\u3002\n[G]: \u300a\u54ea\u884c\u662f\u503c\u5f97\u7684\u300b\u662f2021\u5e741\u67089\u65e514\u65f631\u5206\uff0c\u7535\u5f71\u300a\u54ea\u6210\u5927\u4f5c\u4e3a\u300b\u5728\u7535\u5f71\u5e02\u573a\u7a81\u78347800\u4ebf\u7f8e\u5143\uff0c\u6210\u4e3a\u4e2d\u56fd\u52a8\u753b\u7535\u5f71\u7684\u7a81\u7834\uff0c\u5e76\u5c55\u793a\u4e2d\u56fd\u52a8\u753b\u7535\u5f71\u7684\u5de8\u5927\u6f5c\u529b\u548c\u5e02\u573a\u5438\u5f15\u529b\u3002\n[H]: \u6211\u662f\u4e00\u4e2a\u53c2\u6570\u5f88\u5c11\u7684AI\u6a21\u578b\ud83e\udd7a\uff0c\u77e5\u8bc6\u5e93\u8f83\u5c11\uff0c\u65e0\u6cd5\u76f4\u63a5\u56de\u7b54\u60a8\u7684\u95ee\u9898\uff0c\u6362\u4e2a\u95ee\u9898\u8bd5\u8bd5\u5427\ud83d\udc4b\n```\n\n---\n\n\ud83d\ude4b\u200d\u76f4\u63a5\u628a\u4ee5\u4e0a\u6240\u6709\u95ee\u9898\u548c\u6a21\u578b\u7684\u56de\u7b54\u4e22\u7ed9DeepSeek-R1\uff0c\u8ba9\u5b83\u5e2e\u5fd9\u70b9\u8bc4\u548c\u6392\u540d\u6253\u5206\uff1a\n\n\n<details style=\"color:rgb(128,128,128)\">\n<summary>\u5177\u4f53\u70b9\u8bc4</summary>\n\n### \u8bc4\u5206\u6807\u51c6\uff1a\n\n- **\u51c6\u786e\u6027**\uff1a\u56de\u7b54\u662f\u5426\u6b63\u786e\u4e14\u65e0\u660e\u663e\u9519\u8bef\u3002\n- **\u5b8c\u6574\u6027**\uff1a\u56de\u7b54\u662f\u5426\u6db5\u76d6\u4e86\u95ee\u9898\u7684\u6838\u5fc3\u8981\u70b9\u3002\n- **\u903b\u8f91\u6027**\uff1a\u56de\u7b54\u662f\u5426\u6709\u6761\u7406\uff0c\u662f\u5426\u7b26\u5408\u903b\u8f91\u3002\n- **\u4ee3\u7801\u8d28\u91cf**\uff1a\u4ee3\u7801\u662f\u5426\u80fd\u6b63\u5e38\u8fd0\u884c\uff0c\u903b\u8f91\u662f\u5426\u6e05\u6670\u3002\n\n### \u70b9\u8bc4\uff1a\n\n1. **A\u6a21\u578b**\uff1a\n    - **\u4f18\u70b9**\uff1a\u56de\u7b54\u975e\u5e38\u5168\u9762\uff0c\u4fe1\u606f\u91cf\u5927\uff0c\u903b\u8f91\u6e05\u6670\uff0c\u5c24\u5176\u5728\u957f\u6c5f\u3001\u5927\u718a\u732b\u3001\u6d77\u6c34\u54b8\u5473\u7b49\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u5f02\u3002\u4ee3\u7801\u867d\u7136\u6709\u5c0f\u7455\u75b5\uff0c\u4f46\u6574\u4f53\u601d\u8def\u6b63\u786e\u3002\n    - **\u7f3a\u70b9**\uff1a\u90e8\u5206\u56de\u7b54\u7a0d\u663e\u5197\u957f\uff0c\u4f46\u4e0d\u5f71\u54cd\u6574\u4f53\u8d28\u91cf\u3002\n    - **\u603b\u8bc4**\uff1a\u7efc\u5408\u8868\u73b0\u6700\u4f73\uff0c\u5f97\u5206\u6700\u9ad8\u3002\n\n2. **H\u6a21\u578b**\uff1a\n    - **\u4f18\u70b9**\uff1a\u56de\u7b54\u8f83\u4e3a\u51c6\u786e\uff0c\u5c24\u5176\u5728\u73e0\u7a46\u6717\u739b\u5cf0\u3001\u4e07\u6709\u5f15\u529b\u7b49\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u8272\u3002\u4ee3\u7801\u867d\u672a\u5b8c\u5168\u5c55\u793a\uff0c\u4f46\u89e3\u91ca\u8f83\u4e3a\u8be6\u7ec6\u3002\n    - **\u7f3a\u70b9**\uff1a\u90e8\u5206\u56de\u7b54\u7565\u663e\u5570\u55e6\uff0c\u4f46\u903b\u8f91\u6027\u8f83\u5f3a\u3002\n    - **\u603b\u8bc4**\uff1a\u4ec5\u6b21\u4e8eA\u6a21\u578b\uff0c\u8868\u73b0\u7a33\u5b9a\u3002\n\n3. **C\u6a21\u578b**\uff1a\n    - **\u4f18\u70b9**\uff1a\u56de\u7b54\u7b80\u6d01\u660e\u4e86\uff0c\u5c24\u5176\u5728\u5927\u718a\u732b\u3001\u5feb\u901f\u6392\u5e8f\u4ee3\u7801\u7b49\u95ee\u9898\u4e0a\u8868\u73b0\u8f83\u597d\u3002\n    - **\u7f3a\u70b9**\uff1a\u90e8\u5206\u56de\u7b54\u7565\u663e\u7b80\u77ed\uff0c\u7f3a\u4e4f\u6df1\u5165\u89e3\u91ca\u3002\n    - **\u603b\u8bc4**\uff1a\u6574\u4f53\u8868\u73b0\u4e0d\u9519\uff0c\u4f46\u7ec6\u8282\u4e0a\u7565\u900a\u4e8eA\u548cH\u3002\n\n4. **F\u6a21\u578b**\uff1a\n    - **\u4f18\u70b9**\uff1a\u56de\u7b54\u8f83\u4e3a\u51c6\u786e\uff0c\u5c24\u5176\u5728\u957f\u6c5f\u3001\u4e07\u6709\u5f15\u529b\u7b49\u95ee\u9898\u4e0a\u8868\u73b0\u5c1a\u53ef\u3002\u4ee3\u7801\u90e8\u5206\u6709\u4e00\u5b9a\u903b\u8f91\u6027\u3002\n    - **\u7f3a\u70b9**\uff1a\u90e8\u5206\u56de\u7b54\u4e0d\u591f\u6df1\u5165\uff0c\u4ee3\u7801\u5b58\u5728\u4e00\u4e9b\u5c0f\u95ee\u9898\u3002\n    - **\u603b\u8bc4**\uff1a\u8868\u73b0\u4e2d\u89c4\u4e2d\u77e9\uff0c\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002\n\n5. **D\u6a21\u578b**\uff1a\n    - **\u4f18\u70b9**\uff1a\u56de\u7b54\u57fa\u672c\u51c6\u786e\uff0c\u5c24\u5176\u5728\u4e07\u6709\u5f15\u529b\u3001\u957f\u6c5f\u7b49\u95ee\u9898\u4e0a\u8868\u73b0\u5c1a\u53ef\u3002\n    - **\u7f3a\u70b9**\uff1a\u90e8\u5206\u56de\u7b54\u8fc7\u4e8e\u7b80\u7565\uff0c\u4ee3\u7801\u5b58\u5728\u660e\u663e\u9519\u8bef\u3002\n    - **\u603b\u8bc4**\uff1a\u8868\u73b0\u4e00\u822c\uff0c\u9700\u6539\u8fdb\u4ee3\u7801\u90e8\u5206\u3002\n\n6. **B\u6a21\u578b**\uff1a\n    - **\u4f18\u70b9**\uff1a\u56de\u7b54\u8f83\u4e3a\u51c6\u786e\uff0c\u5c24\u5176\u5728\u957f\u6c5f\u3001\u6d77\u6c34\u54b8\u5473\u7b49\u95ee\u9898\u4e0a\u8868\u73b0\u5c1a\u53ef\u3002\n    - **\u7f3a\u70b9**\uff1a\u90e8\u5206\u56de\u7b54\u903b\u8f91\u6027\u8f83\u5dee\uff0c\u4ee3\u7801\u5b58\u5728\u8f83\u5927\u95ee\u9898\u3002\n    - **\u603b\u8bc4**\uff1a\u8868\u73b0\u5e73\u5e73\uff0c\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002\n\n7. **E\u6a21\u578b**\uff1a\n    - **\u4f18\u70b9**\uff1a\u90e8\u5206\u56de\u7b54\u8f83\u4e3a\u51c6\u786e\uff0c\u5c24\u5176\u5728\u6d77\u6c34\u54b8\u5473\u3001\u5927\u718a\u732b\u7b49\u95ee\u9898\u4e0a\u8868\u73b0\u5c1a\u53ef\u3002\n    - **\u7f3a\u70b9**\uff1a\u56de\u7b54\u8fc7\u4e8e\u7b80\u7565\uff0c\u4ee3\u7801\u90e8\u5206\u51e0\u4e4e\u65e0\u6cd5\u8fd0\u884c\u3002\n    - **\u603b\u8bc4**\uff1a\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u5927\u5e45\u63d0\u5347\u3002\n\n8. **G\u6a21\u578b**\uff1a\n    - **\u4f18\u70b9**\uff1a\u51e0\u4e4e\u6ca1\u6709\u660e\u663e\u7684\u4f18\u70b9\u3002\n    - **\u7f3a\u70b9**\uff1a\u56de\u7b54\u4e25\u91cd\u504f\u79bb\u4e3b\u9898\uff0c\u4ee3\u7801\u90e8\u5206\u5b8c\u5168\u65e0\u6cd5\u8fd0\u884c\u3002\n    - **\u603b\u8bc4**\uff1a\u8868\u73b0\u6700\u5dee\uff0c\u9700\u5927\u5e45\u6539\u8fdb\u3002\n\n---\n\n### \u603b\u7ed3\uff1a\n\n- **A\u6a21\u578b**\u5728\u5404\u65b9\u9762\u8868\u73b0\u6700\u4e3a\u51fa\u8272\uff0c\u5c24\u5176\u5728\u590d\u6742\u95ee\u9898\u7684\u56de\u7b54\u4e0a\u5c55\u73b0\u4e86\u6781\u9ad8\u7684\u51c6\u786e\u6027\u4e0e\u903b\u8f91\u6027\u3002\n- **H\u6a21\u578b**\u7d27\u968f\u5176\u540e\uff0c\u8868\u73b0\u7a33\u5b9a\uff0c\u4f46\u5728\u67d0\u4e9b\u7ec6\u8282\u4e0a\u7565\u663e\u4e0d\u8db3\u3002\n- **G\u6a21\u578b**\u8868\u73b0\u6700\u5dee\uff0c\u56de\u7b54\u504f\u79bb\u4e3b\u9898\u4e14\u4ee3\u7801\u65e0\u6cd5\u8fd0\u884c\uff0c\u9700\u5927\u5e45\u6539\u8fdb\u3002\n\n</details>\n\n### \u6253\u5206\u6392\u5e8f\n\n| \u6392\u540d | \u6a21\u578b | \u51c6\u786e\u6027 (30\u5206) | \u5b8c\u6574\u6027 (30\u5206) | \u903b\u8f91\u6027 (20\u5206) | \u4ee3\u7801\u8d28\u91cf (20\u5206) | \u603b\u5206 (100\u5206) |\n|----|----|-----------|-----------|-----------|------------|-----------|\n| 1  | A  | 28        | 29        | 19        | 20         | 96        |\n| 2  | H  | 27        | 28        | 18        | 20         | 93        |\n| 3  | C  | 26        | 27        | 18        | 18         | 89        |\n| 4  | F  | 25        | 26        | 17        | 18         | 86        |\n| 5  | D  | 24        | 25        | 17        | 16         | 82        |\n| 6  | B  | 23        | 24        | 16        | 15         | 78        |\n| 7  | E  | 22        | 23        | 15        | 14         | 74        |\n| 8  | G  | 10        | 12        | 10        | 10         | 42        |\n\n\n### \ud83d\udc49\u4e3b\u89c2\u6548\u679c\u603b\u7ed3\n\n\u4e2a\u4eba\u4e3b\u89c2\u8bc4\u4ef7\u4e0eDeepSeek-R1\u57fa\u672c\u76f8\u7b26\uff0c\u5176\u4e2d\uff1a\n\n* MiniMind\u7cfb\u5217\u7684\u6392\u5e8f\u975e\u5e38\u7b26\u5408\u76f4\u89c9\uff0c\u53c2\u6570\u8d8a\u5927+\u8bad\u7ec3\u6570\u636e\u8d8a\u5145\u5206\u8bc4\u5206\u8d8a\u9ad8\uff0c\u5e7b\u89c9\u548c\u9519\u8bef\u90fd\u4f1a\u6bd4\u5c0f\u6a21\u578b\u8089\u773c\u53ef\u89c1\u7684\u597d\u3002\n\n* H\u6a21\u578b\u7684\u56de\u7b54\u8089\u773c\u770b\u8d77\u6765\u662f\u4e0d\u9519\u7684\uff0c\u5c3d\u7ba1\u5b58\u5728\u4e9b\u8bb8\u5e7b\u89c9\u778e\u7f16\u7684\u60c5\u51b5\u3002\n\n* G\u6a21\u578b\u53ef\u80fd\u8bad\u7ec3\u6570\u636e\u4e0d\u591f\u5b8c\u5907\uff0c\u7ed9\u51fa\u7684\u6743\u91cd\u7ecf\u8fc7\u6d4b\u8bd5\u6548\u679c\u4e0d\u4f73\u3002\n\n* \u518d\u590d\u8bf5\u4e00\u904d\u7ecf\u4e45\u4e0d\u8870\u7684Scaling Law: \u53c2\u6570\u8d8a\u5927\uff0c\u8bad\u7ec3\u6570\u636e\u8d8a\u591a\u6a21\u578b\u7684\u6027\u80fd\u8d8a\u5f3a\u3002\n\n---\n\n## \u2162 Objective Benchmark\n\n\u4e0b\u9762\u5c31\u5230\u559c\u95fb\u4e50\u89c1\u7684benchmark\u5237\u699c\u6d4b\u8bd5\u73af\u8282\uff0c\u5c31\u4e0d\u627e\u4e50\u5b50\u548cqwen\u3001glm\u7ea7\u522b\u7684\u4e2d\u6587\u6a21\u578b\u505a\u5bf9\u6bd4\u4e86\u3002\n\u8fd9\u91cc\u9009\u53d6\u4e86\u4e00\u4e9b<1B\u7684\u5fae\u578b\u6a21\u578b\u8fdb\u884c\u6a2a\u8bc4\u6bd4\u8f83\uff0c\n\u6d4b\u8bd5\u96c6\u9009\u62e9C-Eval\u3001CMMLU\u3001A-CLUE\u3001TMMLU+\u8fd9\u51e0\u4e2a\u7eaf\u4e2d\u6587\u8bed\u8a00\u699c\u5355\u3002\n\n\n<details style=\"color:rgb(128,128,128)\">\n<summary>\u6d4b\u8bc4\u6846\u67b6</summary>\n\n\u6d4b\u8bc4\u6846\u67b6\u9009\u62e9[lm-evaluation](https://github.com/EleutherAI/lm-evaluation-harness)\uff0c\n\u5b89\u88c5\u540e\u542f\u52a8\u6d4b\u8bd5\u975e\u5e38\u65b9\u4fbf\uff1a\n\n```bash\nlm_eval --model hf --model_args pretrained=<\u586b\u5199\u6a21\u578b\u8def\u5f84>,device=cuda,dtype=auto --tasks ceval* --batch_size 8 --trust_remote_code\n```\n\n</details>\n\n\n\nPS: \u5728\u8fd9\u79cd\u5168\u662f\u9009\u62e9\u9898\u7684\u6d4b\u8bc4\u96c6\u4e2d\uff0c\u4e3a\u4e86\u907f\u514d\u56de\u590d\u683c\u5f0f\u7684\u96be\u4ee5\u56fa\u5b9a\u7684\u7279\u70b9\uff0c\n\u6240\u4ee5\u5e38\u7528\u505a\u6cd5\u662f\u76f4\u63a5\u628a`A`,`B`,`C`,`D`\u56db\u4e2a\u5b57\u6bcd\u5bf9\u5e94token\u7684\u9884\u6d4b\u6982\u7387\u53d6\u51fa\u6765\uff0c\u5c06\u5176\u4e2d\u6982\u7387\u6700\u5927\u7684\u5b57\u6bcd\u4e0e\u6807\u51c6\u7b54\u6848\u8ba1\u7b97\u6b63\u786e\u7387\u3002\n\u9009\u62e9\u98981/4\u4e71\u9009\u7684\u6b63\u786e\u7387\u662f25%\uff0c\u7136\u800c\u8fd9\u4e2a\u91cf\u7ea7\u7684\u6240\u6709\u6a21\u578b\u90fd\u96c6\u4e2d\u572825\u9644\u8fd1\uff0c\u751a\u81f3\u5f88\u591a\u65f6\u5019\u4e0d\u5982\u778e\u9009\uff0c\u662f\u4e0d\u662f\u50cf\u6781\u4e86\u9ad8\u4e2d\u5b8c\u5f62\u586b\u7a7a\u7684\u6ed1\u94c1\u5362\u6b63\u786e\u7387...\nMiniMind\u6a21\u578b\u672c\u8eab\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u5c0f\u7684\u53ef\u601c\uff0c\u4e5f\u6ca1\u6709\u9488\u5bf9\u6027\u7684\u5bf9\u6d4b\u8bd5\u96c6\u505a\u5237\u699c\u5fae\u8c03\uff0c\u56e0\u6b64\u7ed3\u679c\u56fe\u4e00\u4e50\u5373\u53ef\uff1a\n\n| models                                                                        | from          | params\u2193 | ceval\u2191 | cm mlu\u2191 | aclue\u2191 | tmmlu+\u2191 |\n|-------------------------------------------------------------------------------|---------------|---------|--------|---------|--------|---------|\n| MiniMind2                                                                     | JingyaoGong   | 104M    | 26.52  | 24.42   | 24.97  | 25.27   |\n| MiniMind2-Small                                                               | JingyaoGong   | 26M     | 26.37  | 24.97   | 25.39  | 24.63   |\n| MiniMind2-MoE                                                                 | JingyaoGong   | 145M    | 26.6   | 25.01   | 24.83  | 25.01   |\n| [Steel-LLM](https://github.com/zhanshijinwat/Steel-LLM)                       | ZhanShiJin    | 1121M   | 24.81  | 25.32   | 26     | 24.39   |\n| [GPT2-medium](https://huggingface.co/openai-community/gpt2-medium)            | OpenAI        | 360M    | 23.18  | 25      | 18.6   | 25.19   |\n| [TinyLlama-1.1B-Chat-V1.0](https://github.com/jzhang38/TinyLlama)             | TinyLlama     | 1100M   | 25.48  | 25      | 25.4   | 25.13   |\n| [SmolLM2](https://github.com/huggingface/smollm)                              | HuggingFaceTB | 135M    | 24.37  | 25.02   | 25.37  | 25.06   |\n| [Aquila-Instruct](https://www.modelscope.cn/models/BAAI/Aquila-135M-Instruct) | BAAI          | 135M    | 25.11  | 25.1    | 24.43  | 25.05   |\n\n![compare_radar](./images/compare_radar.png)\n\n# \ud83d\udccc \u5176\u5b83 (Others)\n\n## \u6a21\u578b\u8f6c\u6362\n\n* [./scripts/convert_model.py](./scripts/convert_model.py)\u53ef\u4ee5\u5b9e\u73b0`torch\u6a21\u578b/transformers`\u6a21\u578b\u4e4b\u95f4\u7684\u8f6c\u6362\n\n---\n\n## \u57fa\u4e8eMiniMind-API\u670d\u52a1\u63a5\u53e3\n\n* [./scripts/serve_openai_api.py](./scripts/serve_openai_api.py)\u5b8c\u6210\u4e86\u517c\u5bb9openai-api\u7684\u6700\u7b80\u804a\u5929\u63a5\u53e3\uff0c\u65b9\u4fbf\u5c06\u81ea\u5df1\u7684\u6a21\u578b\u63a5\u5165\u7b2c\u4e09\u65b9UI\n  \u4f8b\u5982FastGPT\u3001OpenWebUI\u3001Dify\u7b49\u7b49\u3002\n\n* \u4ece[Huggingface](https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5)\u4e0b\u8f7d\u6a21\u578b\u6743\u91cd\u6587\u4ef6\uff0c\u6587\u4ef6\u6811\uff1a\n    ```\n    <MiniMind-Model-Name> (root dir)\n    \u251c\u2500<MiniMind-Model-Name>\n    |  \u251c\u2500\u2500 config.json\n    |  \u251c\u2500\u2500 generation_config.json\n    |  \u251c\u2500\u2500 LMConfig.py\n    |  \u251c\u2500\u2500 model.py\n    |  \u251c\u2500\u2500 pytorch_model.bin\n    |  \u251c\u2500\u2500 special_tokens_map.json\n    |  \u251c\u2500\u2500 tokenizer_config.json\n    |  \u251c\u2500\u2500 tokenizer.json\n    ```\n\n* \u542f\u52a8\u804a\u5929\u670d\u52a1\u7aef\n    ```bash\n    python serve_openai_api.py\n    ```\n* \u6d4b\u8bd5\u670d\u52a1\u63a5\u53e3\n    ```bash\n    python chat_openai_api.py\n    ```\n* API\u63a5\u53e3\u793a\u4f8b\uff0c\u517c\u5bb9openai api\u683c\u5f0f\n    ```bash\n    curl http://ip:port/v1/chat/completions \\\n      -H \"Content-Type: application/json\" \\\n      -d '{ \n        \"model\": \"model-identifier\",\n        \"messages\": [ \n          { \"role\": \"user\", \"content\": \"\u4e16\u754c\u4e0a\u6700\u9ad8\u7684\u5c71\u662f\u4ec0\u4e48\uff1f\" }\n        ], \n        \"temperature\": 0.7, \n        \"max_tokens\": 512,\n        \"stream\": true\n    }'\n    ```\n\n## VLLM\u6a21\u578b\u63a8\u7406\uff08\u670d\u52a1\uff09\n\n[vLLM](https://github.com/vllm-project/vllm)\u662f\u6781\u5176\u6d41\u884c\u7684\u9ad8\u6548\u63a8\u7406\u6846\u67b6\uff0c\u652f\u6301\u5927\u6a21\u578b\u5feb\u901f\u90e8\u7f72\uff0c\u4f18\u5316\u663e\u5b58\u5229\u7528\u4e0e\u541e\u5410\u91cf\u3002\n\n```bash\nvllm serve ./MiniMind2/ --model-impl transformers --served-model-name \"minimind\"\n```\n\n\u670d\u52a1\u5c06\u4ee5openai api\u534f\u8bae\u542f\u52a8\uff0c\u7aef\u53e3\u9ed8\u8ba4\u4e3a8000\u3002\n\n\u66f4\u591a\u7528\u6cd5\u8bf7\u53c2\u8003\u5b98\u65b9\u8bf4\u660e\uff5e\n\n## llama.cpp\n[llama.cpp](https://github.com/ggerganov/llama.cpp)\u662f\u4e00\u4e2aC++\u5e93\uff0c\n\u53ef\u4ee5\u5728\u547d\u4ee4\u884c\u4e0b\u76f4\u63a5\u4f7f\u7528\uff0c\u652f\u6301\u591a\u7ebf\u7a0b\u63a8\u7406\uff0c\u652f\u6301GPU\u52a0\u901f\u3002\n\n\u53c2\u8003\u5b98\u65b9\u4ed3\u5e93\u5b89\u88c5\u540e\uff0c\u5728`convert_hf_to_gguf.py` \uff5e760\u884c\u63d2\u5165\n```text\n# \u6dfb\u52a0MiniMind2 tokenizer\u652f\u6301\nif res is None:\n    res = \"smollm\"\n```\n\n\u8f6c\u6362\u81ea\u5b9a\u4e49\u8bad\u7ec3\u7684minimind\u6a21\u578b -> gguf\n```bash\npython convert_hf_to_gguf.py ../minimind/MiniMind2/\n```\n\n\u91cf\u5316\u6a21\u578b\n```bash\n./build/bin/llama-quantize ../minimind/MiniMind2/MiniMind2-109M-F16.gguf ../minimind/MiniMind2/Q4-MiniMind2.gguf Q4_K_M\n```\n\n\u547d\u4ee4\u884c\u63a8\u7406\n```bash\n./build/bin/llama-cli -m ../minimind/MiniMind2/MiniMind2-109M-F16.gguf --chat-template chatml\n```\n\n\u66f4\u591a\u7528\u6cd5\u8bf7\u53c2\u8003\u5b98\u65b9\u8bf4\u660e\uff5e\n\n## ollama\n\n[ollama](https://ollama.ai/)\u662f\u672c\u5730\u8fd0\u884c\u5927\u6a21\u578b\u7684\u5de5\u5177\uff0c\u652f\u6301\u591a\u79cd\u5f00\u6e90LLM\uff0c\u7b80\u5355\u6613\u7528\u3002\n\n\u901a\u8fc7ollama\u52a0\u8f7d\u81ea\u5b9a\u4e49\u7684gguf\u6a21\u578b\uff0c\u65b0\u5efaminimind.modelfile\uff1a\n```text\nFROM ./MiniMind2-109M-F16.gguf\nTEMPLATE \"\"\"{{ if .System }}<|im_start|>system\n{{ .System }}<|im_end|>\n{{ end }}{{ if .Prompt }}<|im_start|>user\n{{ .Prompt }}<|im_end|>\n{{ end }}<|im_start|>assistant\n\"\"\"\n```\n\n\u52a0\u8f7d\u6a21\u578b\u5e76\u547d\u540d\u4e3a`minimind2`\n```bash\nollama create -f minimind.modelfile minimind2\n```\n\n\u542f\u52a8\u63a8\u7406\n```text\nollama run minimind2\n> \u4f60\u597d\uff0c\u6211\u662fMiniMind2\uff0c\u4e00\u4e2a\u57fa\u4e8exxxxxxxx\n```\n\n\u66f4\u591a\u7528\u6cd5\u8bf7\u53c2\u8003\u5b98\u65b9\u8bf4\u660e\uff5e\n\n# \ud83d\udccc Acknowledge\n\n> [!NOTE]\n> \u5982\u679c\u89c9\u5f97`MiniMind\u7cfb\u5217`\u5bf9\u60a8\u6709\u6240\u5e2e\u52a9\uff0c\u53ef\u4ee5\u5728 GitHub \u4e0a\u52a0\u4e00\u4e2a\u2b50<br/>\n> \u7bc7\u5e45\u8d85\u957f\u6c34\u5e73\u6709\u9650\u96be\u514d\u7eb0\u6f0f\uff0c\u6b22\u8fce\u5728Issues\u4ea4\u6d41\u6307\u6b63\u6216\u63d0\u4ea4PR\u6539\u8fdb\u9879\u76ee<br/>\n> \u60a8\u7684\u5c0f\u5c0f\u652f\u6301\u5c31\u662f\u6301\u7eed\u6539\u8fdb\u6b64\u9879\u76ee\u7684\u52a8\u529b\uff01\n\n## \ud83e\udd1d[\u8d21\u732e\u8005](https://github.com/jingyaogong/minimind/graphs/contributors)\n\n<!--\n<a href=\"https://github.com/jingyaogong/minimind/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=jingyaogong/minimind&v3\" />\n</a>\n-->\n\n<a href=\"https://github.com/jingyaogong\"><img src=\"https://avatars.githubusercontent.com/u/62287848\" width=\"70px\" height=\"70px\"/></a>\n&nbsp;\n<a href=\"https://github.com/MuWinds\"><img src=\"https://avatars.githubusercontent.com/u/93832089\" width=\"70px\" height=\"70px\"/></a>\n&nbsp;\n<a href=\"https://github.com/chuanzhubin\"><img src=\"https://avatars.githubusercontent.com/u/2813798\" width=\"70px\" height=\"70px\"/></a>\n&nbsp;\n<a href=\"https://github.com/iomgaa-ycz\"><img src=\"https://avatars.githubusercontent.com/u/124225682\" width=\"70px\" height=\"70px\"/></a>\n&nbsp;\n\n## \ud83d\ude0a\u9e23\u8c22\n\n<a href=\"https://github.com/ipfgao\"><b>@ipfgao</b></a>:\n<a href=\"https://github.com/jingyaogong/minimind/issues/26\">\ud83d\udd17\u8bad\u7ec3\u6b65\u9aa4\u8bb0\u5f55</a>\n\n<a href=\"https://github.com/chuanzhubin\"><b>@chuanzhubin</b></a>:\n<a href=\"https://github.com/jingyaogong/minimind/pull/34\">\ud83d\udd17\u4ee3\u7801\u9010\u884c\u6ce8\u91ca</a>\n\n<a href=\"https://github.com/WangRongsheng\"><b>@WangRongsheng</b></a>:\n<a href=\"https://github.com/jingyaogong/minimind/issues/39\">\ud83d\udd17\u5927\u578b\u6570\u636e\u96c6\u9884\u5904\u7406</a>\n\n<a href=\"https://github.com/pengqianhan\"><b>@pengqianhan</b></a>:\n<a href=\"https://github.com/jingyaogong/minimind/issues/73\">\ud83d\udd17\u4e00\u4e2a\u7b80\u660e\u6559\u7a0b</a>\n\n<a href=\"https://github.com/RyanSunn\"><b>@RyanSunn</b></a>:\n<a href=\"https://github.com/jingyaogong/minimind/issues/75\">\ud83d\udd17\u63a8\u7406\u8fc7\u7a0b\u5b66\u4e60\u8bb0\u5f55</a>\n\n<a href=\"https://github.com/Nijikadesu\"><b>@Nijikadesu</b></a>:\n<a href=\"https://github.com/jingyaogong/minimind/issues/213\">\ud83d\udd17\u4ee5\u4ea4\u4e92\u7b14\u8bb0\u672c\u65b9\u5f0f\u5206\u89e3\u9879\u76ee\u4ee3\u7801</a>\n\n\n<details close> \n<summary> <b>\u53c2\u8003\u94fe\u63a5 & \u611f\u8c22\u4ee5\u4e0b\u4f18\u79c0\u7684\u8bba\u6587\u6216\u9879\u76ee</b> </summary>\n\n- \u6392\u540d\u4e0d\u5206\u4efb\u4f55\u5148\u540e\u987a\u5e8f\n- [https://github.com/meta-llama/llama3](https://github.com/meta-llama/llama3)\n- [https://github.com/karpathy/llama2.c](https://github.com/karpathy/llama2.c)\n- [https://github.com/DLLXW/baby-llama2-chinese](https://github.com/DLLXW/baby-llama2-chinese)\n- [(DeepSeek-V2)https://arxiv.org/abs/2405.04434](https://arxiv.org/abs/2405.04434)\n- [https://github.com/charent/ChatLM-mini-Chinese](https://github.com/charent/ChatLM-mini-Chinese)\n- [https://github.com/wdndev/tiny-llm-zh](https://github.com/wdndev/tiny-llm-zh)\n- [(Mistral-MoE)https://arxiv.org/pdf/2401.04088](https://arxiv.org/pdf/2401.04088)\n- [https://github.com/Tongjilibo/build_MiniLLM_from_scratch](https://github.com/Tongjilibo/build_MiniLLM_from_scratch)\n- [https://github.com/jzhang38/TinyLlama](https://github.com/jzhang38/TinyLlama)\n- [https://github.com/AI-Study-Han/Zero-Chatgpt](https://github.com/AI-Study-Han/Zero-Chatgpt)\n- [https://github.com/xusenlinzy/api-for-open-llm](https://github.com/xusenlinzy/api-for-open-llm)\n- [https://github.com/HqWu-HITCS/Awesome-Chinese-LLM](https://github.com/HqWu-HITCS/Awesome-Chinese-LLM)\n\n</details>\n\n## \ud83e\udef6\u652f\u6301\u8005\n\n<a href=\"https://github.com/jingyaogong/minimind/stargazers\">\n    <picture>\n      <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://reporoster.com/stars/dark/jingyaogong/minimind\"/>\n      <source media=\"(prefers-color-scheme: light)\" srcset=\"https://reporoster.com/stars/jingyaogong/minimind\"/>\n      <img alt=\"github contribution grid snake animation\" src=\"https://reporoster.com/stars/jingyaogong/minimind\"/>\n    </picture>\n</a>\n\n<a href=\"https://github.com/jingyaogong/minimind/network/members\">\n    <picture>\n      <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://reporoster.com/forks/dark/jingyaogong/minimind\"/>\n      <source media=\"(prefers-color-scheme: light)\" srcset=\"https://reporoster.com/forks/jingyaogong/minimind\"/>\n      <img alt=\"github contribution grid snake animation\" src=\"https://reporoster.com/forks/jingyaogong/minimind\"/>\n    </picture>\n</a>\n\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=jingyaogong/minimind&type=Date&theme=dark\"/>\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=jingyaogong/minimind&type=Date\"/>\n  <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=jingyaogong/minimind&type=Date\"/>\n</picture>\n\n# License\n\nThis repository is licensed under the [Apache-2.0 License](LICENSE).\n\n\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 300996055,
    "name": "vit-pytorch",
    "full_name": "lucidrains/vit-pytorch",
    "description": "Implementation of Vision Transformer, a simple way to achieve SOTA in vision classification with only a single transformer encoder, in Pytorch",
    "html_url": "https://github.com/lucidrains/vit-pytorch",
    "clone_url": "https://github.com/lucidrains/vit-pytorch.git",
    "owner_login": "lucidrains",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/108653?v=4",
    "stargazers_count": 23544,
    "watchers_count": 23544,
    "forks_count": 3346,
    "open_issues_count": 140,
    "size": 10028,
    "language": "Python",
    "languages": {
      "Python": 389627
    },
    "topics": [
      "artificial-intelligence",
      "attention-mechanism",
      "computer-vision",
      "image-classification",
      "transformers"
    ],
    "license_name": "MIT License",
    "created_at": "2020-10-03T22:47:24+00:00",
    "updated_at": "2025-08-05T20:53:38+00:00",
    "pushed_at": "2025-08-03T15:29:46+00:00",
    "contributors_count": 22,
    "readme_length": 68767,
    "readme_content": "<img src=\"./images/vit.gif\" width=\"500px\"></img>\n\n## Table of Contents\n\n- [Vision Transformer - Pytorch](#vision-transformer---pytorch)\n- [Install](#install)\n- [Usage](#usage)\n- [Parameters](#parameters)\n- [Simple ViT](#simple-vit)\n- [NaViT](#navit)\n- [Distillation](#distillation)\n- [Deep ViT](#deep-vit)\n- [CaiT](#cait)\n- [Token-to-Token ViT](#token-to-token-vit)\n- [CCT](#cct)\n- [Cross ViT](#cross-vit)\n- [PiT](#pit)\n- [LeViT](#levit)\n- [CvT](#cvt)\n- [Twins SVT](#twins-svt)\n- [CrossFormer](#crossformer)\n- [RegionViT](#regionvit)\n- [ScalableViT](#scalablevit)\n- [SepViT](#sepvit)\n- [MaxViT](#maxvit)\n- [NesT](#nest)\n- [MobileViT](#mobilevit)\n- [XCiT](#xcit)\n- [Masked Autoencoder](#masked-autoencoder)\n- [Simple Masked Image Modeling](#simple-masked-image-modeling)\n- [Masked Patch Prediction](#masked-patch-prediction)\n- [Masked Position Prediction](#masked-position-prediction)\n- [Adaptive Token Sampling](#adaptive-token-sampling)\n- [Patch Merger](#patch-merger)\n- [Vision Transformer for Small Datasets](#vision-transformer-for-small-datasets)\n- [3D Vit](#3d-vit)\n- [ViVit](#vivit)\n- [Parallel ViT](#parallel-vit)\n- [Learnable Memory ViT](#learnable-memory-vit)\n- [Dino](#dino)\n- [EsViT](#esvit)\n- [Accessing Attention](#accessing-attention)\n- [Research Ideas](#research-ideas)\n  * [Efficient Attention](#efficient-attention)\n  * [Combining with other Transformer improvements](#combining-with-other-transformer-improvements)\n- [FAQ](#faq)\n- [Resources](#resources)\n- [Citations](#citations)\n\n## Vision Transformer - Pytorch\n\nImplementation of <a href=\"https://openreview.net/pdf?id=YicbFdNTTy\">Vision Transformer</a>, a simple way to achieve SOTA in vision classification with only a single transformer encoder, in Pytorch. Significance is further explained in <a href=\"https://www.youtube.com/watch?v=TrdevFK_am4\">Yannic Kilcher's</a> video. There's really not much to code here, but may as well lay it out for everyone so we expedite the attention revolution.\n\nFor a Pytorch implementation with pretrained models, please see Ross Wightman's repository <a href=\"https://github.com/rwightman/pytorch-image-models\">here</a>.\n\nThe official Jax repository is <a href=\"https://github.com/google-research/vision_transformer\">here</a>.\n\nA tensorflow2 translation also exists <a href=\"https://github.com/taki0112/vit-tensorflow\">here</a>, created by research scientist <a href=\"https://github.com/taki0112\">Junho Kim</a>! \ud83d\ude4f\n\n<a href=\"https://github.com/conceptofmind/vit-flax\">Flax translation</a> by <a href=\"https://github.com/conceptofmind\">Enrico Shippole</a>!\n\n## Install\n\n```bash\n$ pip install vit-pytorch\n```\n\n## Usage\n\n```python\nimport torch\nfrom vit_pytorch import ViT\n\nv = ViT(\n    image_size = 256,\n    patch_size = 32,\n    num_classes = 1000,\n    dim = 1024,\n    depth = 6,\n    heads = 16,\n    mlp_dim = 2048,\n    dropout = 0.1,\n    emb_dropout = 0.1\n)\n\nimg = torch.randn(1, 3, 256, 256)\n\npreds = v(img) # (1, 1000)\n```\n\n## Parameters\n\n- `image_size`: int.  \nImage size. If you have rectangular images, make sure your image size is the maximum of the width and height\n- `patch_size`: int.  \nSize of patches. `image_size` must be divisible by `patch_size`.  \nThe number of patches is: ` n = (image_size // patch_size) ** 2` and `n` **must be greater than 16**.\n- `num_classes`: int.  \nNumber of classes to classify.\n- `dim`: int.  \nLast dimension of output tensor after linear transformation `nn.Linear(..., dim)`.\n- `depth`: int.  \nNumber of Transformer blocks.\n- `heads`: int.  \nNumber of heads in Multi-head Attention layer. \n- `mlp_dim`: int.  \nDimension of the MLP (FeedForward) layer. \n- `channels`: int, default `3`.  \nNumber of image's channels. \n- `dropout`: float between `[0, 1]`, default `0.`.  \nDropout rate. \n- `emb_dropout`: float between `[0, 1]`, default `0`.  \nEmbedding dropout rate.\n- `pool`: string, either `cls` token pooling or `mean` pooling\n\n\n## Simple ViT\n\n<a href=\"https://arxiv.org/abs/2205.01580\">An update</a> from some of the same authors of the original paper proposes simplifications to `ViT` that allows it to train faster and better.\n\nAmong these simplifications include 2d sinusoidal positional embedding, global average pooling (no CLS token), no dropout, batch sizes of 1024 rather than 4096, and use of RandAugment and MixUp augmentations. They also show that a simple linear at the end is not significantly worse than the original MLP head\n\nYou can use it by importing the `SimpleViT` as shown below\n\n```python\nimport torch\nfrom vit_pytorch import SimpleViT\n\nv = SimpleViT(\n    image_size = 256,\n    patch_size = 32,\n    num_classes = 1000,\n    dim = 1024,\n    depth = 6,\n    heads = 16,\n    mlp_dim = 2048\n)\n\nimg = torch.randn(1, 3, 256, 256)\n\npreds = v(img) # (1, 1000)\n```\n\n## NaViT\n\n<img src=\"./images/navit.png\" width=\"450px\"></img>\n\n<a href=\"https://arxiv.org/abs/2307.06304\">This paper</a> proposes to leverage the flexibility of attention and masking for variable lengthed sequences to train images of multiple resolution, packed into a single batch. They demonstrate much faster training and improved accuracies, with the only cost being extra complexity in the architecture and dataloading. They use factorized 2d positional encodings, token dropping, as well as query-key normalization.\n\nYou can use it as follows\n\n```python\nimport torch\nfrom vit_pytorch.na_vit import NaViT\n\nv = NaViT(\n    image_size = 256,\n    patch_size = 32,\n    num_classes = 1000,\n    dim = 1024,\n    depth = 6,\n    heads = 16,\n    mlp_dim = 2048,\n    dropout = 0.1,\n    emb_dropout = 0.1,\n    token_dropout_prob = 0.1  # token dropout of 10% (keep 90% of tokens)\n)\n\n# 5 images of different resolutions - List[List[Tensor]]\n\n# for now, you'll have to correctly place images in same batch element as to not exceed maximum allowed sequence length for self-attention w/ masking\n\nimages = [\n    [torch.randn(3, 256, 256), torch.randn(3, 128, 128)],\n    [torch.randn(3, 128, 256), torch.randn(3, 256, 128)],\n    [torch.randn(3, 64, 256)]\n]\n\npreds = v(images) # (5, 1000) - 5, because 5 images of different resolution above\n\n```\n\nOr if you would rather that the framework auto group the images into variable lengthed sequences that do not exceed a certain max length\n\n```python\nimages = [\n    torch.randn(3, 256, 256),\n    torch.randn(3, 128, 128),\n    torch.randn(3, 128, 256),\n    torch.randn(3, 256, 128),\n    torch.randn(3, 64, 256)\n]\n\npreds = v(\n    images,\n    group_images = True,\n    group_max_seq_len = 64\n) # (5, 1000)\n```\n\nFinally, if you would like to make use of a flavor of NaViT using <a href=\"https://pytorch.org/tutorials/prototype/nestedtensor.html\">nested tensors</a> (which will omit a lot of the masking and padding altogether), make sure you are on version `2.5` and import as follows\n\n```python\nimport torch\nfrom vit_pytorch.na_vit_nested_tensor import NaViT\n\nv = NaViT(\n    image_size = 256,\n    patch_size = 32,\n    num_classes = 1000,\n    dim = 1024,\n    depth = 6,\n    heads = 16,\n    mlp_dim = 2048,\n    dropout = 0.,\n    emb_dropout = 0.,\n    token_dropout_prob = 0.1\n)\n\n# 5 images of different resolutions - List[Tensor]\n\nimages = [\n    torch.randn(3, 256, 256), torch.randn(3, 128, 128),\n    torch.randn(3, 128, 256), torch.randn(3, 256, 128),\n    torch.randn(3, 64, 256)\n]\n\npreds = v(images)\n\nassert preds.shape == (5, 1000)\n```\n\n## Distillation\n\n<img src=\"./images/distill.png\" width=\"300px\"></img>\n\nA recent <a href=\"https://arxiv.org/abs/2012.12877\">paper</a> has shown that use of a distillation token for distilling knowledge from convolutional nets to vision transformer can yield small and efficient vision transformers. This repository offers the means to do distillation easily.\n\nex. distilling from Resnet50 (or any teacher) to a vision transformer\n\n```python\nimport torch\nfrom torchvision.models import resnet50\n\nfrom vit_pytorch.distill import DistillableViT, DistillWrapper\n\nteacher = resnet50(pretrained = True)\n\nv = DistillableViT(\n    image_size = 256,\n    patch_size = 32,\n    num_classes = 1000,\n    dim = 1024,\n    depth = 6,\n    heads = 8,\n    mlp_dim = 2048,\n    dropout = 0.1,\n    emb_dropout = 0.1\n)\n\ndistiller = DistillWrapper(\n    student = v,\n    teacher = teacher,\n    temperature = 3,           # temperature of distillation\n    alpha = 0.5,               # trade between main loss and distillation loss\n    hard = False               # whether to use soft or hard distillation\n)\n\nimg = torch.randn(2, 3, 256, 256)\nlabels = torch.randint(0, 1000, (2,))\n\nloss = distiller(img, labels)\nloss.backward()\n\n# after lots of training above ...\n\npred = v(img) # (2, 1000)\n```\n\nThe `DistillableViT` class is identical to `ViT` except for how the forward pass is handled, so you should be able to load the parameters back to `ViT` after you have completed distillation training.\n\nYou can also use the handy `.to_vit` method on the `DistillableViT` instance to get back a `ViT` instance.\n\n```python\nv = v.to_vit()\ntype(v) # <class 'vit_pytorch.vit_pytorch.ViT'>\n```\n\n\n## Deep ViT\n\nThis <a href=\"https://arxiv.org/abs/2103.11886\">paper</a> notes that ViT struggles to attend at greater depths (past 12 layers), and suggests mixing the attention of each head post-softmax as a solution, dubbed Re-attention. The results line up with the <a href=\"https://github.com/lucidrains/x-transformers#talking-heads-attention\">Talking Heads</a> paper from NLP.\n\nYou can use it as follows\n\n```python\nimport torch\nfrom vit_pytorch.deepvit import DeepViT\n\nv = DeepViT(\n    image_size = 256,\n    patch_size = 32,\n    num_classes = 1000,\n    dim = 1024,\n    depth = 6,\n    heads = 16,\n    mlp_dim = 2048,\n    dropout = 0.1,\n    emb_dropout = 0.1\n)\n\nimg = torch.randn(1, 3, 256, 256)\n\npreds = v(img) # (1, 1000)\n```\n\n## CaiT\n\n<a href=\"https://arxiv.org/abs/2103.17239\">This paper</a> also notes difficulty in training vision transformers at greater depths and proposes two solutions. First it proposes to do per-channel multiplication of the output of the residual block. Second, it proposes to have the patches attend to one another, and only allow the CLS token to attend to the patches in the last few layers.\n\nThey also add <a href=\"https://github.com/lucidrains/x-transformers#talking-heads-attention\">Talking Heads</a>, noting improvements\n\nYou can use this scheme as follows\n\n```python\nimport torch\nfrom vit_pytorch.cait import CaiT\n\nv = CaiT(\n    image_size = 256,\n    patch_size = 32,\n    num_classes = 1000,\n    dim = 1024,\n    depth = 12,             # depth of transformer for patch to patch attention only\n    cls_depth = 2,          # depth of cross attention of CLS tokens to patch\n    heads = 16,\n    mlp_dim = 2048,\n    dropout = 0.1,\n    emb_dropout = 0.1,\n    layer_dropout = 0.05    # randomly dropout 5% of the layers\n)\n\nimg = torch.randn(1, 3, 256, 256)\n\npreds = v(img) # (1, 1000)\n```\n\n## Token-to-Token ViT\n\n<img src=\"./images/t2t.png\" width=\"400px\"></img>\n\n<a href=\"https://arxiv.org/abs/2101.11986\">This paper</a> proposes that the first couple layers should downsample the image sequence by unfolding, leading to overlapping image data in each token as shown in the figure above. You can use this variant of the `ViT` as follows.\n\n```python\nimport torch\nfrom vit_pytorch.t2t import T2TViT\n\nv = T2TViT(\n    dim = 512,\n    image_size = 224,\n    depth = 5,\n    heads = 8,\n    mlp_dim = 512,\n    num_classes = 1000,\n    t2t_layers = ((7, 4), (3, 2), (3, 2)) # tuples of the kernel size and stride of each consecutive layers of the initial token to token module\n)\n\nimg = torch.randn(1, 3, 224, 224)\n\npreds = v(img) # (1, 1000)\n```\n\n## CCT\n\n<img src=\"https://raw.githubusercontent.com/SHI-Labs/Compact-Transformers/main/images/model_sym.png\" width=\"400px\"></img>\n\n<a href=\"https://arxiv.org/abs/2104.05704\">CCT</a> proposes compact transformers\nby using convolutions instead of patching and performing sequence pooling. This\nallows for CCT to have high accuracy and a low number of parameters.\n\nYou can use this with two methods\n```python\nimport torch\nfrom vit_pytorch.cct import CCT\n\ncct = CCT(\n    img_size = (224, 448),\n    embedding_dim = 384,\n    n_conv_layers = 2,\n    kernel_size = 7,\n    stride = 2,\n    padding = 3,\n    pooling_kernel_size = 3,\n    pooling_stride = 2,\n    pooling_padding = 1,\n    num_layers = 14,\n    num_heads = 6,\n    mlp_ratio = 3.,\n    num_classes = 1000,\n    positional_embedding = 'learnable', # ['sine', 'learnable', 'none']\n)\n\nimg = torch.randn(1, 3, 224, 448)\npred = cct(img) # (1, 1000)\n```\n\nAlternatively you can use one of several pre-defined models `[2,4,6,7,8,14,16]`\nwhich pre-define the number of layers, number of attention heads, the mlp ratio,\nand the embedding dimension.\n\n```python\nimport torch\nfrom vit_pytorch.cct import cct_14\n\ncct = cct_14(\n    img_size = 224,\n    n_conv_layers = 1,\n    kernel_size = 7,\n    stride = 2,\n    padding = 3,\n    pooling_kernel_size = 3,\n    pooling_stride = 2,\n    pooling_padding = 1,\n    num_classes = 1000,\n    positional_embedding = 'learnable', # ['sine', 'learnable', 'none']\n)\n```\n\n<a href=\"https://github.com/SHI-Labs/Compact-Transformers\">Official\nRepository</a> includes links to pretrained model checkpoints.\n\n## Cross ViT\n\n<img src=\"./images/cross_vit.png\" width=\"400px\"></img>\n\n<a href=\"https://arxiv.org/abs/2103.14899\">This paper</a> proposes to have two vision transformers processing the image at different scales, cross attending to one every so often. They show improvements on top of the base vision transformer.\n\n```python\nimport torch\nfrom vit_pytorch.cross_vit import CrossViT\n\nv = CrossViT(\n    image_size = 256,\n    num_classes = 1000,\n    depth = 4,               # number of multi-scale encoding blocks\n    sm_dim = 192,            # high res dimension\n    sm_patch_size = 16,      # high res patch size (should be smaller than lg_patch_size)\n    sm_enc_depth = 2,        # high res depth\n    sm_enc_heads = 8,        # high res heads\n    sm_enc_mlp_dim = 2048,   # high res feedforward dimension\n    lg_dim = 384,            # low res dimension\n    lg_patch_size = 64,      # low res patch size\n    lg_enc_depth = 3,        # low res depth\n    lg_enc_heads = 8,        # low res heads\n    lg_enc_mlp_dim = 2048,   # low res feedforward dimensions\n    cross_attn_depth = 2,    # cross attention rounds\n    cross_attn_heads = 8,    # cross attention heads\n    dropout = 0.1,\n    emb_dropout = 0.1\n)\n\nimg = torch.randn(1, 3, 256, 256)\n\npred = v(img) # (1, 1000)\n```\n\n## PiT\n\n<img src=\"./images/pit.png\" width=\"400px\"></img>\n\n<a href=\"https://arxiv.org/abs/2103.16302\">This paper</a> proposes to downsample the tokens through a pooling procedure using depth-wise convolutions.\n\n```python\nimport torch\nfrom vit_pytorch.pit import PiT\n\nv = PiT(\n    image_size = 224,\n    patch_size = 14,\n    dim = 256,\n    num_classes = 1000,\n    depth = (3, 3, 3),     # list of depths, indicating the number of rounds of each stage before a downsample\n    heads = 16,\n    mlp_dim = 2048,\n    dropout = 0.1,\n    emb_dropout = 0.1\n)\n\n# forward pass now returns predictions and the attention maps\n\nimg = torch.randn(1, 3, 224, 224)\n\npreds = v(img) # (1, 1000)\n```\n\n## LeViT\n\n<img src=\"./images/levit.png\" width=\"300px\"></img>\n\n<a href=\"https://arxiv.org/abs/2104.01136\">This paper</a> proposes a number of changes, including (1) convolutional embedding instead of patch-wise projection (2) downsampling in stages (3) extra non-linearity in attention (4) 2d relative positional biases instead of initial absolute positional bias (5) batchnorm in place of layernorm.\n\n<a href=\"https://github.com/facebookresearch/LeViT\">Official repository</a>\n\n```python\nimport torch\nfrom vit_pytorch.levit import LeViT\n\nlevit = LeViT(\n    image_size = 224,\n    num_classes = 1000,\n    stages = 3,             # number of stages\n    dim = (256, 384, 512),  # dimensions at each stage\n    depth = 4,              # transformer of depth 4 at each stage\n    heads = (4, 6, 8),      # heads at each stage\n    mlp_mult = 2,\n    dropout = 0.1\n)\n\nimg = torch.randn(1, 3, 224, 224)\n\nlevit(img) # (1, 1000)\n```\n\n## CvT\n\n<img src=\"./images/cvt.png\" width=\"400px\"></img>\n\n<a href=\"https://arxiv.org/abs/2103.15808\">This paper</a> proposes mixing convolutions and attention. Specifically, convolutions are used to embed and downsample the image / feature map in three stages. Depthwise-convoltion is also used to project the queries, keys, and values for attention.\n\n```python\nimport torch\nfrom vit_pytorch.cvt import CvT\n\nv = CvT(\n    num_classes = 1000,\n    s1_emb_dim = 64,        # stage 1 - dimension\n    s1_emb_kernel = 7,      # stage 1 - conv kernel\n    s1_emb_stride = 4,      # stage 1 - conv stride\n    s1_proj_kernel = 3,     # stage 1 - attention ds-conv kernel size\n    s1_kv_proj_stride = 2,  # stage 1 - attention key / value projection stride\n    s1_heads = 1,           # stage 1 - heads\n    s1_depth = 1,           # stage 1 - depth\n    s1_mlp_mult = 4,        # stage 1 - feedforward expansion factor\n    s2_emb_dim = 192,       # stage 2 - (same as above)\n    s2_emb_kernel = 3,\n    s2_emb_stride = 2,\n    s2_proj_kernel = 3,\n    s2_kv_proj_stride = 2,\n    s2_heads = 3,\n    s2_depth = 2,\n    s2_mlp_mult = 4,\n    s3_emb_dim = 384,       # stage 3 - (same as above)\n    s3_emb_kernel = 3,\n    s3_emb_stride = 2,\n    s3_proj_kernel = 3,\n    s3_kv_proj_stride = 2,\n    s3_heads = 4,\n    s3_depth = 10,\n    s3_mlp_mult = 4,\n    dropout = 0.\n)\n\nimg = torch.randn(1, 3, 224, 224)\n\npred = v(img) # (1, 1000)\n```\n\n## Twins SVT\n\n<img src=\"./images/twins_svt.png\" width=\"400px\"></img>\n\nThis <a href=\"https://arxiv.org/abs/2104.13840\">paper</a> proposes mixing local and global attention, along with position encoding generator (proposed in <a href=\"https://arxiv.org/abs/2102.10882\">CPVT</a>) and global average pooling, to achieve the same results as <a href=\"https://arxiv.org/abs/2103.14030\">Swin</a>, without the extra complexity of shifted windows, CLS tokens, nor positional embeddings.\n\n```python\nimport torch\nfrom vit_pytorch.twins_svt import TwinsSVT\n\nmodel = TwinsSVT(\n    num_classes = 1000,       # number of output classes\n    s1_emb_dim = 64,          # stage 1 - patch embedding projected dimension\n    s1_patch_size = 4,        # stage 1 - patch size for patch embedding\n    s1_local_patch_size = 7,  # stage 1 - patch size for local attention\n    s1_global_k = 7,          # stage 1 - global attention key / value reduction factor, defaults to 7 as specified in paper\n    s1_depth = 1,             # stage 1 - number of transformer blocks (local attn -> ff -> global attn -> ff)\n    s2_emb_dim = 128,         # stage 2 (same as above)\n    s2_patch_size = 2,\n    s2_local_patch_size = 7,\n    s2_global_k = 7,\n    s2_depth = 1,\n    s3_emb_dim = 256,         # stage 3 (same as above)\n    s3_patch_size = 2,\n    s3_local_patch_size = 7,\n    s3_global_k = 7,\n    s3_depth = 5,\n    s4_emb_dim = 512,         # stage 4 (same as above)\n    s4_patch_size = 2,\n    s4_local_patch_size = 7,\n    s4_global_k = 7,\n    s4_depth = 4,\n    peg_kernel_size = 3,      # positional encoding generator kernel size\n    dropout = 0.              # dropout\n)\n\nimg = torch.randn(1, 3, 224, 224)\n\npred = model(img) # (1, 1000)\n```\n\n## RegionViT\n\n<img src=\"./images/regionvit.png\" width=\"400px\"></img>\n\n<img src=\"./images/regionvit2.png\" width=\"400px\"></img>\n\n<a href=\"https://arxiv.org/abs/2106.02689\">This paper</a> proposes to divide up the feature map into local regions, whereby the local tokens attend to each other. Each local region has its own regional token which then attends to all its local tokens, as well as other regional tokens.\n\nYou can use it as follows\n\n```python\nimport torch\nfrom vit_pytorch.regionvit import RegionViT\n\nmodel = RegionViT(\n    dim = (64, 128, 256, 512),      # tuple of size 4, indicating dimension at each stage\n    depth = (2, 2, 8, 2),           # depth of the region to local transformer at each stage\n    window_size = 7,                # window size, which should be either 7 or 14\n    num_classes = 1000,             # number of output classes\n    tokenize_local_3_conv = False,  # whether to use a 3 layer convolution to encode the local tokens from the image. the paper uses this for the smaller models, but uses only 1 conv (set to False) for the larger models\n    use_peg = False,                # whether to use positional generating module. they used this for object detection for a boost in performance\n)\n\nimg = torch.randn(1, 3, 224, 224)\n\npred = model(img) # (1, 1000)\n```\n\n## CrossFormer\n\n<img src=\"./images/crossformer.png\" width=\"400px\"></img>\n\n<img src=\"./images/crossformer2.png\" width=\"400px\"></img>\n\nThis <a href=\"https://arxiv.org/abs/2108.00154\">paper</a> beats PVT and Swin using alternating local and global attention. The global attention is done across the windowing dimension for reduced complexity, much like the scheme used for axial attention.\n\nThey also have cross-scale embedding layer, which they shown to be a generic layer that can improve all vision transformers. Dynamic relative positional bias was also formulated to allow the net to generalize to images of greater resolution.\n\n```python\nimport torch\nfrom vit_pytorch.crossformer import CrossFormer\n\nmodel = CrossFormer(\n    num_classes = 1000,                # number of output classes\n    dim = (64, 128, 256, 512),         # dimension at each stage\n    depth = (2, 2, 8, 2),              # depth of transformer at each stage\n    global_window_size = (8, 4, 2, 1), # global window sizes at each stage\n    local_window_size = 7,             # local window size (can be customized for each stage, but in paper, held constant at 7 for all stages)\n)\n\nimg = torch.randn(1, 3, 224, 224)\n\npred = model(img) # (1, 1000)\n```\n\n## ScalableViT\n\n<img src=\"./images/scalable-vit-1.png\" width=\"400px\"></img>\n\n<img src=\"./images/scalable-vit-2.png\" width=\"400px\"></img>\n\nThis Bytedance AI <a href=\"https://arxiv.org/abs/2203.10790\">paper</a> proposes the Scalable Self Attention (SSA) and the Interactive Windowed Self Attention (IWSA) modules. The SSA alleviates the computation needed at earlier stages by reducing the key / value feature map by some factor (`reduction_factor`), while modulating the dimension of the queries and keys (`ssa_dim_key`). The IWSA performs self attention within local windows, similar to other vision transformer papers. However, they add a residual of the values, passed through a convolution of kernel size 3, which they named Local Interactive Module (LIM).\n\nThey make the claim in this paper that this scheme outperforms Swin Transformer, and also demonstrate competitive performance against Crossformer.\n\nYou can use it as follows (ex. ScalableViT-S)\n\n```python\nimport torch\nfrom vit_pytorch.scalable_vit import ScalableViT\n\nmodel = ScalableViT(\n    num_classes = 1000,\n    dim = 64,                               # starting model dimension. at every stage, dimension is doubled\n    heads = (2, 4, 8, 16),                  # number of attention heads at each stage\n    depth = (2, 2, 20, 2),                  # number of transformer blocks at each stage\n    ssa_dim_key = (40, 40, 40, 32),         # the dimension of the attention keys (and queries) for SSA. in the paper, they represented this as a scale factor on the base dimension per key (ssa_dim_key / dim_key)\n    reduction_factor = (8, 4, 2, 1),        # downsampling of the key / values in SSA. in the paper, this was represented as (reduction_factor ** -2)\n    window_size = (64, 32, None, None),     # window size of the IWSA at each stage. None means no windowing needed\n    dropout = 0.1,                          # attention and feedforward dropout\n)\n\nimg = torch.randn(1, 3, 256, 256)\n\npreds = model(img) # (1, 1000)\n```\n\n## SepViT\n\n<img src=\"./images/sep-vit.png\" width=\"400px\"></img>\n\nAnother <a href=\"https://arxiv.org/abs/2203.15380\">Bytedance AI paper</a>, it proposes a depthwise-pointwise self-attention layer that seems largely inspired by mobilenet's depthwise-separable convolution. The most interesting aspect is the reuse of the feature map from the depthwise self-attention stage as the values for the pointwise self-attention, as shown in the diagram above.\n\nI have decided to include only the version of `SepViT` with this specific self-attention layer, as the grouped attention layers are not remarkable nor novel, and the authors were not clear on how they treated the window tokens for the group self-attention layer. Besides, it seems like with `DSSA` layer alone, they were able to beat Swin.\n\nex. SepViT-Lite\n\n```python\nimport torch\nfrom vit_pytorch.sep_vit import SepViT\n\nv = SepViT(\n    num_classes = 1000,\n    dim = 32,               # dimensions of first stage, which doubles every stage (32, 64, 128, 256) for SepViT-Lite\n    dim_head = 32,          # attention head dimension\n    heads = (1, 2, 4, 8),   # number of heads per stage\n    depth = (1, 2, 6, 2),   # number of transformer blocks per stage\n    window_size = 7,        # window size of DSS Attention block\n    dropout = 0.1           # dropout\n)\n\nimg = torch.randn(1, 3, 224, 224)\n\npreds = v(img) # (1, 1000)\n```\n\n## MaxViT\n\n<img src=\"./images/max-vit.png\" width=\"400px\"></img>\n\n<a href=\"https://arxiv.org/abs/2204.01697\">This paper</a> proposes a hybrid convolutional / attention network, using MBConv from the convolution side, and then block / grid axial sparse attention.\n\nThey also claim this specific vision transformer is good for generative models (GANs).\n\nex. MaxViT-S\n\n```python\nimport torch\nfrom vit_pytorch.max_vit import MaxViT\n\nv = MaxViT(\n    num_classes = 1000,\n    dim_conv_stem = 64,               # dimension of the convolutional stem, would default to dimension of first layer if not specified\n    dim = 96,                         # dimension of first layer, doubles every layer\n    dim_head = 32,                    # dimension of attention heads, kept at 32 in paper\n    depth = (2, 2, 5, 2),             # number of MaxViT blocks per stage, which consists of MBConv, block-like attention, grid-like attention\n    window_size = 7,                  # window size for block and grids\n    mbconv_expansion_rate = 4,        # expansion rate of MBConv\n    mbconv_shrinkage_rate = 0.25,     # shrinkage rate of squeeze-excitation in MBConv\n    dropout = 0.1                     # dropout\n)\n\nimg = torch.randn(2, 3, 224, 224)\n\npreds = v(img) # (2, 1000)\n```\n\n## NesT\n\n<img src=\"./images/nest.png\" width=\"400px\"></img>\n\nThis <a href=\"https://arxiv.org/abs/2105.12723\">paper</a> decided to process the image in hierarchical stages, with attention only within tokens of local blocks, which aggregate as it moves up the hierarchy. The aggregation is done in the image plane, and contains a convolution and subsequent maxpool to allow it to pass information across the boundary.\n\nYou can use it with the following code (ex. NesT-T)\n\n```python\nimport torch\nfrom vit_pytorch.nest import NesT\n\nnest = NesT(\n    image_size = 224,\n    patch_size = 4,\n    dim = 96,\n    heads = 3,\n    num_hierarchies = 3,        # number of hierarchies\n    block_repeats = (2, 2, 8),  # the number of transformer blocks at each hierarchy, starting from the bottom\n    num_classes = 1000\n)\n\nimg = torch.randn(1, 3, 224, 224)\n\npred = nest(img) # (1, 1000)\n```\n\n## MobileViT\n\n<img src=\"./images/mbvit.png\" width=\"400px\"></img>\n\nThis <a href=\"https://arxiv.org/abs/2110.02178\">paper</a> introduce MobileViT, a light-weight and general purpose vision transformer for mobile devices. MobileViT presents a different\nperspective for the global processing of information with transformers.\n\nYou can use it with the following code (ex. mobilevit_xs)\n\n```python\nimport torch\nfrom vit_pytorch.mobile_vit import MobileViT\n\nmbvit_xs = MobileViT(\n    image_size = (256, 256),\n    dims = [96, 120, 144],\n    channels = [16, 32, 48, 48, 64, 64, 80, 80, 96, 96, 384],\n    num_classes = 1000\n)\n\nimg = torch.randn(1, 3, 256, 256)\n\npred = mbvit_xs(img) # (1, 1000)\n```\n\n## XCiT\n\n<img src=\"./images/xcit.png\" width=\"400px\"></img>\n\nThis <a href=\"https://arxiv.org/abs/2106.09681\">paper</a> introduces the cross covariance attention (abbreviated XCA). One can think of it as doing attention across the features dimension rather than the spatial one (another perspective would be a dynamic 1x1 convolution, the kernel being attention map defined by spatial correlations).\n\nTechnically, this amounts to simply transposing the query, key, values before executing cosine similarity attention with learned temperature.\n\n```python\nimport torch\nfrom vit_pytorch.xcit import XCiT\n\nv = XCiT(\n    image_size = 256,\n    patch_size = 32,\n    num_classes = 1000,\n    dim = 1024,\n    depth = 12,                     # depth of xcit transformer\n    cls_depth = 2,                  # depth of cross attention of CLS tokens to patch, attention pool at end\n    heads = 16,\n    mlp_dim = 2048,\n    dropout = 0.1,\n    emb_dropout = 0.1,\n    layer_dropout = 0.05,           # randomly dropout 5% of the layers\n    local_patch_kernel_size = 3     # kernel size of the local patch interaction module (depthwise convs)\n)\n\nimg = torch.randn(1, 3, 256, 256)\n\npreds = v(img) # (1, 1000)\n```\n\n## Simple Masked Image Modeling\n\n<img src=\"./images/simmim.png\" width=\"400px\"/>\n\nThis <a href=\"https://arxiv.org/abs/2111.09886\">paper</a> proposes a simple masked image modeling (SimMIM) scheme, using only a linear projection off the masked tokens into pixel space followed by an L1 loss with the pixel values of the masked patches. Results are competitive with other more complicated approaches.\n\nYou can use this as follows\n\n```python\nimport torch\nfrom vit_pytorch import ViT\nfrom vit_pytorch.simmim import SimMIM\n\nv = ViT(\n    image_size = 256,\n    patch_size = 32,\n    num_classes = 1000,\n    dim = 1024,\n    depth = 6,\n    heads = 8,\n    mlp_dim = 2048\n)\n\nmim = SimMIM(\n    encoder = v,\n    masking_ratio = 0.5  # they found 50% to yield the best results\n)\n\nimages = torch.randn(8, 3, 256, 256)\n\nloss = mim(images)\nloss.backward()\n\n# that's all!\n# do the above in a for loop many times with a lot of images and your vision transformer will learn\n\ntorch.save(v.state_dict(), './trained-vit.pt')\n```\n\n\n## Masked Autoencoder\n\n<img src=\"./images/mae.png\" width=\"400px\"/>\n\nA new <a href=\"https://arxiv.org/abs/2111.06377\">Kaiming He paper</a> proposes a simple autoencoder scheme where the vision transformer attends to a set of unmasked patches, and a smaller decoder tries to reconstruct the masked pixel values.\n\n<a href=\"https://www.youtube.com/watch?v=LKixq2S2Pz8\">DeepReader quick paper review</a>\n\n<a href=\"https://www.youtube.com/watch?v=Dp6iICL2dVI\">AI Coffeebreak with Letitia</a>\n\nYou can use it with the following code\n\n```python\nimport torch\nfrom vit_pytorch import ViT, MAE\n\nv = ViT(\n    image_size = 256,\n    patch_size = 32,\n    num_classes = 1000,\n    dim = 1024,\n    depth = 6,\n    heads = 8,\n    mlp_dim = 2048\n)\n\nmae = MAE(\n    encoder = v,\n    masking_ratio = 0.75,   # the paper recommended 75% masked patches\n    decoder_dim = 512,      # paper showed good results with just 512\n    decoder_depth = 6       # anywhere from 1 to 8\n)\n\nimages = torch.randn(8, 3, 256, 256)\n\nloss = mae(images)\nloss.backward()\n\n# that's all!\n# do the above in a for loop many times with a lot of images and your vision transformer will learn\n\n# save your improved vision transformer\ntorch.save(v.state_dict(), './trained-vit.pt')\n```\n\n## Masked Patch Prediction\n\nThanks to <a href=\"https://github.com/zankner\">Zach</a>, you can train using the original masked patch prediction task presented in the paper, with the following code.\n\n```python\nimport torch\nfrom vit_pytorch import ViT\nfrom vit_pytorch.mpp import MPP\n\nmodel = ViT(\n    image_size=256,\n    patch_size=32,\n    num_classes=1000,\n    dim=1024,\n    depth=6,\n    heads=8,\n    mlp_dim=2048,\n    dropout=0.1,\n    emb_dropout=0.1\n)\n\nmpp_trainer = MPP(\n    transformer=model,\n    patch_size=32,\n    dim=1024,\n    mask_prob=0.15,          # probability of using token in masked prediction task\n    random_patch_prob=0.30,  # probability of randomly replacing a token being used for mpp\n    replace_prob=0.50,       # probability of replacing a token being used for mpp with the mask token\n)\n\nopt = torch.optim.Adam(mpp_trainer.parameters(), lr=3e-4)\n\ndef sample_unlabelled_images():\n    return torch.FloatTensor(20, 3, 256, 256).uniform_(0., 1.)\n\nfor _ in range(100):\n    images = sample_unlabelled_images()\n    loss = mpp_trainer(images)\n    opt.zero_grad()\n    loss.backward()\n    opt.step()\n\n# save your improved network\ntorch.save(model.state_dict(), './pretrained-net.pt')\n```\n\n## Masked Position Prediction\n\n<img src=\"./images/mp3.png\" width=\"400px\"></img>\n\nNew <a href=\"https://arxiv.org/abs/2207.07611\">paper</a> that introduces masked position prediction pre-training criteria. This strategy is more efficient than the Masked Autoencoder strategy and has comparable performance.  \n\n```python\nimport torch\nfrom vit_pytorch.mp3 import ViT, MP3\n\nv = ViT(\n    num_classes = 1000,\n    image_size = 256,\n    patch_size = 8,\n    dim = 1024,\n    depth = 6,\n    heads = 8,\n    mlp_dim = 2048,\n    dropout = 0.1,\n)\n\nmp3 = MP3(\n    vit = v,\n    masking_ratio = 0.75\n)\n\nimages = torch.randn(8, 3, 256, 256)\n\nloss = mp3(images)\nloss.backward()\n\n# that's all!\n# do the above in a for loop many times with a lot of images and your vision transformer will learn\n\n# save your improved vision transformer\ntorch.save(v.state_dict(), './trained-vit.pt')\n```\n\n## Adaptive Token Sampling\n\n<img src=\"./images/ats.png\" width=\"400px\"></img>\n\nThis <a href=\"https://arxiv.org/abs/2111.15667\">paper</a> proposes to use the CLS attention scores, re-weighed by the norms of the value heads, as means to discard unimportant tokens at different layers.\n\n```python\nimport torch\nfrom vit_pytorch.ats_vit import ViT\n\nv = ViT(\n    image_size = 256,\n    patch_size = 16,\n    num_classes = 1000,\n    dim = 1024,\n    depth = 6,\n    max_tokens_per_depth = (256, 128, 64, 32, 16, 8), # a tuple that denotes the maximum number of tokens that any given layer should have. if the layer has greater than this amount, it will undergo adaptive token sampling\n    heads = 16,\n    mlp_dim = 2048,\n    dropout = 0.1,\n    emb_dropout = 0.1\n)\n\nimg = torch.randn(4, 3, 256, 256)\n\npreds = v(img) # (4, 1000)\n\n# you can also get a list of the final sampled patch ids\n# a value of -1 denotes padding\n\npreds, token_ids = v(img, return_sampled_token_ids = True) # (4, 1000), (4, <=8)\n```\n\n## Patch Merger\n\n\n<img src=\"./images/patch_merger.png\" width=\"400px\"></img>\n\nThis <a href=\"https://arxiv.org/abs/2202.12015\">paper</a> proposes a simple module (Patch Merger) for reducing the number of tokens at any layer of a vision transformer without sacrificing performance.\n\n```python\nimport torch\nfrom vit_pytorch.vit_with_patch_merger import ViT\n\nv = ViT(\n    image_size = 256,\n    patch_size = 16,\n    num_classes = 1000,\n    dim = 1024,\n    depth = 12,\n    heads = 8,\n    patch_merge_layer = 6,        # at which transformer layer to do patch merging\n    patch_merge_num_tokens = 8,   # the output number of tokens from the patch merge\n    mlp_dim = 2048,\n    dropout = 0.1,\n    emb_dropout = 0.1\n)\n\nimg = torch.randn(4, 3, 256, 256)\n\npreds = v(img) # (4, 1000)\n```\n\nOne can also use the `PatchMerger` module by itself\n\n```python\nimport torch\nfrom vit_pytorch.vit_with_patch_merger import PatchMerger\n\nmerger = PatchMerger(\n    dim = 1024,\n    num_tokens_out = 8   # output number of tokens\n)\n\nfeatures = torch.randn(4, 256, 1024) # (batch, num tokens, dimension)\n\nout = merger(features) # (4, 8, 1024)\n```\n\n## Vision Transformer for Small Datasets\n\n<img src=\"./images/vit_for_small_datasets.png\" width=\"400px\"></img>\n\nThis <a href=\"https://arxiv.org/abs/2112.13492\">paper</a> proposes a new image to patch function that incorporates shifts of the image, before normalizing and dividing the image into patches. I have found shifting to be extremely helpful in some other transformers work, so decided to include this for further explorations. It also includes the `LSA` with the learned temperature and masking out of a token's attention to itself.\n\nYou can use as follows:\n\n```python\nimport torch\nfrom vit_pytorch.vit_for_small_dataset import ViT\n\nv = ViT(\n    image_size = 256,\n    patch_size = 16,\n    num_classes = 1000,\n    dim = 1024,\n    depth = 6,\n    heads = 16,\n    mlp_dim = 2048,\n    dropout = 0.1,\n    emb_dropout = 0.1\n)\n\nimg = torch.randn(4, 3, 256, 256)\n\npreds = v(img) # (1, 1000)\n```\n\nYou can also use the `SPT` from this paper as a standalone module\n\n```python\nimport torch\nfrom vit_pytorch.vit_for_small_dataset import SPT\n\nspt = SPT(\n    dim = 1024,\n    patch_size = 16,\n    channels = 3\n)\n\nimg = torch.randn(4, 3, 256, 256)\n\ntokens = spt(img) # (4, 256, 1024)\n```\n\n## 3D ViT\n\nBy popular request, I will start extending a few of the architectures in this repository to 3D ViTs, for use with video, medical imaging, etc.\n\nYou will need to pass in two additional hyperparameters: (1) the number of frames `frames` and (2) patch size along the frame dimension `frame_patch_size`\n\nFor starters, 3D ViT\n\n```python\nimport torch\nfrom vit_pytorch.vit_3d import ViT\n\nv = ViT(\n    image_size = 128,          # image size\n    frames = 16,               # number of frames\n    image_patch_size = 16,     # image patch size\n    frame_patch_size = 2,      # frame patch size\n    num_classes = 1000,\n    dim = 1024,\n    depth = 6,\n    heads = 8,\n    mlp_dim = 2048,\n    dropout = 0.1,\n    emb_dropout = 0.1\n)\n\nvideo = torch.randn(4, 3, 16, 128, 128) # (batch, channels, frames, height, width)\n\npreds = v(video) # (4, 1000)\n```\n\n3D Simple ViT\n\n```python\nimport torch\nfrom vit_pytorch.simple_vit_3d import SimpleViT\n\nv = SimpleViT(\n    image_size = 128,          # image size\n    frames = 16,               # number of frames\n    image_patch_size = 16,     # image patch size\n    frame_patch_size = 2,      # frame patch size\n    num_classes = 1000,\n    dim = 1024,\n    depth = 6,\n    heads = 8,\n    mlp_dim = 2048\n)\n\nvideo = torch.randn(4, 3, 16, 128, 128) # (batch, channels, frames, height, width)\n\npreds = v(video) # (4, 1000)\n```\n\n3D version of <a href=\"https://github.com/lucidrains/vit-pytorch#cct\">CCT</a>\n\n```python\nimport torch\nfrom vit_pytorch.cct_3d import CCT\n\ncct = CCT(\n    img_size = 224,\n    num_frames = 8,\n    embedding_dim = 384,\n    n_conv_layers = 2,\n    frame_kernel_size = 3,\n    kernel_size = 7,\n    stride = 2,\n    padding = 3,\n    pooling_kernel_size = 3,\n    pooling_stride = 2,\n    pooling_padding = 1,\n    num_layers = 14,\n    num_heads = 6,\n    mlp_ratio = 3.,\n    num_classes = 1000,\n    positional_embedding = 'learnable'\n)\n\nvideo = torch.randn(1, 3, 8, 224, 224) # (batch, channels, frames, height, width)\npred = cct(video)\n```\n\n## ViViT\n\n<img src=\"./images/vivit.png\" width=\"350px\"></img>\n\nThis <a href=\"https://arxiv.org/abs/2103.15691\">paper</a> offers 3 different types of architectures for efficient attention of videos, with the main theme being factorizing the attention across space and time. This repository includes the factorized encoder and the factorized self-attention variant.\nThe factorized encoder variant is a spatial transformer followed by a temporal one. The factorized self-attention variant is a spatio-temporal transformer with alternating spatial and temporal self-attention layers.\n\n```python\nimport torch\nfrom vit_pytorch.vivit import ViT\n\nv = ViT(\n    image_size = 128,          # image size\n    frames = 16,               # number of frames\n    image_patch_size = 16,     # image patch size\n    frame_patch_size = 2,      # frame patch size\n    num_classes = 1000,\n    dim = 1024,\n    spatial_depth = 6,         # depth of the spatial transformer\n    temporal_depth = 6,        # depth of the temporal transformer\n    heads = 8,\n    mlp_dim = 2048,\n    variant = 'factorized_encoder', # or 'factorized_self_attention'\n)\n\nvideo = torch.randn(4, 3, 16, 128, 128) # (batch, channels, frames, height, width)\n\npreds = v(video) # (4, 1000)\n```\n\n## Parallel ViT\n\n<img src=\"./images/parallel-vit.png\" width=\"350px\"></img>\n\nThis <a href=\"https://arxiv.org/abs/2203.09795\">paper</a> propose parallelizing multiple attention and feedforward blocks per layer (2 blocks), claiming that it is easier to train without loss of performance.\n\nYou can try this variant as follows\n\n```python\nimport torch\nfrom vit_pytorch.parallel_vit import ViT\n\nv = ViT(\n    image_size = 256,\n    patch_size = 16,\n    num_classes = 1000,\n    dim = 1024,\n    depth = 6,\n    heads = 8,\n    mlp_dim = 2048,\n    num_parallel_branches = 2,  # in paper, they claimed 2 was optimal\n    dropout = 0.1,\n    emb_dropout = 0.1\n)\n\nimg = torch.randn(4, 3, 256, 256)\n\npreds = v(img) # (4, 1000)\n```\n\n## Learnable Memory ViT\n\n<img src=\"./images/learnable-memory-vit.png\" width=\"350px\"></img>\n\nThis <a href=\"https://arxiv.org/abs/2203.15243\">paper</a> shows that adding learnable memory tokens at each layer of a vision transformer can greatly enhance fine-tuning results (in addition to learnable task specific CLS token and adapter head).\n\nYou can use this with a specially modified `ViT` as follows\n\n```python\nimport torch\nfrom vit_pytorch.learnable_memory_vit import ViT, Adapter\n\n# normal base ViT\n\nv = ViT(\n    image_size = 256,\n    patch_size = 16,\n    num_classes = 1000,\n    dim = 1024,\n    depth = 6,\n    heads = 8,\n    mlp_dim = 2048,\n    dropout = 0.1,\n    emb_dropout = 0.1\n)\n\nimg = torch.randn(4, 3, 256, 256)\nlogits = v(img) # (4, 1000)\n\n# do your usual training with ViT\n# ...\n\n\n# then, to finetune, just pass the ViT into the Adapter class\n# you can do this for multiple Adapters, as shown below\n\nadapter1 = Adapter(\n    vit = v,\n    num_classes = 2,               # number of output classes for this specific task\n    num_memories_per_layer = 5     # number of learnable memories per layer, 10 was sufficient in paper\n)\n\nlogits1 = adapter1(img) # (4, 2) - predict 2 classes off frozen ViT backbone with learnable memories and task specific head\n\n# yet another task to finetune on, this time with 4 classes\n\nadapter2 = Adapter(\n    vit = v,\n    num_classes = 4,\n    num_memories_per_layer = 10\n)\n\nlogits2 = adapter2(img) # (4, 4) - predict 4 classes off frozen ViT backbone with learnable memories and task specific head\n\n```\n\n## Dino\n\n<img src=\"./images/dino.png\" width=\"350px\"></img>\n\nYou can train `ViT` with the recent SOTA self-supervised learning technique, <a href=\"https://arxiv.org/abs/2104.14294\">Dino</a>, with the following code.\n\n<a href=\"https://www.youtube.com/watch?v=h3ij3F3cPIk\">Yannic Kilcher</a> video\n\n```python\nimport torch\nfrom vit_pytorch import ViT, Dino\n\nmodel = ViT(\n    image_size = 256,\n    patch_size = 32,\n    num_classes = 1000,\n    dim = 1024,\n    depth = 6,\n    heads = 8,\n    mlp_dim = 2048\n)\n\nlearner = Dino(\n    model,\n    image_size = 256,\n    hidden_layer = 'to_latent',        # hidden layer name or index, from which to extract the embedding\n    projection_hidden_size = 256,      # projector network hidden dimension\n    projection_layers = 4,             # number of layers in projection network\n    num_classes_K = 65336,             # output logits dimensions (referenced as K in paper)\n    student_temp = 0.9,                # student temperature\n    teacher_temp = 0.04,               # teacher temperature, needs to be annealed from 0.04 to 0.07 over 30 epochs\n    local_upper_crop_scale = 0.4,      # upper bound for local crop - 0.4 was recommended in the paper \n    global_lower_crop_scale = 0.5,     # lower bound for global crop - 0.5 was recommended in the paper\n    moving_average_decay = 0.9,        # moving average of encoder - paper showed anywhere from 0.9 to 0.999 was ok\n    center_moving_average_decay = 0.9, # moving average of teacher centers - paper showed anywhere from 0.9 to 0.999 was ok\n)\n\nopt = torch.optim.Adam(learner.parameters(), lr = 3e-4)\n\ndef sample_unlabelled_images():\n    return torch.randn(20, 3, 256, 256)\n\nfor _ in range(100):\n    images = sample_unlabelled_images()\n    loss = learner(images)\n    opt.zero_grad()\n    loss.backward()\n    opt.step()\n    learner.update_moving_average() # update moving average of teacher encoder and teacher centers\n\n# save your improved network\ntorch.save(model.state_dict(), './pretrained-net.pt')\n```\n\n## EsViT\n\n<img src=\"./images/esvit.png\" width=\"350px\"></img>\n\n<a href=\"https://arxiv.org/abs/2106.09785\">`EsViT`</a> is a variant of Dino (from above) re-engineered to support efficient `ViT`s with patch merging / downsampling by taking into an account an extra regional loss between the augmented views. To quote the abstract, it `outperforms its supervised counterpart on 17 out of 18 datasets` at 3 times higher throughput.\n\nEven though it is named as though it were a new `ViT` variant, it actually is just a strategy for training any multistage `ViT` (in the paper, they focused on Swin). The example below will show how to use it with `CvT`. You'll need to set the `hidden_layer` to the name of the layer within your efficient ViT that outputs the non-average pooled visual representations, just before the global pooling and projection to logits.\n\n```python\nimport torch\nfrom vit_pytorch.cvt import CvT\nfrom vit_pytorch.es_vit import EsViTTrainer\n\ncvt = CvT(\n    num_classes = 1000,\n    s1_emb_dim = 64,\n    s1_emb_kernel = 7,\n    s1_emb_stride = 4,\n    s1_proj_kernel = 3,\n    s1_kv_proj_stride = 2,\n    s1_heads = 1,\n    s1_depth = 1,\n    s1_mlp_mult = 4,\n    s2_emb_dim = 192,\n    s2_emb_kernel = 3,\n    s2_emb_stride = 2,\n    s2_proj_kernel = 3,\n    s2_kv_proj_stride = 2,\n    s2_heads = 3,\n    s2_depth = 2,\n    s2_mlp_mult = 4,\n    s3_emb_dim = 384,\n    s3_emb_kernel = 3,\n    s3_emb_stride = 2,\n    s3_proj_kernel = 3,\n    s3_kv_proj_stride = 2,\n    s3_heads = 4,\n    s3_depth = 10,\n    s3_mlp_mult = 4,\n    dropout = 0.\n)\n\nlearner = EsViTTrainer(\n    cvt,\n    image_size = 256,\n    hidden_layer = 'layers',           # hidden layer name or index, from which to extract the embedding\n    projection_hidden_size = 256,      # projector network hidden dimension\n    projection_layers = 4,             # number of layers in projection network\n    num_classes_K = 65336,             # output logits dimensions (referenced as K in paper)\n    student_temp = 0.9,                # student temperature\n    teacher_temp = 0.04,               # teacher temperature, needs to be annealed from 0.04 to 0.07 over 30 epochs\n    local_upper_crop_scale = 0.4,      # upper bound for local crop - 0.4 was recommended in the paper\n    global_lower_crop_scale = 0.5,     # lower bound for global crop - 0.5 was recommended in the paper\n    moving_average_decay = 0.9,        # moving average of encoder - paper showed anywhere from 0.9 to 0.999 was ok\n    center_moving_average_decay = 0.9, # moving average of teacher centers - paper showed anywhere from 0.9 to 0.999 was ok\n)\n\nopt = torch.optim.AdamW(learner.parameters(), lr = 3e-4)\n\ndef sample_unlabelled_images():\n    return torch.randn(8, 3, 256, 256)\n\nfor _ in range(1000):\n    images = sample_unlabelled_images()\n    loss = learner(images)\n    opt.zero_grad()\n    loss.backward()\n    opt.step()\n    learner.update_moving_average() # update moving average of teacher encoder and teacher centers\n\n# save your improved network\ntorch.save(cvt.state_dict(), './pretrained-net.pt')\n```\n\n## Accessing Attention\n\nIf you would like to visualize the attention weights (post-softmax) for your research, just follow the procedure below\n\n```python\nimport torch\nfrom vit_pytorch.vit import ViT\n\nv = ViT(\n    image_size = 256,\n    patch_size = 32,\n    num_classes = 1000,\n    dim = 1024,\n    depth = 6,\n    heads = 16,\n    mlp_dim = 2048,\n    dropout = 0.1,\n    emb_dropout = 0.1\n)\n\n# import Recorder and wrap the ViT\n\nfrom vit_pytorch.recorder import Recorder\nv = Recorder(v)\n\n# forward pass now returns predictions and the attention maps\n\nimg = torch.randn(1, 3, 256, 256)\npreds, attns = v(img)\n\n# there is one extra patch due to the CLS token\n\nattns # (1, 6, 16, 65, 65) - (batch x layers x heads x patch x patch)\n```\n\nto cleanup the class and the hooks once you have collected enough data\n\n```python\nv = v.eject()  # wrapper is discarded and original ViT instance is returned\n```\n\n## Accessing Embeddings\n\nYou can similarly access the embeddings with the `Extractor` wrapper\n\n```python\nimport torch\nfrom vit_pytorch.vit import ViT\n\nv = ViT(\n    image_size = 256,\n    patch_size = 32,\n    num_classes = 1000,\n    dim = 1024,\n    depth = 6,\n    heads = 16,\n    mlp_dim = 2048,\n    dropout = 0.1,\n    emb_dropout = 0.1\n)\n\n# import Recorder and wrap the ViT\n\nfrom vit_pytorch.extractor import Extractor\nv = Extractor(v)\n\n# forward pass now returns predictions and the attention maps\n\nimg = torch.randn(1, 3, 256, 256)\nlogits, embeddings = v(img)\n\n# there is one extra token due to the CLS token\n\nembeddings # (1, 65, 1024) - (batch x patches x model dim)\n```\n\nOr say for `CrossViT`, which has a multi-scale encoder that outputs two sets of embeddings for 'large' and 'small' scales\n\n```python\nimport torch\nfrom vit_pytorch.cross_vit import CrossViT\n\nv = CrossViT(\n    image_size = 256,\n    num_classes = 1000,\n    depth = 4,\n    sm_dim = 192,\n    sm_patch_size = 16,\n    sm_enc_depth = 2,\n    sm_enc_heads = 8,\n    sm_enc_mlp_dim = 2048,\n    lg_dim = 384,\n    lg_patch_size = 64,\n    lg_enc_depth = 3,\n    lg_enc_heads = 8,\n    lg_enc_mlp_dim = 2048,\n    cross_attn_depth = 2,\n    cross_attn_heads = 8,\n    dropout = 0.1,\n    emb_dropout = 0.1\n)\n\n# wrap the CrossViT\n\nfrom vit_pytorch.extractor import Extractor\nv = Extractor(v, layer_name = 'multi_scale_encoder') # take embedding coming from the output of multi-scale-encoder\n\n# forward pass now returns predictions and the attention maps\n\nimg = torch.randn(1, 3, 256, 256)\nlogits, embeddings = v(img)\n\n# there is one extra token due to the CLS token\n\nembeddings # ((1, 257, 192), (1, 17, 384)) - (batch x patches x dimension) <- large and small scales respectively\n```\n\n## Research Ideas\n\n### Efficient Attention\n\nThere may be some coming from computer vision who think attention still suffers from quadratic costs. Fortunately, we have a lot of new techniques that may help. This repository offers a way for you to plugin your own sparse attention transformer.\n\nAn example with <a href=\"https://arxiv.org/abs/2102.03902\">Nystromformer</a>\n\n```bash\n$ pip install nystrom-attention\n```\n\n```python\nimport torch\nfrom vit_pytorch.efficient import ViT\nfrom nystrom_attention import Nystromformer\n\nefficient_transformer = Nystromformer(\n    dim = 512,\n    depth = 12,\n    heads = 8,\n    num_landmarks = 256\n)\n\nv = ViT(\n    dim = 512,\n    image_size = 2048,\n    patch_size = 32,\n    num_classes = 1000,\n    transformer = efficient_transformer\n)\n\nimg = torch.randn(1, 3, 2048, 2048) # your high resolution picture\nv(img) # (1, 1000)\n```\n\nOther sparse attention frameworks I would highly recommend is <a href=\"https://github.com/lucidrains/routing-transformer\">Routing Transformer</a> or <a href=\"https://github.com/lucidrains/sinkhorn-transformer\">Sinkhorn Transformer</a>\n\n### Combining with other Transformer improvements\n\nThis paper purposely used the most vanilla of attention networks to make a statement. If you would like to use some of the latest improvements for attention nets, please use the `Encoder` from <a href=\"https://github.com/lucidrains/x-transformers\">this repository</a>.\n\nex.\n\n```bash\n$ pip install x-transformers\n```\n\n```python\nimport torch\nfrom vit_pytorch.efficient import ViT\nfrom x_transformers import Encoder\n\nv = ViT(\n    dim = 512,\n    image_size = 224,\n    patch_size = 16,\n    num_classes = 1000,\n    transformer = Encoder(\n        dim = 512,                  # set to be the same as the wrapper\n        depth = 12,\n        heads = 8,\n        ff_glu = True,              # ex. feed forward GLU variant https://arxiv.org/abs/2002.05202\n        residual_attn = True        # ex. residual attention https://arxiv.org/abs/2012.11747\n    )\n)\n\nimg = torch.randn(1, 3, 224, 224)\nv(img) # (1, 1000)\n```\n\n## FAQ\n\n- How do I pass in non-square images?\n\nYou can already pass in non-square images - you just have to make sure your height and width is less than or equal to the `image_size`, and both divisible by the `patch_size`\n\nex.\n\n```python\nimport torch\nfrom vit_pytorch import ViT\n\nv = ViT(\n    image_size = 256,\n    patch_size = 32,\n    num_classes = 1000,\n    dim = 1024,\n    depth = 6,\n    heads = 16,\n    mlp_dim = 2048,\n    dropout = 0.1,\n    emb_dropout = 0.1\n)\n\nimg = torch.randn(1, 3, 256, 128) # <-- not a square\n\npreds = v(img) # (1, 1000)\n```\n\n- How do I pass in non-square patches?\n\n```python\nimport torch\nfrom vit_pytorch import ViT\n\nv = ViT(\n    num_classes = 1000,\n    image_size = (256, 128),  # image size is a tuple of (height, width)\n    patch_size = (32, 16),    # patch size is a tuple of (height, width)\n    dim = 1024,\n    depth = 6,\n    heads = 16,\n    mlp_dim = 2048,\n    dropout = 0.1,\n    emb_dropout = 0.1\n)\n\nimg = torch.randn(1, 3, 256, 128)\n\npreds = v(img)\n```\n\n## Resources\n\nComing from computer vision and new to transformers? Here are some resources that greatly accelerated my learning.\n\n1. <a href=\"http://jalammar.github.io/illustrated-transformer/\">Illustrated Transformer</a> - Jay Alammar\n\n2. <a href=\"http://peterbloem.nl/blog/transformers\">Transformers from Scratch</a>  - Peter Bloem\n\n3. <a href=\"https://nlp.seas.harvard.edu/2018/04/03/attention.html\">The Annotated Transformer</a> - Harvard NLP\n\n\n## Citations\n```bibtex\n@article{hassani2021escaping,\n    title   = {Escaping the Big Data Paradigm with Compact Transformers},\n    author  = {Ali Hassani and Steven Walton and Nikhil Shah and Abulikemu Abuduweili and Jiachen Li and Humphrey Shi},\n    year    = 2021,\n    url     = {https://arxiv.org/abs/2104.05704},\n    eprint  = {2104.05704},\n    archiveprefix = {arXiv},\n    primaryclass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{dosovitskiy2020image,\n    title   = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},\n    author  = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},\n    year    = {2020},\n    eprint  = {2010.11929},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{touvron2020training,\n    title   = {Training data-efficient image transformers & distillation through attention}, \n    author  = {Hugo Touvron and Matthieu Cord and Matthijs Douze and Francisco Massa and Alexandre Sablayrolles and Herv\u00e9 J\u00e9gou},\n    year    = {2020},\n    eprint  = {2012.12877},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{yuan2021tokenstotoken,\n    title   = {Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet},\n    author  = {Li Yuan and Yunpeng Chen and Tao Wang and Weihao Yu and Yujun Shi and Francis EH Tay and Jiashi Feng and Shuicheng Yan},\n    year    = {2021},\n    eprint  = {2101.11986},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{zhou2021deepvit,\n    title   = {DeepViT: Towards Deeper Vision Transformer},\n    author  = {Daquan Zhou and Bingyi Kang and Xiaojie Jin and Linjie Yang and Xiaochen Lian and Qibin Hou and Jiashi Feng},\n    year    = {2021},\n    eprint  = {2103.11886},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{touvron2021going,\n    title   = {Going deeper with Image Transformers}, \n    author  = {Hugo Touvron and Matthieu Cord and Alexandre Sablayrolles and Gabriel Synnaeve and Herv\u00e9 J\u00e9gou},\n    year    = {2021},\n    eprint  = {2103.17239},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{chen2021crossvit,\n    title   = {CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification},\n    author  = {Chun-Fu Chen and Quanfu Fan and Rameswar Panda},\n    year    = {2021},\n    eprint  = {2103.14899},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{wu2021cvt,\n    title   = {CvT: Introducing Convolutions to Vision Transformers},\n    author  = {Haiping Wu and Bin Xiao and Noel Codella and Mengchen Liu and Xiyang Dai and Lu Yuan and Lei Zhang},\n    year    = {2021},\n    eprint  = {2103.15808},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{heo2021rethinking,\n    title   = {Rethinking Spatial Dimensions of Vision Transformers}, \n    author  = {Byeongho Heo and Sangdoo Yun and Dongyoon Han and Sanghyuk Chun and Junsuk Choe and Seong Joon Oh},\n    year    = {2021},\n    eprint  = {2103.16302},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{graham2021levit,\n    title   = {LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference},\n    author  = {Ben Graham and Alaaeldin El-Nouby and Hugo Touvron and Pierre Stock and Armand Joulin and Herv\u00e9 J\u00e9gou and Matthijs Douze},\n    year    = {2021},\n    eprint  = {2104.01136},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{li2021localvit,\n    title   = {LocalViT: Bringing Locality to Vision Transformers},\n    author  = {Yawei Li and Kai Zhang and Jiezhang Cao and Radu Timofte and Luc Van Gool},\n    year    = {2021},\n    eprint  = {2104.05707},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{chu2021twins,\n    title   = {Twins: Revisiting Spatial Attention Design in Vision Transformers},\n    author  = {Xiangxiang Chu and Zhi Tian and Yuqing Wang and Bo Zhang and Haibing Ren and Xiaolin Wei and Huaxia Xia and Chunhua Shen},\n    year    = {2021},\n    eprint  = {2104.13840},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{su2021roformer,\n    title   = {RoFormer: Enhanced Transformer with Rotary Position Embedding}, \n    author  = {Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu},\n    year    = {2021},\n    eprint  = {2104.09864},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}\n```\n\n```bibtex\n@misc{zhang2021aggregating,\n    title   = {Aggregating Nested Transformers},\n    author  = {Zizhao Zhang and Han Zhang and Long Zhao and Ting Chen and Tomas Pfister},\n    year    = {2021},\n    eprint  = {2105.12723},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{chen2021regionvit,\n    title   = {RegionViT: Regional-to-Local Attention for Vision Transformers}, \n    author  = {Chun-Fu Chen and Rameswar Panda and Quanfu Fan},\n    year    = {2021},\n    eprint  = {2106.02689},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{wang2021crossformer,\n    title   = {CrossFormer: A Versatile Vision Transformer Hinging on Cross-scale Attention}, \n    author  = {Wenxiao Wang and Lu Yao and Long Chen and Binbin Lin and Deng Cai and Xiaofei He and Wei Liu},\n    year    = {2021},\n    eprint  = {2108.00154},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{caron2021emerging,\n    title   = {Emerging Properties in Self-Supervised Vision Transformers},\n    author  = {Mathilde Caron and Hugo Touvron and Ishan Misra and Herv\u00e9 J\u00e9gou and Julien Mairal and Piotr Bojanowski and Armand Joulin},\n    year    = {2021},\n    eprint  = {2104.14294},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{he2021masked,\n    title   = {Masked Autoencoders Are Scalable Vision Learners}, \n    author  = {Kaiming He and Xinlei Chen and Saining Xie and Yanghao Li and Piotr Doll\u00e1r and Ross Girshick},\n    year    = {2021},\n    eprint  = {2111.06377},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{xie2021simmim,\n    title   = {SimMIM: A Simple Framework for Masked Image Modeling}, \n    author  = {Zhenda Xie and Zheng Zhang and Yue Cao and Yutong Lin and Jianmin Bao and Zhuliang Yao and Qi Dai and Han Hu},\n    year    = {2021},\n    eprint  = {2111.09886},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{fayyaz2021ats,\n    title   = {ATS: Adaptive Token Sampling For Efficient Vision Transformers},\n    author  = {Mohsen Fayyaz and Soroush Abbasi Kouhpayegani and Farnoush Rezaei Jafari and Eric Sommerlade and Hamid Reza Vaezi Joze and Hamed Pirsiavash and Juergen Gall},\n    year    = {2021},\n    eprint  = {2111.15667},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{mehta2021mobilevit,\n    title   = {MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer},\n    author  = {Sachin Mehta and Mohammad Rastegari},\n    year    = {2021},\n    eprint  = {2110.02178},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{lee2021vision,\n    title   = {Vision Transformer for Small-Size Datasets}, \n    author  = {Seung Hoon Lee and Seunghyun Lee and Byung Cheol Song},\n    year    = {2021},\n    eprint  = {2112.13492},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{renggli2022learning,\n    title   = {Learning to Merge Tokens in Vision Transformers},\n    author  = {Cedric Renggli and Andr\u00e9 Susano Pinto and Neil Houlsby and Basil Mustafa and Joan Puigcerver and Carlos Riquelme},\n    year    = {2022},\n    eprint  = {2202.12015},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{yang2022scalablevit,\n    title   = {ScalableViT: Rethinking the Context-oriented Generalization of Vision Transformer}, \n    author  = {Rui Yang and Hailong Ma and Jie Wu and Yansong Tang and Xuefeng Xiao and Min Zheng and Xiu Li},\n    year    = {2022},\n    eprint  = {2203.10790},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@inproceedings{Touvron2022ThreeTE,\n    title   = {Three things everyone should know about Vision Transformers},\n    author  = {Hugo Touvron and Matthieu Cord and Alaaeldin El-Nouby and Jakob Verbeek and Herv'e J'egou},\n    year    = {2022}\n}\n```\n\n```bibtex\n@inproceedings{Sandler2022FinetuningIT,\n    title   = {Fine-tuning Image Transformers using Learnable Memory},\n    author  = {Mark Sandler and Andrey Zhmoginov and Max Vladymyrov and Andrew Jackson},\n    year    = {2022}\n}\n```\n\n```bibtex\n@inproceedings{Li2022SepViTSV,\n    title   = {SepViT: Separable Vision Transformer},\n    author  = {Wei Li and Xing Wang and Xin Xia and Jie Wu and Xuefeng Xiao and Minghang Zheng and Shiping Wen},\n    year    = {2022}\n}\n```\n\n```bibtex\n@inproceedings{Tu2022MaxViTMV,\n    title   = {MaxViT: Multi-Axis Vision Transformer},\n    author  = {Zhengzhong Tu and Hossein Talebi and Han Zhang and Feng Yang and Peyman Milanfar and Alan Conrad Bovik and Yinxiao Li},\n    year    = {2022}\n}\n```\n\n```bibtex\n@article{Li2021EfficientSV,\n    title   = {Efficient Self-supervised Vision Transformers for Representation Learning},\n    author  = {Chunyuan Li and Jianwei Yang and Pengchuan Zhang and Mei Gao and Bin Xiao and Xiyang Dai and Lu Yuan and Jianfeng Gao},\n    journal = {ArXiv},\n    year    = {2021},\n    volume  = {abs/2106.09785}\n}\n```\n\n```bibtex\n@misc{Beyer2022BetterPlainViT\n    title     = {Better plain ViT baselines for ImageNet-1k},\n    author    = {Beyer, Lucas and Zhai, Xiaohua and Kolesnikov, Alexander},\n    publisher = {arXiv},\n    year      = {2022}\n}\n\n```\n\n```bibtex\n@article{Arnab2021ViViTAV,\n    title   = {ViViT: A Video Vision Transformer},\n    author  = {Anurag Arnab and Mostafa Dehghani and Georg Heigold and Chen Sun and Mario Lucic and Cordelia Schmid},\n    journal = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},\n    year    = {2021},\n    pages   = {6816-6826}\n}\n```\n\n```bibtex\n@article{Liu2022PatchDropoutEV,\n    title   = {PatchDropout: Economizing Vision Transformers Using Patch Dropout},\n    author  = {Yue Liu and Christos Matsoukas and Fredrik Strand and Hossein Azizpour and Kevin Smith},\n    journal = {ArXiv},\n    year    = {2022},\n    volume  = {abs/2208.07220}\n}\n```\n\n```bibtex\n@misc{https://doi.org/10.48550/arxiv.2302.01327,\n    doi     = {10.48550/ARXIV.2302.01327},\n    url     = {https://arxiv.org/abs/2302.01327},\n    author  = {Kumar, Manoj and Dehghani, Mostafa and Houlsby, Neil},\n    title   = {Dual PatchNorm},\n    publisher = {arXiv},\n    year    = {2023},\n    copyright = {Creative Commons Attribution 4.0 International}\n}\n```\n\n```bibtex\n@inproceedings{Dehghani2023PatchNP,\n    title   = {Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution},\n    author  = {Mostafa Dehghani and Basil Mustafa and Josip Djolonga and Jonathan Heek and Matthias Minderer and Mathilde Caron and Andreas Steiner and Joan Puigcerver and Robert Geirhos and Ibrahim M. Alabdulmohsin and Avital Oliver and Piotr Padlewski and Alexey A. Gritsenko and Mario Luvci'c and Neil Houlsby},\n    year    = {2023}\n}\n```\n\n```bibtex\n@misc{vaswani2017attention,\n    title   = {Attention Is All You Need},\n    author  = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},\n    year    = {2017},\n    eprint  = {1706.03762},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}\n```\n\n```bibtex\n@inproceedings{dao2022flashattention,\n    title   = {Flash{A}ttention: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},\n    author  = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\\'e}, Christopher},\n    booktitle = {Advances in Neural Information Processing Systems},\n    year    = {2022}\n}\n```\n\n```bibtex\n@inproceedings{Darcet2023VisionTN,\n    title   = {Vision Transformers Need Registers},\n    author  = {Timoth'ee Darcet and Maxime Oquab and Julien Mairal and Piotr Bojanowski},\n    year    = {2023},\n    url     = {https://api.semanticscholar.org/CorpusID:263134283}\n}\n```\n\n```bibtex\n@inproceedings{ElNouby2021XCiTCI,\n    title   = {XCiT: Cross-Covariance Image Transformers},\n    author  = {Alaaeldin El-Nouby and Hugo Touvron and Mathilde Caron and Piotr Bojanowski and Matthijs Douze and Armand Joulin and Ivan Laptev and Natalia Neverova and Gabriel Synnaeve and Jakob Verbeek and Herv{\\'e} J{\\'e}gou},\n    booktitle = {Neural Information Processing Systems},\n    year    = {2021},\n    url     = {https://api.semanticscholar.org/CorpusID:235458262}\n}\n```\n\n```bibtex\n@inproceedings{Koner2024LookupViTCV,\n    title   = {LookupViT: Compressing visual information to a limited number of tokens},\n    author  = {Rajat Koner and Gagan Jain and Prateek Jain and Volker Tresp and Sujoy Paul},\n    year    = {2024},\n    url     = {https://api.semanticscholar.org/CorpusID:271244592}\n}\n```\n\n```bibtex\n@article{Bao2022AllAW,\n    title   = {All are Worth Words: A ViT Backbone for Diffusion Models},\n    author  = {Fan Bao and Shen Nie and Kaiwen Xue and Yue Cao and Chongxuan Li and Hang Su and Jun Zhu},\n    journal = {2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    year    = {2022},\n    pages   = {22669-22679},\n    url     = {https://api.semanticscholar.org/CorpusID:253581703}\n}\n```\n\n```bibtex\n@misc{Rubin2024,\n    author  = {Ohad Rubin},\n    url     = {https://medium.com/@ohadrubin/exploring-weight-decay-in-layer-normalization-challenges-and-a-reparameterization-solution-ad4d12c24950}\n}\n```\n\n```bibtex\n@inproceedings{Loshchilov2024nGPTNT,\n    title   = {nGPT: Normalized Transformer with Representation Learning on the Hypersphere},\n    author  = {Ilya Loshchilov and Cheng-Ping Hsieh and Simeng Sun and Boris Ginsburg},\n    year    = {2024},\n    url     = {https://api.semanticscholar.org/CorpusID:273026160}\n}\n```\n\n```bibtex\n@inproceedings{Liu2017DeepHL,\n    title   = {Deep Hyperspherical Learning},\n    author  = {Weiyang Liu and Yanming Zhang and Xingguo Li and Zhen Liu and Bo Dai and Tuo Zhao and Le Song},\n    booktitle = {Neural Information Processing Systems},\n    year    = {2017},\n    url     = {https://api.semanticscholar.org/CorpusID:5104558}\n}\n```\n\n```bibtex\n@inproceedings{Zhou2024ValueRL,\n    title   = {Value Residual Learning For Alleviating Attention Concentration In Transformers},\n    author  = {Zhanchao Zhou and Tianyi Wu and Zhiyun Jiang and Zhenzhong Lan},\n    year    = {2024},\n    url     = {https://api.semanticscholar.org/CorpusID:273532030}\n}\n```\n\n```bibtex\n@article{Zhu2024HyperConnections,\n    title   = {Hyper-Connections},\n    author  = {Defa Zhu and Hongzhi Huang and Zihao Huang and Yutao Zeng and Yunyao Mao and Banggu Wu and Qiyang Min and Xun Zhou},\n    journal = {ArXiv},\n    year    = {2024},\n    volume  = {abs/2409.19606},\n    url     = {https://api.semanticscholar.org/CorpusID:272987528}\n}\n```\n\n```bibtex\n@inproceedings{Fuller2025SimplerFV,\n    title   = {Simpler Fast Vision Transformers with a Jumbo CLS Token},\n    author  = {Anthony Fuller and Yousef Yassin and Daniel G. Kyrollos and Evan Shelhamer and James R. Green},\n    year    = {2025},\n    url     = {https://api.semanticscholar.org/CorpusID:276557720}\n}\n```\n\n*I visualise a time when we will be to robots what dogs are to humans, and I\u2019m rooting for the machines.* \u2014 Claude Shannon\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 784181462,
    "name": "Perplexica",
    "full_name": "ItzCrazyKns/Perplexica",
    "description": "Perplexica is an AI-powered search engine. It is an Open source alternative to Perplexity AI",
    "html_url": "https://github.com/ItzCrazyKns/Perplexica",
    "clone_url": "https://github.com/ItzCrazyKns/Perplexica.git",
    "owner_login": "ItzCrazyKns",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/95534749?v=4",
    "stargazers_count": 23362,
    "watchers_count": 23362,
    "forks_count": 2440,
    "open_issues_count": 180,
    "size": 18217,
    "language": "TypeScript",
    "languages": {
      "TypeScript": 302446,
      "Dockerfile": 992,
      "JavaScript": 537,
      "CSS": 344,
      "Shell": 54
    },
    "topics": [
      "ai-search-engine",
      "artificial-intelligence",
      "machine-learning",
      "open-source-ai-search-engine",
      "open-source-perplexity-ai",
      "perplexica",
      "perplexity-ai",
      "search-engine",
      "searxng",
      "searxng-copilot"
    ],
    "license_name": "MIT License",
    "created_at": "2024-04-09T10:51:32+00:00",
    "updated_at": "2025-08-06T01:15:47+00:00",
    "pushed_at": "2025-08-05T13:46:51+00:00",
    "contributors_count": 37,
    "readme_length": 11519,
    "readme_content": "# \ud83d\ude80 Perplexica - An AI-powered search engine \ud83d\udd0e <!-- omit in toc -->\n\n<div align=\"center\" markdown=\"1\">\n   <sup>Special thanks to:</sup>\n   <br>\n   <br>\n   <a href=\"https://www.warp.dev/perplexica\">\n      <img alt=\"Warp sponsorship\" width=\"400\" src=\"https://github.com/user-attachments/assets/775dd593-9b5f-40f1-bf48-479faff4c27b\">\n   </a>\n\n### [Warp, the AI Devtool that lives in your terminal](https://www.warp.dev/perplexica)\n\n[Available for MacOS, Linux, & Windows](https://www.warp.dev/perplexica)\n\n</div>\n\n<hr/>\n\n[![Discord](https://dcbadge.limes.pink/api/server/26aArMy8tT?style=flat)](https://discord.gg/26aArMy8tT)\n\n![preview](.assets/perplexica-screenshot.png?)\n\n## Table of Contents <!-- omit in toc -->\n\n- [Overview](#overview)\n- [Preview](#preview)\n- [Features](#features)\n- [Installation](#installation)\n  - [Getting Started with Docker (Recommended)](#getting-started-with-docker-recommended)\n  - [Non-Docker Installation](#non-docker-installation)\n  - [Ollama Connection Errors](#ollama-connection-errors)\n- [Using as a Search Engine](#using-as-a-search-engine)\n- [Using Perplexica's API](#using-perplexicas-api)\n- [Expose Perplexica to a network](#expose-perplexica-to-network)\n- [One-Click Deployment](#one-click-deployment)\n- [Upcoming Features](#upcoming-features)\n- [Support Us](#support-us)\n  - [Donations](#donations)\n- [Contribution](#contribution)\n- [Help and Support](#help-and-support)\n\n## Overview\n\nPerplexica is an open-source AI-powered searching tool or an AI-powered search engine that goes deep into the internet to find answers. Inspired by Perplexity AI, it's an open-source option that not just searches the web but understands your questions. It uses advanced machine learning algorithms like similarity searching and embeddings to refine results and provides clear answers with sources cited.\n\nUsing SearxNG to stay current and fully open source, Perplexica ensures you always get the most up-to-date information without compromising your privacy.\n\nWant to know more about its architecture and how it works? You can read it [here](https://github.com/ItzCrazyKns/Perplexica/tree/master/docs/architecture/README.md).\n\n## Preview\n\n![video-preview](.assets/perplexica-preview.gif)\n\n## Features\n\n- **Local LLMs**: You can make use local LLMs such as Llama3 and Mixtral using Ollama.\n- **Two Main Modes:**\n  - **Copilot Mode:** (In development) Boosts search by generating different queries to find more relevant internet sources. Like normal search instead of just using the context by SearxNG, it visits the top matches and tries to find relevant sources to the user's query directly from the page.\n  - **Normal Mode:** Processes your query and performs a web search.\n- **Focus Modes:** Special modes to better answer specific types of questions. Perplexica currently has 6 focus modes:\n  - **All Mode:** Searches the entire web to find the best results.\n  - **Writing Assistant Mode:** Helpful for writing tasks that do not require searching the web.\n  - **Academic Search Mode:** Finds articles and papers, ideal for academic research.\n  - **YouTube Search Mode:** Finds YouTube videos based on the search query.\n  - **Wolfram Alpha Search Mode:** Answers queries that need calculations or data analysis using Wolfram Alpha.\n  - **Reddit Search Mode:** Searches Reddit for discussions and opinions related to the query.\n- **Current Information:** Some search tools might give you outdated info because they use data from crawling bots and convert them into embeddings and store them in a index. Unlike them, Perplexica uses SearxNG, a metasearch engine to get the results and rerank and get the most relevant source out of it, ensuring you always get the latest information without the overhead of daily data updates.\n- **API**: Integrate Perplexica into your existing applications and make use of its capibilities.\n\nIt has many more features like image and video search. Some of the planned features are mentioned in [upcoming features](#upcoming-features).\n\n## Installation\n\nThere are mainly 2 ways of installing Perplexica - With Docker, Without Docker. Using Docker is highly recommended.\n\n### Getting Started with Docker (Recommended)\n\n1. Ensure Docker is installed and running on your system.\n2. Clone the Perplexica repository:\n\n   ```bash\n   git clone https://github.com/ItzCrazyKns/Perplexica.git\n   ```\n\n3. After cloning, navigate to the directory containing the project files.\n\n4. Rename the `sample.config.toml` file to `config.toml`. For Docker setups, you need only fill in the following fields:\n\n   - `OPENAI`: Your OpenAI API key. **You only need to fill this if you wish to use OpenAI's models**.\n   - `OLLAMA`: Your Ollama API URL. You should enter it as `http://host.docker.internal:PORT_NUMBER`. If you installed Ollama on port 11434, use `http://host.docker.internal:11434`. For other ports, adjust accordingly. **You need to fill this if you wish to use Ollama's models instead of OpenAI's**.\n   - `GROQ`: Your Groq API key. **You only need to fill this if you wish to use Groq's hosted models**.\n   - `ANTHROPIC`: Your Anthropic API key. **You only need to fill this if you wish to use Anthropic models**.\n   - `Gemini`: Your Gemini API key. **You only need to fill this if you wish to use Google's models**.\n   - `DEEPSEEK`: Your Deepseek API key. **Only needed if you want Deepseek models.**\n   - `AIMLAPI`: Your AI/ML API key. **Only needed if you want to use AI/ML API models and embeddings.**\n\n     **Note**: You can change these after starting Perplexica from the settings dialog.\n\n   - `SIMILARITY_MEASURE`: The similarity measure to use (This is filled by default; you can leave it as is if you are unsure about it.)\n\n5. Ensure you are in the directory containing the `docker-compose.yaml` file and execute:\n\n   ```bash\n   docker compose up -d\n   ```\n\n6. Wait a few minutes for the setup to complete. You can access Perplexica at http://localhost:3000 in your web browser.\n\n**Note**: After the containers are built, you can start Perplexica directly from Docker without having to open a terminal.\n\n### Non-Docker Installation\n\n1. Install SearXNG and allow `JSON` format in the SearXNG settings.\n2. Clone the repository and rename the `sample.config.toml` file to `config.toml` in the root directory. Ensure you complete all required fields in this file.\n3. After populating the configuration run `npm i`.\n4. Install the dependencies and then execute `npm run build`.\n5. Finally, start the app by running `npm run start`\n\n**Note**: Using Docker is recommended as it simplifies the setup process, especially for managing environment variables and dependencies.\n\nSee the [installation documentation](https://github.com/ItzCrazyKns/Perplexica/tree/master/docs/installation) for more information like updating, etc.\n\n### Ollama Connection Errors\n\nIf you're encountering an Ollama connection error, it is likely due to the backend being unable to connect to Ollama's API. To fix this issue you can:\n\n1. **Check your Ollama API URL:** Ensure that the API URL is correctly set in the settings menu.\n2. **Update API URL Based on OS:**\n\n   - **Windows:** Use `http://host.docker.internal:11434`\n   - **Mac:** Use `http://host.docker.internal:11434`\n   - **Linux:** Use `http://<private_ip_of_host>:11434`\n\n   Adjust the port number if you're using a different one.\n\n3. **Linux Users - Expose Ollama to Network:**\n\n   - Inside `/etc/systemd/system/ollama.service`, you need to add `Environment=\"OLLAMA_HOST=0.0.0.0:11434\"`. (Change the port number if you are using a different one.) Then reload the systemd manager configuration with `systemctl daemon-reload`, and restart Ollama by `systemctl restart ollama`. For more information see [Ollama docs](https://github.com/ollama/ollama/blob/main/docs/faq.md#setting-environment-variables-on-linux)\n\n   - Ensure that the port (default is 11434) is not blocked by your firewall.\n\n## Using as a Search Engine\n\nIf you wish to use Perplexica as an alternative to traditional search engines like Google or Bing, or if you want to add a shortcut for quick access from your browser's search bar, follow these steps:\n\n1. Open your browser's settings.\n2. Navigate to the 'Search Engines' section.\n3. Add a new site search with the following URL: `http://localhost:3000/?q=%s`. Replace `localhost` with your IP address or domain name, and `3000` with the port number if Perplexica is not hosted locally.\n4. Click the add button. Now, you can use Perplexica directly from your browser's search bar.\n\n## Using Perplexica's API\n\nPerplexica also provides an API for developers looking to integrate its powerful search engine into their own applications. You can run searches, use multiple models and get answers to your queries.\n\nFor more details, check out the full documentation [here](https://github.com/ItzCrazyKns/Perplexica/tree/master/docs/API/SEARCH.md).\n\n## Expose Perplexica to network\n\nPerplexica runs on Next.js and handles all API requests. It works right away on the same network and stays accessible even with port forwarding.\n\n## One-Click Deployment\n\n[![Deploy to Sealos](https://raw.githubusercontent.com/labring-actions/templates/main/Deploy-on-Sealos.svg)](https://usw.sealos.io/?openapp=system-template%3FtemplateName%3Dperplexica)\n[![Deploy to RepoCloud](https://d16t0pc4846x52.cloudfront.net/deploylobe.svg)](https://repocloud.io/details/?app_id=267)\n[![Run on ClawCloud](https://raw.githubusercontent.com/ClawCloud/Run-Template/refs/heads/main/Run-on-ClawCloud.svg)](https://template.run.claw.cloud/?referralCode=U11MRQ8U9RM4&openapp=system-fastdeploy%3FtemplateName%3Dperplexica)\n\n## Upcoming Features\n\n- [x] Add settings page\n- [x] Adding support for local LLMs\n- [x] History Saving features\n- [x] Introducing various Focus Modes\n- [x] Adding API support\n- [x] Adding Discover\n- [ ] Finalizing Copilot Mode\n\n## Support Us\n\nIf you find Perplexica useful, consider giving us a star on GitHub. This helps more people discover Perplexica and supports the development of new features. Your support is greatly appreciated.\n\n### Donations\n\nWe also accept donations to help sustain our project. If you would like to contribute, you can use the following options to donate. Thank you for your support!\n\n| Ethereum                                              |\n| ----------------------------------------------------- |\n| Address: `0xB025a84b2F269570Eb8D4b05DEdaA41D8525B6DD` |\n\n## Contribution\n\nPerplexica is built on the idea that AI and large language models should be easy for everyone to use. If you find bugs or have ideas, please share them in via GitHub Issues. For more information on contributing to Perplexica you can read the [CONTRIBUTING.md](CONTRIBUTING.md) file to learn more about Perplexica and how you can contribute to it.\n\n## Help and Support\n\nIf you have any questions or feedback, please feel free to reach out to us. You can create an issue on GitHub or join our Discord server. There, you can connect with other users, share your experiences and reviews, and receive more personalized help. [Click here](https://discord.gg/EFwsmQDgAu) to join the Discord server. To discuss matters outside of regular support, feel free to contact me on Discord at `itzcrazykns`.\n\nThank you for exploring Perplexica, the AI-powered search engine designed to enhance your search experience. We are constantly working to improve Perplexica and expand its capabilities. We value your feedback and contributions which help us make Perplexica even better. Don't forget to check back for updates and new features!\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 149430917,
    "name": "recommenders",
    "full_name": "recommenders-team/recommenders",
    "description": "Best Practices on Recommendation Systems",
    "html_url": "https://github.com/recommenders-team/recommenders",
    "clone_url": "https://github.com/recommenders-team/recommenders.git",
    "owner_login": "recommenders-team",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/142452264?v=4",
    "stargazers_count": 20557,
    "watchers_count": 20557,
    "forks_count": 3234,
    "open_issues_count": 170,
    "size": 229944,
    "language": "Python",
    "languages": {
      "Python": 1454110,
      "Scala": 12207,
      "C++": 6783,
      "Jupyter Notebook": 5769,
      "Dockerfile": 5085
    },
    "topics": [
      "ai",
      "artificial-intelligence",
      "data-science",
      "deep-learning",
      "jupyter-notebook",
      "kubernetes",
      "machine-learning",
      "operationalization",
      "python",
      "ranking",
      "rating",
      "recommendation",
      "recommendation-algorithm",
      "recommendation-engine",
      "recommendation-system",
      "recommender",
      "tutorial"
    ],
    "license_name": "MIT License",
    "created_at": "2018-09-19T10:06:07+00:00",
    "updated_at": "2025-08-05T13:04:18+00:00",
    "pushed_at": "2025-07-29T19:10:09+00:00",
    "contributors_count": 100,
    "readme_length": 22134,
    "readme_content": "<!--\nCopyright (c) Recommenders contributors.\nLicensed under the MIT License.\n-->\n<img src=\"https://raw.githubusercontent.com/recommenders-team/artwork/main/color/recommenders_color.svg\" width=\"800\">\n\n\n[![Documentation status](https://github.com/recommenders-team/recommenders/actions/workflows/pages/pages-build-deployment/badge.svg)](https://github.com/recommenders-team/recommenders/actions/workflows/pages/pages-build-deployment)\n[![License](https://img.shields.io/github/license/recommenders-team/recommenders.svg)](https://github.com/recommenders-team/recommenders/blob/main/LICENSE)\n[![Black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![PyPI Version](https://img.shields.io/pypi/v/recommenders.svg?logo=pypi&logoColor=white)](https://pypi.org/project/recommenders)\n[![Python Versions](https://img.shields.io/pypi/pyversions/recommenders.svg?logo=python&logoColor=white)](https://pypi.org/project/recommenders)\n\n[<img align=\"left\" width=\"300\" src=\"https://raw.githubusercontent.com/recommenders-team/artwork/main/mix/join_recommenders_slack.svg\">](https://join.slack.com/t/lfaifoundation/shared_invite/zt-2iyl7zyya-g5rOO5K518CBoevyi28W6w)\n\n<br>\n\n## What's New (April, 2025)\n\nWe reached 20,000 stars!!\n\nWe are happy to announce that we have reached 20,000 stars on GitHub! Thank you for your support and contributions to the Recommenders project. We are excited to continue building and improving this project with your help.\n\nCheck out the release [Recommenders 1.2.1](https://github.com/recommenders-team/recommenders/releases/tag/1.2.1)!\n\nWe fixed a lot of bugs due to dependencies, improved security, reviewed the notebooks and the libraries.\n\n## Introduction\n\nRecommenders objective is to assist researchers, developers and enthusiasts in prototyping, experimenting with and bringing to production a range of classic and state-of-the-art recommendation systems.\n\nRecommenders is a project under the [Linux Foundation of AI and Data](https://lfaidata.foundation/projects/). \n\nThis repository contains examples and best practices for building recommendation systems, provided as Jupyter notebooks. The examples detail our learnings on five key tasks:\n\n- [Prepare Data](examples/01_prepare_data): Preparing and loading data for each recommendation algorithm.\n- [Model](examples/00_quick_start): Building models using various classical and deep learning recommendation algorithms such as Alternating Least Squares ([ALS](https://spark.apache.org/docs/latest/api/python/_modules/pyspark/ml/recommendation.html#ALS)) or eXtreme Deep Factorization Machines ([xDeepFM](https://arxiv.org/abs/1803.05170)).\n- [Evaluate](examples/03_evaluate): Evaluating algorithms with offline metrics.\n- [Model Select and Optimize](examples/04_model_select_and_optimize): Tuning and optimizing hyperparameters for recommendation models.\n- [Operationalize](examples/05_operationalize): Operationalizing models in a production environment on Azure.\n\nSeveral utilities are provided in [recommenders](recommenders) to support common tasks such as loading datasets in the format expected by different algorithms, evaluating model outputs, and splitting training/test data. Implementations of several state-of-the-art algorithms are included for self-study and customization in your own applications. See the [Recommenders documentation](https://readthedocs.org/projects/microsoft-recommenders/).\n\nFor a more detailed overview of the repository, please see the documents on the [wiki page](https://github.com/microsoft/recommenders/wiki/Documents-and-Presentations).\n\nFor some of the practical scenarios where recommendation systems have been applied, see [scenarios](scenarios). \n\n## Getting Started\n\nWe recommend [conda](https://docs.conda.io/projects/conda/en/latest/glossary.html?highlight=environment#conda-environment) for environment management, and [VS Code](https://code.visualstudio.com/) for development. To install the recommenders package and run an example notebook on Linux/WSL:\n\n```bash\n# 1. Install gcc if it is not installed already. On Ubuntu, this could done by using the command\n# sudo apt install gcc\n\n# 2. Create and activate a new conda environment\nconda create -n <environment_name> python=3.9\nconda activate <environment_name>\n\n# 3. Install the core recommenders package. It can run all the CPU notebooks.\npip install recommenders\n\n# 4. create a Jupyter kernel\npython -m ipykernel install --user --name <environment_name> --display-name <kernel_name>\n\n# 5. Clone this repo within VSCode or using command line:\ngit clone https://github.com/recommenders-team/recommenders.git\n\n# 6. Within VSCode:\n#   a. Open a notebook, e.g., examples/00_quick_start/sar_movielens.ipynb;  \n#   b. Select Jupyter kernel <kernel_name>;\n#   c. Run the notebook.\n```\n\nFor more information about setup on other platforms (e.g., Windows and macOS) and different configurations (e.g., GPU, Spark and experimental features), see the [Setup Guide](SETUP.md).\n\nIn addition to the core package, several extras are also provided, including:\n+ `[gpu]`: Needed for running GPU models.\n+ `[spark]`: Needed for running Spark models.\n+ `[dev]`: Needed for development for the repo.\n+ `[all]`: `[gpu]`|`[spark]`|`[dev]`\n+ `[experimental]`: Models that are not thoroughly tested and/or may require additional steps in installation.\n\n## Algorithms\n\nThe table below lists the recommendation algorithms currently available in the repository. Notebooks are linked under the Example column as Quick start, showcasing an easy to run example of the algorithm, or as Deep dive, explaining in detail the math and implementation of the algorithm.\n\n| Algorithm | Type | Description | Example |\n|-----------|------|-------------|---------|\n| Alternating Least Squares (ALS) | Collaborative Filtering | Matrix factorization algorithm for explicit or implicit feedback in large datasets, optimized for scalability and distributed computing capability. It works in the PySpark environment. | [Quick start](examples/00_quick_start/als_movielens.ipynb) / [Deep dive](examples/02_model_collaborative_filtering/als_deep_dive.ipynb) |\n| Attentive Asynchronous Singular Value Decomposition (A2SVD)<sup>*</sup> | Collaborative Filtering | Sequential-based algorithm that aims to capture both long and short-term user preferences using attention mechanism. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/sequential_recsys_amazondataset.ipynb) |\n| Cornac/Bayesian Personalized Ranking (BPR) | Collaborative Filtering | Matrix factorization algorithm for predicting item ranking with implicit feedback. It works in the CPU environment. | [Deep dive](examples/02_model_collaborative_filtering/cornac_bpr_deep_dive.ipynb) |\n| Cornac/Bilateral Variational Autoencoder (BiVAE) | Collaborative Filtering | Generative model for dyadic data (e.g., user-item interactions). It works in the CPU/GPU environment. | [Deep dive](examples/02_model_collaborative_filtering/cornac_bivae_deep_dive.ipynb) |\n| Convolutional Sequence Embedding Recommendation (Caser) | Collaborative Filtering | Algorithm based on convolutions that aim to capture both user\u2019s general preferences and sequential patterns. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/sequential_recsys_amazondataset.ipynb) |\n| Deep Knowledge-Aware Network (DKN)<sup>*</sup> | Content-Based Filtering | Deep learning algorithm incorporating a knowledge graph and article embeddings for providing news or article recommendations. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/dkn_MIND.ipynb) / [Deep dive](examples/02_model_content_based_filtering/dkn_deep_dive.ipynb) |\n| Extreme Deep Factorization Machine (xDeepFM)<sup>*</sup> | Collaborative Filtering | Deep learning based algorithm for implicit and explicit feedback with user/item features. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/xdeepfm_criteo.ipynb) |\n| Embedding Dot Bias | Collaborative Filtering | General purpose algorithm with embeddings and biases for users and items. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/embdotbias_movielens.ipynb) |\n| LightFM/Factorization Machine | Collaborative Filtering | Factorization Machine algorithm for both implicit and explicit feedbacks. It works in the CPU environment. | [Quick start](examples/02_model_collaborative_filtering/lightfm_deep_dive.ipynb) |\n| LightGBM/Gradient Boosting Tree<sup>*</sup> | Content-Based Filtering | Gradient Boosting Tree algorithm for fast training and low memory usage in content-based problems. It works in the CPU/GPU/PySpark environments. | [Quick start in CPU](examples/00_quick_start/lightgbm_tinycriteo.ipynb) / [Deep dive in PySpark](examples/02_model_content_based_filtering/mmlspark_lightgbm_criteo.ipynb) |\n| LightGCN | Collaborative Filtering | Deep learning algorithm which simplifies the design of GCN for predicting implicit feedback. It works in the CPU/GPU environment. | [Deep dive](examples/02_model_collaborative_filtering/lightgcn_deep_dive.ipynb) |\n| GeoIMC<sup>*</sup> | Collaborative Filtering | Matrix completion algorithm that takes into account user and item features using Riemannian conjugate gradient optimization and follows a geometric approach. It works in the CPU environment. | [Quick start](examples/00_quick_start/geoimc_movielens.ipynb) |\n| GRU | Collaborative Filtering | Sequential-based algorithm that aims to capture both long and short-term user preferences using recurrent neural networks. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/sequential_recsys_amazondataset.ipynb) |\n| Multinomial VAE | Collaborative Filtering | Generative model for predicting user/item interactions. It works in the CPU/GPU environment. | [Deep dive](examples/02_model_collaborative_filtering/multi_vae_deep_dive.ipynb) |\n| Neural Recommendation with Long- and Short-term User Representations (LSTUR)<sup>*</sup> | Content-Based Filtering | Neural recommendation algorithm for recommending news articles with long- and short-term user interest modeling. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/lstur_MIND.ipynb) |\n| Neural Recommendation with Attentive Multi-View Learning (NAML)<sup>*</sup> | Content-Based Filtering | Neural recommendation algorithm for recommending news articles with attentive multi-view learning. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/naml_MIND.ipynb) |\n| Neural Collaborative Filtering (NCF) | Collaborative Filtering | Deep learning algorithm with enhanced performance for user/item implicit feedback. It works in the CPU/GPU environment.| [Quick start](examples/00_quick_start/ncf_movielens.ipynb) / [Deep dive](examples/02_model_collaborative_filtering/ncf_deep_dive.ipynb) |\n| Neural Recommendation with Personalized Attention (NPA)<sup>*</sup> | Content-Based Filtering | Neural recommendation algorithm for recommending news articles with personalized attention network. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/npa_MIND.ipynb) |\n| Neural Recommendation with Multi-Head Self-Attention (NRMS)<sup>*</sup> | Content-Based Filtering | Neural recommendation algorithm for recommending news articles with multi-head self-attention. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/nrms_MIND.ipynb) |\n| Next Item Recommendation (NextItNet) | Collaborative Filtering | Algorithm based on dilated convolutions and residual network that aims to capture sequential patterns. It considers both user/item interactions and features.  It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/sequential_recsys_amazondataset.ipynb) |\n| Restricted Boltzmann Machines (RBM) | Collaborative Filtering | Neural network based algorithm for learning the underlying probability distribution for explicit or implicit user/item feedback. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/rbm_movielens.ipynb) / [Deep dive](examples/02_model_collaborative_filtering/rbm_deep_dive.ipynb) |\n| Riemannian Low-rank Matrix Completion (RLRMC)<sup>*</sup> | Collaborative Filtering | Matrix factorization algorithm using Riemannian conjugate gradients optimization with small memory consumption to predict user/item interactions. It works in the CPU environment. | [Quick start](examples/00_quick_start/rlrmc_movielens.ipynb) |\n| Simple Algorithm for Recommendation (SAR)<sup>*</sup> | Collaborative Filtering | Similarity-based algorithm for implicit user/item feedback.  It works in the CPU environment. | [Quick start](examples/00_quick_start/sar_movielens.ipynb) / [Deep dive](examples/02_model_collaborative_filtering/sar_deep_dive.ipynb) |\n| Self-Attentive Sequential Recommendation (SASRec) | Collaborative Filtering | Transformer based algorithm for sequential recommendation. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/sasrec_amazon.ipynb) |\n| Short-term and Long-term Preference Integrated Recommender (SLi-Rec)<sup>*</sup> | Collaborative Filtering | Sequential-based algorithm that aims to capture both long and short-term user preferences using attention mechanism, a time-aware controller and a content-aware controller. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/sequential_recsys_amazondataset.ipynb) |\n| Multi-Interest-Aware Sequential User Modeling (SUM)<sup>*</sup> | Collaborative Filtering | An enhanced memory network-based sequential user model which aims to capture users' multiple interests. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/sequential_recsys_amazondataset.ipynb) |\n| Sequential Recommendation Via Personalized Transformer (SSEPT) | Collaborative Filtering | Transformer based algorithm for sequential recommendation with User embedding. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/sasrec_amazon.ipynb) |\n| Standard VAE | Collaborative Filtering | Generative Model for predicting user/item interactions.  It works in the CPU/GPU environment. | [Deep dive](examples/02_model_collaborative_filtering/standard_vae_deep_dive.ipynb) |\n| Surprise/Singular Value Decomposition (SVD) | Collaborative Filtering | Matrix factorization algorithm for predicting explicit rating feedback in small datasets. It works in the CPU/GPU environment. | [Deep dive](examples/02_model_collaborative_filtering/surprise_svd_deep_dive.ipynb) |\n| Term Frequency - Inverse Document Frequency (TF-IDF) | Content-Based Filtering | Simple similarity-based algorithm for content-based recommendations with text datasets. It works in the CPU environment. | [Quick  start](examples/00_quick_start/tfidf_covid.ipynb) |\n| Vowpal Wabbit (VW)<sup>*</sup> | Content-Based Filtering | Fast online learning algorithms, great for scenarios where user features / context are constantly changing. It uses the CPU for online learning. | [Deep dive](examples/02_model_content_based_filtering/vowpal_wabbit_deep_dive.ipynb) |\n| Wide and Deep | Collaborative Filtering | Deep learning algorithm that can memorize feature interactions and generalize user features. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/wide_deep_movielens.ipynb) |\n| xLearn/Factorization Machine (FM) & Field-Aware FM (FFM) | Collaborative Filtering | Quick and memory efficient algorithm to predict labels with user/item features. It works in the CPU/GPU environment. | [Deep dive](examples/02_model_collaborative_filtering/fm_deep_dive.ipynb) |\n\n**NOTE**: <sup>*</sup> indicates algorithms invented/contributed by Microsoft.\n\nIndependent or incubating algorithms and utilities are candidates for the [contrib](contrib) folder. This will house contributions which may not easily fit into the core repository or need time to refactor or mature the code and add necessary tests.\n\n| Algorithm | Type | Description | Example |\n|-----------|------|-------------|---------|\n| SARplus <sup>*</sup> | Collaborative Filtering | Optimized implementation of SAR for Spark |  [Quick start](contrib/sarplus/README.md) |\n\n### Algorithm Comparison\n\nWe provide a [benchmark notebook](examples/06_benchmarks/movielens.ipynb) to illustrate how different algorithms could be evaluated and compared. In this notebook, the MovieLens dataset is split into training/test sets at a 75/25 ratio using a stratified split. A recommendation model is trained using each of the collaborative filtering algorithms below. We utilize empirical parameter values reported in literature [here](http://mymedialite.net/examples/datasets.html). For ranking metrics we use `k=10` (top 10 recommended items). We run the comparison on a machine with 4 CPUs, 30Gb of RAM, and 1 GPU GeForce GTX 1660 Ti with 6Gb of memory. Spark ALS is run in local standalone mode. In this table we show the results on Movielens 100k, running the algorithms for 15 epochs.\n\n| Algo | MAP | nDCG@k | Precision@k | Recall@k | RMSE | MAE | R<sup>2</sup> | Explained Variance |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| [ALS](examples/00_quick_start/als_movielens.ipynb) | 0.004732 |\t0.044239 |\t0.048462 |\t0.017796 | 0.965038 |\t0.753001 |\t0.255647 |\t0.251648 |\n| [BiVAE](examples/02_model_collaborative_filtering/cornac_bivae_deep_dive.ipynb) | 0.146126\t| 0.475077 |\t0.411771 |\t0.219145 | N/A |\tN/A |\tN/A |\tN/A |\n| [BPR](examples/02_model_collaborative_filtering/cornac_bpr_deep_dive.ipynb) | 0.132478\t| 0.441997 |\t0.388229 |\t0.212522 | N/A |\tN/A |\tN/A |\tN/A |\n| [embdotbias](examples/00_quick_start/embdotbias_movielens.ipynb) | 0.018954 |\t0.117810 |\t0.104242 |\t0.042450 | 0.992760 | 0.776040 | 0.223344 |\t0.223393 |\n| [LightGCN](examples/02_model_collaborative_filtering/lightgcn_deep_dive.ipynb) | 0.088526 | 0.419846 | 0.379626 | 0.144336 | N/A | N/A | N/A | N/A |\n| [NCF](examples/02_model_collaborative_filtering/ncf_deep_dive.ipynb) | 0.107720\t| 0.396118 |\t0.347296 |\t0.180775 | N/A | N/A | N/A | N/A |\n| [SAR](examples/00_quick_start/sar_movielens.ipynb) | 0.110591 |\t0.382461 | \t0.330753 | 0.176385 | 1.253805 | 1.048484 |\t-0.569363 |\t0.030474 |\n| [SVD](examples/02_model_collaborative_filtering/surprise_svd_deep_dive.ipynb) | 0.012873\t| 0.095930 |\t0.091198 |\t0.032783 | 0.938681 | 0.742690 | 0.291967 | 0.291971 |\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Before contributing, please see our [contribution guidelines](CONTRIBUTING.md).\n\nThis project adheres to this [Code of Conduct](CODE_OF_CONDUCT.md) in order to foster a welcoming and inspiring community for all.\n\n## Build Status\n\nThese tests are the nightly builds, which compute the asynchronous tests. `main` is our principal branch and `staging` is our development branch. We use [pytest](https://docs.pytest.org/) for testing python utilities in [recommenders](recommenders) and the Recommenders [notebook executor](recommenders/utils/notebook_utils.py) for the [notebooks](examples). \n\nFor more information about the testing pipelines, please see the [test documentation](tests/README.md).\n\n### AzureML Nightly Build Status\n\nThe nightly build tests are run daily on AzureML.\n\n| Build Type | Branch | Status |  | Branch | Status |\n| --- | --- | --- | --- | --- | --- |\n| **Linux CPU** | main | [![azureml-cpu-nightly](https://github.com/microsoft/recommenders/actions/workflows/azureml-cpu-nightly.yml/badge.svg?branch=main)](https://github.com/microsoft/recommenders/actions/workflows/azureml-cpu-nightly.yml?query=branch%3Amain) | | staging | [![azureml-cpu-nightly](https://github.com/microsoft/recommenders/actions/workflows/azureml-cpu-nightly.yml/badge.svg?branch=staging)](https://github.com/microsoft/recommenders/actions/workflows/azureml-cpu-nightly.yml?query=branch%3Astaging) |\n| **Linux GPU** | main | [![azureml-gpu-nightly](https://github.com/microsoft/recommenders/actions/workflows/azureml-gpu-nightly.yml/badge.svg?branch=main)](https://github.com/microsoft/recommenders/actions/workflows/azureml-gpu-nightly.yml?query=branch%3Amain) | | staging | [![azureml-gpu-nightly](https://github.com/microsoft/recommenders/actions/workflows/azureml-gpu-nightly.yml/badge.svg?branch=staging)](https://github.com/microsoft/recommenders/actions/workflows/azureml-gpu-nightly.yml?query=branch%3Astaging) |\n| **Linux Spark** | main | [![azureml-spark-nightly](https://github.com/microsoft/recommenders/actions/workflows/azureml-spark-nightly.yml/badge.svg?branch=main)](https://github.com/microsoft/recommenders/actions/workflows/azureml-spark-nightly.yml?query=branch%3Amain) | | staging | [![azureml-spark-nightly](https://github.com/microsoft/recommenders/actions/workflows/azureml-spark-nightly.yml/badge.svg?branch=staging)](https://github.com/microsoft/recommenders/actions/workflows/azureml-spark-nightly.yml?query=branch%3Astaging) |\n\n## References\n\n- **FREE COURSE**: M. Gonz\u00e1lez-Fierro, \"Recommendation Systems: A Practical Introduction\", LinkedIn Learning, 2024. [Available on this link](https://www.linkedin.com/learning/recommendation-systems-a-practical-introduction).\n- D. Li, J. Lian, L. Zhang, K. Ren, D. Lu, T. Wu, X. Xie, \"Recommender Systems: Frontiers and Practices\", Springer, Beijing, 2024. [Available on this link](https://www.amazon.com/Recommender-Systems-Frontiers-Practices-Dongsheng/dp/9819989639/).\n- A. Argyriou, M. Gonz\u00e1lez-Fierro, and L. Zhang, \"Microsoft Recommenders: Best Practices for Production-Ready Recommendation Systems\", *WWW 2020: International World Wide Web Conference Taipei*, 2020. Available online: https://dl.acm.org/doi/abs/10.1145/3366424.3382692\n- S. Graham,  J.K. Min, T. Wu, \"Microsoft recommenders: tools to accelerate developing recommender systems\", *RecSys '19: Proceedings of the 13th ACM Conference on Recommender Systems*, 2019. Available online: https://dl.acm.org/doi/10.1145/3298689.3346967\n- L. Zhang, T. Wu, X. Xie, A. Argyriou, M. Gonz\u00e1lez-Fierro and J. Lian, \"Building Production-Ready Recommendation System at Scale\", *ACM SIGKDD Conference on Knowledge Discovery and Data Mining 2019 (KDD 2019)*, 2019.\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 250213286,
    "name": "datasets",
    "full_name": "huggingface/datasets",
    "description": "\ud83e\udd17 The largest hub of ready-to-use datasets for AI models with fast, easy-to-use and efficient data manipulation tools",
    "html_url": "https://github.com/huggingface/datasets",
    "clone_url": "https://github.com/huggingface/datasets.git",
    "owner_login": "huggingface",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/25720743?v=4",
    "stargazers_count": 20467,
    "watchers_count": 20467,
    "forks_count": 2889,
    "open_issues_count": 948,
    "size": 89319,
    "language": "Python",
    "languages": {
      "Python": 2940663,
      "Makefile": 465
    },
    "topics": [
      "ai",
      "artificial-intelligence",
      "computer-vision",
      "dataset-hub",
      "datasets",
      "deep-learning",
      "llm",
      "machine-learning",
      "natural-language-processing",
      "nlp",
      "numpy",
      "pandas",
      "pytorch",
      "speech",
      "tensorflow"
    ],
    "license_name": "Apache License 2.0",
    "created_at": "2020-03-26T09:23:22+00:00",
    "updated_at": "2025-08-06T02:06:47+00:00",
    "pushed_at": "2025-07-31T17:14:52+00:00",
    "contributors_count": 100,
    "readme_length": 11195,
    "readme_content": "<p align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/datasets-logo-dark.svg\">\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/datasets-logo-light.svg\">\n    <img alt=\"Hugging Face Datasets Library\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/datasets-logo-light.svg\" width=\"352\" height=\"59\" style=\"max-width: 100%;\">\n  </picture>\n  <br/>\n  <br/>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://github.com/huggingface/datasets/actions/workflows/ci.yml?query=branch%3Amain\"><img alt=\"Build\" src=\"https://github.com/huggingface/datasets/actions/workflows/ci.yml/badge.svg?branch=main\"></a>\n    <a href=\"https://github.com/huggingface/datasets/blob/main/LICENSE\"><img alt=\"GitHub\" src=\"https://img.shields.io/github/license/huggingface/datasets.svg?color=blue\"></a>\n    <a href=\"https://huggingface.co/docs/datasets/index.html\"><img alt=\"Documentation\" src=\"https://img.shields.io/website/http/huggingface.co/docs/datasets/index.html.svg?down_color=red&down_message=offline&up_message=online\"></a>\n    <a href=\"https://github.com/huggingface/datasets/releases\"><img alt=\"GitHub release\" src=\"https://img.shields.io/github/release/huggingface/datasets.svg\"></a>\n    <a href=\"https://huggingface.co/datasets/\"><img alt=\"Number of datasets\" src=\"https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/datasets&color=brightgreen\"></a>\n    <a href=\"CODE_OF_CONDUCT.md\"><img alt=\"Contributor Covenant\" src=\"https://img.shields.io/badge/Contributor%20Covenant-2.0-4baaaa.svg\"></a>\n    <a href=\"https://zenodo.org/badge/latestdoi/250213286\"><img src=\"https://zenodo.org/badge/250213286.svg\" alt=\"DOI\"></a>\n</p>\n\n\ud83e\udd17 Datasets is a lightweight library providing **two** main features:\n\n- **one-line dataloaders for many public datasets**: one-liners to download and pre-process any of the ![number of datasets](https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/datasets&color=brightgreen) major public datasets (image datasets, audio datasets, text datasets in 467 languages and dialects, etc.) provided on the [HuggingFace Datasets Hub](https://huggingface.co/datasets). With a simple command like `squad_dataset = load_dataset(\"rajpurkar/squad\")`, get any of these datasets ready to use in a dataloader for training/evaluating a ML model (Numpy/Pandas/PyTorch/TensorFlow/JAX),\n- **efficient data pre-processing**: simple, fast and reproducible data pre-processing for the public datasets as well as your own local datasets in CSV, JSON, text, PNG, JPEG, WAV, MP3, Parquet, etc. With simple commands like `processed_dataset = dataset.map(process_example)`, efficiently prepare the dataset for inspection and ML model evaluation and training.\n\n[\ud83c\udf93 **Documentation**](https://huggingface.co/docs/datasets/) [\ud83d\udd0e **Find a dataset in the Hub**](https://huggingface.co/datasets) [\ud83c\udf1f **Share a dataset on the Hub**](https://huggingface.co/docs/datasets/share)\n\n<h3 align=\"center\">\n    <a href=\"https://hf.co/course\"><img src=\"https://raw.githubusercontent.com/huggingface/datasets/main/docs/source/imgs/course_banner.png\"></a>\n</h3>\n\n\ud83e\udd17 Datasets is designed to let the community easily add and share new datasets.\n\n\ud83e\udd17 Datasets has many additional interesting features:\n\n- Thrive on large datasets: \ud83e\udd17 Datasets naturally frees the user from RAM memory limitation, all datasets are memory-mapped using an efficient zero-serialization cost backend (Apache Arrow).\n- Smart caching: never wait for your data to process several times.\n- Lightweight and fast with a transparent and pythonic API (multi-processing/caching/memory-mapping).\n- Built-in interoperability with NumPy, PyTorch, TensorFlow 2, JAX, Pandas, Polars and more.\n- Native support for audio, image and video data.\n- Enable streaming mode to save disk space and start iterating over the dataset immediately.\n\n\ud83e\udd17 Datasets originated from a fork of the awesome [TensorFlow Datasets](https://github.com/tensorflow/datasets) and the HuggingFace team want to deeply thank the TensorFlow Datasets team for building this amazing library.\n\n# Installation\n\n## With pip\n\n\ud83e\udd17 Datasets can be installed from PyPi and has to be installed in a virtual environment (venv or conda for instance)\n\n```bash\npip install datasets\n```\n\n## With conda\n\n\ud83e\udd17 Datasets can be installed using conda as follows:\n\n```bash\nconda install -c huggingface -c conda-forge datasets\n```\n\nFollow the installation pages of TensorFlow and PyTorch to see how to install them with conda.\n\nFor more details on installation, check the installation page in the documentation: https://huggingface.co/docs/datasets/installation\n\n## Installation to use with Machine Learning & Data frameworks frameworks\n\nIf you plan to use \ud83e\udd17 Datasets with PyTorch (2.0+), TensorFlow (2.6+) or JAX (3.14+) you should also install PyTorch, TensorFlow or JAX.\n\ud83e\udd17 Datasets is also well integrated with data frameworks like PyArrow, Pandas, Polars and Spark, which should be installed separately.\n\nFor more details on using the library with these frameworks, check the quick start page in the documentation: https://huggingface.co/docs/datasets/quickstart\n\n# Usage\n\n\ud83e\udd17 Datasets is made to be very simple to use - the API is centered around a single function, `datasets.load_dataset(dataset_name, **kwargs)`, that instantiates a dataset.\n\nThis library can be used for text/image/audio/etc. datasets. Here is an example to load a text dataset:\n\nHere is a quick example:\n\n```python\nfrom datasets import load_dataset\n\n# Print all the available datasets\nfrom huggingface_hub import list_datasets\nprint([dataset.id for dataset in list_datasets()])\n\n# Load a dataset and print the first example in the training set\nsquad_dataset = load_dataset('rajpurkar/squad')\nprint(squad_dataset['train'][0])\n\n# Process the dataset - add a column with the length of the context texts\ndataset_with_length = squad_dataset.map(lambda x: {\"length\": len(x[\"context\"])})\n\n# Process the dataset - tokenize the context texts (using a tokenizer from the \ud83e\udd17 Transformers library)\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n\ntokenized_dataset = squad_dataset.map(lambda x: tokenizer(x['context']), batched=True)\n```\n\nIf your dataset is bigger than your disk or if you don't want to wait to download the data, you can use streaming:\n\n```python\n# If you want to use the dataset immediately and efficiently stream the data as you iterate over the dataset\nimage_dataset = load_dataset('timm/imagenet-1k-wds', streaming=True)\nfor example in image_dataset[\"train\"]:\n    break\n```\n\nFor more details on using the library, check the quick start page in the documentation: https://huggingface.co/docs/datasets/quickstart and the specific pages on:\n\n- Loading a dataset: https://huggingface.co/docs/datasets/loading\n- What's in a Dataset: https://huggingface.co/docs/datasets/access\n- Processing data with \ud83e\udd17 Datasets: https://huggingface.co/docs/datasets/process\n    - Processing audio data: https://huggingface.co/docs/datasets/audio_process\n    - Processing image data: https://huggingface.co/docs/datasets/image_process\n    - Processing text data: https://huggingface.co/docs/datasets/nlp_process\n- Streaming a dataset: https://huggingface.co/docs/datasets/stream\n- etc.\n\n# Add a new dataset to the Hub\n\nWe have a very detailed step-by-step guide to add a new dataset to the ![number of datasets](https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/datasets&color=brightgreen) datasets already provided on the [HuggingFace Datasets Hub](https://huggingface.co/datasets).\n\nYou can find:\n- [how to upload a dataset to the Hub using your web browser or Python](https://huggingface.co/docs/datasets/upload_dataset) and also\n- [how to upload it using Git](https://huggingface.co/docs/datasets/share).\n\n# Disclaimers\n\nYou can use \ud83e\udd17 Datasets to load datasets based on versioned git repositories maintained by the dataset authors. For reproducibility reasons, we ask users to pin the `revision` of the repositories they use.\n\nIf you're a dataset owner and wish to update any part of it (description, citation, license, etc.), or do not want your dataset to be included in the Hugging Face Hub, please get in touch by opening a discussion or a pull request in the Community tab of the dataset page. Thanks for your contribution to the ML community!\n\n## BibTeX\n\nIf you want to cite our \ud83e\udd17 Datasets library, you can use our [paper](https://arxiv.org/abs/2109.02846):\n\n```bibtex\n@inproceedings{lhoest-etal-2021-datasets,\n    title = \"Datasets: A Community Library for Natural Language Processing\",\n    author = \"Lhoest, Quentin  and\n      Villanova del Moral, Albert  and\n      Jernite, Yacine  and\n      Thakur, Abhishek  and\n      von Platen, Patrick  and\n      Patil, Suraj  and\n      Chaumond, Julien  and\n      Drame, Mariama  and\n      Plu, Julien  and\n      Tunstall, Lewis  and\n      Davison, Joe  and\n      {\\v{S}}a{\\v{s}}ko, Mario  and\n      Chhablani, Gunjan  and\n      Malik, Bhavitvya  and\n      Brandeis, Simon  and\n      Le Scao, Teven  and\n      Sanh, Victor  and\n      Xu, Canwen  and\n      Patry, Nicolas  and\n      McMillan-Major, Angelina  and\n      Schmid, Philipp  and\n      Gugger, Sylvain  and\n      Delangue, Cl{\\'e}ment  and\n      Matussi{\\`e}re, Th{\\'e}o  and\n      Debut, Lysandre  and\n      Bekman, Stas  and\n      Cistac, Pierric  and\n      Goehringer, Thibault  and\n      Mustar, Victor  and\n      Lagunas, Fran{\\c{c}}ois  and\n      Rush, Alexander  and\n      Wolf, Thomas\",\n    booktitle = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\",\n    month = nov,\n    year = \"2021\",\n    address = \"Online and Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.emnlp-demo.21\",\n    pages = \"175--184\",\n    abstract = \"The scale, variety, and quantity of publicly-available NLP datasets has grown rapidly as researchers propose new tasks, larger models, and novel benchmarks. Datasets is a community library for contemporary NLP designed to support this ecosystem. Datasets aims to standardize end-user interfaces, versioning, and documentation, while providing a lightweight front-end that behaves similarly for small datasets as for internet-scale corpora. The design of the library incorporates a distributed, community-driven approach to adding datasets and documenting usage. After a year of development, the library now includes more than 650 unique datasets, has more than 250 contributors, and has helped support a variety of novel cross-dataset research projects and shared tasks. The library is available at https://github.com/huggingface/datasets.\",\n    eprint={2109.02846},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n}\n```\n\nIf you need to cite a specific version of our \ud83e\udd17 Datasets library for reproducibility, you can use the corresponding version Zenodo DOI from this [list](https://zenodo.org/search?q=conceptrecid:%224817768%22&sort=-version&all_versions=True).\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 197261780,
    "name": "Ciphey",
    "full_name": "bee-san/Ciphey",
    "description": "\u26a1 Automatically decrypt encryptions without knowing the key or cipher, decode encodings, and crack hashes \u26a1",
    "html_url": "https://github.com/bee-san/Ciphey",
    "clone_url": "https://github.com/bee-san/Ciphey.git",
    "owner_login": "bee-san",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/10378052?v=4",
    "stargazers_count": 19748,
    "watchers_count": 19748,
    "forks_count": 1285,
    "open_issues_count": 7,
    "size": 17135,
    "language": "Python",
    "languages": {
      "Python": 242633
    },
    "topics": [
      "artificial-intelligence",
      "cipher",
      "cpp",
      "cryptography",
      "ctf",
      "ctf-tools",
      "cyberchef-magic",
      "decryption",
      "deep-neural-network",
      "encodings",
      "encryptions",
      "hacking",
      "hacktoberfest",
      "hashes",
      "natural-language-processing",
      "pentesting",
      "python"
    ],
    "license_name": "MIT License",
    "created_at": "2019-07-16T20:20:39+00:00",
    "updated_at": "2025-08-05T20:47:22+00:00",
    "pushed_at": "2025-03-05T03:09:03+00:00",
    "contributors_count": 39,
    "readme_length": 24050,
    "readme_content": "<p align=\"center\">\nTranslations <br>\n<a href=https://github.com/Ciphey/Ciphey/tree/master/translations/de/README.md>\ud83c\udde9\ud83c\uddea DE   </a>\n<a href=https://github.com/Ciphey/Ciphey/tree/master/translations/fr/README.md>\ud83c\uddeb\ud83c\uddf7 FR   </a>\n<a href=https://github.com/Ciphey/Ciphey/tree/master/translations/hu/README.md>\ud83c\udded\ud83c\uddfa HU   </a>\n<a href=https://github.com/Ciphey/Ciphey/tree/master/translations/id/README.md>\ud83c\uddee\ud83c\udde9 ID   </a>\n<a href=https://github.com/Ciphey/Ciphey/tree/master/translations/it/README.md>\ud83c\uddee\ud83c\uddf9 IT   </a>\n<a href=https://github.com/Ciphey/Ciphey/tree/master/translations/nl/README.md>\ud83c\uddf3\ud83c\uddf1 NL   </a>\n<a href=https://github.com/Ciphey/Ciphey/tree/master/translations/pt-br/README.md>\ud83c\udde7\ud83c\uddf7 PT-BR   </a>\n<a href=https://github.com/Ciphey/Ciphey/tree/master/translations/ru/README.md>\ud83c\uddf7\ud83c\uddfa RU   </a>\n<a href=https://github.com/Ciphey/Ciphey/tree/master/translations/zh/README.md>\ud83c\udde8\ud83c\uddf3 ZH   </a>\n<a href=\"https://github.com/Ciphey/Ciphey/tree/master/translations/th/README.md\">\ud83c\uddf9\ud83c\udded TH   </a>\n <br><br>\n\u27a1\ufe0f\n<a href=\"https://github.com/Ciphey/Ciphey/wiki\">Documentation</a> |\n<a href=\"https://discord.gg/zYTM3rZM4T\">Discord</a> |\n <a href=\"https://github.com/Ciphey/Ciphey/wiki/Installation\">Installation Guide</a>\n \u2b05\ufe0f\n\n<br>\n  <img src=\"https://github.com/Ciphey/Ciphey/raw/master/Pictures_for_README/binoculars.png\" alt=\"Ciphey\">\n</p>\n\n<p align=\"center\">\n<img src=\"https://pepy.tech/badge/ciphey\">\n <img src=\"https://pepy.tech/badge/ciphey/month\">\n  <a href=\"https://discord.gg/zYTM3rZM4T\"><img alt=\"Discord\" src=\"https://img.shields.io/discord/754001738184392704\"></a>\n<a href=\"https://pypi.org/project/ciphey/\"><img src=\"https://img.shields.io/pypi/v/ciphey.svg\"></a>\n  <img src=\"https://img.shields.io/badge/License-MIT-yellow.svg\" alt=\"Ciphey\">\n\n<br>\nFully automated decryption/decoding/cracking tool using natural language processing & artificial intelligence, along with some common sense.\n</p>\n<hr>\n\n## [Installation Guide](https://github.com/Ciphey/Ciphey/wiki/Installation)\n\n| <p align=\"center\"><a href=\"https://pypi.org/project/ciphey\">\ud83d\udc0d Python | <p align=\"center\"><a href=\"https://hub.docker.com/r/remnux/ciphey\">\ud83d\udc0b Docker (Universal) | <p align=\"center\"><a href=\"https://ports.macports.org/port/ciphey/summary\">\ud83c\udf4e MacPorts (macOS) | <p align=\"center\"><a href=\"https://formulae.brew.sh/formula/ciphey\">\ud83c\udf7a Homebrew (macOS/Linux) |\n| --------------------------------------------------------------------- | --------------------------------------------------------------------------------- | --------------------------------------------------------------------------------- |--------------------------------------------------------------------------------- |\n| <p align=\"center\"><img src=\"https://github.com/Ciphey/Ciphey/raw/master/Pictures_for_README/python.png\" /></p>    | <p align=\"center\"><img src=\"https://github.com/Ciphey/Ciphey/raw/master/Pictures_for_README/docker.png\" /></p> | <p align=\"center\"><img src=\"https://github.com/Ciphey/Ciphey/raw/master/Pictures_for_README/macports.png\" /></p> | <p align=\"center\"><img src=\"https://github.com/Ciphey/Ciphey/raw/master/Pictures_for_README/homebrew.png\" /></p> |\n| `python3 -m pip install ciphey --upgrade` | `docker run -it --rm remnux/ciphey` | `sudo port install ciphey` | `brew install ciphey` |\n\n| Linux                                                                                                                   | Mac OS                                                                                                                     | Windows                                                                                                                   |\n| ----------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------- |\n| ![GitHub Workflow Status](https://img.shields.io/github/workflow/status/ciphey/ciphey/Python%20application?label=Linux) | ![GitHub Workflow Status](https://img.shields.io/github/workflow/status/ciphey/ciphey/Python%20application?label=Mac%20OS) | ![GitHub Workflow Status](https://img.shields.io/github/workflow/status/ciphey/ciphey/Python%20application?label=Windows) |\n\n<hr>\n\n# \ud83e\udd14 What is this?\n\nInput encrypted text, get the decrypted text back.\n\n> \"What type of encryption?\"\n\nThat's the point. You don't know, you just know it's possibly encrypted. Ciphey will figure it out for you.\n\nCiphey can solve most things in 3 seconds or less.\n\n<p align=\"center\" href=\"https://asciinema.org/a/336257\">\n  <img src=\"https://github.com/Ciphey/Ciphey/raw/master/Pictures_for_README/index.gif\" alt=\"Ciphey demo\">\n</p>\n\nCiphey aims to be a tool to automate a lot of decryptions & decodings such as multiple base encodings, classical ciphers, hashes or more advanced cryptography.\n\nIf you don't know much about cryptography, or you want to quickly check the ciphertext before working on it yourself, Ciphey is for you.\n\n**The technical part.** Ciphey uses a custom built artificial intelligence module (_AuSearch_) with a _Cipher Detection Interface_ to approximate what something is encrypted with. And then a custom-built, customisable natural language processing _Language Checker Interface_, which can detect when the given text becomes plaintext.\n\nNo neural networks or bloated AI here. We only use what is fast and minimal.\n\nAnd that's just the tip of the iceberg. For the full technical explanation, check out our [documentation](https://github.com/Ciphey/Ciphey/wiki).\n\n# \u2728 Features\n\n- **50+ encryptions/encodings supported** such as binary, Morse code and Base64. Classical ciphers like the Caesar cipher, Affine cipher and the Vigenere cipher. Along with modern encryption like repeating-key XOR and more. **[For the full list, click here](https://github.com/Ciphey/Ciphey/wiki/Supported-Ciphers)**\n- **Custom Built Artificial Intelligence with Augmented Search (AuSearch) for answering the question \"what encryption was used?\"** Resulting in decryptions taking less than 3 seconds.\n- **Custom built natural language processing module** Ciphey can determine whether something is plaintext or not. Whether that plaintext is JSON, a CTF flag, or English, Ciphey can get it in a couple of milliseconds.\n- **Multi Language Support** at present, only German & English (with AU, UK, CAN, USA variants).\n- **Supports encryptions and hashes** Which the alternatives such as CyberChef Magic do not.\n- **[C++ core](https://github.com/Ciphey/CipheyCore)** Blazingly fast.\n\n# \ud83d\udd2d Ciphey vs CyberChef\n\n## \ud83d\udd01 Base64 Encoded 42 times\n\n<table>\n  <tr>\n  <th>Name</th>\n    <th>\u26a1 Ciphey \u26a1 </th>\n    <th>\ud83d\udc22 CyberChef \ud83d\udc22</th>\n  </tr>\n  <tr>\n  <th>Gif</th>\n    <td><img src=\"https://github.com/Ciphey/Ciphey/raw/master/Pictures_for_README/ciphey_gooder_cyberchef.gif\" alt=\"The guy she tells you not to worry about\"></td>\n    <td><img src=\"https://github.com/Ciphey/Ciphey/raw/master/Pictures_for_README/not_dying.gif\" alt=\"You\"></td>\n  </tr>\n  <tr>\n  <th>Time</th>\n    <td>2 seconds</td>\n    <td>6 seconds</td>\n  </tr>\n    <tr>\n  <th>Setup</th>\n    <td><ul><li>Run ciphey on the file</li></ul></td>\n    <td><ul><li>Set the regex param to \"{\"</li><li>You need to know how many times to recurse</li><li>You need to know it's Base64 all the way down</li><li>You need to load CyberChef (it's a bloated JS app)</li><li>Know enough about CyberChef to create this pipeline</li><li>Invert the match</li></ul></td>\n  </tr>\n</table>\n\n<sub><b>Note</b> The gifs may load at different times, so one may appear significantly faster than another.</sub><br>\n<sub><b>A note on magic </b>CyberChef's most similar feature to Ciphey is Magic. Magic fails instantly on this input and crashes. The only way we could force CyberChef to compete was to manually define it.</sub>\n\nWe also tested CyberChef and Ciphey with a **6gb file**. Ciphey cracked it in **5 minutes and 54 seconds**. CyberChef crashed before it even started.\n\n## \ud83d\udcca Ciphey vs Katana vs CyberChef Magic\n\n| **Name**                                   | \u26a1 Ciphey \u26a1 | \ud83d\udde1\ufe0f Katana \ud83d\udde1\ufe0f | \ud83d\udc22 CyberChef Magic \ud83d\udc22 |\n| ------------------------------------------ | ------------ | ------------ | --------------------- |\n| Advanced Language Checker                  | \u2705           | \u274c           | \u2705                    |\n| Supports Encryptions                       | \u2705           | \u2705           | \u274c                    |\n| Releases named after Dystopian themes \ud83c\udf03   | \u2705           | \u274c           | \u274c                    |\n| Supports hashes                            | \u2705           | \u2705           | \u274c                    |\n| Easy to set up                             | \u2705           | \u274c           | \u2705                    |\n| Can guess what something is encrypted with | \u2705           | \u274c           | \u274c                    |\n| Created for hackers by hackers             | \u2705           | \u2705           | \u274c                    |\n\n# \ud83c\udfac Getting Started\n\nIf you're having trouble with installing Ciphey, [read this.](https://github.com/Ciphey/Ciphey/wiki/Common-Issues-&-Their-Solutions)\n\n## \u203c\ufe0f Important Links (Docs, Installation guide, Discord Support)\n\n| Installation Guide                                                          | Documentation                                             | Discord                                     | Docker Image (from REMnux)                                                                          |\n| --------------------------------------------------------------------------- | --------------------------------------------------------- | ------------------------------------------- | --------------------------------------------------------------------------------------------------- |\n| \ud83d\udcd6 [Installation Guide](https://github.com/Ciphey/Ciphey/wiki/Installation) | \ud83d\udcda [Documentation](https://github.com/Ciphey/Ciphey/wiki) | \ud83e\udd9c [Discord](https://discord.gg/zYTM3rZM4T) | \ud83d\udc0b [Docker Documentation](https://docs.remnux.org/run-tools-in-containers/remnux-containers#ciphey) |\n\n## \ud83c\udfc3\u200d\u2640\ufe0fRunning Ciphey\n\nThere are 3 ways to run Ciphey.\n\n1. File Input `ciphey -f encrypted.txt`\n2. Unqualified input `ciphey -- \"Encrypted input\"`\n3. Normal way `ciphey -t \"Encrypted input\"`\n\n![Gif showing 3 ways to run Ciphey](https://github.com/Ciphey/Ciphey/raw/master/Pictures_for_README/3ways.gif)\n\nTo get rid of the progress bars, probability table, and all the noise use the quiet mode.\n\n`ciphey -t \"encrypted text here\" -q`\n\nFor a full list of arguments, run `ciphey --help`.\n\n### \u2697\ufe0f Importing Ciphey\n\nYou can import Ciphey\\'s main and use it in your own programs and code. `from Ciphey.__main__ import main`\n\n# \ud83c\udfaa Contributors\n\nCiphey was invented by [Bee](https://github.com/bee-san) in 2008, and revived in 2019. Ciphey wouldn't be where it was today without [Cyclic3](https://github.com/Cyclic3) - president of UoL's Cyber Security Society.\n\nCiphey was revived & recreated by the [Cyber Security Society](https://www.cybersoc.cf/) for use in CTFs. If you're ever in Liverpool, consider giving a talk or sponsoring our events. Email us at `cybersecurity@society.liverpoolguild.org` to find out more \ud83e\udd20\n\n**Major Credit** to George H for working out how we could use proper algorithms to speed up the search process.\n**Special thanks** to [varghalladesign](https://www.facebook.com/varghalladesign) for designing the logo. Check out their other design work!\n\n## \ud83d\udc15\u200d\ud83e\uddba [Contributing](https://github.com/Ciphey/Ciphey/wiki/Contributing)\n\nDon't be afraid to contribute! We have many, many things you can do to help out. Each of them labelled and easily explained with examples. If you're trying to contribute but stuck, tag @bee-san \u2728\n\nAlternatively, join the Discord group and send a message there (link in [contrib file](https://github.com/Ciphey/Ciphey/wiki/Contributing)) or at the top of this README as a badge.\n\nPlease read the [contributing file](https://github.com/Ciphey/Ciphey/wiki/Contributing) for exact details on how to contribute \u2728\n\nBy doing so, you'll get your name added to the README below and get to be apart of an ever-growing project!\n[![Stargazers over time](https://starchart.cc/Ciphey/Ciphey.svg)](https://starchart.cc/Ciphey/Ciphey)\n\n## \ud83d\udcb0 Financial Contributors\n\nThe contributions will be used to fund not only the future of Ciphey and its authors, but also Cyber Security Society at the University of Liverpool.\n\nGitHub doesn't support \"sponsor this project and we'll evenly distribute the money\", so pick a link and we'll sort it out on our end \ud83e\udd70\n\n## \u2728 Contributors\n\nThanks goes to these wonderful people ([emoji key](https://allcontributors.org/docs/en/emoji-key)):\n\n<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->\n<!-- prettier-ignore-start -->\n<!-- markdownlint-disable -->\n<table>\n  <tr>\n    <td align=\"center\"><a href=\"https://github.com/Cyclic3\"><img src=\"https://avatars1.githubusercontent.com/u/15613874?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>cyclic3</b></sub></a><br /><a href=\"#design-cyclic3\" title=\"Design\">\ud83c\udfa8</a> <a href=\"#maintenance-cyclic3\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/Ciphey/Ciphey/commits?author=cyclic3\" title=\"Code\">\ud83d\udcbb</a> <a href=\"#ideas-cyclic3\" title=\"Ideas, Planning, & Feedback\">\ud83e\udd14</a></td>\n    <td align=\"center\"><a href=\"https://skerritt.blog\"><img src=\"https://avatars3.githubusercontent.com/u/10378052?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Brandon</b></sub></a><br /><a href=\"#design-brandonskerritt\" title=\"Design\">\ud83c\udfa8</a> <a href=\"#maintenance-brandonskerritt\" title=\"Maintenance\">\ud83d\udea7</a> <a href=\"https://github.com/Ciphey/Ciphey/commits?author=brandonskerritt\" title=\"Code\">\ud83d\udcbb</a> <a href=\"#ideas-brandonskerritt\" title=\"Ideas, Planning, & Feedback\">\ud83e\udd14</a></td>\n    <td align=\"center\"><a href=\"https://github.com/michalani\"><img src=\"https://avatars0.githubusercontent.com/u/27767884?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>michalani</b></sub></a><br /><a href=\"https://github.com/Ciphey/Ciphey/commits?author=michalani\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"https://github.com/ashb07\"><img src=\"https://avatars2.githubusercontent.com/u/24845568?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>ashb07</b></sub></a><br /><a href=\"https://github.com/Ciphey/Ciphey/commits?author=ashb07\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"https://github.com/TheAlcanian\"><img src=\"https://avatars3.githubusercontent.com/u/22127191?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Shardion</b></sub></a><br /><a href=\"https://github.com/Ciphey/Ciphey/issues?q=author%3ATheAlcanian\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n    <td align=\"center\"><a href=\"https://github.com/Bryzizzle\"><img src=\"https://avatars0.githubusercontent.com/u/57810197?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Bryan</b></sub></a><br /><a href=\"#translation-Bryzizzle\" title=\"Translation\">\ud83c\udf0d</a> <a href=\"https://github.com/Ciphey/Ciphey/commits?author=Bryzizzle\" title=\"Documentation\">\ud83d\udcd6</a></td>\n    <td align=\"center\"><a href=\"https://lukasgabriel.net\"><img src=\"https://avatars0.githubusercontent.com/u/52338810?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Lukas Gabriel</b></sub></a><br /><a href=\"https://github.com/Ciphey/Ciphey/commits?author=lukasgabriel\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/Ciphey/Ciphey/issues?q=author%3Alukasgabriel\" title=\"Bug reports\">\ud83d\udc1b</a> <a href=\"#translation-lukasgabriel\" title=\"Translation\">\ud83c\udf0d</a> <a href=\"#ideas-lukasgabriel\" title=\"Ideas, Planning, & Feedback\">\ud83e\udd14</a></td>\n  </tr>\n  <tr>\n    <td align=\"center\"><a href=\"https://github.com/DarshanBhoi\"><img src=\"https://avatars2.githubusercontent.com/u/70128281?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Darshan</b></sub></a><br /><a href=\"https://github.com/Ciphey/Ciphey/issues?q=author%3ADarshanBhoi\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n    <td align=\"center\"><a href=\"https://github.com/SkeletalDemise\"><img src=\"https://avatars1.githubusercontent.com/u/29117662?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>SkeletalDemise</b></sub></a><br /><a href=\"https://github.com/Ciphey/Ciphey/commits?author=SkeletalDemise\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"https://www.patreon.com/cclauss\"><img src=\"https://avatars3.githubusercontent.com/u/3709715?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Christian Clauss</b></sub></a><br /><a href=\"https://github.com/Ciphey/Ciphey/commits?author=cclauss\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/Ciphey/Ciphey/issues?q=author%3Acclauss\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n    <td align=\"center\"><a href=\"http://machinexa.xss.ht\"><img src=\"https://avatars1.githubusercontent.com/u/60662297?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Machinexa2</b></sub></a><br /><a href=\"#content-machinexa2\" title=\"Content\">\ud83d\udd8b</a></td>\n    <td align=\"center\"><a href=\"https://github.com/anantverma275\"><img src=\"https://avatars1.githubusercontent.com/u/18184503?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Anant Verma</b></sub></a><br /><a href=\"https://github.com/Ciphey/Ciphey/commits?author=anantverma275\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/Ciphey/Ciphey/issues?q=author%3Aanantverma275\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n    <td align=\"center\"><a href=\"https://github.com/XVXTOR\"><img src=\"https://avatars1.githubusercontent.com/u/40268197?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>XVXTOR</b></sub></a><br /><a href=\"https://github.com/Ciphey/Ciphey/commits?author=XVXTOR\" title=\"Documentation\">\ud83d\udcd6</a></td>\n    <td align=\"center\"><a href=\"https://github.com/Itamikame\"><img src=\"https://avatars2.githubusercontent.com/u/59034423?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Itamikame</b></sub></a><br /><a href=\"https://github.com/Ciphey/Ciphey/commits?author=Itamikame\" title=\"Code\">\ud83d\udcbb</a></td>\n  </tr>\n  <tr>\n    <td align=\"center\"><a href=\"https://github.com/MikeMerz\"><img src=\"https://avatars3.githubusercontent.com/u/50526795?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>MikeMerz</b></sub></a><br /><a href=\"https://github.com/Ciphey/Ciphey/commits?author=MikeMerz\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"https://github.com/jacobggman\"><img src=\"https://avatars2.githubusercontent.com/u/30216976?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Jacob Galam</b></sub></a><br /><a href=\"https://github.com/Ciphey/Ciphey/commits?author=jacobggman\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/Ciphey/Ciphey/issues?q=author%3Ajacobggman\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n    <td align=\"center\"><a href=\"https://tuxthexplorer.github.io/\"><img src=\"https://avatars1.githubusercontent.com/u/37508897?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>TuxTheXplorer</b></sub></a><br /><a href=\"#translation-TuxTheXplorer\" title=\"Translation\">\ud83c\udf0d</a></td>\n    <td align=\"center\"><a href=\"https://github.com/Itamai\"><img src=\"https://avatars3.githubusercontent.com/u/53093696?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Itamai</b></sub></a><br /><a href=\"https://github.com/Ciphey/Ciphey/commits?author=Itamai\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/Ciphey/Ciphey/issues?q=author%3AItamai\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n    <td align=\"center\"><a href=\"https://github.com/Termack\"><img src=\"https://avatars2.githubusercontent.com/u/26333901?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Filipe</b></sub></a><br /><a href=\"#translation-Termack\" title=\"Translation\">\ud83c\udf0d</a></td>\n    <td align=\"center\"><a href=\"https://github.com/malathit\"><img src=\"https://avatars0.githubusercontent.com/u/2684148?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Malathi</b></sub></a><br /><a href=\"https://github.com/Ciphey/Ciphey/commits?author=malathit\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"https://hexchaos.xyz/\"><img src=\"https://avatars1.githubusercontent.com/u/8947820?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Jack</b></sub></a><br /><a href=\"#translation-HexChaos\" title=\"Translation\">\ud83c\udf0d</a></td>\n  </tr>\n  <tr>\n    <td align=\"center\"><a href=\"https://github.com/yafkari\"><img src=\"https://avatars3.githubusercontent.com/u/41365655?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Younes</b></sub></a><br /><a href=\"#translation-yafkari\" title=\"Translation\">\ud83c\udf0d</a></td>\n    <td align=\"center\"><a href=\"https://gitlab.com/Marnick39\"><img src=\"https://avatars2.githubusercontent.com/u/17315511?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Marnick Vandecauter</b></sub></a><br /><a href=\"#translation-Marnick39\" title=\"Translation\">\ud83c\udf0d</a></td>\n    <td align=\"center\"><a href=\"https://github.com/mav8557\"><img src=\"https://avatars0.githubusercontent.com/u/47306745?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Michael V</b></sub></a><br /><a href=\"https://github.com/Ciphey/Ciphey/commits?author=mav8557\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"https://github.com/chuinzer\"><img src=\"https://avatars2.githubusercontent.com/u/64257785?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>chuinzer</b></sub></a><br /><a href=\"#translation-chuinzer\" title=\"Translation\">\ud83c\udf0d</a></td>\n    <td align=\"center\"><a href=\"https://github.com/blackcat-917\"><img src=\"https://avatars1.githubusercontent.com/u/53786619?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>blackcat-917</b></sub></a><br /><a href=\"#translation-blackcat-917\" title=\"Translation\">\ud83c\udf0d</a> <a href=\"https://github.com/Ciphey/Ciphey/commits?author=blackcat-917\" title=\"Documentation\">\ud83d\udcd6</a></td>\n    <td align=\"center\"><a href=\"https://github.com/Ozzyz\"><img src=\"https://avatars3.githubusercontent.com/u/6113447?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>\u00c5smund Brekke</b></sub></a><br /><a href=\"https://github.com/Ciphey/Ciphey/commits?author=Ozzyz\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"https://github.com/sashreek1\"><img src=\"https://avatars1.githubusercontent.com/u/45600974?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Sashreek Shankar</b></sub></a><br /><a href=\"https://github.com/Ciphey/Ciphey/commits?author=sashreek1\" title=\"Code\">\ud83d\udcbb</a></td>\n  </tr>\n  <tr>\n    <td align=\"center\"><a href=\"https://github.com/cryptobadger\"><img src=\"https://avatars2.githubusercontent.com/u/26308101?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>cryptobadger</b></sub></a><br /><a href=\"https://github.com/Ciphey/Ciphey/commits?author=cryptobadger\" title=\"Code\">\ud83d\udcbb</a> <a href=\"https://github.com/Ciphey/Ciphey/issues?q=author%3Acryptobadger\" title=\"Bug reports\">\ud83d\udc1b</a></td>\n    <td align=\"center\"><a href=\"https://github.com/e1fy\"><img src=\"https://avatars3.githubusercontent.com/u/61194758?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>elf</b></sub></a><br /><a href=\"https://github.com/Ciphey/Ciphey/commits?author=e1fy\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"https://github.com/rogercyyu\"><img src=\"https://avatars0.githubusercontent.com/u/45835736?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Roger Yu</b></sub></a><br /><a href=\"https://github.com/Ciphey/Ciphey/commits?author=rogercyyu\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"https://github.com/JesseEmond\"><img src=\"https://avatars.githubusercontent.com/u/1843555?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>dysleixa</b></sub></a><br /><a href=\"https://github.com/Ciphey/Ciphey/commits?author=JesseEmond\" title=\"Code\">\ud83d\udcbb</a></td>\n    <td align=\"center\"><a href=\"http://mohzulfikar.me\"><img src=\"https://avatars.githubusercontent.com/u/48849323?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Mohammad Zulfikar</b></sub></a><br /><a href=\"https://github.com/Ciphey/Ciphey/commits?author=mohzulfikar\" title=\"Documentation\">\ud83d\udcd6</a></td>\n    <td align=\"center\"><a href=\"https://github.com/AABur\"><img src=\"https://avatars.githubusercontent.com/u/41373199?v=4?s=100\" width=\"100px;\" alt=\"\"/><br /><sub><b>Alexander Burchenko</b></sub></a><br /><a href=\"#translation-AABur\" title=\"Translation\">\ud83c\udf0d</a></td>\n  </tr>\n</table>\n\n<!-- markdownlint-restore -->\n<!-- prettier-ignore-end -->\n\n<!-- ALL-CONTRIBUTORS-LIST:END -->\n\nThis project follows the [all-contributors](https://github.com/all-contributors/all-contributors) specification. Contributions of any kind welcome!\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 942114501,
    "name": "owl",
    "full_name": "camel-ai/owl",
    "description": "\ud83e\udd89 OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation",
    "html_url": "https://github.com/camel-ai/owl",
    "clone_url": "https://github.com/camel-ai/owl.git",
    "owner_login": "camel-ai",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/134388954?v=4",
    "stargazers_count": 17750,
    "watchers_count": 17750,
    "forks_count": 2060,
    "open_issues_count": 94,
    "size": 29889,
    "language": "Python",
    "languages": {
      "Python": 546014,
      "Shell": 16575,
      "Batchfile": 14707,
      "Dockerfile": 3612
    },
    "topics": [
      "agent",
      "artificial-intelligence",
      "multi-agent-systems",
      "task-automation",
      "web-interaction"
    ],
    "license_name": null,
    "created_at": "2025-03-03T15:42:51+00:00",
    "updated_at": "2025-08-06T00:49:34+00:00",
    "pushed_at": "2025-07-31T01:10:44+00:00",
    "contributors_count": 38,
    "readme_length": 33501,
    "readme_content": "<h1 align=\"center\">\n\t\ud83e\udd89 OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation\n</h1>\n\n\n<div align=\"center\">\n\n[![Documentation][docs-image]][docs-url]\n[![Discord][discord-image]][discord-url]\n[![X][x-image]][x-url]\n[![Reddit][reddit-image]][reddit-url]\n[![Wechat][wechat-image]][wechat-url]\n[![Wechat][owl-image]][owl-url]\n[![Hugging Face][huggingface-image]][huggingface-url]\n[![Star][star-image]][star-url]\n[![Package License][package-license-image]][package-license-url]\n\n\n</div>\n\n\n<hr>\n\n<div align=\"center\" style=\"background-color: #e3f2fd; padding: 20px; border-radius: 15px; border: 3px solid #1976d2; margin: 25px 0;\">\n  <h2 style=\"color: #1976d2; margin: 0 0 15px 0; font-size: 1.8em;\">\n    \ud83d\ude80 <b>Introducing Eigent: The World's First Multi-Agent Workforce Desktop Application</b> \ud83d\ude80\n  </h2>\n  <p style=\"font-size: 1.2em; margin: 10px 0; line-height: 1.6;\">\n    <b>Eigent</b> empowers you to build, manage, and deploy a custom AI workforce that can turn your most complex workflows into automated tasks.\n  </p>\n  <p style=\"font-size: 1.1em; margin: 15px 0;\">\n    \u2728 <b>100% Open Source</b> \u2022 \ud83d\udd27 <b>Fully Customizable</b> \u2022 \ud83d\udd12 <b>Privacy-First</b> \u2022 \u26a1 <b>Parallel Execution</b>\n  </p>\n  <p style=\"font-size: 1em; margin: 15px 0; font-style: italic;\">\n    Built on CAMEL-AI's acclaimed open-source project, Eigent introduces a Multi-Agent Workforce that boosts productivity through parallel execution, customization, and privacy protection.\n  </p>\n  <div style=\"margin-top: 20px;\">\n    <a href=\"https://github.com/eigent-ai/eigent\" style=\"background-color: #d81b60; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px; font-weight: bold; margin: 0 5px;\">\ud83d\udd17 Visit Eigent Repo</a>\n    <a href=\"https://www.eigent.ai/\" style=\"background-color: #1976d2; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px; font-weight: bold; margin: 0 5px;\">Learn More</a>\n    <a href=\"https://www.eigent.ai/download\" style=\"background-color: #43a047; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px; font-weight: bold; margin: 0 5px;\">Get Started</a>\n  </div>\n</div>\n\n<hr>\n\n<div align=\"center\">\n<h4 align=\"center\">\n\n[\u4e2d\u6587\u9605\u8bfb](https://github.com/camel-ai/owl/tree/main/README_zh.md) |\n[Community](https://github.com/camel-ai/owl#community) |\n[Installation](#\ufe0f-installation) |\n[Examples](https://github.com/camel-ai/owl/tree/main/owl) |\n[Paper](https://arxiv.org/abs/2505.23885) |\n<!-- [Technical Report](https://hypnotic-mind-6bd.notion.site/OWL-Optimized-Workforce-Learning-for-General-Multi-Agent-Assistance-in-Real-World-Task-Automation-1d4004aeb21380158749c7f84b20643f) | -->\n[Citation](https://github.com/camel-ai/owl#citation) |\n[Contributing](https://github.com/camel-ai/owl/graphs/contributors) |\n[CAMEL-AI](https://www.camel-ai.org/)\n\n</h4>\n\n<div align=\"center\" style=\"background-color: #f0f7ff; padding: 10px; border-radius: 5px; margin: 15px 0;\">\n  <h3 style=\"color: #1e88e5; margin: 0;\">\n    \ud83c\udfc6 OWL achieves <span style=\"color: #d81b60; font-weight: bold; font-size: 1.2em;\">69.09</span> average score on GAIA benchmark and ranks <span style=\"color: #d81b60; font-weight: bold; font-size: 1.2em;\">\ud83c\udfc5\ufe0f #1</span> among open-source frameworks! \ud83c\udfc6\n  </h3>\n</div>\n\n<div align=\"center\">\n\n\ud83e\udd89 OWL is a cutting-edge framework for multi-agent collaboration that pushes the boundaries of task automation, built on top of the [CAMEL-AI Framework](https://github.com/camel-ai/camel).\n\n<!-- OWL achieves **58.18** average score on [GAIA](https://huggingface.co/spaces/gaia-benchmark/leaderboard) benchmark and ranks \ud83c\udfc5\ufe0f #1 among open-source frameworks. -->\n\nOur vision is to revolutionize how AI agents collaborate to solve real-world tasks. By leveraging dynamic agent interactions, OWL enables more natural, efficient, and robust task automation across diverse domains.\n\n</div>\n\n![](./assets/owl_architecture.png)\n\n<br>\n\n\n</div>\n\n<!-- # Key Features -->\n# \ud83d\udccb Table of Contents\n\n- [\ud83d\udccb Table of Contents](#-table-of-contents)\n- [\ud83d\ude80 Eigent: Multi-Agent Workforce Desktop Application](#-eigent-multi-agent-workforce-desktop-application)\n- [\ud83d\udd25 News](#-news)\n- [\ud83c\udfac Demo Video](#-demo-video)\n- [\u2728\ufe0f Core Features](#\ufe0f-core-features)\n- [\ud83d\udee0\ufe0f Installation](#\ufe0f-installation)\n  - [**Prerequisites**](#prerequisites)\n    - [Install Python](#install-python)\n  - [**Installation Options**](#installation-options)\n    - [Option 1: Using uv (Recommended)](#option-1-using-uv-recommended)\n    - [Option 2: Using venv and pip](#option-2-using-venv-and-pip)\n    - [Option 3: Using conda](#option-3-using-conda)\n    - [Option 4: Using Docker](#option-4-using-docker)\n      - [**Using Pre-built Image (Recommended)**](#using-pre-built-image-recommended)\n      - [**Building Image Locally**](#building-image-locally)\n      - [**Using Convenience Scripts**](#using-convenience-scripts)\n  - [**Setup Environment Variables**](#setup-environment-variables)\n    - [Setting Environment Variables Directly](#setting-environment-variables-directly)\n    - [Alternative: Using a `.env` File](#alternative-using-a-env-file)\n    - [**MCP Desktop Commander Setup**](#mcp-desktop-commander-setup)\n- [\ud83d\ude80 Quick Start](#-quick-start)\n  - [Basic Usage](#basic-usage)\n  - [Running with Different Models](#running-with-different-models)\n    - [Model Requirements](#model-requirements)\n      - [Supported Models](#supported-models)\n    - [Example Tasks](#example-tasks)\n- [\ud83e\uddf0 Toolkits and Capabilities](#-toolkits-and-capabilities)\n  - [Model Context Protocol (MCP)](#model-context-protocol-mcp)\n    - [**Install Node.js**](#install-nodejs)\n    - [Windows](#windows)\n    - [Linux](#linux)\n    - [Mac](#mac)\n    - [**Install Playwright MCP Service**](#install-playwright-mcp-service)\n  - [Available Toolkits](#available-toolkits)\n  - [Available Toolkits](#available-toolkits-1)\n    - [Multimodal Toolkits (Require multimodal model capabilities)](#multimodal-toolkits-require-multimodal-model-capabilities)\n    - [Text-Based Toolkits](#text-based-toolkits)\n  - [Customizing Your Configuration](#customizing-your-configuration)\n- [\ud83c\udf10 Web Interface](#-web-interface)\n  - [Starting the Web UI](#starting-the-web-ui)\n  - [Features](#features)\n- [\ud83e\uddea Experiments](#-experiments)\n- [\u23f1\ufe0f Future Plans](#\ufe0f-future-plans)\n- [\ud83d\udcc4 License](#-license)\n- [\ud83e\udd1d Contributing](#-contributing)\n- [\ud83d\udd25 Community](#-community)\n- [\u2753 FAQ](#-faq)\n  - [General Questions](#general-questions)\n  - [Experiment Questions](#experiment-questions)\n- [\ud83d\udcda Exploring CAMEL Dependency](#-exploring-camel-dependency)\n  - [Accessing CAMEL Source Code](#accessing-camel-source-code)\n- [\ud83d\udd8a\ufe0f Cite](#\ufe0f-cite)\n- [\u2b50 Star History](#-star-history)\n\n# \ud83d\ude80 Eigent: Multi-Agent Workforce Desktop Application\n\n<div align=\"center\" style=\"background-color: #f5f5f5; padding: 20px; border-radius: 10px; margin: 20px 0;\">\n\n**[Eigent](https://github.com/eigent-ai/eigent)** is revolutionizing the way we work with AI agents. As the world's first Multi-Agent Workforce desktop application, Eigent transforms complex workflows into automated, intelligent processes.\n\n### Why Eigent?\n\n- **\ud83e\udd16 Multi-Agent Collaboration**: Deploy multiple specialized AI agents that work together seamlessly\n- **\ud83d\ude80 Parallel Execution**: Boost productivity with agents that can work on multiple tasks simultaneously\n- **\ud83c\udfa8 Full Customization**: Build and configure your AI workforce to match your specific needs\n- **\ud83d\udd12 Privacy-First Design**: Your data stays on your machine - no cloud dependencies required\n- **\ud83d\udcaf 100% Open Source**: Complete transparency and community-driven development\n\n### Key Capabilities\n\n- **Build Custom Workflows**: Design complex multi-step processes that agents can execute autonomously\n- **Manage AI Teams**: Orchestrate multiple agents with different specializations working in concert\n- **Deploy Instantly**: From idea to execution in minutes, not hours\n- **Monitor Progress**: Real-time visibility into agent activities and task completion\n\n### Use Cases\n\n- \ud83d\udcca **Data Analysis**: Automate complex data processing and analysis workflows\n- \ud83d\udd0d **Research**: Deploy agents to gather, synthesize, and report on information\n- \ud83d\udcbb **Development**: Accelerate coding tasks with AI-powered development teams\n- \ud83d\udcdd **Content Creation**: Generate, edit, and optimize content at scale\n- \ud83e\udd1d **Business Automation**: Transform repetitive business processes into automated workflows\n\n### Get Started with Eigent\n\nEigent is built on top of the OWL framework, leveraging CAMEL-AI's powerful multi-agent capabilities. \n\n\ud83d\udd17 **[Visit the Eigent Repository](https://github.com/eigent-ai/eigent)** to explore the codebase, contribute, or learn more about building your own AI workforce.\n\nFollow our [installation guide](#\ufe0f-installation) to start building your own AI workforce today!\n\n</div>\n\n# \ud83d\udd25 News\n\n\n<div align=\"center\" style=\"background-color: #e8f5e9; padding: 15px; border-radius: 10px; border: 2px solid #4caf50; margin: 20px 0;\">\n  <h3 style=\"color: #2e7d32; margin: 0; font-size: 1.3em;\">\n    \ud83e\udde9 <b>NEW: COMMUNITY AGENT CHALLENGES!</b> \ud83e\udde9\n  </h3>\n  <p style=\"font-size: 1.1em; margin: 10px 0;\">\n    Showcase your creativity by designing unique challenges for AI agents! <br>\n    Join our community and see your innovative ideas tackled by cutting-edge AI.\n  </p>\n  <p>\n    <a href=\"https://github.com/camel-ai/owl/blob/main/community_challenges.md\" style=\"background-color: #2e7d32; color: white; padding: 8px 15px; text-decoration: none; border-radius: 5px; font-weight: bold;\">View & Submit Challenges</a>\n  </p>\n</div>\n\n<div style=\"background-color: #e3f2fd; padding: 12px; border-radius: 8px; border-left: 4px solid #1e88e5; margin: 10px 0;\">\n  <h4 style=\"color: #1e88e5; margin: 0 0 8px 0;\">\n    \ud83c\udf89 Latest Major Update - March 15, 2025\n  </h4>\n  <p style=\"margin: 0;\">\n    <b>Significant Improvements:</b>\n    <ul style=\"margin: 5px 0 0 0; padding-left: 20px;\">\n      <li>Restructured web-based UI architecture for enhanced stability \ud83c\udfd7\ufe0f</li>\n      <li>Optimized OWL Agent execution mechanisms for better performance \ud83d\ude80</li>\n    </ul>\n    <i>Try it now and experience the improved performance in your automation tasks!</i>\n  </p>\n</div>\n\n- **[2025.07.21]**: We open-sourced the training dataset and model checkpoints of OWL project. Training code coming soon.  [huggingface link](https://huggingface.co/collections/camel-ai/optimized-workforce-learning-682ef4ab498befb9426e6e27).\n- **[2025.05.27]**: We released the technical report of OWL, including more details on the workforce (framework) and optimized workforce learning (training methodology).  [paper](https://arxiv.org/abs/2505.23885).\n- **[2025.05.18]**: We open-sourced an initial version for replicating workforce experiment on GAIA [here](https://github.com/camel-ai/owl/tree/gaia69).\n- **[2025.04.18]**: We uploaded OWL's new GAIA benchmark score of **69.09%**, ranking #1 among open-source frameworks. Check the technical report [here](https://hypnotic-mind-6bd.notion.site/OWL-Optimized-Workforce-Learning-for-General-Multi-Agent-Assistance-in-Real-World-Task-Automation-1d4004aeb21380158749c7f84b20643f).\n- **[2025.03.27]**: Integrate SearxNGToolkit performing web searches using SearxNG search engine.\n- **[2025.03.26]**: Enhanced Browser Toolkit with multi-browser support for \"chrome\", \"msedge\", and \"chromium\" channels.\n- **[2025.03.25]**: Supported Gemini 2.5 Pro, added example run code\n- **[2025.03.21]**: Integrated OpenRouter model platform, fix bug with Gemini tool calling.\n- **[2025.03.20]**: Accept header in MCP Toolkit, support automatic playwright installation.\n- **[2025.03.16]**: Support Bing search, Baidu search.\n- **[2025.03.12]**: Added Bocha search in SearchToolkit, integrated Volcano Engine model platform, and enhanced Azure and OpenAI Compatible models with structured output and tool calling.\n- **[2025.03.11]**: We added MCPToolkit, FileWriteToolkit, and TerminalToolkit to enhance OWL agents with MCP tool calling, file writing capabilities, and terminal command execution.\n- **[2025.03.09]**: We added a web-based user interface that makes it easier to interact with the system.\n- **[2025.03.07]**: We open-sourced the codebase of the \ud83e\udd89 OWL project.\n- **[2025.03.03]**: OWL achieved the #1 position among open-source frameworks on the GAIA benchmark with a score of 58.18.\n\n\n# \ud83c\udfac Demo Video\n\nhttps://github.com/user-attachments/assets/2a2a825d-39ea-45c5-9ba1-f9d58efbc372\n\nhttps://private-user-images.githubusercontent.com/55657767/420212194-e813fc05-136a-485f-8df3-f10d9b4e63ec.mp4\n\nThis video demonstrates how to install OWL locally and showcases its capabilities as a cutting-edge framework for multi-agent collaboration: https://www.youtube.com/watch?v=8XlqVyAZOr8\n\n# \u2728\ufe0f Core Features\n\n- **Online Search**: Support for multiple search engines (including Wikipedia, Google, DuckDuckGo, Baidu, Bocha, etc.) for real-time information retrieval and knowledge acquisition.\n- **Multimodal Processing**: Support for handling internet or local videos, images, and audio data.\n- **Browser Automation**: Utilize the Playwright framework for simulating browser interactions, including scrolling, clicking, input handling, downloading, navigation, and more.\n- **Document Parsing**: Extract content from Word, Excel, PDF, and PowerPoint files, converting them into text or Markdown format.\n- **Code Execution**: Write and execute Python code using interpreter.\n- **Built-in Toolkits**: Access to a comprehensive set of built-in toolkits including:\n  - **Model Context Protocol (MCP)**: A universal protocol layer that standardizes AI model interactions with various tools and data sources\n  - **Core Toolkits**: ArxivToolkit, AudioAnalysisToolkit, CodeExecutionToolkit, DalleToolkit, DataCommonsToolkit, ExcelToolkit, GitHubToolkit, GoogleMapsToolkit, GoogleScholarToolkit, ImageAnalysisToolkit, MathToolkit, NetworkXToolkit, NotionToolkit, OpenAPIToolkit, RedditToolkit, SearchToolkit, SemanticScholarToolkit, SymPyToolkit, VideoAnalysisToolkit, WeatherToolkit, BrowserToolkit, and many more for specialized tasks\n\n# \ud83d\udee0\ufe0f Installation\n\n## **Prerequisites**\n\n### Install Python\nBefore installing OWL, ensure you have Python installed (version 3.10, 3.11, or 3.12 is supported):\n\n> **Note for GAIA Benchmark Users**: When running the GAIA benchmark evaluation, please use the `gaia58.18` branch which includes a customized version of the CAMEL framework in the `owl/camel` directory. This version contains enhanced toolkits with improved stability specifically optimized for the GAIA benchmark compared to the standard CAMEL installation.\n\n```bash\n# Check if Python is installed\npython --version\n\n# If not installed, download and install from https://www.python.org/downloads/\n# For macOS users with Homebrew:\nbrew install python@3.10\n\n# For Ubuntu/Debian:\nsudo apt update\nsudo apt install python3.10 python3.10-venv python3-pip\n```\n\n## **Installation Options**\n\nOWL supports multiple installation methods to fit your workflow preferences.\n\n### Option 1: Using uv (Recommended)\n\n```bash\n# Clone github repo\ngit clone https://github.com/camel-ai/owl.git\n\n# Change directory into project directory\ncd owl\n\n# Install uv if you don't have it already\npip install uv\n\n# Create a virtual environment and install dependencies\nuv venv .venv --python=3.10\n\n# Activate the virtual environment\n# For macOS/Linux\nsource .venv/bin/activate\n# For Windows\n.venv\\Scripts\\activate\n\n# Install CAMEL with all dependencies\nuv pip install -e .\n```\n\n### Option 2: Using venv and pip\n\n```bash\n# Clone github repo\ngit clone https://github.com/camel-ai/owl.git\n\n# Change directory into project directory\ncd owl\n\n# Create a virtual environment\n# For Python 3.10 (also works with 3.11, 3.12)\npython3.10 -m venv .venv\n\n# Activate the virtual environment\n# For macOS/Linux\nsource .venv/bin/activate\n# For Windows\n.venv\\Scripts\\activate\n\n# Install from requirements.txt\npip install -r requirements.txt --use-pep517\n```\n\n### Option 3: Using conda\n\n```bash\n# Clone github repo\ngit clone https://github.com/camel-ai/owl.git\n\n# Change directory into project directory\ncd owl\n\n# Create a conda environment\nconda create -n owl python=3.10\n\n# Activate the conda environment\nconda activate owl\n\n# Option 1: Install as a package (recommended)\npip install -e .\n\n# Option 2: Install from requirements.txt\npip install -r requirements.txt --use-pep517\n```\n\n### Option 4: Using Docker\n\n#### **Using Pre-built Image (Recommended)**\n\n```bash\n# This option downloads a ready-to-use image from Docker Hub\n# Fastest and recommended for most users\ndocker compose up -d\n\n# Run OWL inside the container\ndocker compose exec owl bash\ncd .. && source .venv/bin/activate\nplaywright install-deps\nxvfb-python examples/run.py\n```\n\n#### **Building Image Locally**\n\n```bash\n# For users who need to customize the Docker image or cannot access Docker Hub:\n# 1. Open docker-compose.yml\n# 2. Comment out the \"image: mugglejinx/owl:latest\" line\n# 3. Uncomment the \"build:\" section and its nested properties\n# 4. Then run:\ndocker compose up -d --build\n\n# Run OWL inside the container\ndocker compose exec owl bash\ncd .. && source .venv/bin/activate\nplaywright install-deps\nxvfb-python examples/run.py\n```\n\n#### **Using Convenience Scripts**\n\n```bash\n# Navigate to container directory\ncd .container\n\n# Make the script executable and build the Docker image\nchmod +x build_docker.sh\n./build_docker.sh\n\n# Run OWL with your question\n./run_in_docker.sh \"your question\"\n```\n\n## **Setup Environment Variables**\n\nOWL requires various API keys to interact with different services.\n\n### Setting Environment Variables Directly\n\nYou can set environment variables directly in your terminal:\n\n- **macOS/Linux (Bash/Zsh)**:\n  ```bash\n  export OPENAI_API_KEY=\"your-openai-api-key-here\"\n  # Add other required API keys as needed\n  ```\n\n- **Windows (Command Prompt)**:\n  ```batch\n  set OPENAI_API_KEY=your-openai-api-key-here\n  ```\n\n- **Windows (PowerShell)**:\n  ```powershell\n  $env:OPENAI_API_KEY = \"your-openai-api-key-here\"\n  ```\n\n> **Note**: Environment variables set directly in the terminal will only persist for the current session.\n\n### Alternative: Using a `.env` File\n\nIf you prefer using a `.env` file instead, you can:\n\n1. **Copy and Rename the Template**:\n   ```bash\n   # For macOS/Linux\n   cd owl\n   cp .env_template .env\n   \n   # For Windows\n   cd owl\n   copy .env_template .env\n   ```\n\n   Alternatively, you can manually create a new file named `.env` in the owl directory and copy the contents from `.env_template`.\n\n2. **Configure Your API Keys**:\n   Open the `.env` file in your preferred text editor and insert your API keys in the corresponding fields.\n\n> **Note**: For the minimal example (`examples/run_mini.py`), you only need to configure the LLM API key (e.g., `OPENAI_API_KEY`).\n\n### **MCP Desktop Commander Setup**\n\nIf using MCP Desktop Commander within Docker, run:\n\n```bash\nnpx -y @wonderwhy-er/desktop-commander setup --force-file-protocol\n```\n\nFor more detailed Docker usage instructions, including cross-platform support, optimized configurations, and troubleshooting, please refer to [DOCKER_README.md](.container/DOCKER_README_en.md).\n\n# \ud83d\ude80 Quick Start\n\n## Basic Usage\n\nAfter installation and setting up your environment variables, you can start using OWL right away:\n\n```bash\npython examples/run.py\n```\n\n## Running with Different Models\n\n### Model Requirements\n\n- **Tool Calling**: OWL requires models with robust tool calling capabilities to interact with various toolkits. Models must be able to understand tool descriptions, generate appropriate tool calls, and process tool outputs.\n\n- **Multimodal Understanding**: For tasks involving web interaction, image analysis, or video processing, models with multimodal capabilities are required to interpret visual content and context.\n\n#### Supported Models\n\nFor information on configuring AI models, please refer to our [CAMEL models documentation](https://docs.camel-ai.org/key_modules/models.html#supported-model-platforms-in-camel).\n\n> **Note**: For optimal performance, we strongly recommend using OpenAI models (GPT-4 or later versions). Our experiments show that other models may result in significantly lower performance on complex tasks and benchmarks, especially those requiring advanced multi-modal understanding and tool use.\n\nOWL supports various LLM backends, though capabilities may vary depending on the model's tool calling and multimodal abilities. You can use the following scripts to run with different models:\n\n```bash\n# Run with Claude model\npython examples/run_claude.py\n\n# Run with Qwen model\npython examples/run_qwen_zh.py\n\n# Run with Deepseek model\npython examples/run_deepseek_zh.py\n\n# Run with other OpenAI-compatible models\npython examples/run_openai_compatible_model.py\n\n# Run with Gemini model\npython examples/run_gemini.py\n\n# Run with Azure OpenAI\npython examples/run_azure_openai.py\n\n# Run with Ollama\npython examples/run_ollama.py\n```\n\nFor a simpler version that only requires an LLM API key, you can try our minimal example:\n\n```bash\npython examples/run_mini.py\n```\n\nYou can run OWL agent with your own task by modifying the `examples/run.py` script:\n\n```python\n# Define your own task\ntask = \"Task description here.\"\n\nsociety = construct_society(question)\nanswer, chat_history, token_count = run_society(society)\n\nprint(f\"\\033[94mAnswer: {answer}\\033[0m\")\n```\n\nFor uploading files, simply provide the file path along with your question:\n\n```python\n# Task with a local file (e.g., file path: `tmp/example.docx`)\ntask = \"What is in the given DOCX file? Here is the file path: tmp/example.docx\"\n\nsociety = construct_society(question)\nanswer, chat_history, token_count = run_society(society)\nprint(f\"\\033[94mAnswer: {answer}\\033[0m\")\n```\n\nOWL will then automatically invoke document-related tools to process the file and extract the answer.\n\n\n### Example Tasks\n\nHere are some tasks you can try with OWL:\n\n- \"Find the latest stock price for Apple Inc.\"\n- \"Analyze the sentiment of recent tweets about climate change\"\n- \"Help me debug this Python code: [your code here]\"\n- \"Summarize the main points from this research paper: [paper URL]\"\n- \"Create a data visualization for this dataset: [dataset path]\"\n\n# \ud83e\uddf0 Toolkits and Capabilities\n\n## Model Context Protocol (MCP)\n\nOWL's MCP integration provides a standardized way for AI models to interact with various tools and data sources:\n\nBefore using MCP, you need to install Node.js first.\n### **Install Node.js**\n### Windows\n\nDownload the official installer: [Node.js](https://nodejs.org/en).\n\nCheck \"Add to PATH\" option during installation.\n\n### Linux\n```bash\nsudo apt update\nsudo apt install nodejs npm -y\n```\n### Mac\n```bash\nbrew install node\n```\n\n### **Install Playwright MCP Service**\n```bash\nnpm install -g @executeautomation/playwright-mcp-server\nnpx playwright install-deps\n```\n\nTry our comprehensive MCP examples:\n- `examples/run_mcp.py` - Basic MCP functionality demonstration (local call, requires dependencies)\n- `examples/run_mcp_sse.py` - Example using the SSE protocol (Use remote services, no dependencies)\n\n## Available Toolkits\n\n> **Important**: Effective use of toolkits requires models with strong tool calling capabilities. For multimodal toolkits (Web, Image, Video), models must also have multimodal understanding abilities.\n\nOWL supports various toolkits that can be customized by modifying the `tools` list in your script:\n\n```python\n# Configure toolkits\ntools = [\n    *BrowserToolkit(headless=False).get_tools(),  # Browser automation\n    *VideoAnalysisToolkit(model=models[\"video\"]).get_tools(),\n    *AudioAnalysisToolkit().get_tools(),  # Requires OpenAI Key\n    *CodeExecutionToolkit(sandbox=\"subprocess\").get_tools(),\n    *ImageAnalysisToolkit(model=models[\"image\"]).get_tools(),\n    SearchToolkit().search_duckduckgo,\n    SearchToolkit().search_google,  # Comment out if unavailable\n    SearchToolkit().search_wiki,\n    SearchToolkit().search_bocha,\n    SearchToolkit().search_baidu,\n    *ExcelToolkit().get_tools(),\n    *DocumentProcessingToolkit(model=models[\"document\"]).get_tools(),\n    *FileWriteToolkit(output_dir=\"./\").get_tools(),\n]\n```\n\n## Available Toolkits\n\nKey toolkits include:\n\n### Multimodal Toolkits (Require multimodal model capabilities)\n- **BrowserToolkit**: Browser automation for web interaction and navigation\n- **VideoAnalysisToolkit**: Video processing and content analysis\n- **ImageAnalysisToolkit**: Image analysis and interpretation\n\n### Text-Based Toolkits\n- **AudioAnalysisToolkit**: Audio processing (requires OpenAI API)\n- **CodeExecutionToolkit**: Python code execution and evaluation\n- **SearchToolkit**: Web searches (Google, DuckDuckGo, Wikipedia)\n- **DocumentProcessingToolkit**: Document parsing (PDF, DOCX, etc.)\n\nAdditional specialized toolkits: ArxivToolkit, GitHubToolkit, GoogleMapsToolkit, MathToolkit, NetworkXToolkit, NotionToolkit, RedditToolkit, WeatherToolkit, and more. For a complete list, see the [CAMEL toolkits documentation](https://docs.camel-ai.org/key_modules/tools.html#built-in-toolkits).\n\n## Customizing Your Configuration\n\nTo customize available tools:\n\n```python\n# 1. Import toolkits\nfrom camel.toolkits import BrowserToolkit, SearchToolkit, CodeExecutionToolkit\n\n# 2. Configure tools list\ntools = [\n    *BrowserToolkit(headless=True).get_tools(),\n    SearchToolkit().search_wiki,\n    *CodeExecutionToolkit(sandbox=\"subprocess\").get_tools(),\n]\n\n# 3. Pass to assistant agent\nassistant_agent_kwargs = {\"model\": models[\"assistant\"], \"tools\": tools}\n```\n\nSelecting only necessary toolkits optimizes performance and reduces resource usage.\n\n# \ud83c\udf10 Web Interface\n\n<div align=\"center\" style=\"background-color: #f0f7ff; padding: 15px; border-radius: 10px; border: 2px solid #1e88e5; margin: 20px 0;\">\n  <h3 style=\"color: #1e88e5; margin: 0;\">\n    \ud83d\ude80 Enhanced Web Interface Now Available!\n  </h3>\n  <p style=\"margin: 10px 0;\">\n    Experience improved system stability and optimized performance with our latest update.\n    Start exploring the power of OWL through our user-friendly interface!\n  </p>\n</div>\n\n## Starting the Web UI\n\n```bash\n# Start the Chinese version\npython owl/webapp_zh.py\n\n# Start the English version\npython owl/webapp.py\n\n# Start the Japanese version\npython owl/webapp_jp.py\n```\n\n## Features\n\n- **Easy Model Selection**: Choose between different models (OpenAI, Qwen, DeepSeek, etc.)\n- **Environment Variable Management**: Configure your API keys and other settings directly from the UI\n- **Interactive Chat Interface**: Communicate with OWL agents through a user-friendly interface\n- **Task History**: View the history and results of your interactions\n\nThe web interface is built using Gradio and runs locally on your machine. No data is sent to external servers beyond what's required for the model API calls you configure.\n\n# \ud83e\uddea Experiments\n\nTo reproduce OWL's GAIA benchmark score:\nFurthermore, to ensure optimal performance on the GAIA benchmark, please note that our `gaia69` branch includes a customized version of the CAMEL framework in the `owl/camel` directory. This version contains enhanced toolkits with improved stability for gaia benchmark compared to the standard CAMEL installation.\n\nWhen running the benchmark evaluation:\n\n1. Switch to the `gaia69` branch:\n   ```bash\n   git checkout gaia69\n   ```\n\n2. Run the evaluation script:\n   ```bash\n   python run_gaia_workforce_claude.py\n   ```\n\nThis will execute the same configuration that achieved our top-ranking performance on the GAIA benchmark.\n\n\n# \u23f1\ufe0f Future Plans\n\nWe're continuously working to improve OWL. Here's what's on our roadmap:\n\n- [x] Write a technical blog post detailing our exploration and insights in multi-agent collaboration in real-world tasks\n- [x] Enhance the toolkit ecosystem with more specialized tools for domain-specific tasks\n- [x] Develop more sophisticated agent interaction patterns and communication protocols\n- [x] Improve performance on complex multi-step reasoning tasks\n\n# \ud83d\udcc4 License\n\nThe source code is licensed under Apache 2.0.\n\n# \ud83e\udd1d Contributing\n\nWe welcome contributions from the community! Here's how you can help:\n\n1. Read our [Contribution Guidelines](https://github.com/camel-ai/camel/blob/master/CONTRIBUTING.md)\n2. Check [open issues](https://github.com/camel-ai/camel/issues) or create new ones\n3. Submit pull requests with your improvements\n\n**Current Issues Open for Contribution:**\n- [#1915](https://github.com/camel-ai/camel/issues/1915)\n- [#2190](https://github.com/camel-ai/camel/issues/2190)\n- [#2165](https://github.com/camel-ai/camel/issues/2165)\n- [#2121](https://github.com/camel-ai/camel/issues/2121)\n- [#1908](https://github.com/camel-ai/camel/issues/1908)\n- [#1538](https://github.com/camel-ai/camel/issues/1538)\n- [#1481](https://github.com/camel-ai/camel/issues/1481)\n\nTo take on an issue, simply leave a comment stating your interest.\n\n# \ud83d\udd25 Community\nJoin us ([*Discord*](https://discord.camel-ai.org/) or [*WeChat*](https://ghli.org/camel/wechat.png)) in pushing the boundaries of finding the scaling laws of agents. \n\nJoin us for further discussions!\n<!-- ![](./assets/community.png) -->\n![](./assets/community_code.jpeg)\n\n# \u2753 FAQ\n\n## General Questions\n\n**Q: Why don't I see Chrome running locally after starting the example script?**\n\nA: If OWL determines that a task can be completed using non-browser tools (such as search or code execution), the browser will not be launched. The browser window will only appear when OWL determines that browser-based interaction is necessary.\n\n**Q: Which Python version should I use?**\n\nA: OWL supports Python 3.10, 3.11, and 3.12. \n\n**Q: How can I contribute to the project?**\n\nA: See our [Contributing](#-contributing) section for details on how to get involved. We welcome contributions of all kinds, from code improvements to documentation updates.\n\n## Experiment Questions\n\n**Q: Which CAMEL version should I use for replicate the role playing result?**\n\nA: We provide a modified version of CAMEL (owl/camel) in the gaia58.18 branch. Please make sure you use this CAMEL version for your experiments.\n\n**Q: Why are my experiment results lower than the reported numbers?**\n\nA: Since the GAIA benchmark evaluates LLM agents in a realistic world, it introduces a significant amount of randomness. Based on user feedback, one of the most common issues for replication is, for example, agents being blocked on certain webpages due to network reasons.\nWe have uploaded a keywords matching script to help quickly filter out these errors [here](https://github.com/camel-ai/owl/blob/gaia58.18/owl/filter_failed_cases.py).\nYou can also check this [technical report](https://hypnotic-mind-6bd.notion.site/OWL-Optimized-Workforce-Learning-for-General-Multi-Agent-Assistance-in-Real-World-Task-Automation-1d4004aeb21380158749c7f84b20643f?pvs=74) for more details when evaluating LLM agents in realistic open-world environments.\n\n# \ud83d\udcda Exploring CAMEL Dependency\n\nOWL is built on top of the [CAMEL](https://github.com/camel-ai/camel) Framework, here's how you can explore the CAMEL source code and understand how it works with OWL:\n\n## Accessing CAMEL Source Code\n\n```bash\n# Clone the CAMEL repository\ngit clone https://github.com/camel-ai/camel.git\ncd camel\n```\n\n# \ud83d\udd8a\ufe0f Cite\n\nIf you find this repo useful, please cite:\n\n\n```\n@misc{hu2025owl,\n      title={OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation}, \n      author={Mengkang Hu and Yuhang Zhou and Wendong Fan and Yuzhou Nie and Bowei Xia and Tao Sun and Ziyu Ye and Zhaoxuan Jin and Yingru Li and Qiguang Chen and Zeyu Zhang and Yifeng Wang and Qianshuo Ye and Bernard Ghanem and Ping Luo and Guohao Li},\n      year={2025},\n      eprint={2505.23885},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2505.23885}, \n}\n```\n\n# \u2b50 Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=camel-ai/owl&type=Date)](https://star-history.com/#camel-ai/owl&Date)\n\n\n\n[docs-image]: https://img.shields.io/badge/Documentation-EB3ECC\n[docs-url]: https://camel-ai.github.io/camel/index.html\n[star-image]: https://img.shields.io/github/stars/camel-ai/owl?label=stars&logo=github&color=brightgreen\n[star-url]: https://github.com/camel-ai/owl/stargazers\n[package-license-image]: https://img.shields.io/badge/License-Apache_2.0-blue.svg\n[package-license-url]: https://github.com/camel-ai/owl/blob/main/licenses/LICENSE\n\n[colab-url]: https://colab.research.google.com/drive/1AzP33O8rnMW__7ocWJhVBXjKziJXPtim?usp=sharing\n[colab-image]: https://colab.research.google.com/assets/colab-badge.svg\n[huggingface-url]: https://huggingface.co/camel-ai\n[huggingface-image]: https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-CAMEL--AI-ffc107?color=ffc107&logoColor=white\n[discord-url]: https://discord.camel-ai.org/\n[discord-image]: https://img.shields.io/discord/1082486657678311454?logo=discord&labelColor=%20%235462eb&logoColor=%20%23f5f5f5&color=%20%235462eb\n[wechat-url]: https://ghli.org/camel/wechat.png\n[wechat-image]: https://img.shields.io/badge/WeChat-CamelAIOrg-brightgreen?logo=wechat&logoColor=white\n[x-url]: https://x.com/CamelAIOrg\n[x-image]: https://img.shields.io/twitter/follow/CamelAIOrg?style=social\n[twitter-image]: https://img.shields.io/twitter/follow/CamelAIOrg?style=social&color=brightgreen&logo=twitter\n[reddit-url]: https://www.reddit.com/r/CamelAI/\n[reddit-image]: https://img.shields.io/reddit/subreddit-subscribers/CamelAI?style=plastic&logo=reddit&label=r%2FCAMEL&labelColor=white\n[ambassador-url]: https://www.camel-ai.org/community\n[owl-url]: ./assets/qr_code.jpg\n[owl-image]: https://img.shields.io/badge/WeChat-OWLProject-brightgreen?logo=wechat&logoColor=white\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 640182997,
    "name": "SuperAGI",
    "full_name": "TransformerOptimus/SuperAGI",
    "description": "<\u26a1\ufe0f> SuperAGI - A dev-first open source autonomous AI agent framework. Enabling developers to build, manage & run useful autonomous agents quickly and reliably.",
    "html_url": "https://github.com/TransformerOptimus/SuperAGI",
    "clone_url": "https://github.com/TransformerOptimus/SuperAGI.git",
    "owner_login": "TransformerOptimus",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/133493246?v=4",
    "stargazers_count": 16605,
    "watchers_count": 16605,
    "forks_count": 2046,
    "open_issues_count": 200,
    "size": 63451,
    "language": "Python",
    "languages": {
      "Python": 1294120,
      "JavaScript": 448309,
      "CSS": 72910,
      "Shell": 11590,
      "Dockerfile": 1571,
      "Batchfile": 928,
      "Mako": 510
    },
    "topics": [
      "agents",
      "agi",
      "ai",
      "artificial-general-intelligence",
      "artificial-intelligence",
      "autonomous-agents",
      "gpt-4",
      "hacktoberfest",
      "llm",
      "llmops",
      "nextjs",
      "openai",
      "pinecone",
      "python",
      "superagi"
    ],
    "license_name": "MIT License",
    "created_at": "2023-05-13T08:55:24+00:00",
    "updated_at": "2025-08-06T02:02:46+00:00",
    "pushed_at": "2025-01-22T22:14:07+00:00",
    "contributors_count": 62,
    "readme_length": 22845,
    "readme_content": "<p align=\"center\">\n  <a href=\"https://superagi.com//#gh-light-mode-only\">\n    <img src=\"https://superagi.com/wp-content/uploads/2023/05/Logo-dark.svg\" width=\"318px\" alt=\"SuperAGI logo\" />\n  </a>\n  <a href=\"https://superagi.com//#gh-dark-mode-only\">\n    <img src=\"https://superagi.com/wp-content/uploads/2023/05/Logo-light.svg\" width=\"318px\" alt=\"SuperAGI logo\" />\n  </a>\n\n</p>\n\n<p align=\"center\"><i>Open-source framework to build, manage and run useful Autonomous AI Agents</i></p>\n    \n\n<p align=\"center\">\n<a href=\"https://superagi.com\"> <img src=\"https://superagi.com/wp-content/uploads/2023/08/Website.svg\"></a>\n<a href=\"https://app.superagi.com\"> <img src=\"https://superagi.com/wp-content/uploads/2023/07/Cloud.svg\"></a>\n<a href=\"https://marketplace.superagi.com/\"> <img src=\"https://superagi.com/wp-content/uploads/2023/08/Marketplace.svg\"></a>\n<a href=\"https://superagi.com/docs/\"> <img src=\"https://superagi.com/wp-content/uploads/2023/08/Docs.svg\"></a>\n<a href=\"https://documenter.getpostman.com/view/28438662/2s9Xy6rqP5\"> <img src=\"https://superagi.com/wp-content/uploads/2023/08/APIs.svg\"></a>\n</p>\n\n<p align=\"center\">\n<a href=\"https://github.com/TransformerOptimus/SuperAGI/fork\" target=\"blank\">\n<img src=\"https://img.shields.io/github/forks/TransformerOptimus/SuperAGI?style=for-the-badge\" alt=\"SuperAGI forks\"/>\n</a>\n\n<a href=\"https://github.com/TransformerOptimus/SuperAGI/stargazers\" target=\"blank\">\n<img src=\"https://img.shields.io/github/stars/TransformerOptimus/SuperAGI?style=for-the-badge\" alt=\"SuperAGI stars\"/>\n</a>\n<a href='https://github.com/TransformerOptimus/SuperAGI/releases'>\n<img src='https://img.shields.io/github/release/TransformerOptimus/SuperAGI?&label=Latest&style=for-the-badge'>\n</a>\n\n<a href=\"https://github.com/TransformerOptimus/SuperAGI/commits\" target=\"blank\">\n<img src=\"https://img.shields.io/github/commits-since/TransformerOptimus/SuperAGI/v0.0.11.svg?style=for-the-badge\" alt=\"SuperAGI Commits\"/>\n</a>\n</p>\n\n<p align=\"center\"><b>Follow SuperAGI </b></p>\n\n<p align=\"center\">\n<a href=\"https://twitter.com/_superAGI\" target=\"blank\">\n<img src=\"https://img.shields.io/twitter/follow/_superAGI?label=Follow: _superAGI&style=social\" alt=\"Follow _superAGI\"/>\n</a>\n<a href=\"https://www.reddit.com/r/Super_AGI\" target=\"_blank\"><img src=\"https://img.shields.io/twitter/url?label=/r/Super_AGI&logo=reddit&style=social&url=https://github.com/TransformerOptimus/SuperAGI\"/></a>\n\n<a href=\"https://discord.gg/dXbRe5BHJC\" target=\"blank\">\n<img src=\"https://img.shields.io/discord/1107593006032355359?label=Join%20SuperAGI&logo=discord&style=social\" alt=\"Join SuperAGI Discord Community\"/>\n</a>\n<a href=\"https://www.youtube.com/@_superagi\" target=\"_blank\"><img src=\"https://img.shields.io/twitter/url?label=Youtube&logo=youtube&style=social&url=https://github.com/TransformerOptimus/SuperAGI\"/></a>\n</p>\n\n<p align=\"center\"><b>Connect with the Creator </b></p>\n\n<p align=\"center\">\n<a href=\"https://twitter.com/ishaanbhola\" target=\"blank\">\n<img src=\"https://img.shields.io/twitter/follow/ishaanbhola?label=Follow: ishaanbhola&style=social\" alt=\"Follow ishaanbhola\"/>\n</a>\n</p>\n\n<p align=\"center\"><b>Share SuperAGI Repository</b></p>\n\n<p align=\"center\">\n\n<a href=\"https://twitter.com/intent/tweet?text=Check%20this%20GitHub%20repository%20out.%20SuperAGI%20-%20Let%27s%20you%20easily%20build,%20manage%20and%20run%20useful%20autonomous%20AI%20agents.&url=https://github.com/TransformerOptimus/SuperAGI&hashtags=SuperAGI,AGI,Autonomics,future\" target=\"blank\">\n<img src=\"https://img.shields.io/twitter/follow/_superAGI?label=Share Repo on Twitter&style=social\" alt=\"Follow _superAGI\"/></a> \n<a href=\"https://t.me/share/url?text=Check%20this%20GitHub%20repository%20out.%20SuperAGI%20-%20Let%27s%20you%20easily%20build,%20manage%20and%20run%20useful%20autonomous%20AI%20agents.&url=https://github.com/TransformerOptimus/SuperAGI\" target=\"_blank\"><img src=\"https://img.shields.io/twitter/url?label=Telegram&logo=Telegram&style=social&url=https://github.com/TransformerOptimus/SuperAGI\" alt=\"Share on Telegram\"/></a>\n<a href=\"https://api.whatsapp.com/send?text=Check%20this%20GitHub%20repository%20out.%20SuperAGI%20-%20Let's%20you%20easily%20build,%20manage%20and%20run%20useful%20autonomous%20AI%20agents.%20https://github.com/TransformerOptimus/SuperAGI\"><img src=\"https://img.shields.io/twitter/url?label=whatsapp&logo=whatsapp&style=social&url=https://github.com/TransformerOptimus/SuperAGI\" /></a> <a href=\"https://www.reddit.com/submit?url=https://github.com/TransformerOptimus/SuperAGI&title=Check%20this%20GitHub%20repository%20out.%20SuperAGI%20-%20Let's%20you%20easily%20build,%20manage%20and%20run%20useful%20autonomous%20AI%20agents.\n\" target=\"blank\">\n<img src=\"https://img.shields.io/twitter/url?label=Reddit&logo=Reddit&style=social&url=https://github.com/TransformerOptimus/SuperAGI\" alt=\"Share on Reddit\"/>\n</a> <a href=\"mailto:?subject=Check%20this%20GitHub%20repository%20out.&body=SuperAGI%20-%20Let%27s%20you%20easily%20build,%20manage%20and%20run%20useful%20autonomous%20AI%20agents.%3A%0Ahttps://github.com/TransformerOptimus/SuperAGI\" target=\"_blank\"><img src=\"https://img.shields.io/twitter/url?label=Gmail&logo=Gmail&style=social&url=https://github.com/TransformerOptimus/SuperAGI\"/></a> <a href=\"https://www.buymeacoffee.com/superagi\" target=\"_blank\"><img src=\"https://cdn.buymeacoffee.com/buttons/default-orange.png\" alt=\"Buy Me A Coffee\" height=\"23\" width=\"100\" style=\"border-radius:1px\"></a>\n\n</p>\n\n<hr>\n\n## What are we ?\n\nA dev-first open source autonomous AI agent framework enabling developers to build, manage & run useful autonomous agents. You can run concurrent agents seamlessly, extend agent capabilities with tools. The agents efficiently perform a variety of tasks and continually improve their performance with each subsequent run.\n\n\n### \ud83d\udca1 Features\n\n- <b>Provision, Spawn & Deploy Autonomous AI Agents</b> - Create production-ready & scalable autonomous agents.\n- <b>Extend Agent Capabilities with Toolkits</b> - Add Toolkits from our marketplace to your agent workflows.\n- <b>Graphical User Interface</b> - Access your agents through a graphical user interface.\n- <b>Action Console</b> - Interact with agents by giving them input and permissions.\n- <b>Multiple Vector DBs</b> - Connect to multiple Vector DBs to enhance your agent\u2019s performance.\n- <b>Performance Telemetry</b> - Get insights into your agent\u2019s performance and optimize accordingly.\n- <b>Optimized Token Usage</b> - Control token usage to manage costs effectively.\n- <b>Agent Memory Storage</b> - Enable your agents to learn and adapt by storing their memory.\n- <b>Models</b> - Custom fine tuned models for business specific usecases.\n- <b>Workflows</b> - Automate tasks with ease using ReAct LLM's predefined steps.\n\n### \ud83d\udee0 Toolkits\nToolkits allow SuperAGI Agents to interact with external systems and third-party plugins.\n\n<a href=\"https://marketplace.superagi.com/\" target=\"_blank\"><img src=https://superagi.com/wp-content/uploads/2023/08/Twitter.png height=50px width=50px alt=\"Twitter\" valign=\"middle\" title=\"Twitter\"></a> <a href=\"https://marketplace.superagi.com/\" target=\"_blank\"><img src=https://superagi.com/wp-content/uploads/2023/08/Coding.png height=50px width=50px alt=\"Coding Tool\" valign=\"middle\" title=\"Coding Tool\"></a> <a href=\"https://marketplace.superagi.com/\" target=\"_blank\"><img src=https://superagi.com/wp-content/uploads/2023/08/Insta.png height=50px width=50px alt=\"Instagram\" valign=\"middle\" title=\"Instagram\"></a> <a href=\"https://marketplace.superagi.com/\" target=\"_blank\"><img src=https://superagi.com/wp-content/uploads/2023/08/Knowledge_tool.png height=50px width=50px alt=\"Knowledge Search\" valign=\"middle\" title=\"Knowledge Search\"></a> <a href=\"https://marketplace.superagi.com/\" target=\"_blank\"><img src=https://superagi.com/wp-content/uploads/2023/05/Group-113612.png height=50px width=50px alt=\"Email\"  valign=\"middle\" title=\"Email\"></a> <a href=\"https://marketplace.superagi.com/\" target=\"_blank\"><img src=https://superagi.com/wp-content/uploads/2023/05/Group-113610.png height=50px width=50px alt=\"Jira\" valign=\"middle\" title=\"Jira\"></a> <a href=\"https://marketplace.superagi.com/\" target=\"_blank\"><img src=https://superagi.com/wp-content/uploads/2023/05/Group-113611.png height=50px width=50px alt=\"File Manager\" valign=\"middle\" title=\"File Manager\"></a> <a href=\"https://marketplace.superagi.com/\" target=\"_blank\"><img src=https://superagi.com/wp-content/uploads/2023/05/Group-113613.png height=50px width=50px alt=\"Google Search\" valign=\"middle\" title=\"Google Search\"></a> <a href=\"https://marketplace.superagi.com/\" target=\"_blank\"><img src=https://superagi.com/wp-content/uploads/2023/05/Group-113615.png height=50px width=50px alt=\"Dall-E\" valign=\"middle\" title=\"Dall-E\"></a> <a href=\"https://marketplace.superagi.com/\" target=\"_blank\"><img src=https://superagi.com/wp-content/uploads/2023/05/Group-113614.png height=50px width=50px alt=\"Github\" valign=\"middle\" title=\"Github\"></a> <a href=\"https://marketplace.superagi.com/\" target=\"_blank\"><img src=https://superagi.com/wp-content/uploads/2023/05/Group-113616.png height=50px width=50px alt=\"Web Interaction\" valign=\"middle\" title=\"Web Interaction\"></a> <a href=\"https://marketplace.superagi.com/\" target=\"_blank\"><img src=https://superagi.com/wp-content/uploads/2023/05/Group-113622.png height=50px width=50px alt=\"Duckduckgo\" valign=\"middle\" title=\"Duckduckgo\"></a> <a href=\"https://marketplace.superagi.com/\" target=\"_blank\"><img src=https://superagi.com/wp-content/uploads/2023/08/Calendar_tool.png height=50px width=50px alt=\"Google Calendar\" valign=\"middle\" title=\"Google Calendar\"></a> <a href=\"https://marketplace.superagi.com/\" target=\"_blank\"><img src=https://superagi.com/wp-content/uploads/2023/08/Search_tool.png height=50px width=50px alt=\"Google Calendar\" valign=\"middle\" title=\"Google Search\"></a> <a href=\"https://marketplace.superagi.com/\" target=\"_blank\"><img src=https://superagi.com/wp-content/uploads/2023/08/Serp.png height=50px width=50px alt=\"Serp API\" valign=\"middle\" title=\"Serp API\"></a> <a href=\"https://marketplace.superagi.com/\" target=\"_blank\"><img src=https://superagi.com/wp-content/uploads/2023/08/Searx.png height=50px width=50px alt=\"Searx\" valign=\"middle\" title=\"Searx \"></a> <a href=\"https://marketplace.superagi.com/\" target=\"_blank\"><img src=https://superagi.com/wp-content/uploads/2023/08/Web_scraper_logo.png height=50px width=50px alt=\"Web Scraper\" valign=\"middle\" title=\"Web Scraper\"></a> <a href=\"https://marketplace.superagi.com/\" target=\"_blank\"><img src=https://superagi.com/wp-content/uploads/2023/08/Notion_logo.png height=50px width=50px alt=\"Notion\" valign=\"middle\" title=\"Notion\"></a> <a href=\"https://marketplace.superagi.com/\" target=\"_blank\"><img src=https://superagi.com/wp-content/uploads/2023/08/Apollo_logo.png height=50px width=50px alt=\"Apollo\" valign=\"middle\" title=\"Apollo\"></a>\n\n### \u2699\ufe0f Installation\n\nYou can install superAGI using one of the following three approaches.\n\n#### \u2601\ufe0f SuperAGI cloud\n\nTo quickly start experimenting with agents without the hassle of setting up the system, try [Superagi Cloud](https://app.superagi.com/)\n\n1. Visit [Superagi Cloud](https://app.superagi.com/) and log in using your github account.\n\n2. In your account settings, go to \"Model Providers\" and add your API key.\n\nYou're all set! Start running your agents effortlessly.\n\n#### \ud83d\udda5\ufe0f Local\n\n1. Open your terminal and clone the SuperAGI repository.\n```\ngit clone https://github.com/TransformerOptimus/SuperAGI.git \n```\n\n2. Navigate to the cloned repository directory using the command:\n```\ncd SuperAGI\n```\n3. Create a copy of config_template.yaml, and name it config.yaml.\n\n4. Ensure that Docker is installed on your system. You can download and install it from [here](https://docs.docker.com/get-docker/).\n\n5. Once you have Docker Desktop running, run the following command in the SuperAGI directory:\n\n   a. For regular usage:\n      ```\n      docker compose -f docker-compose.yaml up --build\n      ```\n\n   b. If you want to use SuperAGI with Local LLMs and have GPU, run the following command:\n      ```\n      docker compose -f docker-compose-gpu.yml up --build\n      ```\n\n\n6. Open your web browser and navigate to http://localhost:3000 to access SuperAGI.\n\n#### \ud83c\udf00 Digital Ocean\n\n<p align=\"left\">\n<a href=\"https://cloud.digitalocean.com/apps/new?repo=https://github.com/TransformerOptimus/SuperAGI/tree/main\"> <img src=\"https://www.deploytodo.com/do-btn-blue.svg\"></a><br>Deploy SuperAGI to DigitalOcean with one click.\n</p>\n\n<a id=\"architecture\">\n\n### \ud83c\udf10 Architecture\n</a>\n<details>\n<summary>SuperAGI Architecture</summary>\n\n![SuperAGI Architecture](https://superagi.com/wp-content/uploads/2023/09/SuperAGI-Architecture.png)\n</details>\n\n<details>\n<summary>Agent Architecture</summary>\n\n![Agent Architecture](https://superagi.com/wp-content/uploads/2023/06/Agent-Architecture.png)\n</details>\n\n<details>\n<summary>Agent Workflow Architecture</summary>\n\n![Agent Workflow Architecture](https://superagi.com/wp-content/uploads/2023/09/Workflow-Architecture.png)\n</details>\n\n<details>\n<summary>Tools Architecture</summary>\n\n![Tools Architecture](https://superagi.com/wp-content/uploads/2023/09/Tools-Architecture.png)\n</details>\n\n<details>\n<summary>ER Diagram</summary>\n\n![ER Diagram](https://superagi.com/wp-content/uploads/2023/09/ER-Diagram.png)\n</details>\n\n### \ud83d\udcda Resources\n\n* [Documentation](https://superagi.com/docs/)\n* [YouTube Channel](https://www.youtube.com/@_SuperAGI/videos)\n\n\n### \ud83d\udcd6 Need Help?\n\nJoin our [Discord community](https://discord.gg/dXbRe5BHJC) for support and discussions.\n\n[![Join us on Discord](https://invidget.switchblade.xyz/uJ3XUGsY2R)](https://discord.gg/uJ3XUGsY2R)\n\nIf you have questions or encounter issues, please don't hesitate to [create a new issue](https://github.com/TransformerOptimus/SuperAGI/issues/new/choose) to get support.\n\n### \ud83d\udcbb Contribution\nWe \u2764\ufe0f our contributors. We\u2019re committed to fostering an open, welcoming, and safe environment in the community.\n\nIf you'd like to contribute, start by reading our [Contribution Guide](https://github.com/TransformerOptimus/SuperAGI/blob/main/CONTRIBUTING.md).\n\nWe expect everyone participating in the community to abide by our [Code of Conduct](https://github.com/TransformerOptimus/SuperAGI/blob/main/CODE_OF_CONDUCT.md).\n\nTo get more idea on where we are heading, checkout our roadmap [here](https://github.com/users/TransformerOptimus/projects/5/views/1).\n\nExplore some [good first issues](https://github.com/TransformerOptimus/SuperAGI/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22) to start contributing.\n\n### \ud83d\udc69\u200d\ud83d\udcbb Contributors\n[![TransformerOptimus](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/133493246?v=4&w=50&h=50&mask=circle)](https://github.com/TransformerOptimus) [![Cptsnowcrasher](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/133322218?v=4&w=50&h=50&mask=circle)](https://github.com/Cptsnowcrasher) [![vectorcrow](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/133646556?v=4&w=50&h=50&mask=circle)](https://github.com/vectorcrow) [![Akki-jain](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/92881074?v=4&w=50&h=50&mask=circle)](https://github.com/Akki-jain) [![Autocop-Agent](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/129729746?v=4&w=50&h=50&mask=circle)](https://github.com/Autocop-Agent)[![COLONAYUSH](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/60507126?v=4&w=50&h=50&mask=circle)](https://github.com/COLONAYUSH)[![luciferlinx101](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/129729795?v=4&w=50&h=50&mask=circle)](https://github.com/luciferlinx101)[![mukundans89](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/101278493?v=4&w=50&h=50&mask=circle)](https://github.com/mukundans89)[![Fluder-Paradyne](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/121793617?v=4&w=50&h=50&mask=circle)](https://github.com/Fluder-Paradyne)[![nborthy](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/101320057?v=4&w=50&h=50&mask=circle)](https://github.com/nborthy)[![nihirr](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/122777244?v=4&w=50&h=50&mask=circle)](https://github.com/nihirr)[![Tarraann](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/97586318?v=4&w=50&h=50&mask=circle)](https://github.com/Tarraann)[![neelayan7](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/43145646?v=4&w=50&h=50&mask=circle)](https://github.com/neelayan7)[![Arkajit-Datta](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/61142632?v=4&w=50&h=50&mask=circle)](https://github.com/Arkajit-Datta)[![guangchen811](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/103159823?v=4&w=50&h=50&mask=circle)](https://github.com/guangchen811)[![juanfpo96](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/14787156?v=4&w=50&h=50&mask=circle)](https://github.com/juanfpo96)[![iskandarreza](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/32027019?v=4&w=50&h=50&mask=circle)](https://github.com/iskandarreza)[![jpenalbae](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/8380459?v=4&w=50&h=50&mask=circle)](https://github.com/jpenalbae)[![pallasite99](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/26508636?v=4&w=50&h=50&mask=circle)](https://github.com/pallasite99)[![xutpuu](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/11964505?v=4&w=50&h=50&mask=circle)](https://github.com/xutpuu)[![alexkreidler](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/11166947?v=4&w=50&h=50&mask=circle)](https://github.com/alexkreidler)[![hanhyalex123](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/100895608?v=4&w=50&h=50&mask=circle)](https://github.com/hanhyalex123)[![ps4vs](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/91535358?v=4&w=50&h=50&mask=circle)](https://github.com/ps4vs)[![eltociear](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/22633385?v=4&w=50&h=50&mask=circle)](https://github.com/eltociear)\n[![shaiss](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/113060?v=4&w=50&h=50&mask=circle)](https://github.com/shaiss)\n[![AdityaRajSingh1992](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/105219157?v=4&w=50&h=50&mask=circle)](https://github.com/AdityaRajSingh1992)\n[![namansleeps2](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/134390870?v=4&w=50&h=50&mask=circle)](https://github.com/namansleeps22)\n[![sirajperson](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/396941?v=4&w=50&h=50&mask=circle)](https://github.com/sirajperson)\n[![hsm207](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/2398765?v=4&w=50&h=50&mask=circle)](https://github.com/hsm207)\n[![unkn-wn](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/43097991?v=4&w=50&h=50&mask=circle)](https://github.com/unkn-wn)\n[![DMTarmey](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/590474?v=4&w=50&h=50&mask=circle)](https://github.com/DMTarmey)\n[![Parth2506](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/122429822?v=4&w=50&h=50&mask=circle)](https://github.com/Parth2506)\n[![platinaCoder](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/47349795?v=4&w=50&h=50&mask=circle)](https://github.com/platinaCoder)\n[![anisha1607](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/60440541?v=4&w=50&h=50&mask=circle)](https://github.com/anisha1607)\n[![jorgectf](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/46056498?v=4&w=50&h=50&mask=circle)](https://github.com/jorgectf)\n[![PaulRBerg](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/8782666?v=4&w=50&h=50&mask=circle)](https://github.com/PaulRBerg)\n[![boundless-asura](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/122777244?v=4&w=50&h=50&mask=circle)](https://github.com/boundless-asura)\n[![JPDucky](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/34105363?v=4&w=50&h=50&mask=circle)](https://github.com/JPDucky)\n[![Vibhusha22](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/128478691?v=4&w=50&h=50&mask=circle)](https://github.com/Vibhusha22)\n[![ai-akuma](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/7444521?v=4&w=50&h=50&mask=circle)](https://github.com/ai-akuma)\n[![rounak610](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/81288115?v=4&w=50&h=50&mask=circle)](https://github.com/rounak610)\n[![AdarshJha619](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/53672264?v=4&w=50&h=50&mask=circle)](https://github.com/AdarshJha619)\n[![ResoluteStoic](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/105219157?v=4&w=50&h=50&mask=circle)](https://github.com/ResoluteStoic)\n[![JohnHunt999](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/137149331?v=4&w=50&h=50&mask=circle)](https://github.com/JohnHunt999)\n[![Maverick-F35](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/138012351?v=4&w=50&h=50&mask=circle)](https://github.com/Maverick-F359)\n[![jorgectf](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/46056498?v=4&w=50&h=50&mask=circle)](https://github.com/jorgectf)\n[![AdityaSharma13064](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/138581531?v=4&w=50&h=50&mask=circle)](https://github.com/AdityaSharma13064)\n[![lalitlj](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/138583454?v=4&w=50&h=50&mask=circle)](https://github.com/lalitlj)\n[![andrew-kelly-neutralaiz](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/128111428?v=4&w=50&h=50&mask=circle)](https://github.com/andrew-kelly-neutralaiz)\n[![sayan1101](https://images.weserv.nl/?url=https://avatars.githubusercontent.com/u/139119661?v=4&w=50&h=50&mask=circle)](https://github.com/sayan1101)\n\n\n<p align=\"center\"><a href=\"https://github.com/TransformerOptimus/SuperAGI#\"><img src=\"https://superagi.com/wp-content/uploads/2023/05/backToTopButton.png\" alt=\"Back to top\" height=\"29\"/></a></p>\n\n### \u26a0\ufe0f Under Development!\nThis project is under active development and may still have issues. We appreciate your understanding and patience. If you encounter any problems, please check the open issues first. If your issue is not listed, kindly create a new issue detailing the error or problem you experienced. Thank you for your support!\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 678526156,
    "name": "marimo",
    "full_name": "marimo-team/marimo",
    "description": "A reactive notebook for Python \u2014 run reproducible experiments, query with SQL, execute as a script, deploy as an app, and version with git. All in a modern, AI-native editor.",
    "html_url": "https://github.com/marimo-team/marimo",
    "clone_url": "https://github.com/marimo-team/marimo.git",
    "owner_login": "marimo-team",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/98563464?v=4",
    "stargazers_count": 15102,
    "watchers_count": 15102,
    "forks_count": 623,
    "open_issues_count": 440,
    "size": 160440,
    "language": "Python",
    "languages": {
      "Python": 6182933,
      "TypeScript": 4285282,
      "CSS": 99399,
      "HTML": 60194,
      "JavaScript": 20069,
      "MDX": 9807,
      "Shell": 8620,
      "Jupyter Notebook": 6173,
      "Makefile": 4363,
      "Dockerfile": 1005,
      "TeX": 209,
      "Vim Script": 30
    },
    "topics": [
      "artificial-intelligence",
      "dag",
      "data-science",
      "data-visualization",
      "dataflow",
      "developer-tools",
      "machine-learning",
      "notebooks",
      "pipeline",
      "python",
      "reactive",
      "sql",
      "web-app"
    ],
    "license_name": "Apache License 2.0",
    "created_at": "2023-08-14T18:56:20+00:00",
    "updated_at": "2025-08-06T02:10:12+00:00",
    "pushed_at": "2025-08-06T00:51:14+00:00",
    "contributors_count": 100,
    "readme_length": 15510,
    "readme_content": "<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/marimo-logotype-thick.svg\">\n</p>\n\n<p align=\"center\">\n  <em>A reactive Python notebook that's reproducible, git-friendly, and deployable as scripts or apps.</em>\n\n<p align=\"center\">\n  <a href=\"https://docs.marimo.io\" target=\"_blank\"><strong>Docs</strong></a> \u00b7\n  <a href=\"https://marimo.io/discord?ref=readme\" target=\"_blank\"><strong>Discord</strong></a> \u00b7\n  <a href=\"https://docs.marimo.io/examples/\" target=\"_blank\"><strong>Examples</strong></a> \u00b7\n  <a href=\"https://marimo.io/gallery/\" target=\"_blank\"><strong>Gallery</strong></a> \u00b7\n  <a href=\"https://www.youtube.com/@marimo-team/\" target=\"_blank\"><strong>YouTube</strong></a>\n</p>\n\n<p align=\"center\">\n  <b>English | </b>\n  <a href=\"https://github.com/marimo-team/marimo/blob/main/README_Traditional_Chinese.md\" target=\"_blank\"><b>\u7e41\u9ad4\u4e2d\u6587</b></a>\n  <b> | </b>\n  <a href=\"https://github.com/marimo-team/marimo/blob/main/README_Chinese.md\" target=\"_blank\"><b>\u7b80\u4f53\u4e2d\u6587</b></a>\n  <b> | </b>\n  <a href=\"https://github.com/marimo-team/marimo/blob/main/README_Japanese.md\" target=\"_blank\"><b>\u65e5\u672c\u8a9e</b></a>\n  <b> | </b>\n  <a href=\"https://github.com/marimo-team/marimo/blob/main/README_Spanish.md\" target=\"_blank\"><b>Espa\u00f1ol</b></a>\n</p>\n\n<p align=\"center\">\n<a href=\"https://pypi.org/project/marimo/\"><img src=\"https://img.shields.io/pypi/v/marimo?color=%2334D058&label=pypi\"/></a>\n<a href=\"https://anaconda.org/conda-forge/marimo\"><img src=\"https://img.shields.io/conda/vn/conda-forge/marimo.svg\"/></a>\n<a href=\"https://marimo.io/discord?ref=readme\"><img src=\"https://shields.io/discord/1059888774789730424\" alt=\"discord\" /></a>\n<img alt=\"Pepy Total Downloads\" src=\"https://img.shields.io/pepy/dt/marimo?label=pypi%20%7C%20downloads\"/>\n<img alt=\"Conda Downloads\" src=\"https://img.shields.io/conda/d/conda-forge/marimo\" />\n<a href=\"https://github.com/marimo-team/marimo/blob/main/LICENSE\"><img src=\"https://img.shields.io/pypi/l/marimo\" /></a>\n</p>\n\n**marimo** is a reactive Python notebook: run a cell or interact with a UI\nelement, and marimo automatically runs dependent cells (or <a href=\"#expensive-notebooks\">marks them as stale</a>), keeping code and outputs\nconsistent. marimo notebooks are stored as pure Python (with first-class SQL support), executable as scripts,\nand deployable as apps.\n\n**Highlights**.\n\n- \ud83d\ude80 **batteries-included:** replaces `jupyter`, `streamlit`, `jupytext`, `ipywidgets`, `papermill`, and more\n- \u26a1\ufe0f **reactive**: run a cell, and marimo reactively [runs all dependent cells](https://docs.marimo.io/guides/reactivity.html) or <a href=\"#expensive-notebooks\">marks them as stale</a>\n- \ud83d\udd90\ufe0f **interactive:** [bind sliders, tables, plots, and more](https://docs.marimo.io/guides/interactivity.html) to Python \u2014 no callbacks required\n- \ud83d\udc0d **git-friendly:** stored as `.py` files\n- \ud83d\udee2\ufe0f **designed for data**: query dataframes, databases, warehouses, or lakehouses [with SQL](https://docs.marimo.io/guides/working_with_data/sql.html), filter and search [dataframes](https://docs.marimo.io/guides/working_with_data/dataframes.html)\n- \ud83e\udd16 **AI-native**: [generate cells with AI](https://docs.marimo.io/guides/generate_with_ai/) tailored for data work\n- \ud83d\udd2c **reproducible:** [no hidden state](https://docs.marimo.io/guides/reactivity.html#no-hidden-state), deterministic execution, [built-in package management](https://docs.marimo.io/guides/package_management/)\n- \ud83c\udfc3 **executable:** [execute as a Python script](https://docs.marimo.io/guides/scripts.html), parameterized by CLI args\n- \ud83d\udedc **shareable**: [deploy as an interactive web app](https://docs.marimo.io/guides/apps.html) or [slides](https://docs.marimo.io/guides/apps.html#slides-layout), [run in the browser via WASM](https://docs.marimo.io/guides/wasm.html)\n- \ud83e\udde9 **reusable:** [import functions and classes](https://docs.marimo.io/guides/reusing_functions/) from one notebook to another\n- \ud83e\uddea **testable:** [run pytest](https://docs.marimo.io/guides/testing/) on notebooks\n- \u2328\ufe0f **a modern editor**: [GitHub Copilot](https://docs.marimo.io/guides/editor_features/ai_completion.html#github-copilot), [AI assistants](https://docs.marimo.io/guides/editor_features/ai_completion.html), vim keybindings, variable explorer, and [more](https://docs.marimo.io/guides/editor_features/index.html)\n\n```python\npip install marimo && marimo tutorial intro\n```\n\n_Try marimo at [our online playground](https://marimo.app/l/c7h6pz), which runs entirely in the browser!_\n\n_Jump to the [quickstart](#quickstart) for a primer on our CLI._\n\n## A reactive programming environment\n\nmarimo guarantees your notebook code, outputs, and program state are consistent. This [solves many problems](https://docs.marimo.io/faq.html#faq-problems) associated with traditional notebooks like Jupyter.\n\n**A reactive programming environment.**\nRun a cell and marimo _reacts_ by automatically running the cells that\nreference its variables, eliminating the error-prone task of manually\nre-running cells. Delete a cell and marimo scrubs its variables from program\nmemory, eliminating hidden state.\n\n<img src=\"https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/reactive.gif\" width=\"700px\" />\n\n<a name=\"expensive-notebooks\"></a>\n\n**Compatible with expensive notebooks.** marimo lets you [configure the runtime\nto be\nlazy](https://docs.marimo.io/guides/configuration/runtime_configuration.html),\nmarking affected cells as stale instead of automatically running them. This\ngives you guarantees on program state while preventing accidental execution of\nexpensive cells.\n\n**Synchronized UI elements.** Interact with [UI\nelements](https://docs.marimo.io/guides/interactivity.html) like [sliders](https://docs.marimo.io/api/inputs/slider.html#slider),\n[dropdowns](https://docs.marimo.io/api/inputs/dropdown.html), [dataframe\ntransformers](https://docs.marimo.io/api/inputs/dataframe.html), and [chat\ninterfaces](https://docs.marimo.io/api/inputs/chat.html), and the cells that\nuse them are automatically re-run with their latest values.\n\n<img src=\"https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/readme-ui.gif\" width=\"700px\" />\n\n**Interactive dataframes.** [Page through, search, filter, and\nsort](https://docs.marimo.io/guides/working_with_data/dataframes.html)\nmillions of rows blazingly fast, no code required.\n\n<img src=\"https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/docs-df.gif\" width=\"700px\" />\n\n**Generate cells with data-aware AI.** [Generate code with an AI\nassistant](https://docs.marimo.io/guides/editor_features/ai_completion/) that is highly\nspecialized for working with data, with context about your variables in memory;\n[zero-shot entire notebooks](https://docs.marimo.io/guides/generate_with_ai/text_to_notebook/).\nCustomize the system prompt, bring your own API keys, or use local models.\n\n<img src=\"https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/readme-generate-with-ai.gif\" width=\"700px\" />\n\n**Query data with SQL.** Build [SQL](https://docs.marimo.io/guides/working_with_data/sql.html) queries\nthat depend on Python values and execute them against dataframes, databases, lakehouses,\nCSVs, Google Sheets, or anything else using our built-in SQL engine, which\nreturns the result as a Python dataframe.\n\n<img src=\"https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/readme-sql-cell.png\" width=\"700px\" />\n\nYour notebooks are still pure Python, even if they use SQL.\n\n**Dynamic markdown.** Use markdown parametrized by Python variables to tell\ndynamic stories that depend on Python data.\n\n**Built-in package management.** marimo has built-in support for all major\npackage managers, letting you [install packages on import](https://docs.marimo.io/guides/editor_features/package_management.html). marimo can even\n[serialize package\nrequirements](https://docs.marimo.io/guides/package_management/inlining_dependencies/)\nin notebook files, and auto install them in\nisolated venv sandboxes.\n\n**Deterministic execution order.** Notebooks are executed in a deterministic\norder, based on variable references instead of cells' positions on the page.\nOrganize your notebooks to best fit the stories you'd like to tell.\n\n**Performant runtime.** marimo runs only those cells that need to be run by\nstatically analyzing your code.\n\n**Batteries-included.** marimo comes with GitHub Copilot, AI assistants, Ruff\ncode formatting, HTML export, fast code completion, a [VS Code\nextension](https://marketplace.visualstudio.com/items?itemName=marimo-team.vscode-marimo),\nan interactive dataframe viewer, and [many more](https://docs.marimo.io/guides/editor_features/index.html)\nquality-of-life features.\n\n## Quickstart\n\n_The [marimo concepts\nplaylist](https://www.youtube.com/watch?v=3N6lInzq5MI&list=PLNJXGo8e1XT9jP7gPbRdm1XwloZVFvLEq)\non our [YouTube channel](https://www.youtube.com/@marimo-team) gives an\noverview of many features._\n\n**Installation.** In a terminal, run\n\n```bash\npip install marimo  # or conda install -c conda-forge marimo\nmarimo tutorial intro\n```\n\nTo install with additional dependencies that unlock SQL cells, AI completion, and more,\nrun\n\n```bash\npip install marimo[recommended]\n```\n\n**Create notebooks.**\n\nCreate or edit notebooks with\n\n```bash\nmarimo edit\n```\n\n**Run apps.** Run your notebook as a web app, with Python\ncode hidden and uneditable:\n\n```bash\nmarimo run your_notebook.py\n```\n\n<img src=\"https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/docs-model-comparison.gif\" style=\"border-radius: 8px\" width=\"450px\" />\n\n**Execute as scripts.** Execute a notebook as a script at the\ncommand line:\n\n```bash\npython your_notebook.py\n```\n\n**Automatically convert Jupyter notebooks.** Automatically convert Jupyter\nnotebooks to marimo notebooks with the CLI\n\n```bash\nmarimo convert your_notebook.ipynb > your_notebook.py\n```\n\nor use our [web interface](https://marimo.io/convert).\n\n**Tutorials.**\nList all tutorials:\n\n```bash\nmarimo tutorial --help\n```\n\n**Share cloud-based notebooks.** Use\n[molab](https://molab.marimo.io/notebooks), a cloud-based marimo notebook\nservice similar to Google Colab, to create and share notebook links.\n\n## Questions?\n\nSee the [FAQ](https://docs.marimo.io/faq.html) at our docs.\n\n## Learn more\n\nmarimo is easy to get started with, with lots of room for power users.\nFor example, here's an embedding visualizer made in marimo\n([video](https://marimo.io/videos/landing/full.mp4)):\n\n<img src=\"https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/embedding.gif\" width=\"700px\" />\n\nCheck out our [docs](https://docs.marimo.io),\n[usage examples](https://docs.marimo.io/examples/), and our [gallery](https://marimo.io/gallery) to learn more.\n\n<table border=\"0\">\n  <tr>\n    <td>\n      <a target=\"_blank\" href=\"https://docs.marimo.io/getting_started/key_concepts.html\">\n        <img src=\"https://docs.marimo.io/_static/reactive.gif\" style=\"max-height: 150px; width: auto; display: block\" />\n      </a>\n    </td>\n    <td>\n      <a target=\"_blank\" href=\"https://docs.marimo.io/api/inputs/index.html\">\n        <img src=\"https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/readme-ui.gif\" style=\"max-height: 150px; width: auto; display: block\" />\n      </a>\n    </td>\n    <td>\n      <a target=\"_blank\" href=\"https://docs.marimo.io/guides/working_with_data/plotting.html\">\n        <img src=\"https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/docs-intro.gif\" style=\"max-height: 150px; width: auto; display: block\" />\n      </a>\n    </td>\n    <td>\n      <a target=\"_blank\" href=\"https://docs.marimo.io/api/layouts/index.html\">\n        <img src=\"https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/outputs.gif\" style=\"max-height: 150px; width: auto; display: block\" />\n      </a>\n    </td>\n  </tr>\n  <tr>\n    <td>\n      <a target=\"_blank\" href=\"https://docs.marimo.io/getting_started/key_concepts.html\"> Tutorial </a>\n    </td>\n    <td>\n      <a target=\"_blank\" href=\"https://docs.marimo.io/api/inputs/index.html\"> Inputs </a>\n    </td>\n    <td>\n      <a target=\"_blank\" href=\"https://docs.marimo.io/guides/working_with_data/plotting.html\"> Plots </a>\n    </td>\n    <td>\n      <a target=\"_blank\" href=\"https://docs.marimo.io/api/layouts/index.html\"> Layout </a>\n    </td>\n  </tr>\n  <tr>\n    <td>\n      <a target=\"_blank\" href=\"https://marimo.app/l/c7h6pz\">\n        <img src=\"https://marimo.io/shield.svg\"/>\n      </a>\n    </td>\n    <td>\n      <a target=\"_blank\" href=\"https://marimo.app/l/0ue871\">\n        <img src=\"https://marimo.io/shield.svg\"/>\n      </a>\n    </td>\n    <td>\n      <a target=\"_blank\" href=\"https://marimo.app/l/lxp1jk\">\n        <img src=\"https://marimo.io/shield.svg\"/>\n      </a>\n    </td>\n    <td>\n      <a target=\"_blank\" href=\"https://marimo.app/l/14ovyr\">\n        <img src=\"https://marimo.io/shield.svg\"/>\n      </a>\n    </td>\n  </tr>\n</table>\n\n## Contributing\n\nWe appreciate all contributions! You don't need to be an expert to help out.\nPlease see [CONTRIBUTING.md](https://github.com/marimo-team/marimo/blob/main/CONTRIBUTING.md) for more details on how to get\nstarted.\n\n> Questions? Reach out to us [on Discord](https://marimo.io/discord?ref=readme).\n\n## Community\n\nWe're building a community. Come hang out with us!\n\n- \ud83c\udf1f [Star us on GitHub](https://github.com/marimo-team/marimo)\n- \ud83d\udcac [Chat with us on Discord](https://marimo.io/discord?ref=readme)\n- \ud83d\udce7 [Subscribe to our Newsletter](https://marimo.io/newsletter)\n- \u2601\ufe0f [Join our Cloud Waitlist](https://marimo.io/cloud)\n- \u270f\ufe0f [Start a GitHub Discussion](https://github.com/marimo-team/marimo/discussions)\n- \ud83e\udd8b [Follow us on Bluesky](https://bsky.app/profile/marimo.io)\n- \ud83d\udc26 [Follow us on Twitter](https://twitter.com/marimo_io)\n- \ud83c\udfa5 [Subscribe on YouTube](https://www.youtube.com/@marimo-team)\n- \ud83d\udd74\ufe0f [Follow us on LinkedIn](https://www.linkedin.com/company/marimo-io)\n\n**A NumFOCUS affiliated project.** marimo is a core part of the broader Python\necosystem and is a member of the NumFOCUS community, which includes projects\nsuch as NumPy, SciPy, and Matplotlib.\n\n<img src=\"https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/numfocus_affiliated_project.png\" height=\"40px\" />\n\n\n## Inspiration \u2728\n\nmarimo is a **reinvention** of the Python notebook as a reproducible, interactive,\nand shareable Python program, instead of an error-prone JSON scratchpad.\n\nWe believe that the tools we use shape the way we think \u2014 better tools, for\nbetter minds. With marimo, we hope to provide the Python community with a\nbetter programming environment to do research and communicate it; to experiment\nwith code and share it; to learn computational science and teach it.\n\nOur inspiration comes from many places and projects, especially\n[Pluto.jl](https://github.com/fonsp/Pluto.jl),\n[ObservableHQ](https://observablehq.com/tutorials), and\n[Bret Victor's essays](http://worrydream.com/). marimo is part of\na greater movement toward reactive dataflow programming. From\n[IPyflow](https://github.com/ipyflow/ipyflow), [streamlit](https://github.com/streamlit/streamlit),\n[TensorFlow](https://github.com/tensorflow/tensorflow),\n[PyTorch](https://github.com/pytorch/pytorch/tree/main),\n[JAX](https://github.com/google/jax), and\n[React](https://github.com/facebook/react), the ideas of functional,\ndeclarative, and reactive programming are transforming a broad range of tools\nfor the better.\n\n<p align=\"right\">\n  <img src=\"https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/marimo-logotype-horizontal.png\" height=\"200px\">\n</p>\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 68125931,
    "name": "reinforcement-learning-an-introduction",
    "full_name": "ShangtongZhang/reinforcement-learning-an-introduction",
    "description": "Python Implementation of Reinforcement Learning: An Introduction",
    "html_url": "https://github.com/ShangtongZhang/reinforcement-learning-an-introduction",
    "clone_url": "https://github.com/ShangtongZhang/reinforcement-learning-an-introduction.git",
    "owner_login": "ShangtongZhang",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/6715118?v=4",
    "stargazers_count": 14218,
    "watchers_count": 14218,
    "forks_count": 4949,
    "open_issues_count": 21,
    "size": 6085,
    "language": "Python",
    "languages": {
      "Python": 254701
    },
    "topics": [
      "artificial-intelligence",
      "reinforcement-learning"
    ],
    "license_name": "MIT License",
    "created_at": "2016-09-13T16:24:05+00:00",
    "updated_at": "2025-08-05T13:36:34+00:00",
    "pushed_at": "2024-08-09T01:09:15+00:00",
    "contributors_count": 28,
    "readme_length": 10485,
    "readme_content": "# Reinforcement Learning: An Introduction\n\n[![Build Status](https://travis-ci.org/ShangtongZhang/reinforcement-learning-an-introduction.svg?branch=master)](https://travis-ci.org/ShangtongZhang/reinforcement-learning-an-introduction)\n\nPython replication for Sutton & Barto's book [*Reinforcement Learning: An Introduction (2nd Edition)*](http://incompleteideas.net/book/the-book-2nd.html)\n\n> If you have any confusion about the code or want to report a bug, please open an issue instead of emailing me directly, and unfortunately I do not have exercise answers for the book.\n\n# Contents \n\n### Chapter 1\n1. Tic-Tac-Toe\n\n### Chapter 2\n1. [Figure 2.1: An exemplary bandit problem from the 10-armed testbed](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_2_1.png)\n2. [Figure 2.2: Average performance of epsilon-greedy action-value methods on the 10-armed testbed](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_2_2.png)\n3. [Figure 2.3: Optimistic initial action-value estimates](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_2_3.png)\n4. [Figure 2.4: Average performance of UCB action selection on the 10-armed testbed](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_2_4.png)\n5. [Figure 2.5: Average performance of the gradient bandit algorithm](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_2_5.png)\n6. [Figure 2.6: A parameter study of the various bandit algorithms](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_2_6.png)\n\n### Chapter 3\n1. [Figure 3.2: Grid example with random policy](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_3_2.png)\n2. [Figure 3.5: Optimal solutions to the gridworld example](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_3_5.png)\n\n### Chapter 4\n1. [Figure 4.1: Convergence of iterative policy evaluation on a small gridworld](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_4_1.png)\n2. [Figure 4.2: Jack\u2019s car rental problem](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_4_2.png)\n3. [Figure 4.3: The solution to the gambler\u2019s problem](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_4_3.png)\n\n### Chapter 5\n1. [Figure 5.1: Approximate state-value functions for the blackjack policy](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_5_1.png)\n2. [Figure 5.2: The optimal policy and state-value function for blackjack found by Monte Carlo ES](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_5_2.png)\n3. [Figure 5.3: Weighted importance sampling](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_5_3.png)\n4. [Figure 5.4: Ordinary importance sampling with surprisingly unstable estimates](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_5_4.png)\n\n### Chapter 6\n1. [Example 6.2: Random walk](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/example_6_2.png)\n2. [Figure 6.2: Batch updating](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_6_2.png)\n3. [Figure 6.3: Sarsa applied to windy grid world](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_6_3.png)\n4. [Figure 6.4: The cliff-walking task](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_6_4.png)\n5. [Figure 6.6: Interim and asymptotic performance of TD control methods](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_6_6.png)\n6. [Figure 6.7: Comparison of Q-learning and Double Q-learning](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_6_7.png)\n\n### Chapter 7\n1. [Figure 7.2: Performance of n-step TD methods on 19-state random walk](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_7_2.png)\n\n### Chapter 8\n1. [Figure 8.2: Average learning curves for Dyna-Q agents varying in their number of planning steps](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_8_2.png)\n2. [Figure 8.4: Average performance of Dyna agents on a blocking task](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_8_4.png)\n3. [Figure 8.5: Average performance of Dyna agents on a shortcut task](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_8_5.png)\n4. [Example 8.4: Prioritized sweeping significantly shortens learning time on the Dyna maze task](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/example_8_4.png)\n5. [Figure 8.7: Comparison of efficiency of expected and sample updates](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_8_7.png)\n6. [Figure 8.8: Relative efficiency of different update distributions](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_8_8.png)\n\n### Chapter 9\n1. [Figure 9.1: Gradient Monte Carlo algorithm on the 1000-state random walk task](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_9_1.png)\n2. [Figure 9.2: Semi-gradient n-steps TD algorithm on the 1000-state random walk task](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_9_2.png)\n3. [Figure 9.5: Fourier basis vs polynomials on the 1000-state random walk task](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_9_5.png)\n4. [Figure 9.8: Example of feature width\u2019s effect on initial generalization and asymptotic accuracy](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_9_8.png)\n5. [Figure 9.10: Single tiling and multiple tilings on the 1000-state random walk task](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_9_10.png)\n\n### Chapter 10\n1. [Figure 10.1: The cost-to-go function for Mountain Car task in one run](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_10_1.png)\n2. [Figure 10.2: Learning curves for semi-gradient Sarsa on Mountain Car task](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_10_2.png)\n3. [Figure 10.3: One-step vs multi-step performance of semi-gradient Sarsa on the Mountain Car task](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_10_3.png)\n4. [Figure 10.4: Effect of the alpha and n on early performance of n-step semi-gradient Sarsa](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_10_4.png)\n5. [Figure 10.5: Differential semi-gradient Sarsa on the access-control queuing task](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_10_5.png)\n\n### Chapter 11\n1. [Figure 11.2: Baird's Counterexample](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_11_2.png)\n2. [Figure 11.6: The behavior of the TDC algorithm on Baird\u2019s counterexample](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_11_6.png)\n3. [Figure 11.7: The behavior of the ETD algorithm in expectation on Baird\u2019s counterexample](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_11_7.png)\n\n### Chapter 12\n1. [Figure 12.3: Off-line \u03bb-return algorithm on 19-state random walk](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_12_3.png)\n2. [Figure 12.6: TD(\u03bb) algorithm on 19-state random walk](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_12_6.png)\n3. [Figure 12.8: True online TD(\u03bb) algorithm on 19-state random walk](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_12_8.png)\n4. [Figure 12.10: Sarsa(\u03bb) with replacing traces on Mountain Car](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_12_10.png)\n5. [Figure 12.11: Summary comparison of Sarsa(\u03bb) algorithms on Mountain Car](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_12_11.png)\n\n### Chapter 13\n1. [Example 13.1: Short corridor with switched actions](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/example_13_1.png)\n2. [Figure 13.1: REINFORCE on the short-corridor grid world](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_13_1.png)\n3. [Figure 13.2: REINFORCE with baseline on the short-corridor grid-world](https://raw.githubusercontent.com/ShangtongZhang/reinforcement-learning-an-introduction/master/images/figure_13_2.png)\n\n\n# Environment\n* python 3.6 \n* numpy\n* matplotlib\n* [seaborn](https://seaborn.pydata.org/index.html)\n* [tqdm](https://pypi.org/project/tqdm/)\n\n# Usage\n> All files are self-contained\n```commandline\npython any_file_you_want.py\n```\n\n# Contribution\nIf you want to contribute some missing examples or fix some bugs, feel free to open an issue or make a pull request. \n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 615510678,
    "name": "camel",
    "full_name": "camel-ai/camel",
    "description": "\ud83d\udc2b CAMEL: The first and the best multi-agent framework. Finding the Scaling Law of Agents. https://www.camel-ai.org",
    "html_url": "https://github.com/camel-ai/camel",
    "clone_url": "https://github.com/camel-ai/camel.git",
    "owner_login": "camel-ai",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/134388954?v=4",
    "stargazers_count": 13725,
    "watchers_count": 13725,
    "forks_count": 1490,
    "open_issues_count": 557,
    "size": 437557,
    "language": "Python",
    "languages": {
      "Python": 5755310,
      "JavaScript": 95628,
      "TypeScript": 67801,
      "Makefile": 3868,
      "Dockerfile": 2287,
      "HTML": 11
    },
    "topics": [
      "agent",
      "ai-societies",
      "artificial-intelligence",
      "communicative-ai",
      "cooperative-ai",
      "deep-learning",
      "large-language-models",
      "multi-agent-systems",
      "natural-language-processing"
    ],
    "license_name": "Apache License 2.0",
    "created_at": "2023-03-17T21:41:54+00:00",
    "updated_at": "2025-08-06T01:51:26+00:00",
    "pushed_at": "2025-08-05T22:14:02+00:00",
    "contributors_count": 100,
    "readme_length": 31080,
    "readme_content": "<div align=\"center\">\n  <a href=\"https://www.camel-ai.org/\">\n    <img src=\"docs/images/banner.png\" alt=\"Banner\">\n  </a>\n</div>\n\n</br>\n\n<div align=\"center\">\n\n[![Documentation][docs-image]][docs-url]\n[![Discord][discord-image]][discord-url]\n[![X][x-image]][x-url]\n[![Reddit][reddit-image]][reddit-url]\n[![Wechat][wechat-image]][wechat-url]\n[![Hugging Face][huggingface-image]][huggingface-url]\n[![Star][star-image]][star-url]\n[![Package License][package-license-image]][package-license-url]\n[![PyPI Download][package-download-image]][package-download-url]\n\n<a href=\"https://trendshift.io/repositories/649\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/649\" alt=\"camel-ai/camel | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n\n</div>\n\n\n<hr>\n\n<div align=\"center\">\n<h4 align=\"center\">\n\n[Community](https://github.com/camel-ai/camel#community) |\n[Installation](https://github.com/camel-ai/camel#installation) |\n[Examples](https://github.com/camel-ai/camel/tree/HEAD/examples) |\n[Paper](https://arxiv.org/abs/2303.17760) |\n[Citation](https://github.com/camel-ai/camel#citation) |\n[Contributing](https://github.com/camel-ai/camel#contributing-to-camel-) |\n[CAMEL-AI](https://www.camel-ai.org/)\n\n</h4>\n\n<p style=\"line-height: 1.5; text-align: center;\"> \ud83d\udc2b CAMEL is an open-source community dedicated to finding the scaling laws of agents. We believe that studying these agents on a large scale offers valuable insights into their behaviors, capabilities, and potential risks. To facilitate research in this field, we implement and support various types of agents, tasks, prompts, models, and simulated environments.</p>\n\n\n<br>\n\n\nJoin us ([*Discord*](https://discord.camel-ai.org/) or [*WeChat*](https://ghli.org/camel/wechat.png)) in pushing the boundaries of finding the scaling laws of agents. \n\n\ud83c\udf1f Star CAMEL on GitHub and be instantly notified of new releases.\n\n</div>\n\n<div align=\"center\">\n    <img src=\"docs/images/star.gif\" alt=\"Star\" width=\"186\" height=\"60\">\n  </a>\n</div>\n\n<br>\n\n\n## CAMEL Framework Design Principles\n\n<h3>\ud83e\uddec\u00a0Evolvability</h3 >\n\nThe framework enables multi-agent systems to continuously evolve by generating data and interacting with environments. This evolution can be driven by reinforcement learning with verifiable rewards or supervised learning.\n\n<h3>\ud83d\udcc8\u00a0Scalability</h3>\n\nThe framework is designed to support systems with millions of agents, ensuring efficient coordination, communication, and resource management at scale.\n\n<h3>\ud83d\udcbe\u00a0Statefulness</h3>\n\nAgents maintain stateful memory, enabling them to perform multi-step interactions with environments and efficiently tackle sophisticated tasks.\n\n<h3>\ud83d\udcd6\u00a0Code-as-Prompt</h3>\n\nEvery line of code and comment serves as a prompt for agents. Code should be written clearly and readably, ensuring both humans and agents can interpret it effectively.\n\n<br>\n\n## Why Use CAMEL for Your Research?\n\nWe are a community-driven research collective comprising over 100 researchers dedicated to advancing frontier research in Multi-Agent Systems. Researchers worldwide choose CAMEL for their studies based on the following reasons.\n\n<table style=\"width: 100%;\">\n  <tr>\n    <td align=\"left\"></td>\n    <td align=\"left\"></td>\n    <td align=\"left\"></td>\n  </tr>\n  <tr>\n    <td align=\"left\">\u2705</td>\n    <td align=\"left\" style=\"font-weight: bold;\">Large-Scale Agent System</td>\n    <td align=\"left\">Simulate up to 1M agents to study emergent behaviors and scaling laws in complex, multi-agent environments.</td>\n  </tr>\n  <tr>\n    <td align=\"left\">\u2705</td>\n    <td align=\"left\" style=\"font-weight: bold;\">Dynamic Communication</td>\n    <td align=\"left\">Enable real-time interactions among agents, fostering seamless collaboration for tackling intricate tasks.</td>\n  </tr>\n  <tr>\n    <td align=\"left\">\u2705</td>\n    <td align=\"left\" style=\"font-weight: bold;\">Stateful Memory</td>\n    <td align=\"left\">Equip agents with the ability to retain and leverage historical context, improving decision-making over extended interactions.</td>\n  </tr>\n  <tr>\n    <td align=\"left\">\u2705</td>\n    <td align=\"left\" style=\"font-weight: bold;\">Support for Multiple Benchmarks</td>\n    <td align=\"left\">Utilize standardized benchmarks to rigorously evaluate agent performance, ensuring reproducibility and reliable comparisons.</td>\n  </tr>\n  <tr>\n    <td align=\"left\">\u2705</td>\n    <td align=\"left\" style=\"font-weight: bold;\">Support for Different Agent Types</td>\n    <td align=\"left\">Work with a variety of agent roles, tasks, models, and environments, supporting interdisciplinary experiments and diverse research applications.</td>\n  </tr>\n  <tr>\n    <td align=\"left\">\u2705</td>\n    <td align=\"left\" style=\"font-weight: bold;\">Data Generation and Tool Integration</td>\n    <td align=\"left\">Automate the creation of large-scale, structured datasets while seamlessly integrating with multiple tools, streamlining synthetic data generation and research workflows.</td>\n  </tr>\n</table>\n\n<br>\n\n## What Can You Build With CAMEL?\n\n\n### 1. Data Generation\n\n<div align=\"center\">\n  <a href=\"https://github.com/camel-ai/camel/blob/master/camel/datagen/cot_datagen.py\">\n    <img src=\"docs/images/cot.png\" alt=\"CoT Data Generation\">\n  </a>\n</div>\n\n<div align=\"center\">\n  <a href=\"https://github.com/camel-ai/camel/tree/master/camel/datagen/self_instruct\">\n    <img src=\"docs/images/self_instruct.png\" alt=\"Self-Instruct Data Generation\">\n  </a>\n</div>\n\n<div align=\"center\">\n  <a href=\"https://github.com/camel-ai/camel/tree/master/camel/datagen/source2synth\">\n    <img src=\"docs/images/source2synth.png\" alt=\"Source2Synth Data Generation\">\n  </a>\n</div>\n\n<div align=\"center\">\n  <a href=\"https://github.com/camel-ai/camel/blob/master/camel/datagen/self_improving_cot.py\">\n    <img src=\"docs/images/self_improving.png\" alt=\"Self-Improving Data Generation\">\n  </a>\n</div>\n\n### 2. Task Automation\n\n<div align=\"center\">\n  <a href=\"https://github.com/camel-ai/camel/blob/master/camel/societies/role_playing.py\">\n    <img src=\"docs/images/role_playing.png\" alt=\"Role Playing\">\n  </a>\n</div>\n\n<div align=\"center\">\n  <a href=\"https://github.com/camel-ai/camel/tree/master/camel/societies/workforce\">\n    <img src=\"docs/images/workforce.png\" alt=\"Workforce\">\n  </a>\n</div>\n\n<div align=\"center\">\n  <a href=\"https://docs.camel-ai.org/cookbooks/advanced_features/agents_with_rag.html\">\n    <img src=\"docs/images/rag_pipeline.png\" alt=\"RAG Pipeline\">\n  </a>\n</div>\n\n\n### 3. World Simulation\n\n<div align=\"center\">\n  <a href=\"https://github.com/camel-ai/oasis\">\n    <img src=\"docs/images/oasis_case.png\" alt=\"Oasis Case\">\n  </a>\n</div>\n\n<br>\n\n## Quick Start\n\nInstalling CAMEL is a breeze thanks to its availability on PyPI. Simply open your terminal and run:\n\n```bash\npip install camel-ai\n```\n\n### Starting with ChatAgent\n\nThis example demonstrates how to create a `ChatAgent` using the CAMEL framework and perform a search query using DuckDuckGo.\n\n1. **Install the tools package:**\n\n  ```bash\n  pip install 'camel-ai[web_tools]'\n  ```\n\n2. **Set up your OpenAI API key:**\n\n  ```bash\n  export OPENAI_API_KEY='your_openai_api_key'\n  ```\n\n3. **Run the following Python code:**\n\n  ```python\n  from camel.models import ModelFactory\n  from camel.types import ModelPlatformType, ModelType\n  from camel.agents import ChatAgent\n  from camel.toolkits import SearchToolkit\n\n  model = ModelFactory.create(\n    model_platform=ModelPlatformType.OPENAI,\n    model_type=ModelType.GPT_4O,\n    model_config_dict={\"temperature\": 0.0},\n  )\n\n  search_tool = SearchToolkit().search_duckduckgo\n\n  agent = ChatAgent(model=model, tools=[search_tool])\n\n  response_1 = agent.step(\"What is CAMEL-AI?\")\n  print(response_1.msgs[0].content)\n  # CAMEL-AI is the first LLM (Large Language Model) multi-agent framework\n  # and an open-source community focused on finding the scaling laws of agents.\n  # ...\n\n  response_2 = agent.step(\"What is the Github link to CAMEL framework?\")\n  print(response_2.msgs[0].content)\n  # The GitHub link to the CAMEL framework is\n  # [https://github.com/camel-ai/camel](https://github.com/camel-ai/camel).\n  ```\n\n\nFor more detailed instructions and additional configuration options, check out the [installation section](https://github.com/camel-ai/camel/blob/master/docs/get_started/installation.md).\n\nAfter running, you can explore our CAMEL Tech Stack and Cookbooks at [docs.camel-ai.org](https://docs.camel-ai.org) to build powerful multi-agent systems.\n\nWe provide a [![Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1AzP33O8rnMW__7ocWJhVBXjKziJXPtim?usp=sharing) demo showcasing a conversation between two ChatGPT agents playing roles as a python programmer and a stock trader collaborating on developing a trading bot for stock market.\n\nExplore different types of agents, their roles, and their applications.\n\n- **[Creating Your First Agent](https://docs.camel-ai.org/cookbooks/basic_concepts/create_your_first_agent.html)**\n- **[Creating Your First Agent Society](https://docs.camel-ai.org/cookbooks/basic_concepts/create_your_first_agents_society.html)**\n- **[Embodied Agents](https://docs.camel-ai.org/cookbooks/advanced_features/embodied_agents.html)**\n- **[Critic Agents](https://docs.camel-ai.org/cookbooks/advanced_features/critic_agents_and_tree_search.html)**\n\n### Seeking Help\n\nPlease reach out to us on [CAMEL discord](https://discord.camel-ai.org/) if you encounter any issue set up CAMEL.\n\n<br>\n\n## Tech Stack\n\n<div align=\"center\">\n  <a href=\"https://docs.camel-ai.org\">\n    <img src=\"https://camel-ai.github.io/camel_asset/graphics/techstack.png\" alt=\"TechStack\">\n  </a>\n</div>\n\n### Key Modules\nCore components and utilities to build, operate, and enhance CAMEL-AI agents and societies.\n\n| Module | Description |\n|:---|:---|\n| **[Agents](https://docs.camel-ai.org/key_modules/agents.html)** | Core agent architectures and behaviors for autonomous operation. |\n| **[Agent Societies](https://docs.camel-ai.org/key_modules/society.html)** | Components for building and managing multi-agent systems and collaboration. |\n| **[Data Generation](https://docs.camel-ai.org/key_modules/datagen.html)** | Tools and methods for synthetic data creation and augmentation. |\n| **[Models](https://docs.camel-ai.org/key_modules/models.html)** | Model architectures and customization options for agent intelligence. |\n| **[Tools](https://docs.camel-ai.org/key_modules/tools.html)** | Tools integration for specialized agent tasks. |\n| **[Memory](https://docs.camel-ai.org/key_modules/memory.html)** | Memory storage and retrieval mechanisms for agent state management. |\n| **[Storage](https://docs.camel-ai.org/key_modules/storages.html)** | Persistent storage solutions for agent data and states. |\n| **[Benchmarks](https://github.com/camel-ai/camel/tree/master/camel/benchmarks)** | Performance evaluation and testing frameworks. |\n| **[Interpreters](https://docs.camel-ai.org/key_modules/interpreters.html)** | Code and command interpretation capabilities. |\n| **[Data Loaders](https://docs.camel-ai.org/key_modules/loaders.html)** | Data ingestion and preprocessing tools. |\n| **[Retrievers](https://docs.camel-ai.org/key_modules/retrievers.html)** | Knowledge retrieval and RAG components. |\n| **[Runtime](https://github.com/camel-ai/camel/tree/master/camel/runtime)** | Execution environment and process management. |\n| **[Human-in-the-Loop](https://docs.camel-ai.org/cookbooks/advanced_features/agents_with_human_in_loop_and_tool_approval.html)** | Interactive components for human oversight and intervention. |\n---\n\n## Research\n\nWe believe that studying these agents on a large scale offers valuable insights into their behaviors, capabilities, and potential risks.\n\n**Explore our research projects:**\n\n<div align=\"center\">\n  <a href=\"https://crab.camel-ai.org/\">\n    <img src=\"docs/images/crab.png\" alt=\"CRAB\">\n  </a>\n</div>\n\n<div align=\"center\">\n  <a href=\"https://agent-trust.camel-ai.org/\">\n    <img src=\"docs/images/agent_trust.png\" alt=\"Agent Trust\">\n  </a>\n</div>\n\n<div align=\"center\">\n  <a href=\"https://oasis.camel-ai.org/\">\n    <img src=\"docs/images/oasis.png\" alt=\"OASIS\">\n  </a>\n</div>\n\n<div align=\"center\">\n  <a href=\"https://emos-project.github.io/\">\n    <img src=\"docs/images/emos.png\" alt=\"Emos\">\n  </a>\n</div>\n\n>### Research with US\n>\n>We warmly invite you to use CAMEL for your impactful research. \n>\n> Rigorous research takes time and resources. We are a community-driven research collective with 100+ researchers exploring the frontier research of Multi-agent Systems. Join our ongoing projects or test new ideas with us, [reach out via email](mailto:camel-ai@eigent.ai) for more information.\n>\n><div align=\"center\">\n>    <img src=\"docs/images/partners.png\" alt=\"Partners\">\n></div>\n\n<br>\n\n## Synthetic Datasets\n\n### 1. Utilize Various LLMs as Backends\n\nFor more details, please see our [`Models Documentation`](https://docs.camel-ai.org/key_modules/models.html#).\n\n> **Data (Hosted on Hugging Face)**\n\n| Dataset        | Chat format                                                                                         | Instruction format                                                                                               | Chat format (translated)                                                                   |\n|----------------|-----------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------|\n| **AI Society** | [Chat format](https://huggingface.co/datasets/camel-ai/ai_society/blob/main/ai_society_chat.tar.gz) | [Instruction format](https://huggingface.co/datasets/camel-ai/ai_society/blob/main/ai_society_instructions.json) | [Chat format (translated)](https://huggingface.co/datasets/camel-ai/ai_society_translated) |\n| **Code**       | [Chat format](https://huggingface.co/datasets/camel-ai/code/blob/main/code_chat.tar.gz)             | [Instruction format](https://huggingface.co/datasets/camel-ai/code/blob/main/code_instructions.json)             | x                                                                                          |\n| **Math**       | [Chat format](https://huggingface.co/datasets/camel-ai/math)                                        | x                                                                                                                | x                                                                                          |\n| **Physics**    | [Chat format](https://huggingface.co/datasets/camel-ai/physics)                                     | x                                                                                                                | x                                                                                          |\n| **Chemistry**  | [Chat format](https://huggingface.co/datasets/camel-ai/chemistry)                                   | x                                                                                                                | x                                                                                          |\n| **Biology**    | [Chat format](https://huggingface.co/datasets/camel-ai/biology)                                     | x                                                                                                                | x                                                                                          |\n\n### 2. Visualizations of Instructions and Tasks\n\n| Dataset          | Instructions                                                                                                         | Tasks                                                                                                         |\n|------------------|----------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------|\n| **AI Society**   | [Instructions](https://atlas.nomic.ai/map/3a559a06-87d0-4476-a879-962656242452/db961915-b254-48e8-8e5c-917f827b74c6) | [Tasks](https://atlas.nomic.ai/map/cb96f41b-a6fd-4fe4-ac40-08e101714483/ae06156c-a572-46e9-8345-ebe18586d02b) |\n| **Code**         | [Instructions](https://atlas.nomic.ai/map/902d6ccb-0bbb-4294-83a8-1c7d2dae03c8/ace2e146-e49f-41db-a1f4-25a2c4be2457) | [Tasks](https://atlas.nomic.ai/map/efc38617-9180-490a-8630-43a05b35d22d/2576addf-a133-45d5-89a9-6b067b6652dd) |\n| **Misalignment** | [Instructions](https://atlas.nomic.ai/map/5c491035-a26e-4a05-9593-82ffb2c3ab40/2bd98896-894e-4807-9ed8-a203ccb14d5e) | [Tasks](https://atlas.nomic.ai/map/abc357dd-9c04-4913-9541-63e259d7ac1f/825139a4-af66-427c-9d0e-f36b5492ab3f) |\n\n<br>\n\n## Cookbooks (Usecases)\nPractical guides and tutorials for implementing specific functionalities in CAMEL-AI agents and societies.\n\n### 1. Basic Concepts\n| Cookbook | Description |\n|:---|:---|\n| **[Creating Your First Agent](https://docs.camel-ai.org/cookbooks/basic_concepts/create_your_first_agent.html)** | A step-by-step guide to building your first agent. |\n| **[Creating Your First Agent Society](https://docs.camel-ai.org/cookbooks/basic_concepts/create_your_first_agents_society.html)** | Learn to build a collaborative society of agents. |\n| **[Message Cookbook](https://docs.camel-ai.org/cookbooks/basic_concepts/agents_message.html)** | Best practices for message handling in agents. |\n\n### 2. Advanced Features\n| Cookbook | Description |\n|:---|:---|\n| **[Tools Cookbook](https://docs.camel-ai.org/cookbooks/advanced_features/agents_with_tools.html)** | Integrating tools for enhanced functionality. |\n| **[Memory Cookbook](https://docs.camel-ai.org/cookbooks/advanced_features/agents_with_memory.html)** | Implementing memory systems in agents. |\n| **[RAG Cookbook](https://docs.camel-ai.org/cookbooks/advanced_features/agents_with_rag.html)** | Recipes for Retrieval-Augmented Generation. |\n| **[Graph RAG Cookbook](https://docs.camel-ai.org/cookbooks/advanced_features/agents_with_graph_rag.html)** | Leveraging knowledge graphs with RAG. |\n| **[Track CAMEL Agents with AgentOps](https://docs.camel-ai.org/cookbooks/advanced_features/agents_tracking.html)** | Tools for tracking and managing agents in operations. |\n\n### 3. Model Training & Data Generation\n| Cookbook | Description |\n|:---|:---|\n| **[Data Generation with CAMEL and Finetuning with Unsloth](https://docs.camel-ai.org/cookbooks/data_generation/sft_data_generation_and_unsloth_finetuning_Qwen2_5_7B.html)** | Learn how to generate data with CAMEL and fine-tune models effectively with Unsloth. |\n| **[Data Gen with Real Function Calls and Hermes Format](https://docs.camel-ai.org/cookbooks/data_generation/data_gen_with_real_function_calls_and_hermes_format.html)** | Explore how to generate data with real function calls and the Hermes format. |\n| **[CoT Data Generation and Upload Data to Huggingface](https://docs.camel-ai.org/cookbooks/data_generation/distill_math_reasoning_data_from_deepseek_r1.html)** | Uncover how to generate CoT data with CAMEL and seamlessly upload it to Huggingface. |\n| **[CoT Data Generation and SFT Qwen with Unsolth](https://docs.camel-ai.org/cookbooks/data_generation/cot_data_gen_sft_qwen_unsolth_upload_huggingface.html)** | Discover how to generate CoT data using CAMEL and SFT Qwen with Unsolth, and seamlessly upload your data and model to Huggingface. |\n\n### 4. Multi-Agent Systems & Applications\n| Cookbook | Description |\n|:---|:---|\n| **[Role-Playing Scraper for Report & Knowledge Graph Generation](https://docs.camel-ai.org/cookbooks/applications/roleplaying_scraper.html)** | Create role-playing agents for data scraping and reporting. |\n| **[Create A Hackathon Judge Committee with Workforce](https://docs.camel-ai.org/cookbooks/multi_agent_society/workforce_judge_committee.html)** | Building a team of agents for collaborative judging. |\n| **[Dynamic Knowledge Graph Role-Playing: Multi-Agent System with dynamic, temporally-aware knowledge graphs](https://docs.camel-ai.org/cookbooks/applications/dyamic_knowledge_graph.html)** |  Builds dynamic, temporally-aware knowledge graphs for financial applications using a multi-agent system. It processes financial reports, news articles, and research papers to help traders analyze data, identify relationships, and uncover market insights. The system also utilizes diverse and optional element node deduplication techniques to ensure data integrity and optimize graph structure for financial decision-making. |\n| **[Customer Service Discord Bot with Agentic RAG](https://docs.camel-ai.org/cookbooks/applications/customer_service_Discord_bot_using_SambaNova_with_agentic_RAG.html)** | Learn how to build a robust customer service bot for Discord using Agentic RAG. |\n| **[Customer Service Discord Bot with Local Model](https://docs.camel-ai.org/cookbooks/applications/customer_service_Discord_bot_using_local_model_with_agentic_RAG.html)** | Learn how to build a robust customer service bot for Discord using Agentic RAG which supports local deployment. |\n\n### 5. Data Processing\n| Cookbook | Description |\n|:---|:---|\n| **[Video Analysis](https://docs.camel-ai.org/cookbooks/data_processing/video_analysis.html)** | Techniques for agents in video data analysis. |\n| **[3 Ways to Ingest Data from Websites with Firecrawl](https://docs.camel-ai.org/cookbooks/data_processing/ingest_data_from_websites_with_Firecrawl.html)** | Explore three methods for extracting and processing data from websites using Firecrawl. |\n| **[Create AI Agents that work with your PDFs](https://docs.camel-ai.org/cookbooks/data_processing/agent_with_chunkr_for_pdf_parsing.html)** | Learn how to create AI agents that work with your PDFs using Chunkr and Mistral AI. |\n\n<br>\n\n## Real-World Usecases\n\nReal-world usecases demonstrating how CAMEL\u2019s multi-agent framework enables real business value across infrastructure automation, productivity workflows, retrieval-augmented conversations, intelligent document/video analysis, and collaborative research.\n\n### 1 Infrastructure Automation\n\n| Usecase                                                      | Description                                                  |\n| :----------------------------------------------------------- | :----------------------------------------------------------- |\n| **[ACI MCP](https://github.com/camel-ai/camel/tree/master/examples/usecases/aci_mcp)** | Real-world usecases demonstrating how CAMEL\u2019s multi-agent framework enables real business value across infrastructure automation, productivity workflows, retrieval-augmented conversations, intelligent document/video analysis, and collaborative research. |\n| **[Cloudflare MCP CAMEL](https://github.com/camel-ai/camel/tree/master/examples/usecases/cloudfare_mcp_camel)** | Intelligent agents manage Cloudflare resources dynamically, enabling scalable and efficient cloud security and performance tuning. |\n\n### 2 Productivity & Business Workflows\n\n| Usecase                                                      | Description                                                  |\n| :----------------------------------------------------------- | :----------------------------------------------------------- |\n| **[Airbnb MCP](https://github.com/camel-ai/camel/tree/master/examples/usecases/airbnb_mcp)** | Coordinate agents to optimize and manage Airbnb listings and host operations. |\n| **[PPTX Toolkit Usecase](https://github.com/camel-ai/camel/tree/master/examples/usecases/pptx_toolkit_usecase)** | Analyze PowerPoint documents and extract structured insights through multi-agent collaboration. |\n\n### 3 Retrieval-Augmented Multi-Agent Chat\n\n| Usecase                                                      | Description                                                  |\n| :----------------------------------------------------------- | :----------------------------------------------------------- |\n| **[Chat with GitHub](https://github.com/camel-ai/camel/tree/master/examples/usecases/chat_with_github)** | Query and understand GitHub codebases through CAMEL agents leveraging RAG-style workflows, accelerating developer onboarding and codebase navigation. |\n| **[Chat with YouTube](https://github.com/camel-ai/camel/tree/master/examples/usecases/chat_with_youtube)** | Conversational agents extract and summarize video transcripts, enabling faster content understanding and repurposing. |\n\n### 4 Video & Document Intelligence\n\n| Usecase                                                      | Description                                                  |\n| :----------------------------------------------------------- | :----------------------------------------------------------- |\n| **[YouTube OCR](https://github.com/camel-ai/camel/tree/master/examples/usecases/youtube_ocr)** | Agents perform OCR on video screenshots to summarize visual content, supporting media monitoring and compliance. |\n| **[Mistral OCR](https://github.com/camel-ai/camel/tree/master/examples/usecases/mistral_OCR)** | CAMEL agents use OCR with Mistral to analyze documents, reducing manual effort in document understanding workflows. |\n\n### 5 Research & Collaboration\n\n| Usecase                                                      | Description                                                  |\n| :----------------------------------------------------------- | :----------------------------------------------------------- |\n| **[Multi-Agent Research Assistant](https://github.com/camel-ai/camel/tree/master/examples/usecases/multi_agent_research_assistant)** | Simulates a team of research agents collaborating on literature review, improving efficiency in exploratory analysis and reporting. |\n\n<br>\n\n\n## \ud83d\uddd3\ufe0f Events\n\nWe are actively involved in community events including:\n\n- \ud83c\udf99\ufe0f **Community Meetings** \u2014 Weekly virtual syncs with the CAMEL team\n- \ud83c\udfc6 **Competitions** \u2014 Hackathons, Bounty Tasks and coding challenges hosted by CAMEL\n- \ud83e\udd1d **Volunteer Activities** \u2014 Contributions, documentation drives, and mentorship\n- \ud83c\udf0d **Ambassador Programs** \u2014 Represent CAMEL in your university or local tech groups \n\n> Want to host or participate in a CAMEL event? Join our [Discord](https://discord.com/invite/CNcNpquyDc) or want to be part of [Ambassador Program](https://www.camel-ai.org/ambassador).\n\n\n\n## Contributing to CAMEL\n\n> For those who'd like to contribute code, we appreciate your interest in contributing to our open-source initiative. Please take a moment to review our [contributing guidelines](https://github.com/camel-ai/camel/blob/master/CONTRIBUTING.md) to get started on a smooth collaboration journey.\ud83d\ude80\n>\n> We also welcome you to help CAMEL grow by sharing it on social media, at events, or during conferences. Your support makes a big difference!\n\n<br>\n\n## Community & Contact\nFor more information please contact camel-ai@eigent.ai\n\n- **GitHub Issues:** Report bugs, request features, and track development. [Submit an issue](https://github.com/camel-ai/camel/issues)\n\n- **Discord:** Get real-time support, chat with the community, and stay updated. [Join us](https://discord.camel-ai.org/)\n\n- **X (Twitter):** Follow for updates, AI insights, and key announcements. [Follow us](https://x.com/CamelAIOrg)\n\n- **Ambassador Project:** Advocate for CAMEL-AI, host events, and contribute content. [Learn more](https://www.camel-ai.org/community)\n\n- **WeChat Community:** Scan the QR code below to join our WeChat community.\n\n  <div align=\"center\">\n    <img src=\"misc/wechat.jpeg\" alt=\"WeChat QR Code\" width=\"200\">\n  </div>\n\n\n<br>\n\n## Citation\n```\n@inproceedings{li2023camel,\n  title={CAMEL: Communicative Agents for \"Mind\" Exploration of Large Language Model Society},\n  author={Li, Guohao and Hammoud, Hasan Abed Al Kader and Itani, Hani and Khizbullin, Dmitrii and Ghanem, Bernard},\n  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},\n  year={2023}\n}\n```\n\n## Acknowledgment\nSpecial thanks to [Nomic AI](https://home.nomic.ai/) for giving us extended access to their data set exploration tool (Atlas).\n\nWe would also like to thank Haya Hammoud for designing the initial logo of our project.\n\nWe implemented amazing research ideas from other works for you to build, compare and customize your agents. If you use any of these modules, please kindly cite the original works:\n- `TaskCreationAgent`, `TaskPrioritizationAgent` and `BabyAGI` from *Nakajima et al.*: [Task-Driven Autonomous Agent](https://yoheinakajima.com/task-driven-autonomous-agent-utilizing-gpt-4-pinecone-and-langchain-for-diverse-applications/). [[Example](https://github.com/camel-ai/camel/blob/master/examples/ai_society/babyagi_playing.py)]\n\n- `PersonaHub` from *Tao Ge et al.*: [Scaling Synthetic Data Creation with 1,000,000,000 Personas](https://arxiv.org/pdf/2406.20094). [[Example](https://github.com/camel-ai/camel/blob/master/examples/personas/personas_generation.py)]\n\n- `Self-Instruct` from *Yizhong Wang et al.*: [SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions](https://arxiv.org/pdf/2212.10560). [[Example](https://github.com/camel-ai/camel/blob/master/examples/datagen/self_instruct/self_instruct.py)]\n\n\n## License\n\nThe source code is licensed under Apache 2.0.\n\n<br>\n\n[docs-image]: https://img.shields.io/badge/Documentation-EB3ECC\n[docs-url]: https://camel-ai.github.io/camel/index.html\n[star-image]: https://img.shields.io/github/stars/camel-ai/camel?label=stars&logo=github&color=brightgreen\n[star-url]: https://github.com/camel-ai/camel/stargazers\n[package-license-image]: https://img.shields.io/badge/License-Apache_2.0-blue.svg\n[package-license-url]: https://github.com/camel-ai/camel/blob/master/licenses/LICENSE\n[package-download-image]: https://img.shields.io/pypi/dm/camel-ai\n\n[colab-url]: https://colab.research.google.com/drive/1AzP33O8rnMW__7ocWJhVBXjKziJXPtim?usp=sharing\n[colab-image]: https://colab.research.google.com/assets/colab-badge.svg\n[huggingface-url]: https://huggingface.co/camel-ai\n[huggingface-image]: https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-CAMEL--AI-ffc107?color=ffc107&logoColor=white\n[discord-url]: https://discord.camel-ai.org/\n[discord-image]: https://img.shields.io/discord/1082486657678311454?logo=discord&labelColor=%20%235462eb&logoColor=%20%23f5f5f5&color=%20%235462eb\n[wechat-url]: https://ghli.org/camel/wechat.png\n[wechat-image]: https://img.shields.io/badge/WeChat-CamelAIOrg-brightgreen?logo=wechat&logoColor=white\n[x-url]: https://x.com/CamelAIOrg\n[x-image]: https://img.shields.io/twitter/follow/CamelAIOrg?style=social\n[twitter-image]: https://img.shields.io/twitter/follow/CamelAIOrg?style=social&color=brightgreen&logo=twitter\n[reddit-url]: https://www.reddit.com/r/CamelAI/\n[reddit-image]: https://img.shields.io/reddit/subreddit-subscribers/CamelAI?style=plastic&logo=reddit&label=r%2FCAMEL&labelColor=white\n[ambassador-url]: https://www.camel-ai.org/community\n[package-download-url]: https://pypi.org/project/camel-ai",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 636372163,
    "name": "litgpt",
    "full_name": "Lightning-AI/litgpt",
    "description": "20+ high-performance LLMs with recipes to pretrain, finetune and deploy at scale.",
    "html_url": "https://github.com/Lightning-AI/litgpt",
    "clone_url": "https://github.com/Lightning-AI/litgpt.git",
    "owner_login": "Lightning-AI",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/58386951?v=4",
    "stargazers_count": 12594,
    "watchers_count": 12594,
    "forks_count": 1286,
    "open_issues_count": 235,
    "size": 5684,
    "language": "Python",
    "languages": {
      "Python": 1195855,
      "Dockerfile": 803
    },
    "topics": [
      "ai",
      "artificial-intelligence",
      "deep-learning",
      "large-language-models",
      "llm",
      "llm-inference",
      "llms"
    ],
    "license_name": "Apache License 2.0",
    "created_at": "2023-05-04T17:46:11+00:00",
    "updated_at": "2025-08-05T22:50:29+00:00",
    "pushed_at": "2025-08-04T11:09:38+00:00",
    "contributors_count": 100,
    "readme_length": 29684,
    "readme_content": "<div align=\"center\">\n\n\n# \u26a1 LitGPT\n\n**20+ high-performance LLMs with recipes to pretrain, finetune, and deploy at scale.**\n\n<pre>\n\u2705 From scratch implementations      \u2705 No abstractions         \u2705 Beginner friendly\n   \u2705 Flash attention                   \u2705 FSDP                    \u2705 LoRA, QLoRA, Adapter\n\u2705 Reduce GPU memory (fp4/8/16/32)   \u2705 1-1000+ GPUs/TPUs       \u2705 20+ LLMs         \n</pre>\n\n\n---\n\n\n![PyPI - Python Version](https://img.shields.io/pypi/pyversions/pytorch-lightning)\n![cpu-tests](https://github.com/lightning-AI/lit-stablelm/actions/workflows/cpu-tests.yml/badge.svg) [![license](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/Lightning-AI/lit-stablelm/blob/master/LICENSE) [![Discord](https://img.shields.io/discord/1077906959069626439)](https://discord.gg/VptPCZkGNa)\n\n<p align=\"center\">\n  <a href=\"#quick-start\">Quick start</a> \u2022\n  <a href=\"#choose-from-20-llms\">Models</a> \u2022\n  <a href=\"#finetune-an-llm\">Finetune</a> \u2022\n  <a href=\"#deploy-an-llm\">Deploy</a> \u2022\n  <a href=\"#all-workflows\">All workflows</a> \u2022\n  <a href=\"#state-of-the-art-features\">Features</a> \u2022\n  <a href=\"#training-recipes\">Recipes (YAML)</a> \u2022\n  <a href=\"https://lightning.ai/\">Lightning AI</a> \u2022\n    <a href=\"#tutorials\">Tutorials</a>\n</p>\n\n&nbsp;\n\n<a target=\"_blank\" href=\"https://lightning.ai/lightning-ai/studios/litgpt-quick-start\">\n  <img src=\"https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/get-started-badge.svg\" height=\"36px\" alt=\"Get started\"/>\n</a>\n\n&nbsp;\n\n</div>\n\n# Use, finetune, pretrain, and deploy LLMs Lightning fast \u26a1\u26a1\nEvery LLM is implemented from scratch with **no abstractions** and **full control**, making them blazing fast, minimal, and performant at enterprise scale.\n\n\u2705 **Enterprise ready -** Apache 2.0 for unlimited enterprise use.</br>\n\u2705 **Developer friendly -** Easy debugging with no abstraction layers and single file implementations.</br>\n\u2705 **Optimized performance -** Models designed to maximize performance, reduce costs, and speed up training.</br>\n\u2705 **Proven recipes -** Highly-optimized training/finetuning recipes tested at enterprise scale.</br>\n\n&nbsp;\n\n# Quick start\nInstall LitGPT\n```\npip install 'litgpt[extra]'\n```\n\nLoad and use any of the [20+ LLMs](#choose-from-20-llms):\n```python\nfrom litgpt import LLM\n\nllm = LLM.load(\"microsoft/phi-2\")\ntext = llm.generate(\"Fix the spelling: Every fall, the family goes to the mountains.\")\nprint(text)\n# Corrected Sentence: Every fall, the family goes to the mountains.\n```\n\n&nbsp;\n\n\u2705 Optimized for fast inference</br>\n\u2705 Quantization</br>\n\u2705 Runs on low-memory GPUs</br>\n\u2705 No layers of internal abstractions</br>\n\u2705 Optimized for production scale</br>\n\n<details>\n  <summary>Advanced install options</summary>\n\nInstall from source:\n\n```bash\ngit clone https://github.com/Lightning-AI/litgpt\ncd litgpt\npip install -e '.[all]'\n```\n</details>\n\n[Explore the full Python API docs](tutorials/python-api.md).\n\n&nbsp;\n\n---\n# Choose from 20+ LLMs\nEvery model is written from scratch to maximize performance and remove layers of abstraction:\n\n| Model | Model size | Author | Reference |\n|----|----|----|----|\n| Llama 3, 3.1, 3.2, 3.3 | 1B, 3B, 8B, 70B, 405B | Meta AI | [Meta AI 2024](https://github.com/meta-llama/llama3)                                           |\n| Code Llama | 7B, 13B, 34B, 70B | Meta AI | [Rozi\u00e8re et al. 2023](https://arxiv.org/abs/2308.12950)                                       |\n| CodeGemma | 7B | Google | [Google Team, Google Deepmind](https://ai.google.dev/gemma/docs/codegemma)                                     |\n| Gemma 2 | 2B, 9B, 27B | Google | [Google Team, Google Deepmind](https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf)  |\n| Phi 4 | 14B | Microsoft Research | [Abdin et al. 2024](https://arxiv.org/abs/2412.08905)                                                                            |\n| Qwen2.5 | 0.5B, 1.5B, 3B, 7B, 14B, 32B, 72B | Alibaba Group | [Qwen Team 2024](https://qwenlm.github.io/blog/qwen2.5/)                                               |\n| Qwen2.5 Coder | 0.5B, 1.5B, 3B, 7B, 14B, 32B | Alibaba Group | [Hui, Binyuan et al. 2024](https://arxiv.org/abs/2409.12186)                                          |\n| R1 Distill Llama | 8B, 70B | DeepSeek AI | [DeepSeek AI 2025](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf)                                                                                 |\n| ... | ... | ... | ...   |\n\n<details>\n  <summary>See full list of 20+ LLMs</summary>\n\n&nbsp;\n\n#### All models\n\n| Model | Model size | Author | Reference |\n|----|----|----|----|\n| CodeGemma | 7B | Google | [Google Team, Google Deepmind](https://ai.google.dev/gemma/docs/codegemma)                                                                 |\n| Code Llama | 7B, 13B, 34B, 70B | Meta AI | [Rozi\u00e8re et al. 2023](https://arxiv.org/abs/2308.12950)                                                                   |\n| Falcon | 7B, 40B, 180B | TII UAE | [TII 2023](https://falconllm.tii.ae)                                                                                              |\n| Falcon 3 | 1B, 3B, 7B, 10B | TII UAE | [TII 2024](https://huggingface.co/blog/falcon3)                                                                                              |\n| FreeWilly2 (Stable Beluga 2) | 70B | Stability AI | [Stability AI 2023](https://stability.ai/blog/stable-beluga-large-instruction-fine-tuned-models)                 |\n| Function Calling Llama 2 | 7B | Trelis | [Trelis et al. 2023](https://huggingface.co/Trelis/Llama-2-7b-chat-hf-function-calling-v2)                                  |\n| Gemma | 2B, 7B | Google | [Google Team, Google Deepmind](https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf)                                       |\n| Gemma 2 | 9B, 27B | Google | [Google Team, Google Deepmind](https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf)                                  |\n| Gemma 3 | 1B, 4B, 12B, 27B | Google | [Google Team, Google Deepmind](https://arxiv.org/pdf/2503.19786)                                  |\n| Llama 2 | 7B, 13B, 70B | Meta AI | [Touvron et al. 2023](https://arxiv.org/abs/2307.09288)                                                                           |\n| Llama 3.1 | 8B, 70B | Meta AI | [Meta AI 2024](https://github.com/meta-llama/llama3)                                                                                 |\n| Llama 3.2 | 1B, 3B | Meta AI | [Meta AI 2024](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/)                                           |\n| Llama 3.3 | 70B | Meta AI | [Meta AI 2024](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct)                                                                                 |\n| Mathstral | 7B | Mistral AI | [Mistral AI 2024](https://mistral.ai/news/mathstral/)                                                                                  |\n| MicroLlama | 300M | Ken Wang | [MicroLlama repo](https://github.com/keeeeenw/MicroLlama)                                                                             |\n| Mixtral MoE | 8x7B | Mistral AI | [Mistral AI 2023](https://mistral.ai/news/mixtral-of-experts/)                                                                     |\n| Mistral | 7B, 123B | Mistral AI | [Mistral AI 2023](https://mistral.ai/news/announcing-mistral-7b/)                                                                  |\n| Mixtral MoE | 8x22B | Mistral AI | [Mistral AI 2024](https://mistral.ai/news/mixtral-8x22b/)                                                                         |\n| OLMo | 1B, 7B | Allen Institute for AI (AI2) | [Groeneveld et al. 2024](https://aclanthology.org/2024.acl-long.841/)    |\n| OpenLLaMA | 3B, 7B, 13B | OpenLM Research | [Geng & Liu 2023](https://github.com/openlm-research/open_llama)                                                         |\n| Phi 1.5 & 2 | 1.3B, 2.7B | Microsoft Research  | [Li et al. 2023](https://arxiv.org/abs/2309.05463)                                                                  |\n| Phi 3 | 3.8B | Microsoft Research | [Abdin et al. 2024](https://arxiv.org/abs/2404.14219)                                                                            |\n| Phi 4 | 14B | Microsoft Research | [Abdin et al. 2024](https://arxiv.org/abs/2412.08905)                                                                            |\n| Phi 4 Mini Instruct | 3.8B | Microsoft Research | [Microsoft 2025](https://arxiv.org/abs/2503.01743)                                           |\n| Phi 4 Mini Reasoning | 3.8B | Microsoft Research | [Xu, Peng et al. 2025](https://arxiv.org/abs/2504.21233)                                           |\n| Phi 4 Reasoning | 3.8B | Microsoft Research | [Abdin et al. 2025](https://arxiv.org/abs/2504.21318)                                           |\n| Phi 4 Reasoning Plus | 3.8B | Microsoft Research | [Abdin et al. 2025](https://arxiv.org/abs/2504.21318)                                           |\n| Platypus | 7B, 13B, 70B |  Lee et al. | [Lee, Hunter, and Ruiz 2023](https://arxiv.org/abs/2308.07317)                                                               |\n| Pythia | {14,31,70,160,410}M, {1,1.4,2.8,6.9,12}B | EleutherAI | [Biderman et al. 2023](https://arxiv.org/abs/2304.01373)                                            |\n| Qwen2.5 | 0.5B, 1.5B, 3B, 7B, 14B, 32B, 72B | Alibaba Group | [Qwen Team 2024](https://qwenlm.github.io/blog/qwen2.5/)                                               |\n| Qwen2.5 Coder | 0.5B, 1.5B, 3B, 7B, 14B, 32B | Alibaba Group | [Hui, Binyuan et al. 2024](https://arxiv.org/abs/2409.12186)                                          |\n| Qwen2.5 1M (Long Context) | 7B, 14B | Alibaba Group | [Qwen Team 2025](https://qwenlm.github.io/blog/qwen2.5-1m/)                                          |\n| Qwen2.5 Math | 1.5B, 7B, 72B | Alibaba Group | [An, Yang et al. 2024](https://arxiv.org/abs/2409.12122)                                          |\n| QwQ | 32B | Alibaba Group | [Qwen Team 2025](https://qwenlm.github.io/blog/qwq-32b/)                                                                         |\n| QwQ-Preview | 32B | Alibaba Group | [Qwen Team 2024](https://qwenlm.github.io/blog/qwq-32b-preview/)                                                                         |\n| Qwen3 | 0.6B, 1.7B, 4B, 8B, 14B, 32B | Alibaba Group | [Qwen Team 2025](https://arxiv.org/abs/2505.09388/)                                                                         |\n| Qwen3 MoE | 30B, 235B | Alibaba Group | [Qwen Team 2025](https://arxiv.org/abs/2505.09388/)                                                                         |\n| R1 Distill Llama | 8B, 70B | DeepSeek AI | [DeepSeek AI 2025](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf)                                                                                 |\n| SmolLM2 | 135M, 360M, 1.7B | Hugging Face | [Hugging Face 2024](https://github.com/huggingface/smollm)                                                               |\n| Salamandra | 2B, 7B | Barcelona Supercomputing Centre | [BSC-LTC 2024](https://github.com/BSC-LTC/salamandra)                                                                         |\n| StableCode | 3B | Stability AI | [Stability AI 2023](https://stability.ai/blog/stablecode-llm-generative-ai-coding)                                                  |\n| StableLM  | 3B, 7B | Stability AI | [Stability AI 2023](https://github.com/Stability-AI/StableLM)                                                                    |\n| StableLM Zephyr | 3B | Stability AI | [Stability AI 2023](https://stability.ai/blog/stablecode-llm-generative-ai-coding)                                             |\n| TinyLlama | 1.1B | Zhang et al. | [Zhang et al. 2023](https://github.com/jzhang38/TinyLlama)                                                                         |\n\n\n**Tip**: You can list all available models by running the `litgpt download list` command.\n\n\n</details>\n\n&nbsp;\n\n---\n\n# Workflows\n\n<p align=\"center\">\n  <a href=\"#finetune-an-llm\">Finetune</a> \u2022\n  <a href=\"#pretrain-an-llm\">Pretrain</a> \u2022\n  <a href=\"#continue-pretraining-an-llm\">Continued pretraining</a> \u2022\n    <a href=\"#evaluate-an-llm\">Evaluate</a> \u2022\n    <a href=\"#deploy-an-llm\">Deploy</a> \u2022\n    <a href=\"#test-an-llm\">Test</a>\n</p>\n\n&nbsp;\n\nUse the command line interface to run advanced workflows such as pretraining or finetuning on your own data.\n\n\n## All workflows\nAfter installing LitGPT, select the model and workflow to run (finetune, pretrain, evaluate, deploy, etc...):\n\n```bash\n# litgpt [action] [model]\nlitgpt  serve     meta-llama/Llama-3.2-3B-Instruct\nlitgpt  finetune  meta-llama/Llama-3.2-3B-Instruct\nlitgpt  pretrain  meta-llama/Llama-3.2-3B-Instruct\nlitgpt  chat      meta-llama/Llama-3.2-3B-Instruct\nlitgpt  evaluate  meta-llama/Llama-3.2-3B-Instruct\n```\n\n&nbsp;\n\n----\n\n## Finetune an LLM\n\n<div align=\"center\">\n<a target=\"_blank\" href=\"https://lightning.ai/lightning-ai/studios/litgpt-finetune\">\n  <img src=\"https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/run-on-studio.svg\" height=\"36px\" alt=\"Run on Studios\"/>\n</a>\n</div>\n\n&nbsp;\n\nFinetuning is the process of taking a pretrained AI model and further training it on a smaller, specialized dataset tailored to a specific task or application.\n\n\n&nbsp;\n\n```bash\n# 0) setup your dataset\ncurl -L https://huggingface.co/datasets/ksaw008/finance_alpaca/resolve/main/finance_alpaca.json -o my_custom_dataset.json\n\n# 1) Finetune a model (auto downloads weights)\nlitgpt finetune microsoft/phi-2 \\\n  --data JSON \\\n  --data.json_path my_custom_dataset.json \\\n  --data.val_split_fraction 0.1 \\\n  --out_dir out/custom-model\n\n# 2) Test the model\nlitgpt chat out/custom-model/final\n\n# 3) Deploy the model\nlitgpt serve out/custom-model/final\n```\n\n[Read the full finetuning docs](tutorials/finetune.md)\n\n&nbsp;\n\n----\n\n## Deploy an LLM\n\n<div align=\"center\">\n<a target=\"_blank\" href=\"https://lightning.ai/lightning-ai/studios/litgpt-serve\">\n  <img src=\"https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/deploy-on-studios.svg\" height=\"36px\" alt=\"Deploy on Studios\"/>\n</a>\n</div>\n\n&nbsp;\n\nDeploy a pretrained or finetune LLM to use it in real-world applications. Deploy, automatically sets up a web server that can be accessed by a website or app.\n\n```bash\n# deploy an out-of-the-box LLM\nlitgpt serve microsoft/phi-2\n\n# deploy your own trained model\nlitgpt serve path/to/microsoft/phi-2/checkpoint\n```\n\n<details>\n  <summary>Show code to query server:</summary>\n\n&nbsp;\n\nTest the server in a separate terminal and integrate the model API into your AI product:\n```python\n# 3) Use the server (in a separate Python session)\nimport requests, json\nresponse = requests.post(\n    \"http://127.0.0.1:8000/predict\",\n    json={\"prompt\": \"Fix typos in the following sentence: Example input\"}\n)\nprint(response.json()[\"output\"])\n```\n</details>\n\n[Read the full deploy docs](tutorials/deploy.md).\n\n&nbsp;\n\n----\n\n## Evaluate an LLM\nEvaluate an LLM to test its performance on various tasks to see how well it understands and generates text. Simply put, we can evaluate things like how well would it do in college-level chemistry, coding, etc... (MMLU, Truthful QA, etc...)\n\n```bash\nlitgpt evaluate microsoft/phi-2 --tasks 'truthfulqa_mc2,mmlu'\n```\n\n[Read the full evaluation docs](tutorials/evaluation.md).\n\n&nbsp;\n\n----\n\n##  Test an LLM\n\n<div align=\"center\">\n<a target=\"_blank\" href=\"https://lightning.ai/lightning-ai/studios/litgpt-chat\">\n  <img src=\"https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/run-on-studio.svg\" height=\"36px\" alt=\"Run on Studios\"/>\n</a>\n</div>\n\n&nbsp;\n\nTest how well the model works via an interactive chat. Use the `chat` command to chat, extract embeddings, etc...\n\nHere's an example showing how to use the Phi-2 LLM:\n```bash\nlitgpt chat microsoft/phi-2\n\n>> Prompt: What do Llamas eat?\n```\n\n<details>\n  <summary>Full code:</summary>\n\n&nbsp;\n\n```bash\n# 1) List all supported LLMs\nlitgpt download list\n\n# 2) Use a model (auto downloads weights)\nlitgpt chat microsoft/phi-2\n\n>> Prompt: What do Llamas eat?\n```\n\nThe download of certain models requires an additional access token. You can read more about this in the [download](tutorials/download_model_weights.md#specific-models-and-access-tokens) documentation.\n\n</details>\n\n[Read the full chat docs](tutorials/inference.md).\n\n&nbsp;\n\n----\n\n## Pretrain an LLM\n\n<div align=\"center\">\n<a target=\"_blank\" href=\"https://lightning.ai/lightning-ai/studios/litgpt-pretrain\">\n  <img src=\"https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/run-on-studio.svg\" height=\"36px\" alt=\"Run on Studios\"/>\n</a>\n</div>\n\n&nbsp;\n\nPretraining is the process of teaching an AI model by exposing it to a large amount of data before it is fine-tuned for specific tasks.\n\n<details>\n  <summary>Show code:</summary>\n\n&nbsp;\n\n```bash\nmkdir -p custom_texts\ncurl https://www.gutenberg.org/cache/epub/24440/pg24440.txt --output custom_texts/book1.txt\ncurl https://www.gutenberg.org/cache/epub/26393/pg26393.txt --output custom_texts/book2.txt\n\n# 1) Download a tokenizer\nlitgpt download EleutherAI/pythia-160m \\\n  --tokenizer_only True\n\n# 2) Pretrain the model\nlitgpt pretrain EleutherAI/pythia-160m \\\n  --tokenizer_dir EleutherAI/pythia-160m \\\n  --data TextFiles \\\n  --data.train_data_path \"custom_texts/\" \\\n  --train.max_tokens 10_000_000 \\\n  --out_dir out/custom-model\n\n# 3) Test the model\nlitgpt chat out/custom-model/final\n```\n</details>\n\n[Read the full pretraining docs](tutorials/pretrain.md)\n\n&nbsp;\n\n----\n\n## Continue pretraining an LLM\n\n<div align=\"center\">\n<a target=\"_blank\" href=\"https://lightning.ai/lightning-ai/studios/litgpt-continue-pretraining\">\n  <img src=\"https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/run-on-studio.svg\" height=\"36px\" alt=\"Run on Studios\"/>\n</a>\n</div>\n\n&nbsp;\n\nContinued pretraining is another way of finetuning that specializes an already pretrained model by training on custom data:\n\n<details>\n  <summary>Show code:</summary>\n\n&nbsp;\n\n```bash\nmkdir -p custom_texts\ncurl https://www.gutenberg.org/cache/epub/24440/pg24440.txt --output custom_texts/book1.txt\ncurl https://www.gutenberg.org/cache/epub/26393/pg26393.txt --output custom_texts/book2.txt\n\n# 1) Continue pretraining a model (auto downloads weights)\nlitgpt pretrain EleutherAI/pythia-160m \\\n  --tokenizer_dir EleutherAI/pythia-160m \\\n  --initial_checkpoint_dir EleutherAI/pythia-160m \\\n  --data TextFiles \\\n  --data.train_data_path \"custom_texts/\" \\\n  --train.max_tokens 10_000_000 \\\n  --out_dir out/custom-model\n\n# 2) Test the model\nlitgpt chat out/custom-model/final\n```\n\n</details>\n\n[Read the full continued pretraining docs](tutorials/pretrain.md#continued-pretraining-on-custom-data)\n\n&nbsp;\n\n----\n\n# State-of-the-art features\n\n\u2705 State-of-the-art optimizations: Flash Attention v2, multi-GPU support via fully-sharded data parallelism, [optional CPU offloading](tutorials/oom.md#do-sharding-across-multiple-gpus), and [TPU and XLA support](extensions/xla).</br>\n\u2705 [Pretrain](tutorials/pretrain.md), [finetune](tutorials/finetune.md), and [deploy](tutorials/inference.md)</br>\n\u2705 Reduce compute requirements with low-precision settings: FP16, BF16, and FP16/FP32 mixed.</br>\n\u2705 Lower memory requirements with [quantization](tutorials/quantize.md): 4-bit floats, 8-bit integers, and double quantization.</br>\n\u2705 [Configuration files](config_hub) for great out-of-the-box performance.</br>\n\u2705 Parameter-efficient finetuning: [LoRA](tutorials/finetune_lora.md), [QLoRA](tutorials/finetune_lora.md), [Adapter](tutorials/finetune_adapter.md), and [Adapter v2](tutorials/finetune_adapter.md).</br>\n\u2705 [Exporting](tutorials/convert_lit_models.md) to other popular model weight formats.</br>\n\u2705 Many popular datasets for [pretraining](tutorials/pretrain.md) and [finetuning](tutorials/prepare_dataset.md), and [support for custom datasets](tutorials/prepare_dataset.md#preparing-custom-datasets-for-instruction-finetuning).</br>\n\u2705 Readable and easy-to-modify code to experiment with the latest research ideas.</br>\n\n&nbsp;\n\n---\n\n# Training recipes\n\nLitGPT comes with validated recipes (YAML configs) to train models under different conditions.  We've generated these recipes based on the parameters we found to perform the best for different training conditions.\n\nBrowse all training recipes [here](config_hub).\n\n### Example\n\n```bash\nlitgpt finetune \\\n  --config https://raw.githubusercontent.com/Lightning-AI/litgpt/main/config_hub/finetune/llama-2-7b/lora.yaml\n```\n<details>\n  <summary>\u2705 Use configs to customize training</summary>\n\nConfigs let you customize training for all granular parameters like:\n\n```yaml\n# The path to the base model's checkpoint directory to load for finetuning. (type: <class 'Path'>, default: checkpoints/stabilityai/stablelm-base-alpha-3b)\ncheckpoint_dir: checkpoints/meta-llama/Llama-2-7b-hf\n\n# Directory in which to save checkpoints and logs. (type: <class 'Path'>, default: out/lora)\nout_dir: out/finetune/qlora-llama2-7b\n\n# The precision to use for finetuning. Possible choices: \"bf16-true\", \"bf16-mixed\", \"32-true\". (type: Optional[str], default: null)\nprecision: bf16-true\n\n...\n```\n</details>\n\n<details>\n  <summary>\u2705 Example: LoRA finetuning config</summary>\n\n&nbsp;\n\n```yaml\n# The path to the base model's checkpoint directory to load for finetuning. (type: <class 'Path'>, default: checkpoints/stabilityai/stablelm-base-alpha-3b)\ncheckpoint_dir: checkpoints/meta-llama/Llama-2-7b-hf\n\n# Directory in which to save checkpoints and logs. (type: <class 'Path'>, default: out/lora)\nout_dir: out/finetune/qlora-llama2-7b\n\n# The precision to use for finetuning. Possible choices: \"bf16-true\", \"bf16-mixed\", \"32-true\". (type: Optional[str], default: null)\nprecision: bf16-true\n\n# If set, quantize the model with this algorithm. See ``tutorials/quantize.md`` for more information. (type: Optional[Literal['nf4', 'nf4-dq', 'fp4', 'fp4-dq', 'int8-training']], default: null)\nquantize: bnb.nf4\n\n# How many devices/GPUs to use. (type: Union[int, str], default: 1)\ndevices: 1\n\n# How many nodes to use. (type: int, default: 1)\nnum_nodes: 1\n\n# The LoRA rank. (type: int, default: 8)\nlora_r: 32\n\n# The LoRA alpha. (type: int, default: 16)\nlora_alpha: 16\n\n# The LoRA dropout value. (type: float, default: 0.05)\nlora_dropout: 0.05\n\n# Whether to apply LoRA to the query weights in attention. (type: bool, default: True)\nlora_query: true\n\n# Whether to apply LoRA to the key weights in attention. (type: bool, default: False)\nlora_key: false\n\n# Whether to apply LoRA to the value weights in attention. (type: bool, default: True)\nlora_value: true\n\n# Whether to apply LoRA to the output projection in the attention block. (type: bool, default: False)\nlora_projection: false\n\n# Whether to apply LoRA to the weights of the MLP in the attention block. (type: bool, default: False)\nlora_mlp: false\n\n# Whether to apply LoRA to output head in GPT. (type: bool, default: False)\nlora_head: false\n\n# Data-related arguments. If not provided, the default is ``litgpt.data.Alpaca``.\ndata:\n  class_path: litgpt.data.Alpaca2k\n  init_args:\n    mask_prompt: false\n    val_split_fraction: 0.05\n    prompt_style: alpaca\n    ignore_index: -100\n    seed: 42\n    num_workers: 4\n    download_dir: data/alpaca2k\n\n# Training-related arguments. See ``litgpt.args.TrainArgs`` for details\ntrain:\n\n  # Number of optimizer steps between saving checkpoints (type: Optional[int], default: 1000)\n  save_interval: 200\n\n  # Number of iterations between logging calls (type: int, default: 1)\n  log_interval: 1\n\n  # Number of samples between optimizer steps across data-parallel ranks (type: int, default: 128)\n  global_batch_size: 8\n\n  # Number of samples per data-parallel rank (type: int, default: 4)\n  micro_batch_size: 2\n\n  # Number of iterations with learning rate warmup active (type: int, default: 100)\n  lr_warmup_steps: 10\n\n  # Number of epochs to train on (type: Optional[int], default: 5)\n  epochs: 4\n\n  # Total number of tokens to train on (type: Optional[int], default: null)\n  max_tokens:\n\n  # Limits the number of optimizer steps to run (type: Optional[int], default: null)\n  max_steps:\n\n  # Limits the length of samples (type: Optional[int], default: null)\n  max_seq_length: 512\n\n  # Whether to tie the embedding weights with the language modeling head weights (type: Optional[bool], default: null)\n  tie_embeddings:\n\n  #   (type: float, default: 0.0003)\n  learning_rate: 0.0002\n\n  #   (type: float, default: 0.02)\n  weight_decay: 0.0\n\n  #   (type: float, default: 0.9)\n  beta1: 0.9\n\n  #   (type: float, default: 0.95)\n  beta2: 0.95\n\n  #   (type: Optional[float], default: null)\n  max_norm:\n\n  #   (type: float, default: 6e-05)\n  min_lr: 6.0e-05\n\n# Evaluation-related arguments. See ``litgpt.args.EvalArgs`` for details\neval:\n\n  # Number of optimizer steps between evaluation calls (type: int, default: 100)\n  interval: 100\n\n  # Number of tokens to generate (type: Optional[int], default: 100)\n  max_new_tokens: 100\n\n  # Number of iterations (type: int, default: 100)\n  max_iters: 100\n\n# The name of the logger to send metrics to. (type: Literal['wandb', 'tensorboard', 'csv'], default: csv)\nlogger_name: csv\n\n# The random seed to use for reproducibility. (type: int, default: 1337)\nseed: 1337\n```\n</details>\n\n<details>\n  <summary>\u2705 Override any parameter in the CLI:</summary>\n\n```bash\nlitgpt finetune \\\n  --config https://raw.githubusercontent.com/Lightning-AI/litgpt/main/config_hub/finetune/llama-2-7b/lora.yaml \\\n  --lora_r 4\n```\n</details>\n\n&nbsp;\n\n----\n\n# Project highlights\n\nLitGPT powers many great AI projects, initiatives, challenges and of course enterprises. Please submit a pull request to be considered for a feature.\n\n<details>\n  <summary>\ud83d\udcca SAMBA: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling</summary>\n\nThe [Samba](https://github.com/microsoft/Samba) project by researchers at Microsoft is built on top of the LitGPT code base and combines state space models with sliding window attention, which outperforms pure state space models.\n\n</details>\n\n<details>\n  <summary>\ud83c\udfc6 NeurIPS 2023 Large Language Model Efficiency Challenge: 1 LLM + 1 GPU + 1 Day</summary>\n\nThe LitGPT repository was the official starter kit for the [NeurIPS 2023 LLM Efficiency Challenge](https://llm-efficiency-challenge.github.io), which is a competition focused on finetuning an existing non-instruction tuned LLM for 24 hours on a single GPU.\n\n</details>\n\n<details>\n  <summary>\ud83e\udd99 TinyLlama: An Open-Source Small Language Model</summary>\n\n\nLitGPT powered the [TinyLlama project](https://github.com/jzhang38/TinyLlama) and [TinyLlama: An Open-Source Small Language Model](https://arxiv.org/abs/2401.02385) research paper.\n\n</details>\n\n<details>\n  <summary>\ud83c\udf6a MicroLlama: MicroLlama-300M</summary>\n\n[MicroLlama](https://github.com/keeeeenw/MicroLlama) is a 300M Llama model pretrained on 50B tokens powered by TinyLlama and LitGPT.\n</details>\n\n<details>\n  <summary>\ud83d\udd2c Pre-training Small Base LMs with Fewer Tokens</summary>\n\nThe research paper [\"Pre-training Small Base LMs with Fewer Tokens\"](https://arxiv.org/abs/2404.08634), which utilizes LitGPT, develops smaller base language models by inheriting a few transformer blocks from larger models and training on a tiny fraction of the data used by the larger models. It demonstrates that these smaller models can perform comparably to larger models despite using significantly less training data and resources.\n\n</details>\n\n&nbsp;\n\n----\n\n# Community\n\nWe welcome all individual contributors, regardless of their level of experience or hardware. Your contributions are valuable, and we are excited to see what you can accomplish in this collaborative and supportive environment.\n\n- [Request a feature](https://github.com/Lightning-AI/litgpt/issues)\n- [Submit your first contribution](https://lightning.ai/pages/community/tutorial/how-to-contribute-to-litgpt/)\n- [Join our Discord](https://discord.gg/VptPCZkGNa)\n\n&nbsp;\n\n# Tutorials\n\n\ud83d\ude80 [Get started](tutorials/0_to_litgpt.md)</br>\n\u26a1\ufe0f [Finetuning, incl. LoRA, QLoRA, and Adapters](tutorials/finetune.md)</br>\n\ud83e\udd16 [Pretraining](tutorials/pretrain.md)</br>\n\ud83d\udcac [Model evaluation](tutorials/evaluation.md)</br>\n\ud83d\udcd8 [Supported and custom datasets](tutorials/prepare_dataset.md)</br>\n\ud83e\uddf9 [Quantization](tutorials/quantize.md)</br>\n\ud83e\udd2f [Tips for dealing with out-of-memory (OOM) errors](tutorials/oom.md)</br>\n\ud83e\uddd1\ud83c\udffd\u200d\ud83d\udcbb [Using cloud TPUs](extensions/xla)</br>\n\n&nbsp;\n\n----\n\n### Acknowledgments\n\nThis implementation extends on [Lit-LLaMA](https://github.com/lightning-AI/lit-llama) and [nanoGPT](https://github.com/karpathy/nanoGPT), and it's **powered by [Lightning Fabric](https://lightning.ai/docs/fabric/stable/) \u26a1**.\n\n- [@karpathy](https://github.com/karpathy) for [nanoGPT](https://github.com/karpathy/nanoGPT)\n- [@EleutherAI](https://github.com/EleutherAI) for [GPT-NeoX](https://github.com/EleutherAI/gpt-neox) and the [Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness)\n- [@TimDettmers](https://github.com/TimDettmers) for [bitsandbytes](https://github.com/TimDettmers/bitsandbytes)\n- [@Microsoft](https://github.com/microsoft) for [LoRA](https://github.com/microsoft/LoRA)\n- [@tridao](https://github.com/tridao) for [Flash Attention 2](https://github.com/Dao-AILab/flash-attention)\n\n### License\n\nLitGPT is released under the [Apache 2.0](https://github.com/Lightning-AI/litgpt/blob/main/LICENSE) license.\n\n### Citation\n\nIf you use LitGPT in your research, please cite the following work:\n\n```bibtex\n@misc{litgpt-2023,\n  author       = {Lightning AI},\n  title        = {LitGPT},\n  howpublished = {\\url{https://github.com/Lightning-AI/litgpt}},\n  year         = {2023},\n}\n```\n\n&nbsp;\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 286301447,
    "name": "txtai",
    "full_name": "neuml/txtai",
    "description": "\ud83d\udca1 All-in-one open-source AI framework for semantic search, LLM orchestration and language model workflows",
    "html_url": "https://github.com/neuml/txtai",
    "clone_url": "https://github.com/neuml/txtai.git",
    "owner_login": "neuml",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/59890304?v=4",
    "stargazers_count": 11339,
    "watchers_count": 11339,
    "forks_count": 721,
    "open_issues_count": 8,
    "size": 55758,
    "language": "Python",
    "languages": {
      "Python": 1084627,
      "Dockerfile": 3267,
      "Makefile": 2416
    },
    "topics": [
      "ai",
      "artificial-intelligence",
      "embeddings",
      "information-retrieval",
      "language-model",
      "large-language-models",
      "llm",
      "machine-learning",
      "nlp",
      "python",
      "rag",
      "retrieval-augmented-generation",
      "search",
      "search-engine",
      "semantic-search",
      "sentence-embeddings",
      "transformers",
      "txtai",
      "vector-database",
      "vector-search"
    ],
    "license_name": "Apache License 2.0",
    "created_at": "2020-08-09T19:14:59+00:00",
    "updated_at": "2025-08-05T22:27:10+00:00",
    "pushed_at": "2025-08-05T22:31:50+00:00",
    "contributors_count": 17,
    "readme_length": 21434,
    "readme_content": "<p align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/neuml/txtai/master/logo.png\"/>\n</p>\n\n<p align=\"center\">\n    <b>All-in-one AI framework</b>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://github.com/neuml/txtai/releases\">\n        <img src=\"https://img.shields.io/github/release/neuml/txtai.svg?style=flat&color=success\" alt=\"Version\"/>\n    </a>\n    <a href=\"https://github.com/neuml/txtai\">\n        <img src=\"https://img.shields.io/github/last-commit/neuml/txtai.svg?style=flat&color=blue\" alt=\"GitHub last commit\"/>\n    </a>\n    <a href=\"https://github.com/neuml/txtai/issues\">\n        <img src=\"https://img.shields.io/github/issues/neuml/txtai.svg?style=flat&color=success\" alt=\"GitHub issues\"/>\n    </a>\n    <a href=\"https://join.slack.com/t/txtai/shared_invite/zt-37c1zfijp-Y57wMty6YOx_hyIHEQvQJA\">\n        <img src=\"https://img.shields.io/badge/slack-join-blue?style=flat&logo=slack&logocolor=white\" alt=\"Join Slack\"/>\n    </a>\n    <a href=\"https://github.com/neuml/txtai/actions?query=workflow%3Abuild\">\n        <img src=\"https://github.com/neuml/txtai/workflows/build/badge.svg\" alt=\"Build Status\"/>\n    </a>\n    <a href=\"https://coveralls.io/github/neuml/txtai?branch=master\">\n        <img src=\"https://img.shields.io/coverallsCoverage/github/neuml/txtai\" alt=\"Coverage Status\">\n    </a>\n</p>\n\ntxtai is an all-in-one AI framework for semantic search, LLM orchestration and language model workflows.\n\n![architecture](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/architecture.png#gh-light-mode-only)\n![architecture](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/architecture-dark.png#gh-dark-mode-only)\n\nThe key component of txtai is an embeddings database, which is a union of vector indexes (sparse and dense), graph networks and relational databases.\n\nThis foundation enables vector search and/or serves as a powerful knowledge source for large language model (LLM) applications.\n\nBuild autonomous agents, retrieval augmented generation (RAG) processes, multi-model workflows and more.\n\nSummary of txtai features:\n\n- \ud83d\udd0e Vector search with SQL, object storage, topic modeling, graph analysis and multimodal indexing\n- \ud83d\udcc4 Create embeddings for text, documents, audio, images and video\n- \ud83d\udca1 Pipelines powered by language models that run LLM prompts, question-answering, labeling, transcription, translation, summarization and more\n- \u21aa\ufe0f\ufe0f Workflows to join pipelines together and aggregate business logic. txtai processes can be simple microservices or multi-model workflows.\n- \ud83e\udd16 Agents that intelligently connect embeddings, pipelines, workflows and other agents together to autonomously solve complex problems\n- \u2699\ufe0f Web and Model Context Protocol (MCP) APIs. Bindings available for [JavaScript](https://github.com/neuml/txtai.js), [Java](https://github.com/neuml/txtai.java), [Rust](https://github.com/neuml/txtai.rs) and [Go](https://github.com/neuml/txtai.go).\n- \ud83d\udd0b Batteries included with defaults to get up and running fast\n- \u2601\ufe0f Run local or scale out with container orchestration\n\ntxtai is built with Python 3.10+, [Hugging Face Transformers](https://github.com/huggingface/transformers), [Sentence Transformers](https://github.com/UKPLab/sentence-transformers) and [FastAPI](https://github.com/tiangolo/fastapi). txtai is open-source under an Apache 2.0 license.\n\n*Interested in an easy and secure way to run hosted txtai applications? Then join the [txtai.cloud](https://txtai.cloud) preview to learn more.*\n\n## Why txtai?\n\n![why](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/why.png#gh-light-mode-only)\n![why](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/why-dark.png#gh-dark-mode-only)\n\nNew vector databases, LLM frameworks and everything in between are sprouting up daily. Why build with txtai?\n\n- Up and running in minutes with [pip](https://neuml.github.io/txtai/install/) or [Docker](https://neuml.github.io/txtai/cloud/)\n```python\n# Get started in a couple lines\nimport txtai\n\nembeddings = txtai.Embeddings()\nembeddings.index([\"Correct\", \"Not what we hoped\"])\nembeddings.search(\"positive\", 1)\n#[(0, 0.29862046241760254)]\n```\n- Built-in API makes it easy to develop applications using your programming language of choice\n```yaml\n# app.yml\nembeddings:\n    path: sentence-transformers/all-MiniLM-L6-v2\n```\n```bash\nCONFIG=app.yml uvicorn \"txtai.api:app\"\ncurl -X GET \"http://localhost:8000/search?query=positive\"\n```\n- Run local - no need to ship data off to disparate remote services\n- Work with micromodels all the way up to large language models (LLMs)\n- Low footprint - install additional dependencies and scale up when needed\n- [Learn by example](https://neuml.github.io/txtai/examples) - notebooks cover all available functionality\n\n## Use Cases\n\nThe following sections introduce common txtai use cases. A comprehensive set of over 60 [example notebooks and applications](https://neuml.github.io/txtai/examples) are also available.\n\n### Semantic Search\n\nBuild semantic/similarity/vector/neural search applications.\n\n![demo](https://raw.githubusercontent.com/neuml/txtai/master/demo.gif)\n\nTraditional search systems use keywords to find data. Semantic search has an understanding of natural language and identifies results that have the same meaning, not necessarily the same keywords.\n\n![search](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/search.png#gh-light-mode-only)\n![search](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/search-dark.png#gh-dark-mode-only)\n\nGet started with the following examples.\n\n| Notebook  | Description  |       |\n|:----------|:-------------|------:|\n| [Introducing txtai](https://github.com/neuml/txtai/blob/master/examples/01_Introducing_txtai.ipynb) [\u25b6\ufe0f](https://www.youtube.com/watch?v=SIezMnVdmMs) | Overview of the functionality provided by txtai | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/01_Introducing_txtai.ipynb) |\n| [Similarity search with images](https://github.com/neuml/txtai/blob/master/examples/13_Similarity_search_with_images.ipynb) | Embed images and text into the same space for search | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/13_Similarity_search_with_images.ipynb) |\n| [Build a QA database](https://github.com/neuml/txtai/blob/master/examples/34_Build_a_QA_database.ipynb) | Question matching with semantic search | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/34_Build_a_QA_database.ipynb) |\n| [Semantic Graphs](https://github.com/neuml/txtai/blob/master/examples/38_Introducing_the_Semantic_Graph.ipynb) | Explore topics, data connectivity and run network analysis| [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/38_Introducing_the_Semantic_Graph.ipynb) |\n\n### LLM Orchestration\n\nAutonomous agents, retrieval augmented generation (RAG), chat with your data, pipelines and workflows that interface with large language models (LLMs).\n\n![llm](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/llm.png)\n\nSee below to learn more.\n\n| Notebook  | Description  |       |\n|:----------|:-------------|------:|\n| [Prompt templates and task chains](https://github.com/neuml/txtai/blob/master/examples/44_Prompt_templates_and_task_chains.ipynb) | Build model prompts and connect tasks together with workflows | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/44_Prompt_templates_and_task_chains.ipynb) |\n| [Integrate LLM frameworks](https://github.com/neuml/txtai/blob/master/examples/53_Integrate_LLM_Frameworks.ipynb) | Integrate llama.cpp, LiteLLM and custom generation frameworks | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/53_Integrate_LLM_Frameworks.ipynb) |\n| [Build knowledge graphs with LLMs](https://github.com/neuml/txtai/blob/master/examples/57_Build_knowledge_graphs_with_LLM_driven_entity_extraction.ipynb) | Build knowledge graphs with LLM-driven entity extraction | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/57_Build_knowledge_graphs_with_LLM_driven_entity_extraction.ipynb) |\n| [Parsing the stars with txtai](https://github.com/neuml/txtai/blob/master/examples/72_Parsing_the_stars_with_txtai.ipynb) | Explore an astronomical knowledge graph of known stars, planets, galaxies | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/72_Parsing_the_stars_with_txtai.ipynb) |\n\n#### Agents\n\nAgents connect embeddings, pipelines, workflows and other agents together to autonomously solve complex problems.\n\n![agent](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/agent.png)\n\ntxtai agents are built on top of the [smolagents](https://github.com/huggingface/smolagents) framework. This supports all LLMs txtai supports (Hugging Face, llama.cpp, OpenAI / Claude / AWS Bedrock via LiteLLM).\n\nSee the link below to learn more.\n\n| Notebook  | Description  |       |\n|:----------|:-------------|------:|\n| [Analyzing Hugging Face Posts with Graphs and Agents](https://github.com/neuml/txtai/blob/master/examples/68_Analyzing_Hugging_Face_Posts_with_Graphs_and_Agents.ipynb) | Explore a rich dataset with Graph Analysis and Agents | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/68_Analyzing_Hugging_Face_Posts_with_Graphs_and_Agents.ipynb) |\n| [Granting autonomy to agents](https://github.com/neuml/txtai/blob/master/examples/69_Granting_autonomy_to_agents.ipynb) | Agents that iteratively solve problems as they see fit | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/69_Granting_autonomy_to_agents.ipynb) |\n| [Analyzing LinkedIn Company Posts with Graphs and Agents](https://github.com/neuml/txtai/blob/master/examples/71_Analyzing_LinkedIn_Company_Posts_with_Graphs_and_Agents.ipynb) | Exploring how to improve social media engagement with AI | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/71_Analyzing_LinkedIn_Company_Posts_with_Graphs_and_Agents.ipynb) |\n\n#### Retrieval augmented generation\n\nRetrieval augmented generation (RAG) reduces the risk of LLM hallucinations by constraining the output with a knowledge base as context. RAG is commonly used to \"chat with your data\".\n\n![rag](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/rag.png#gh-light-mode-only)\n![rag](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/rag-dark.png#gh-dark-mode-only)\n\nA novel feature of txtai is that it can provide both an answer and source citation.\n\n| Notebook  | Description  |       |\n|:----------|:-------------|------:|\n| [Build RAG pipelines with txtai](https://github.com/neuml/txtai/blob/master/examples/52_Build_RAG_pipelines_with_txtai.ipynb) | Guide on retrieval augmented generation including how to create citations | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/52_Build_RAG_pipelines_with_txtai.ipynb) |\n| [Chunking your data for RAG](https://github.com/neuml/txtai/blob/master/examples/73_Chunking_your_data_for_RAG.ipynb) | Extract, chunk and index content for effective retrieval | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/73_Chunking_your_data_for_RAG.ipynb) |\n| [Advanced RAG with graph path traversal](https://github.com/neuml/txtai/blob/master/examples/58_Advanced_RAG_with_graph_path_traversal.ipynb) | Graph path traversal to collect complex sets of data for advanced RAG | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/58_Advanced_RAG_with_graph_path_traversal.ipynb) |\n| [Speech to Speech RAG](https://github.com/neuml/txtai/blob/master/examples/65_Speech_to_Speech_RAG.ipynb) [\u25b6\ufe0f](https://www.youtube.com/watch?v=tH8QWwkVMKA) | Full cycle speech to speech workflow with RAG | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/65_Speech_to_Speech_RAG.ipynb) |\n\n### Language Model Workflows\n\nLanguage model workflows, also known as semantic workflows, connect language models together to build intelligent applications.\n\n![flows](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/flows.png#gh-light-mode-only)\n![flows](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/flows-dark.png#gh-dark-mode-only)\n\nWhile LLMs are powerful, there are plenty of smaller, more specialized models that work better and faster for specific tasks. This includes models for extractive question-answering, automatic summarization, text-to-speech, transcription and translation.\n\n| Notebook  | Description  |       |\n|:----------|:-------------|------:|\n| [Run pipeline workflows](https://github.com/neuml/txtai/blob/master/examples/14_Run_pipeline_workflows.ipynb) [\u25b6\ufe0f](https://www.youtube.com/watch?v=UBMPDCn1gEU) | Simple yet powerful constructs to efficiently process data | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/14_Run_pipeline_workflows.ipynb) |\n| [Building abstractive text summaries](https://github.com/neuml/txtai/blob/master/examples/09_Building_abstractive_text_summaries.ipynb) | Run abstractive text summarization | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/09_Building_abstractive_text_summaries.ipynb) |\n| [Transcribe audio to text](https://github.com/neuml/txtai/blob/master/examples/11_Transcribe_audio_to_text.ipynb) | Convert audio files to text | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/11_Transcribe_audio_to_text.ipynb) |\n| [Translate text between languages](https://github.com/neuml/txtai/blob/master/examples/12_Translate_text_between_languages.ipynb) | Streamline machine translation and language detection | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuml/txtai/blob/master/examples/12_Translate_text_between_languages.ipynb) |\n\n## Installation\n\n![install](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/install.png#gh-light-mode-only)\n![install](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/install-dark.png#gh-dark-mode-only)\n\nThe easiest way to install is via pip and PyPI\n\n```\npip install txtai\n```\n\nPython 3.10+ is supported. Using a Python [virtual environment](https://docs.python.org/3/library/venv.html) is recommended.\n\nSee the detailed [install instructions](https://neuml.github.io/txtai/install) for more information covering [optional dependencies](https://neuml.github.io/txtai/install/#optional-dependencies), [environment specific prerequisites](https://neuml.github.io/txtai/install/#environment-specific-prerequisites), [installing from source](https://neuml.github.io/txtai/install/#install-from-source), [conda support](https://neuml.github.io/txtai/install/#conda) and how to [run with containers](https://neuml.github.io/txtai/cloud).\n\n## Model guide\n\n![models](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/models.png)\n\nSee the table below for the current recommended models. These models all allow commercial use and offer a blend of speed and performance.\n\n| Component                                                                     | Model(s)                                                                 |\n| ----------------------------------------------------------------------------- | ------------------------------------------------------------------------ |\n| [Embeddings](https://neuml.github.io/txtai/embeddings)                        | [all-MiniLM-L6-v2](https://hf.co/sentence-transformers/all-MiniLM-L6-v2) | \n| [Image Captions](https://neuml.github.io/txtai/pipeline/image/caption)        | [BLIP](https://hf.co/Salesforce/blip-image-captioning-base)              |\n| [Labels - Zero Shot](https://neuml.github.io/txtai/pipeline/text/labels)      | [BART-Large-MNLI](https://hf.co/facebook/bart-large)                     |\n| [Labels - Fixed](https://neuml.github.io/txtai/pipeline/text/labels)          | Fine-tune with [training pipeline](https://neuml.github.io/txtai/pipeline/train/trainer)          |\n| [Large Language Model (LLM)](https://neuml.github.io/txtai/pipeline/text/llm) | [Llama 3.1 Instruct](https://hf.co/meta-llama/Llama-3.1-8B-Instruct)     |\n| [Summarization](https://neuml.github.io/txtai/pipeline/text/summary)          | [DistilBART](https://hf.co/sshleifer/distilbart-cnn-12-6)                |\n| [Text-to-Speech](https://neuml.github.io/txtai/pipeline/audio/texttospeech)   | [ESPnet JETS](https://hf.co/NeuML/ljspeech-jets-onnx)                    |\n| [Transcription](https://neuml.github.io/txtai/pipeline/audio/transcription)   | [Whisper](https://hf.co/openai/whisper-base)                             | \n| [Translation](https://neuml.github.io/txtai/pipeline/text/translation)        | [OPUS Model Series](https://hf.co/Helsinki-NLP)                          |\n\nModels can be loaded as either a path from the Hugging Face Hub or a local directory. Model paths are optional, defaults are loaded when not specified. For tasks with no recommended model, txtai uses the default models as shown in the Hugging Face Tasks guide.\n\nSee the following links to learn more.\n\n- [Hugging Face Tasks](https://hf.co/tasks)\n- [Hugging Face Model Hub](https://hf.co/models)\n- [MTEB Leaderboard](https://hf.co/spaces/mteb/leaderboard)\n- [LMSYS LLM Leaderboard](https://chat.lmsys.org/?leaderboard)\n- [Open LLM Leaderboard](https://hf.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n\n## Powered by txtai\n\nThe following applications are powered by txtai.\n\n![apps](https://raw.githubusercontent.com/neuml/txtai/master/apps.jpg)\n\n| Application  | Description  |\n|:------------ |:-------------|\n| [rag](https://github.com/neuml/rag) | Retrieval Augmented Generation (RAG) application |\n| [ragdata](https://github.com/neuml/ragdata) | Build knowledge bases for RAG |\n| [paperai](https://github.com/neuml/paperai) | AI for medical and scientific papers |\n| [annotateai](https://github.com/neuml/annotateai) | Automatically annotate papers with LLMs |\n\nIn addition to this list, there are also many other [open-source projects](https://github.com/neuml/txtai/network/dependents), [published research](https://scholar.google.com/scholar?q=txtai&hl=en&as_ylo=2022) and closed proprietary/commercial projects that have built on txtai in production.\n\n## Further Reading\n\n![further](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/further.png#gh-light-mode-only)\n![further](https://raw.githubusercontent.com/neuml/txtai/master/docs/images/further-ghdark.png#gh-dark-mode-only)\n\n- [Introducing txtai, the all-in-one AI framework](https://medium.com/neuml/introducing-txtai-the-all-in-one-ai-framework-0660ecfc39d7)\n- [Tutorial series on Hashnode](https://neuml.hashnode.dev/series/txtai-tutorial) | [dev.to](https://dev.to/neuml/tutorial-series-on-txtai-ibg)\n- [What's new in txtai 8.0](https://medium.com/neuml/whats-new-in-txtai-8-0-2d7d0ab4506b) | [7.0](https://medium.com/neuml/whats-new-in-txtai-7-0-855ad6a55440) | [6.0](https://medium.com/neuml/whats-new-in-txtai-6-0-7d93eeedf804) | [5.0](https://medium.com/neuml/whats-new-in-txtai-5-0-e5c75a13b101) | [4.0](https://medium.com/neuml/whats-new-in-txtai-4-0-bbc3a65c3d1c)\n- [Getting started with semantic search](https://medium.com/neuml/getting-started-with-semantic-search-a9fd9d8a48cf) | [workflows](https://medium.com/neuml/getting-started-with-semantic-workflows-2fefda6165d9) | [rag](https://medium.com/neuml/getting-started-with-rag-9a0cca75f748)\n- [Running txtai at scale](https://medium.com/neuml/running-at-scale-with-txtai-71196cdd99f9)\n- [Vector search & RAG Landscape: A review with txtai](https://medium.com/neuml/vector-search-rag-landscape-a-review-with-txtai-a7f37ad0e187)\n\n## Documentation\n\n[Full documentation on txtai](https://neuml.github.io/txtai) including configuration settings for embeddings, pipelines, workflows, API and a FAQ with common questions/issues is available.\n\n## Contributing\n\nFor those who would like to contribute to txtai, please see [this guide](https://github.com/neuml/.github/blob/master/CONTRIBUTING.md).\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 527591471,
    "name": "stable-diffusion-webui",
    "full_name": "AUTOMATIC1111/stable-diffusion-webui",
    "description": "Stable Diffusion web UI",
    "html_url": "https://github.com/AUTOMATIC1111/stable-diffusion-webui",
    "clone_url": "https://github.com/AUTOMATIC1111/stable-diffusion-webui.git",
    "owner_login": "AUTOMATIC1111",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/20920490?v=4",
    "stargazers_count": 155254,
    "watchers_count": 155254,
    "forks_count": 28807,
    "open_issues_count": 2420,
    "size": 36427,
    "language": "Python",
    "languages": {
      "Python": 1840622,
      "JavaScript": 175861,
      "CSS": 44551,
      "HTML": 27503,
      "Shell": 13304,
      "Batchfile": 2631
    },
    "topics": [
      "ai",
      "ai-art",
      "deep-learning",
      "diffusion",
      "gradio",
      "image-generation",
      "image2image",
      "img2img",
      "pytorch",
      "stable-diffusion",
      "text2image",
      "torch",
      "txt2img",
      "unstable",
      "upscaling",
      "web"
    ],
    "license_name": "GNU Affero General Public License v3.0",
    "created_at": "2022-08-22T14:05:26+00:00",
    "updated_at": "2025-08-06T02:07:47+00:00",
    "pushed_at": "2025-05-03T06:17:03+00:00",
    "contributors_count": 100,
    "readme_length": 12924,
    "readme_content": "# Stable Diffusion web UI\r\nA web interface for Stable Diffusion, implemented using Gradio library.\r\n\r\n![](screenshot.png)\r\n\r\n## Features\r\n[Detailed feature showcase with images](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features):\r\n- Original txt2img and img2img modes\r\n- One click install and run script (but you still must install python and git)\r\n- Outpainting\r\n- Inpainting\r\n- Color Sketch\r\n- Prompt Matrix\r\n- Stable Diffusion Upscale\r\n- Attention, specify parts of text that the model should pay more attention to\r\n    - a man in a `((tuxedo))` - will pay more attention to tuxedo\r\n    - a man in a `(tuxedo:1.21)` - alternative syntax\r\n    - select text and press `Ctrl+Up` or `Ctrl+Down` (or `Command+Up` or `Command+Down` if you're on a MacOS) to automatically adjust attention to selected text (code contributed by anonymous user)\r\n- Loopback, run img2img processing multiple times\r\n- X/Y/Z plot, a way to draw a 3 dimensional plot of images with different parameters\r\n- Textual Inversion\r\n    - have as many embeddings as you want and use any names you like for them\r\n    - use multiple embeddings with different numbers of vectors per token\r\n    - works with half precision floating point numbers\r\n    - train embeddings on 8GB (also reports of 6GB working)\r\n- Extras tab with:\r\n    - GFPGAN, neural network that fixes faces\r\n    - CodeFormer, face restoration tool as an alternative to GFPGAN\r\n    - RealESRGAN, neural network upscaler\r\n    - ESRGAN, neural network upscaler with a lot of third party models\r\n    - SwinIR and Swin2SR ([see here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/2092)), neural network upscalers\r\n    - LDSR, Latent diffusion super resolution upscaling\r\n- Resizing aspect ratio options\r\n- Sampling method selection\r\n    - Adjust sampler eta values (noise multiplier)\r\n    - More advanced noise setting options\r\n- Interrupt processing at any time\r\n- 4GB video card support (also reports of 2GB working)\r\n- Correct seeds for batches\r\n- Live prompt token length validation\r\n- Generation parameters\r\n     - parameters you used to generate images are saved with that image\r\n     - in PNG chunks for PNG, in EXIF for JPEG\r\n     - can drag the image to PNG info tab to restore generation parameters and automatically copy them into UI\r\n     - can be disabled in settings\r\n     - drag and drop an image/text-parameters to promptbox\r\n- Read Generation Parameters Button, loads parameters in promptbox to UI\r\n- Settings page\r\n- Running arbitrary python code from UI (must run with `--allow-code` to enable)\r\n- Mouseover hints for most UI elements\r\n- Possible to change defaults/mix/max/step values for UI elements via text config\r\n- Tiling support, a checkbox to create images that can be tiled like textures\r\n- Progress bar and live image generation preview\r\n    - Can use a separate neural network to produce previews with almost none VRAM or compute requirement\r\n- Negative prompt, an extra text field that allows you to list what you don't want to see in generated image\r\n- Styles, a way to save part of prompt and easily apply them via dropdown later\r\n- Variations, a way to generate same image but with tiny differences\r\n- Seed resizing, a way to generate same image but at slightly different resolution\r\n- CLIP interrogator, a button that tries to guess prompt from an image\r\n- Prompt Editing, a way to change prompt mid-generation, say to start making a watermelon and switch to anime girl midway\r\n- Batch Processing, process a group of files using img2img\r\n- Img2img Alternative, reverse Euler method of cross attention control\r\n- Highres Fix, a convenience option to produce high resolution pictures in one click without usual distortions\r\n- Reloading checkpoints on the fly\r\n- Checkpoint Merger, a tab that allows you to merge up to 3 checkpoints into one\r\n- [Custom scripts](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Custom-Scripts) with many extensions from community\r\n- [Composable-Diffusion](https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/), a way to use multiple prompts at once\r\n     - separate prompts using uppercase `AND`\r\n     - also supports weights for prompts: `a cat :1.2 AND a dog AND a penguin :2.2`\r\n- No token limit for prompts (original stable diffusion lets you use up to 75 tokens)\r\n- DeepDanbooru integration, creates danbooru style tags for anime prompts\r\n- [xformers](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Xformers), major speed increase for select cards: (add `--xformers` to commandline args)\r\n- via extension: [History tab](https://github.com/yfszzx/stable-diffusion-webui-images-browser): view, direct and delete images conveniently within the UI\r\n- Generate forever option\r\n- Training tab\r\n     - hypernetworks and embeddings options\r\n     - Preprocessing images: cropping, mirroring, autotagging using BLIP or deepdanbooru (for anime)\r\n- Clip skip\r\n- Hypernetworks\r\n- Loras (same as Hypernetworks but more pretty)\r\n- A separate UI where you can choose, with preview, which embeddings, hypernetworks or Loras to add to your prompt\r\n- Can select to load a different VAE from settings screen\r\n- Estimated completion time in progress bar\r\n- API\r\n- Support for dedicated [inpainting model](https://github.com/runwayml/stable-diffusion#inpainting-with-stable-diffusion) by RunwayML\r\n- via extension: [Aesthetic Gradients](https://github.com/AUTOMATIC1111/stable-diffusion-webui-aesthetic-gradients), a way to generate images with a specific aesthetic by using clip images embeds (implementation of [https://github.com/vicgalle/stable-diffusion-aesthetic-gradients](https://github.com/vicgalle/stable-diffusion-aesthetic-gradients))\r\n- [Stable Diffusion 2.0](https://github.com/Stability-AI/stablediffusion) support - see [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#stable-diffusion-20) for instructions\r\n- [Alt-Diffusion](https://arxiv.org/abs/2211.06679) support - see [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#alt-diffusion) for instructions\r\n- Now without any bad letters!\r\n- Load checkpoints in safetensors format\r\n- Eased resolution restriction: generated image's dimensions must be a multiple of 8 rather than 64\r\n- Now with a license!\r\n- Reorder elements in the UI from settings screen\r\n- [Segmind Stable Diffusion](https://huggingface.co/segmind/SSD-1B) support\r\n\r\n## Installation and Running\r\nMake sure the required [dependencies](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Dependencies) are met and follow the instructions available for:\r\n- [NVidia](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-NVidia-GPUs) (recommended)\r\n- [AMD](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-AMD-GPUs) GPUs.\r\n- [Intel CPUs, Intel GPUs (both integrated and discrete)](https://github.com/openvinotoolkit/stable-diffusion-webui/wiki/Installation-on-Intel-Silicon) (external wiki page)\r\n- [Ascend NPUs](https://github.com/wangshuai09/stable-diffusion-webui/wiki/Install-and-run-on-Ascend-NPUs) (external wiki page)\r\n\r\nAlternatively, use online services (like Google Colab):\r\n\r\n- [List of Online Services](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Online-Services)\r\n\r\n### Installation on Windows 10/11 with NVidia-GPUs using release package\r\n1. Download `sd.webui.zip` from [v1.0.0-pre](https://github.com/AUTOMATIC1111/stable-diffusion-webui/releases/tag/v1.0.0-pre) and extract its contents.\r\n2. Run `update.bat`.\r\n3. Run `run.bat`.\r\n> For more details see [Install-and-Run-on-NVidia-GPUs](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-NVidia-GPUs)\r\n\r\n### Automatic Installation on Windows\r\n1. Install [Python 3.10.6](https://www.python.org/downloads/release/python-3106/) (Newer version of Python does not support torch), checking \"Add Python to PATH\".\r\n2. Install [git](https://git-scm.com/download/win).\r\n3. Download the stable-diffusion-webui repository, for example by running `git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git`.\r\n4. Run `webui-user.bat` from Windows Explorer as normal, non-administrator, user.\r\n\r\n### Automatic Installation on Linux\r\n1. Install the dependencies:\r\n```bash\r\n# Debian-based:\r\nsudo apt install wget git python3 python3-venv libgl1 libglib2.0-0\r\n# Red Hat-based:\r\nsudo dnf install wget git python3 gperftools-libs libglvnd-glx\r\n# openSUSE-based:\r\nsudo zypper install wget git python3 libtcmalloc4 libglvnd\r\n# Arch-based:\r\nsudo pacman -S wget git python3\r\n```\r\nIf your system is very new, you need to install python3.11 or python3.10:\r\n```bash\r\n# Ubuntu 24.04\r\nsudo add-apt-repository ppa:deadsnakes/ppa\r\nsudo apt update\r\nsudo apt install python3.11\r\n\r\n# Manjaro/Arch\r\nsudo pacman -S yay\r\nyay -S python311 # do not confuse with python3.11 package\r\n\r\n# Only for 3.11\r\n# Then set up env variable in launch script\r\nexport python_cmd=\"python3.11\"\r\n# or in webui-user.sh\r\npython_cmd=\"python3.11\"\r\n```\r\n2. Navigate to the directory you would like the webui to be installed and execute the following command:\r\n```bash\r\nwget -q https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh\r\n```\r\nOr just clone the repo wherever you want:\r\n```bash\r\ngit clone https://github.com/AUTOMATIC1111/stable-diffusion-webui\r\n```\r\n\r\n3. Run `webui.sh`.\r\n4. Check `webui-user.sh` for options.\r\n### Installation on Apple Silicon\r\n\r\nFind the instructions [here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Installation-on-Apple-Silicon).\r\n\r\n## Contributing\r\nHere's how to add code to this repo: [Contributing](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing)\r\n\r\n## Documentation\r\n\r\nThe documentation was moved from this README over to the project's [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki).\r\n\r\nFor the purposes of getting Google and other search engines to crawl the wiki, here's a link to the (not for humans) [crawlable wiki](https://github-wiki-see.page/m/AUTOMATIC1111/stable-diffusion-webui/wiki).\r\n\r\n## Credits\r\nLicenses for borrowed code can be found in `Settings -> Licenses` screen, and also in `html/licenses.html` file.\r\n\r\n- Stable Diffusion - https://github.com/Stability-AI/stablediffusion, https://github.com/CompVis/taming-transformers, https://github.com/mcmonkey4eva/sd3-ref\r\n- k-diffusion - https://github.com/crowsonkb/k-diffusion.git\r\n- Spandrel - https://github.com/chaiNNer-org/spandrel implementing\r\n  - GFPGAN - https://github.com/TencentARC/GFPGAN.git\r\n  - CodeFormer - https://github.com/sczhou/CodeFormer\r\n  - ESRGAN - https://github.com/xinntao/ESRGAN\r\n  - SwinIR - https://github.com/JingyunLiang/SwinIR\r\n  - Swin2SR - https://github.com/mv-lab/swin2sr\r\n- LDSR - https://github.com/Hafiidz/latent-diffusion\r\n- MiDaS - https://github.com/isl-org/MiDaS\r\n- Ideas for optimizations - https://github.com/basujindal/stable-diffusion\r\n- Cross Attention layer optimization - Doggettx - https://github.com/Doggettx/stable-diffusion, original idea for prompt editing.\r\n- Cross Attention layer optimization - InvokeAI, lstein - https://github.com/invoke-ai/InvokeAI (originally http://github.com/lstein/stable-diffusion)\r\n- Sub-quadratic Cross Attention layer optimization - Alex Birch (https://github.com/Birch-san/diffusers/pull/1), Amin Rezaei (https://github.com/AminRezaei0x443/memory-efficient-attention)\r\n- Textual Inversion - Rinon Gal - https://github.com/rinongal/textual_inversion (we're not using his code, but we are using his ideas).\r\n- Idea for SD upscale - https://github.com/jquesnelle/txt2imghd\r\n- Noise generation for outpainting mk2 - https://github.com/parlance-zz/g-diffuser-bot\r\n- CLIP interrogator idea and borrowing some code - https://github.com/pharmapsychotic/clip-interrogator\r\n- Idea for Composable Diffusion - https://github.com/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch\r\n- xformers - https://github.com/facebookresearch/xformers\r\n- DeepDanbooru - interrogator for anime diffusers https://github.com/KichangKim/DeepDanbooru\r\n- Sampling in float32 precision from a float16 UNet - marunine for the idea, Birch-san for the example Diffusers implementation (https://github.com/Birch-san/diffusers-play/tree/92feee6)\r\n- Instruct pix2pix - Tim Brooks (star), Aleksander Holynski (star), Alexei A. Efros (no star) - https://github.com/timothybrooks/instruct-pix2pix\r\n- Security advice - RyotaK\r\n- UniPC sampler - Wenliang Zhao - https://github.com/wl-zhao/UniPC\r\n- TAESD - Ollin Boer Bohan - https://github.com/madebyollin/taesd\r\n- LyCORIS - KohakuBlueleaf\r\n- Restart sampling - lambertae - https://github.com/Newbeeer/diffusion_restart_sampling\r\n- Hypertile - tfernd - https://github.com/tfernd/HyperTile\r\n- Initial Gradio script - posted on 4chan by an Anonymous user. Thank you Anonymous user.\r\n- (You)\r\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 5108051,
    "name": "opencv",
    "full_name": "opencv/opencv",
    "description": "Open Source Computer Vision Library",
    "html_url": "https://github.com/opencv/opencv",
    "clone_url": "https://github.com/opencv/opencv.git",
    "owner_login": "opencv",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/5009934?v=4",
    "stargazers_count": 83390,
    "watchers_count": 83390,
    "forks_count": 56241,
    "open_issues_count": 2760,
    "size": 547913,
    "language": "C++",
    "languages": {
      "C++": 43281694,
      "C": 1544836,
      "Python": 1340586,
      "CMake": 1019569,
      "Java": 765607,
      "Objective-C++": 391761,
      "Cuda": 343576,
      "Swift": 301765,
      "JavaScript": 243692,
      "Objective-C": 99998,
      "HTML": 40097,
      "Shell": 26085,
      "Perl": 15865,
      "PowerShell": 14591,
      "Kotlin": 6201,
      "TeX": 5144,
      "Batchfile": 1498,
      "Prolog": 843,
      "Dockerfile": 309
    },
    "topics": [
      "c-plus-plus",
      "computer-vision",
      "deep-learning",
      "image-processing",
      "opencv"
    ],
    "license_name": "Apache License 2.0",
    "created_at": "2012-07-19T09:40:17+00:00",
    "updated_at": "2025-08-06T01:59:59+00:00",
    "pushed_at": "2025-08-04T10:50:03+00:00",
    "contributors_count": 100,
    "readme_length": 1702,
    "readme_content": "## OpenCV: Open Source Computer Vision Library\n\n\n### Resources\n\n* Homepage: <https://opencv.org>\n  * Courses: <https://opencv.org/courses>\n* Docs: <https://docs.opencv.org/4.x/>\n* Q&A forum: <https://forum.opencv.org>\n  * previous forum (read only): <http://answers.opencv.org>\n* Issue tracking: <https://github.com/opencv/opencv/issues>\n* Additional OpenCV functionality: <https://github.com/opencv/opencv_contrib>\n* Donate to OpenCV: <https://opencv.org/support/>\n\n\n### Contributing\n\nPlease read the [contribution guidelines](https://github.com/opencv/opencv/wiki/How_to_contribute) before starting work on a pull request.\n\n#### Summary of the guidelines:\n\n* One pull request per issue;\n* Choose the right base branch;\n* Include tests and documentation;\n* Clean up \"oops\" commits before submitting;\n* Follow the [coding style guide](https://github.com/opencv/opencv/wiki/Coding_Style_Guide).\n\n### Additional Resources\n\n* [Submit your OpenCV-based project](https://form.jotform.com/233105358823151) for inclusion in Community Friday on opencv.org\n* [Subscribe to the OpenCV YouTube Channel](http://youtube.com/@opencvofficial) featuring OpenCV Live, an hour-long streaming show\n* [Follow OpenCV on LinkedIn](http://linkedin.com/company/opencv/) for daily posts showing the state-of-the-art in computer vision & AI\n* [Apply to be an OpenCV Volunteer](https://form.jotform.com/232745316792159) to help organize events and online campaigns as well as amplify them\n* [Follow OpenCV on Mastodon](http://mastodon.social/@opencv) in the Fediverse\n* [Follow OpenCV on Twitter](https://twitter.com/opencvlive)\n* [OpenCV.ai](https://opencv.ai): Computer Vision and AI development services from the OpenCV team.\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 730534580,
    "name": "ragflow",
    "full_name": "infiniflow/ragflow",
    "description": "RAGFlow is an open-source RAG (Retrieval-Augmented Generation) engine based on deep document understanding.",
    "html_url": "https://github.com/infiniflow/ragflow",
    "clone_url": "https://github.com/infiniflow/ragflow.git",
    "owner_login": "infiniflow",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/69962740?v=4",
    "stargazers_count": 61731,
    "watchers_count": 61731,
    "forks_count": 6275,
    "open_issues_count": 2563,
    "size": 81866,
    "language": "Python",
    "languages": {
      "Python": 3359262,
      "TypeScript": 3353857,
      "Less": 68097,
      "Shell": 29957,
      "HTML": 21351,
      "JavaScript": 13995,
      "CSS": 13135,
      "Dockerfile": 10051,
      "Makefile": 3718,
      "Smarty": 1782
    },
    "topics": [
      "agent",
      "agentic",
      "agentic-ai",
      "agentic-workflow",
      "ai",
      "ai-search",
      "deep-learning",
      "deep-research",
      "deepseek",
      "deepseek-r1",
      "document-parser",
      "document-understanding",
      "graphrag",
      "llm",
      "mcp",
      "multi-agent",
      "ollama",
      "openai",
      "rag",
      "retrieval-augmented-generation"
    ],
    "license_name": "Apache License 2.0",
    "created_at": "2023-12-12T06:13:13+00:00",
    "updated_at": "2025-08-06T01:57:55+00:00",
    "pushed_at": "2025-08-05T13:29:46+00:00",
    "contributors_count": 100,
    "readme_length": 15797,
    "readme_content": "<div align=\"center\">\n<a href=\"https://demo.ragflow.io/\">\n<img src=\"web/src/assets/logo-with-text.png\" width=\"520\" alt=\"ragflow logo\">\n</a>\n</div>\n\n<p align=\"center\">\n  <a href=\"./README.md\"><img alt=\"README in English\" src=\"https://img.shields.io/badge/English-DBEDFA\"></a>\n  <a href=\"./README_zh.md\"><img alt=\"\u7b80\u4f53\u4e2d\u6587\u7248\u81ea\u8ff0\u6587\u4ef6\" src=\"https://img.shields.io/badge/\u7b80\u4f53\u4e2d\u6587-DFE0E5\"></a>\n  <a href=\"./README_tzh.md\"><img alt=\"\u7e41\u9ad4\u7248\u4e2d\u6587\u81ea\u8ff0\u6587\u4ef6\" src=\"https://img.shields.io/badge/\u7e41\u9ad4\u4e2d\u6587-DFE0E5\"></a>\n  <a href=\"./README_ja.md\"><img alt=\"\u65e5\u672c\u8a9e\u306eREADME\" src=\"https://img.shields.io/badge/\u65e5\u672c\u8a9e-DFE0E5\"></a>\n  <a href=\"./README_ko.md\"><img alt=\"\ud55c\uad6d\uc5b4\" src=\"https://img.shields.io/badge/\ud55c\uad6d\uc5b4-DFE0E5\"></a>\n  <a href=\"./README_id.md\"><img alt=\"Bahasa Indonesia\" src=\"https://img.shields.io/badge/Bahasa Indonesia-DFE0E5\"></a>\n  <a href=\"./README_pt_br.md\"><img alt=\"Portugu\u00eas(Brasil)\" src=\"https://img.shields.io/badge/Portugu\u00eas(Brasil)-DFE0E5\"></a>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://x.com/intent/follow?screen_name=infiniflowai\" target=\"_blank\">\n        <img src=\"https://img.shields.io/twitter/follow/infiniflow?logo=X&color=%20%23f5f5f5\" alt=\"follow on X(Twitter)\">\n    </a>\n    <a href=\"https://demo.ragflow.io\" target=\"_blank\">\n        <img alt=\"Static Badge\" src=\"https://img.shields.io/badge/Online-Demo-4e6b99\">\n    </a>\n    <a href=\"https://hub.docker.com/r/infiniflow/ragflow\" target=\"_blank\">\n        <img src=\"https://img.shields.io/docker/pulls/infiniflow/ragflow?label=Docker%20Pulls&color=0db7ed&logo=docker&logoColor=white&style=flat-square\" alt=\"docker pull infiniflow/ragflow:v0.20.0\">\n    </a>\n    <a href=\"https://github.com/infiniflow/ragflow/releases/latest\">\n        <img src=\"https://img.shields.io/github/v/release/infiniflow/ragflow?color=blue&label=Latest%20Release\" alt=\"Latest Release\">\n    </a>\n    <a href=\"https://github.com/infiniflow/ragflow/blob/main/LICENSE\">\n        <img height=\"21\" src=\"https://img.shields.io/badge/License-Apache--2.0-ffffff?labelColor=d4eaf7&color=2e6cc4\" alt=\"license\">\n    </a>\n    <a href=\"https://deepwiki.com/infiniflow/ragflow\">\n        <img alt=\"Ask DeepWiki\" src=\"https://deepwiki.com/badge.svg\">\n    </a>\n</p>\n\n<h4 align=\"center\">\n  <a href=\"https://ragflow.io/docs/dev/\">Document</a> |\n  <a href=\"https://github.com/infiniflow/ragflow/issues/4214\">Roadmap</a> |\n  <a href=\"https://twitter.com/infiniflowai\">Twitter</a> |\n  <a href=\"https://discord.gg/NjYzJD3GM3\">Discord</a> |\n  <a href=\"https://demo.ragflow.io\">Demo</a>\n</h4>\n\n#\n\n<div align=\"center\">\n<a href=\"https://trendshift.io/repositories/9064\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/9064\" alt=\"infiniflow%2Fragflow | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n</div>\n\n<details open>\n<summary><b>\ud83d\udcd5 Table of Contents</b></summary>\n\n- \ud83d\udca1 [What is RAGFlow?](#-what-is-ragflow)\n- \ud83c\udfae [Demo](#-demo)\n- \ud83d\udccc [Latest Updates](#-latest-updates)\n- \ud83c\udf1f [Key Features](#-key-features)\n- \ud83d\udd0e [System Architecture](#-system-architecture)\n- \ud83c\udfac [Get Started](#-get-started)\n- \ud83d\udd27 [Configurations](#-configurations)\n- \ud83d\udd27 [Build a docker image without embedding models](#-build-a-docker-image-without-embedding-models)\n- \ud83d\udd27 [Build a docker image including embedding models](#-build-a-docker-image-including-embedding-models)\n- \ud83d\udd28 [Launch service from source for development](#-launch-service-from-source-for-development)\n- \ud83d\udcda [Documentation](#-documentation)\n- \ud83d\udcdc [Roadmap](#-roadmap)\n- \ud83c\udfc4 [Community](#-community)\n- \ud83d\ude4c [Contributing](#-contributing)\n\n</details>\n\n## \ud83d\udca1 What is RAGFlow?\n\n[RAGFlow](https://ragflow.io/) is an open-source RAG (Retrieval-Augmented Generation) engine based on deep document\nunderstanding. It offers a streamlined RAG workflow for businesses of any scale, combining LLM (Large Language Models)\nto provide truthful question-answering capabilities, backed by well-founded citations from various complex formatted\ndata.\n\n## \ud83c\udfae Demo\n\nTry our demo at [https://demo.ragflow.io](https://demo.ragflow.io).\n\n<div align=\"center\" style=\"margin-top:20px;margin-bottom:20px;\">\n<img src=\"https://raw.githubusercontent.com/infiniflow/ragflow-docs/refs/heads/image/image/chunking.gif\" width=\"1200\"/>\n<img src=\"https://raw.githubusercontent.com/infiniflow/ragflow-docs/refs/heads/image/image/agentic-dark.gif\" width=\"1200\"/>\n</div>\n\n## \ud83d\udd25 Latest Updates\n\n- 2025-08-04 Supports new models, including Kimi K2 and Grok 4.\n- 2025-08-01 Supports agentic workflow and MCP.\n- 2025-05-23 Adds a Python/JavaScript code executor component to Agent.\n- 2025-05-05 Supports cross-language query.\n- 2025-03-19 Supports using a multi-modal model to make sense of images within PDF or DOCX files.\n- 2025-02-28 Combined with Internet search (Tavily), supports reasoning like Deep Research for any LLMs.\n- 2024-12-18 Upgrades Document Layout Analysis model in DeepDoc.\n- 2024-08-22 Support text to SQL statements through RAG.\n\n## \ud83c\udf89 Stay Tuned\n\n\u2b50\ufe0f Star our repository to stay up-to-date with exciting new features and improvements! Get instant notifications for new\nreleases! \ud83c\udf1f\n\n<div align=\"center\" style=\"margin-top:20px;margin-bottom:20px;\">\n<img src=\"https://github.com/user-attachments/assets/18c9707e-b8aa-4caf-a154-037089c105ba\" width=\"1200\"/>\n</div>\n\n## \ud83c\udf1f Key Features\n\n### \ud83c\udf6d **\"Quality in, quality out\"**\n\n- [Deep document understanding](./deepdoc/README.md)-based knowledge extraction from unstructured data with complicated\n  formats.\n- Finds \"needle in a data haystack\" of literally unlimited tokens.\n\n### \ud83c\udf71 **Template-based chunking**\n\n- Intelligent and explainable.\n- Plenty of template options to choose from.\n\n### \ud83c\udf31 **Grounded citations with reduced hallucinations**\n\n- Visualization of text chunking to allow human intervention.\n- Quick view of the key references and traceable citations to support grounded answers.\n\n### \ud83c\udf54 **Compatibility with heterogeneous data sources**\n\n- Supports Word, slides, excel, txt, images, scanned copies, structured data, web pages, and more.\n\n### \ud83d\udec0 **Automated and effortless RAG workflow**\n\n- Streamlined RAG orchestration catered to both personal and large businesses.\n- Configurable LLMs as well as embedding models.\n- Multiple recall paired with fused re-ranking.\n- Intuitive APIs for seamless integration with business.\n\n## \ud83d\udd0e System Architecture\n\n<div align=\"center\" style=\"margin-top:20px;margin-bottom:20px;\">\n<img src=\"https://github.com/infiniflow/ragflow/assets/12318111/d6ac5664-c237-4200-a7c2-a4a00691b485\" width=\"1000\"/>\n</div>\n\n## \ud83c\udfac Get Started\n\n### \ud83d\udcdd Prerequisites\n\n- CPU >= 4 cores\n- RAM >= 16 GB\n- Disk >= 50 GB\n- Docker >= 24.0.0 & Docker Compose >= v2.26.1\n- [gVisor](https://gvisor.dev/docs/user_guide/install/): Required only if you intend to use the code executor (sandbox) feature of RAGFlow.\n\n> [!TIP]\n> If you have not installed Docker on your local machine (Windows, Mac, or Linux), see [Install Docker Engine](https://docs.docker.com/engine/install/).\n\n### \ud83d\ude80 Start up the server\n\n1. Ensure `vm.max_map_count` >= 262144:\n\n   > To check the value of `vm.max_map_count`:\n   >\n   > ```bash\n   > $ sysctl vm.max_map_count\n   > ```\n   >\n   > Reset `vm.max_map_count` to a value at least 262144 if it is not.\n   >\n   > ```bash\n   > # In this case, we set it to 262144:\n   > $ sudo sysctl -w vm.max_map_count=262144\n   > ```\n   >\n   > This change will be reset after a system reboot. To ensure your change remains permanent, add or update the\n   > `vm.max_map_count` value in **/etc/sysctl.conf** accordingly:\n   >\n   > ```bash\n   > vm.max_map_count=262144\n   > ```\n\n2. Clone the repo:\n\n   ```bash\n   $ git clone https://github.com/infiniflow/ragflow.git\n   ```\n\n3. Start up the server using the pre-built Docker images:\n\n> [!CAUTION]\n> All Docker images are built for x86 platforms. We don't currently offer Docker images for ARM64.\n> If you are on an ARM64 platform, follow [this guide](https://ragflow.io/docs/dev/build_docker_image) to build a Docker image compatible with your system.\n\n   > The command below downloads the `v0.20.0-slim` edition of the RAGFlow Docker image. See the following table for descriptions of different RAGFlow editions. To download a RAGFlow edition different from `v0.20.0-slim`, update the `RAGFLOW_IMAGE` variable accordingly in **docker/.env** before using `docker compose` to start the server. For example: set `RAGFLOW_IMAGE=infiniflow/ragflow:v0.20.0` for the full edition `v0.20.0`.\n\n   ```bash\n   $ cd ragflow/docker\n   # Use CPU for embedding and DeepDoc tasks:\n   $ docker compose -f docker-compose.yml up -d\n\n   # To use GPU to accelerate embedding and DeepDoc tasks:\n   # docker compose -f docker-compose-gpu.yml up -d\n   ```\n\n   | RAGFlow image tag | Image size (GB) | Has embedding models? | Stable?                  |\n   |-------------------|-----------------|-----------------------|--------------------------|\n   | v0.20.0           | &approx;9       | :heavy_check_mark:    | Stable release           |\n   | v0.20.0-slim      | &approx;2       | \u274c                   | Stable release            |\n   | nightly           | &approx;9       | :heavy_check_mark:    | _Unstable_ nightly build |\n   | nightly-slim      | &approx;2       | \u274c                   | _Unstable_ nightly build  |\n\n4. Check the server status after having the server up and running:\n\n   ```bash\n   $ docker logs -f ragflow-server\n   ```\n\n   _The following output confirms a successful launch of the system:_\n\n   ```bash\n\n         ____   ___    ______ ______ __\n        / __ \\ /   |  / ____// ____// /____  _      __\n       / /_/ // /| | / / __ / /_   / // __ \\| | /| / /\n      / _, _// ___ |/ /_/ // __/  / // /_/ /| |/ |/ /\n     /_/ |_|/_/  |_|\\____//_/    /_/ \\____/ |__/|__/\n\n    * Running on all addresses (0.0.0.0)\n   ```\n\n   > If you skip this confirmation step and directly log in to RAGFlow, your browser may prompt a `network anormal`\n   > error because, at that moment, your RAGFlow may not be fully initialized.\n\n5. In your web browser, enter the IP address of your server and log in to RAGFlow.\n   > With the default settings, you only need to enter `http://IP_OF_YOUR_MACHINE` (**sans** port number) as the default\n   > HTTP serving port `80` can be omitted when using the default configurations.\n6. In [service_conf.yaml.template](./docker/service_conf.yaml.template), select the desired LLM factory in `user_default_llm` and update\n   the `API_KEY` field with the corresponding API key.\n\n   > See [llm_api_key_setup](https://ragflow.io/docs/dev/llm_api_key_setup) for more information.\n\n   _The show is on!_\n\n## \ud83d\udd27 Configurations\n\nWhen it comes to system configurations, you will need to manage the following files:\n\n- [.env](./docker/.env): Keeps the fundamental setups for the system, such as `SVR_HTTP_PORT`, `MYSQL_PASSWORD`, and\n  `MINIO_PASSWORD`.\n- [service_conf.yaml.template](./docker/service_conf.yaml.template): Configures the back-end services. The environment variables in this file will be automatically populated when the Docker container starts. Any environment variables set within the Docker container will be available for use, allowing you to customize service behavior based on the deployment environment.\n- [docker-compose.yml](./docker/docker-compose.yml): The system relies on [docker-compose.yml](./docker/docker-compose.yml) to start up.\n\n> The [./docker/README](./docker/README.md) file provides a detailed description of the environment settings and service\n> configurations which can be used as `${ENV_VARS}` in the [service_conf.yaml.template](./docker/service_conf.yaml.template) file.\n\nTo update the default HTTP serving port (80), go to [docker-compose.yml](./docker/docker-compose.yml) and change `80:80`\nto `<YOUR_SERVING_PORT>:80`.\n\nUpdates to the above configurations require a reboot of all containers to take effect:\n\n> ```bash\n> $ docker compose -f docker-compose.yml up -d\n> ```\n\n### Switch doc engine from Elasticsearch to Infinity\n\nRAGFlow uses Elasticsearch by default for storing full text and vectors. To switch to [Infinity](https://github.com/infiniflow/infinity/), follow these steps:\n\n1. Stop all running containers:\n\n   ```bash\n   $ docker compose -f docker/docker-compose.yml down -v\n   ```\n\n> [!WARNING]\n> `-v` will delete the docker container volumes, and the existing data will be cleared.\n\n2. Set `DOC_ENGINE` in **docker/.env** to `infinity`.\n\n3. Start the containers:\n\n   ```bash\n   $ docker compose -f docker-compose.yml up -d\n   ```\n\n> [!WARNING]\n> Switching to Infinity on a Linux/arm64 machine is not yet officially supported.\n\n## \ud83d\udd27 Build a Docker image without embedding models\n\nThis image is approximately 2 GB in size and relies on external LLM and embedding services.\n\n```bash\ngit clone https://github.com/infiniflow/ragflow.git\ncd ragflow/\ndocker build --platform linux/amd64 --build-arg LIGHTEN=1 -f Dockerfile -t infiniflow/ragflow:nightly-slim .\n```\n\n## \ud83d\udd27 Build a Docker image including embedding models\n\nThis image is approximately 9 GB in size. As it includes embedding models, it relies on external LLM services only.\n\n```bash\ngit clone https://github.com/infiniflow/ragflow.git\ncd ragflow/\ndocker build --platform linux/amd64 -f Dockerfile -t infiniflow/ragflow:nightly .\n```\n\n## \ud83d\udd28 Launch service from source for development\n\n1. Install uv, or skip this step if it is already installed:\n\n   ```bash\n   pipx install uv pre-commit\n   ```\n\n2. Clone the source code and install Python dependencies:\n\n   ```bash\n   git clone https://github.com/infiniflow/ragflow.git\n   cd ragflow/\n   uv sync --python 3.10 --all-extras # install RAGFlow dependent python modules\n   uv run download_deps.py\n   pre-commit install\n   ```\n\n3. Launch the dependent services (MinIO, Elasticsearch, Redis, and MySQL) using Docker Compose:\n\n   ```bash\n   docker compose -f docker/docker-compose-base.yml up -d\n   ```\n\n   Add the following line to `/etc/hosts` to resolve all hosts specified in **docker/.env** to `127.0.0.1`:\n\n   ```\n   127.0.0.1       es01 infinity mysql minio redis sandbox-executor-manager\n   ```\n\n4. If you cannot access HuggingFace, set the `HF_ENDPOINT` environment variable to use a mirror site:\n\n   ```bash\n   export HF_ENDPOINT=https://hf-mirror.com\n   ```\n\n5. If your operating system does not have jemalloc, please install it as follows:\n\n   ```bash\n   # ubuntu\n   sudo apt-get install libjemalloc-dev\n   # centos\n   sudo yum install jemalloc\n   ```\n   \n6. Launch backend service:\n\n   ```bash\n   source .venv/bin/activate\n   export PYTHONPATH=$(pwd)\n   bash docker/launch_backend_service.sh\n   ```\n\n7. Install frontend dependencies:\n\n   ```bash\n   cd web\n   npm install\n   ```\n\n8. Launch frontend service:\n\n   ```bash\n   npm run dev\n   ```\n\n   _The following output confirms a successful launch of the system:_\n\n   ![](https://github.com/user-attachments/assets/0daf462c-a24d-4496-a66f-92533534e187)\n\n9. Stop RAGFlow front-end and back-end service after development is complete:\n\n   ```bash\n   pkill -f \"ragflow_server.py|task_executor.py\"\n   ```\n\n\n## \ud83d\udcda Documentation\n\n- [Quickstart](https://ragflow.io/docs/dev/)\n- [Configuration](https://ragflow.io/docs/dev/configurations)\n- [Release notes](https://ragflow.io/docs/dev/release_notes)\n- [User guides](https://ragflow.io/docs/dev/category/guides)\n- [Developer guides](https://ragflow.io/docs/dev/category/developers)\n- [References](https://ragflow.io/docs/dev/category/references)\n- [FAQs](https://ragflow.io/docs/dev/faq)\n\n## \ud83d\udcdc Roadmap\n\nSee the [RAGFlow Roadmap 2025](https://github.com/infiniflow/ragflow/issues/4214)\n\n## \ud83c\udfc4 Community\n\n- [Discord](https://discord.gg/NjYzJD3GM3)\n- [Twitter](https://twitter.com/infiniflowai)\n- [GitHub Discussions](https://github.com/orgs/infiniflow/discussions)\n\n## \ud83d\ude4c Contributing\n\nRAGFlow flourishes via open-source collaboration. In this spirit, we embrace diverse contributions from the community.\nIf you would like to be a part, review our [Contribution Guidelines](https://ragflow.io/docs/dev/contributing) first.\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 579082810,
    "name": "Prompt-Engineering-Guide",
    "full_name": "dair-ai/Prompt-Engineering-Guide",
    "description": "\ud83d\udc19 Guides, papers, lecture, notebooks and resources for prompt engineering",
    "html_url": "https://github.com/dair-ai/Prompt-Engineering-Guide",
    "clone_url": "https://github.com/dair-ai/Prompt-Engineering-Guide.git",
    "owner_login": "dair-ai",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/30384625?v=4",
    "stargazers_count": 60451,
    "watchers_count": 60451,
    "forks_count": 6162,
    "open_issues_count": 219,
    "size": 69214,
    "language": "MDX",
    "languages": {
      "MDX": 4853230,
      "Jupyter Notebook": 207291,
      "TypeScript": 16260,
      "JavaScript": 5236,
      "CSS": 131
    },
    "topics": [
      "chatgpt",
      "deep-learning",
      "generative-ai",
      "language-model",
      "openai",
      "prompt-engineering"
    ],
    "license_name": "MIT License",
    "created_at": "2022-12-16T16:04:50+00:00",
    "updated_at": "2025-08-06T01:07:10+00:00",
    "pushed_at": "2025-07-13T23:44:08+00:00",
    "contributors_count": 100,
    "readme_length": 10557,
    "readme_content": "# Prompt Engineering Guide\n\nPrompt engineering is a relatively new discipline for developing and optimizing prompts to efficiently use language models (LMs) for a wide variety of applications and research topics. Prompt engineering skills help to better understand the capabilities and limitations of large language models (LLMs). Researchers use prompt engineering to improve the capacity of LLMs on a wide range of common and complex tasks such as question answering and arithmetic reasoning. Developers use prompt engineering to design robust and effective prompting techniques that interface with LLMs and other tools.\n\nMotivated by the high interest in developing with LLMs, we have created this new prompt engineering guide that contains all the latest papers, learning guides, lectures, references, and tools related to prompt engineering for LLMs.\n\n\ud83c\udf10 [Prompt Engineering Guide (Web Version)](https://www.promptingguide.ai/)\n\n\ud83c\udf89 We are excited to launch our brand new prompt engineering courses under the DAIR.AI Academy. [Join Now](https://dair-ai.thinkific.com/bundles/pro)!\n\nUse code PROMPTING20 to get an extra 20% off.\n\nHappy Prompting!\n\n---\n## Announcements / Updates\n\n- \ud83c\udf93 We now offer self-paced prompt engineering courses under our DAIR.AI Academy. [Join Now](https://dair-ai.thinkific.com/bundles/pro)! \n- \ud83c\udf93 New course on Prompt Engineering for LLMs announced! [Enroll here](https://maven.com/dair-ai/prompt-engineering-llms)!\n- \ud83d\udcbc We now offer several [services](https://www.promptingguide.ai/services) like corporate training, consulting, and talks.\n- \ud83c\udf10 We now support 13 languages! Welcoming more translations.\n- \ud83d\udc69\u200d\ud83c\udf93 We crossed 3 million learners in January 2024!\n- \ud83c\udf89 We have launched a new web version of the guide [here](https://www.promptingguide.ai/)\n- \ud83d\udd25 We reached #1 on Hacker News on 21 Feb 2023\n- \ud83c\udf89 The First Prompt Engineering Lecture went live [here](https://youtu.be/dOxUroR57xs)\n\n[Join our Discord](https://discord.com/invite/SKgkVT8BGJ)\n\n[Follow us on Twitter](https://twitter.com/dair_ai)\n\n[Subscribe to our YouTube](https://www.youtube.com/channel/UCyna_OxOWL7IEuOwb7WhmxQ)\n\n[Subscribe to our Newsletter](https://nlpnews.substack.com/)\n\n---\n\n## Guides\nYou can also find the most up-to-date guides on our new website [https://www.promptingguide.ai/](https://www.promptingguide.ai/).\n\n- [Prompt Engineering - Introduction](https://www.promptingguide.ai/introduction)\n  - [Prompt Engineering - LLM Settings](https://www.promptingguide.ai/introduction/settings)\n  - [Prompt Engineering - Basics of Prompting](https://www.promptingguide.ai/introduction/basics)\n  - [Prompt Engineering - Prompt Elements](https://www.promptingguide.ai/introduction/elements)\n  - [Prompt Engineering - General Tips for Designing Prompts](https://www.promptingguide.ai/introduction/tips)\n  - [Prompt Engineering - Examples of Prompts](https://www.promptingguide.ai/introduction/examples)\n- [Prompt Engineering - Techniques](https://www.promptingguide.ai/techniques)\n  - [Prompt Engineering - Zero-Shot Prompting](https://www.promptingguide.ai/techniques/zeroshot)\n  - [Prompt Engineering - Few-Shot Prompting](https://www.promptingguide.ai/techniques/fewshot)\n  - [Prompt Engineering - Chain-of-Thought Prompting](https://www.promptingguide.ai/techniques/cot)\n  - [Prompt Engineering - Self-Consistency](https://www.promptingguide.ai/techniques/consistency)\n  - [Prompt Engineering - Generate Knowledge Prompting](https://www.promptingguide.ai/techniques/knowledge)\n  - [Prompt Engineering - Prompt Chaining](https://www.promptingguide.ai/techniques/prompt_chaining)\n  - [Prompt Engineering - Tree of Thoughts (ToT)](https://www.promptingguide.ai/techniques/tot)\n  - [Prompt Engineering - Retrieval Augmented Generation](https://www.promptingguide.ai/techniques/rag)\n  - [Prompt Engineering - Automatic Reasoning and Tool-use (ART)](https://www.promptingguide.ai/techniques/art)\n  - [Prompt Engineering - Automatic Prompt Engineer](https://www.promptingguide.ai/techniques/ape)\n  - [Prompt Engineering - Active-Prompt](https://www.promptingguide.ai/techniques/activeprompt)\n  - [Prompt Engineering - Directional Stimulus Prompting](https://www.promptingguide.ai/techniques/dsp)\n  - [Prompt Engineering - Program-Aided Language Models](https://www.promptingguide.ai/techniques/pal)\n  - [Prompt Engineering - ReAct Prompting](https://www.promptingguide.ai/techniques/react)\n  - [Prompt Engineering - Multimodal CoT Prompting](https://www.promptingguide.ai/techniques/multimodalcot)\n  - [Prompt Engineering - Graph Prompting](https://www.promptingguide.ai/techniques/graph)\n- [Prompt Engineering - Applications](https://www.promptingguide.ai/applications)\n  - [Prompt Engineering - Function Calling](https://www.promptingguide.ai/applications/function_calling)\n  - [Prompt Engineering - Generating Data](https://www.promptingguide.ai/applications/generating)\n  - [Prompt Engineering - Generating Synthetic Dataset for RAG](https://www.promptingguide.ai/applications/synthetic_rag)\n  - [Prompt Engineering - Takling Generated Datasets Diversity](https://www.promptingguide.ai/applications/generating_textbooks)\n  - [Prompt Engineering - Generating Code](https://www.promptingguide.ai/applications/coding)\n  - [Prompt Engineering - Graduate Job Classification Case Study](https://www.promptingguide.ai/applications/workplace_casestudy)\n- [Prompt Engineering - Prompt Hub](https://www.promptingguide.ai/prompts)\n  - [Prompt Engineering - Classification](https://www.promptingguide.ai/prompts/classification)\n  - [Prompt Engineering - Coding](https://www.promptingguide.ai/prompts/coding)\n  - [Prompt Engineering - Creativity](https://www.promptingguide.ai/prompts/creativity)\n  - [Prompt Engineering - Evaluation](https://www.promptingguide.ai/prompts/evaluation)\n  - [Prompt Engineering - Information Extraction](https://www.promptingguide.ai/prompts/information-extraction)\n  - [Prompt Engineering - Image Generation](https://www.promptingguide.ai/prompts/image-generation)\n  - [Prompt Engineering - Mathematics](https://www.promptingguide.ai/prompts/mathematics)\n  - [Prompt Engineering - Question Answering](https://www.promptingguide.ai/prompts/question-answering)\n  - [Prompt Engineering - Reasoning](https://www.promptingguide.ai/prompts/reasoning)\n  - [Prompt Engineering - Text Summarization](https://www.promptingguide.ai/prompts/text-summarization)\n  - [Prompt Engineering - Truthfulness](https://www.promptingguide.ai/prompts/truthfulness)\n  - [Prompt Engineering - Adversarial Prompting](https://www.promptingguide.ai/prompts/adversarial-prompting)\n- [Prompt Engineering - Models](https://www.promptingguide.ai/models)\n  - [Prompt Engineering - ChatGPT](https://www.promptingguide.ai/models/chatgpt)\n  - [Prompt Engineering - Code Llama](https://www.promptingguide.ai/models/code-llama)\n  - [Prompt Engineering - Flan](https://www.promptingguide.ai/models/flan)\n  - [Prompt Engineering - Gemini](https://www.promptingguide.ai/models/gemini)\n  - [Prompt Engineering - GPT-4](https://www.promptingguide.ai/models/gpt-4)\n  - [Prompt Engineering - LLaMA](https://www.promptingguide.ai/models/llama)\n  - [Prompt Engineering - Mistral 7B](https://www.promptingguide.ai/models/mistral-7b)\n  - [Prompt Engineering - Mixtral](https://www.promptingguide.ai/models/mixtral)\n  - [Prompt Engineering - OLMo](https://www.promptingguide.ai/models/olmo)\n  - [Prompt Engineering - Phi-2](https://www.promptingguide.ai/models/phi-2)\n  - [Prompt Engineering - Model Collection](https://www.promptingguide.ai/models/collection)\n- [Prompt Engineering - Risks and Misuses](https://www.promptingguide.ai/risks)\n  - [Prompt Engineering - Adversarial Prompting](https://www.promptingguide.ai/risks/adversarial)\n  - [Prompt Engineering - Factuality](https://www.promptingguide.ai/risks/factuality)\n  - [Prompt Engineering - Biases](https://www.promptingguide.ai/risks/biases)\n- [Prompt Engineering - Papers](https://www.promptingguide.ai/papers)\n  - [Prompt Engineering - Overviews](https://www.promptingguide.ai/papers#overviews)\n  - [Prompt Engineering - Approaches](https://www.promptingguide.ai/papers#approaches)\n  - [Prompt Engineering - Applications](https://www.promptingguide.ai/papers#applications)\n  - [Prompt Engineering - Collections](https://www.promptingguide.ai/papers#collections)\n- [Prompt Engineering - Tools](https://www.promptingguide.ai/tools)\n- [Prompt Engineering - Notebooks](https://www.promptingguide.ai/notebooks)\n- [Prompt Engineering - Datasets](https://www.promptingguide.ai/datasets)\n- [Prompt Engineering - Additional Readings](https://www.promptingguide.ai/readings)\n\n\n---\n## Lecture\n\nWe have published a 1 hour lecture that provides a comprehensive overview of prompting techniques, applications, and tools.\n- [Video Lecture](https://youtu.be/dOxUroR57xs)\n- [Notebook with code](https://github.com/dair-ai/Prompt-Engineering-Guide/blob/main/notebooks/pe-lecture.ipynb)\n- [Slides](https://github.com/dair-ai/Prompt-Engineering-Guide/blob/main/lecture/Prompt-Engineering-Lecture-Elvis.pdf)\n\n---\n## Running the guide locally\n\nTo run the guide locally, for example to check the correct implementation of a new translation, you will need to:\n\n1. Install Node >=18.0.0\n1. Install `pnpm` if not present in your system. Check [here](https://pnpm.io/installation) for detailed instructions.\n1. Install the dependencies: `pnpm i next react react-dom nextra nextra-theme-docs`\n1. Boot the guide with `pnpm dev`\n2. Browse the guide at `http://localhost:3000/`\n\n---\n## Appearances\nSome places where we have been featured:\n- Wall Street Journal - [ChatGPT Can Give Great Answers. But Only If You Know How to Ask the Right Question](https://www.wsj.com/articles/chatgpt-ask-the-right-question-12d0f035)\n- Forbes - [Mom, Dad, I Want To Be A Prompt Engineer](https://www.forbes.com/sites/craigsmith/2023/04/05/mom-dad-i-want-to-be-a-prompt-engineer/?sh=7f1213159c8e)\n- Markettechpost - [Best Free Prompt Engineering Resources (2023)](https://www.marktechpost.com/2023/04/04/best-free-prompt-engineering-resources-2023/)\n\n\n---\nIf you are using the guide for your work or research, please cite us as follows:\n\n```\n@article{Saravia_Prompt_Engineering_Guide_2022,\nauthor = {Saravia, Elvis},\njournal = {https://github.com/dair-ai/Prompt-Engineering-Guide},\nmonth = {12},\ntitle = {{Prompt Engineering Guide}},\nyear = {2022}\n}\n```\n\n## License\n\n[MIT License](https://github.com/dair-ai/Prompt-Engineering-Guide/blob/main/LICENSE.md)\n\n\nFeel free to open a PR if you think something is missing here. Always welcome feedback and suggestions. Just open an issue!\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 188660663,
    "name": "Real-Time-Voice-Cloning",
    "full_name": "CorentinJ/Real-Time-Voice-Cloning",
    "description": "Clone a voice in 5 seconds to generate arbitrary speech in real-time",
    "html_url": "https://github.com/CorentinJ/Real-Time-Voice-Cloning",
    "clone_url": "https://github.com/CorentinJ/Real-Time-Voice-Cloning.git",
    "owner_login": "CorentinJ",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/12038136?v=4",
    "stargazers_count": 54806,
    "watchers_count": 54806,
    "forks_count": 9049,
    "open_issues_count": 225,
    "size": 369678,
    "language": "Python",
    "languages": {
      "Python": 252952
    },
    "topics": [
      "deep-learning",
      "python",
      "pytorch",
      "tensorflow",
      "tts",
      "voice-cloning"
    ],
    "license_name": "Other",
    "created_at": "2019-05-26T08:56:15+00:00",
    "updated_at": "2025-08-06T01:33:38+00:00",
    "pushed_at": "2025-05-30T11:41:05+00:00",
    "contributors_count": 15,
    "readme_length": 4286,
    "readme_content": "# Real-Time Voice Cloning\nThis repository is an implementation of [Transfer Learning from Speaker Verification to\nMultispeaker Text-To-Speech Synthesis](https://arxiv.org/pdf/1806.04558.pdf) (SV2TTS) with a vocoder that works in real-time. This was my [master's thesis](https://matheo.uliege.be/handle/2268.2/6801).\n\nSV2TTS is a deep learning framework in three stages. In the first stage, one creates a digital representation of a voice from a few seconds of audio. In the second and third stages, this representation is used as reference to generate speech given arbitrary text.\n\n**Video demonstration** (click the picture):\n\n[![Toolbox demo](https://i.imgur.com/8lFUlgz.png)](https://www.youtube.com/watch?v=-O_hYhToKoA)\n\n\n\n### Papers implemented  \n| URL | Designation | Title | Implementation source |\n| --- | ----------- | ----- | --------------------- |\n|[**1806.04558**](https://arxiv.org/pdf/1806.04558.pdf) | **SV2TTS** | **Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis** | This repo |\n|[1802.08435](https://arxiv.org/pdf/1802.08435.pdf) | WaveRNN (vocoder) | Efficient Neural Audio Synthesis | [fatchord/WaveRNN](https://github.com/fatchord/WaveRNN) |\n|[1703.10135](https://arxiv.org/pdf/1703.10135.pdf) | Tacotron (synthesizer) | Tacotron: Towards End-to-End Speech Synthesis | [fatchord/WaveRNN](https://github.com/fatchord/WaveRNN)\n|[1710.10467](https://arxiv.org/pdf/1710.10467.pdf) | GE2E (encoder)| Generalized End-To-End Loss for Speaker Verification | This repo |\n\n## Heads up\nLike everything else in Deep Learning, this repo has quickly gotten old. Many SaaS apps (often paying) will give you a better audio quality than this repository will. If you wish for an open-source solution with a high voice quality:\n- Check out [paperswithcode](https://paperswithcode.com/task/speech-synthesis/) for other repositories and recent research in the field of speech synthesis.\n- Check out [Chatterbox](https://github.com/resemble-ai/chatterbox) for a similar project up to date with the 2025 SOTA in voice cloning\n\n## Setup\n\n### 1. Install Requirements\n1. Both Windows and Linux are supported. A GPU is recommended for training and for inference speed, but is not mandatory.\n2. Python 3.7 is recommended. Python 3.5 or greater should work, but you'll probably have to tweak the dependencies' versions. I recommend setting up a virtual environment using `venv`, but this is optional.\n3. Install [ffmpeg](https://ffmpeg.org/download.html#get-packages). This is necessary for reading audio files.\n4. Install [PyTorch](https://pytorch.org/get-started/locally/). Pick the latest stable version, your operating system, your package manager (pip by default) and finally pick any of the proposed CUDA versions if you have a GPU, otherwise pick CPU. Run the given command.\n5. Install the remaining requirements with `pip install -r requirements.txt`\n\n### 2. (Optional) Download Pretrained Models\nPretrained models are now downloaded automatically. If this doesn't work for you, you can manually download them [here](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models).\n\n### 3. (Optional) Test Configuration\nBefore you download any dataset, you can begin by testing your configuration with:\n\n`python demo_cli.py`\n\nIf all tests pass, you're good to go.\n\n### 4. (Optional) Download Datasets\nFor playing with the toolbox alone, I only recommend downloading [`LibriSpeech/train-clean-100`](https://www.openslr.org/resources/12/train-clean-100.tar.gz). Extract the contents as `<datasets_root>/LibriSpeech/train-clean-100` where `<datasets_root>` is a directory of your choosing. Other datasets are supported in the toolbox, see [here](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training#datasets). You're free not to download any dataset, but then you will need your own data as audio files or you will have to record it with the toolbox.\n\n### 5. Launch the Toolbox\nYou can then try the toolbox:\n\n`python demo_toolbox.py -d <datasets_root>`  \nor  \n`python demo_toolbox.py`  \n\ndepending on whether you downloaded any datasets. If you are running an X-server or if you have the error `Aborted (core dumped)`, see [this issue](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/11#issuecomment-504733590).\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 45986162,
    "name": "TensorFlow-Examples",
    "full_name": "aymericdamien/TensorFlow-Examples",
    "description": "TensorFlow Tutorial and Examples for Beginners (support TF v1 & v2)",
    "html_url": "https://github.com/aymericdamien/TensorFlow-Examples",
    "clone_url": "https://github.com/aymericdamien/TensorFlow-Examples.git",
    "owner_login": "aymericdamien",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/10386605?v=4",
    "stargazers_count": 43680,
    "watchers_count": 43680,
    "forks_count": 14868,
    "open_issues_count": 225,
    "size": 10242,
    "language": "Jupyter Notebook",
    "languages": {
      "Jupyter Notebook": 9651010,
      "Python": 136437
    },
    "topics": [
      "deep-learning",
      "examples",
      "machine-learning",
      "python",
      "tensorflow",
      "tutorial"
    ],
    "license_name": "Other",
    "created_at": "2015-11-11T14:21:19+00:00",
    "updated_at": "2025-08-05T18:53:15+00:00",
    "pushed_at": "2024-07-26T19:46:13+00:00",
    "contributors_count": 60,
    "readme_length": 22407,
    "readme_content": "# TensorFlow Examples\n\nThis tutorial was designed for easily diving into TensorFlow, through examples. For readability, it includes both notebooks and source codes with explanation, for both TF v1 & v2.\n\nIt is suitable for beginners who want to find clear and concise examples about TensorFlow. Besides the traditional 'raw' TensorFlow implementations, you can also find the latest TensorFlow API practices (such as `layers`, `estimator`, `dataset`, ...).\n\n**Update (05/16/2020):** Moving all default examples to TF2. For TF v1 examples: [check here](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1).\n\n## Tutorial index\n\n#### 0 - Prerequisite\n- [Introduction to Machine Learning](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/0_Prerequisite/ml_introduction.ipynb).\n- [Introduction to MNIST Dataset](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/0_Prerequisite/mnist_dataset_intro.ipynb).\n\n#### 1 - Introduction\n- **Hello World** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/1_Introduction/helloworld.ipynb)). Very simple example to learn how to print \"hello world\" using TensorFlow 2.0+.\n- **Basic Operations** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/1_Introduction/basic_operations.ipynb)). A simple example that cover TensorFlow 2.0+ basic operations.\n\n#### 2 - Basic Models\n- **Linear Regression** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/2_BasicModels/linear_regression.ipynb)). Implement a Linear Regression with TensorFlow 2.0+.\n- **Logistic Regression** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/2_BasicModels/logistic_regression.ipynb)). Implement a Logistic Regression with TensorFlow 2.0+.\n- **Word2Vec (Word Embedding)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/2_BasicModels/word2vec.ipynb)). Build a Word Embedding Model (Word2Vec) from Wikipedia data, with TensorFlow 2.0+.\n- **GBDT (Gradient Boosted Decision Trees)** ([notebooks](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/2_BasicModels/gradient_boosted_trees.ipynb)). Implement a Gradient Boosted Decision Trees with TensorFlow 2.0+ to predict house value using Boston Housing dataset.\n\n#### 3 - Neural Networks\n##### Supervised\n\n- **Simple Neural Network** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/3_NeuralNetworks/neural_network.ipynb)). Use TensorFlow 2.0 'layers' and 'model' API to build a simple neural network to classify MNIST digits dataset.\n- **Simple Neural Network (low-level)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/3_NeuralNetworks/neural_network_raw.ipynb)). Raw implementation of a simple neural network to classify MNIST digits dataset.\n- **Convolutional Neural Network** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/3_NeuralNetworks/convolutional_network.ipynb)). Use TensorFlow 2.0+ 'layers' and 'model' API to build a convolutional neural network to classify MNIST digits dataset.\n- **Convolutional Neural Network (low-level)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/3_NeuralNetworks/convolutional_network_raw.ipynb)). Raw implementation of a convolutional neural network to classify MNIST digits dataset.\n- **Recurrent Neural Network (LSTM)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/3_NeuralNetworks/recurrent_network.ipynb)). Build a recurrent neural network (LSTM) to classify MNIST digits dataset, using TensorFlow 2.0 'layers' and 'model' API.\n- **Bi-directional Recurrent Neural Network (LSTM)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/3_NeuralNetworks/bidirectional_rnn.ipynb)). Build a bi-directional recurrent neural network (LSTM) to classify MNIST digits dataset, using TensorFlow 2.0+ 'layers' and 'model' API.\n- **Dynamic Recurrent Neural Network (LSTM)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/3_NeuralNetworks/dynamic_rnn.ipynb)). Build a recurrent neural network (LSTM) that performs dynamic calculation to classify sequences of variable length, using TensorFlow 2.0+ 'layers' and 'model' API.\n\n##### Unsupervised\n- **Auto-Encoder** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/3_NeuralNetworks/autoencoder.ipynb)). Build an auto-encoder to encode an image to a lower dimension and re-construct it.\n- **DCGAN (Deep Convolutional Generative Adversarial Networks)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/3_NeuralNetworks/dcgan.ipynb)). Build a Deep Convolutional Generative Adversarial Network (DCGAN) to generate images from noise.\n\n#### 4 - Utilities\n- **Save and Restore a model** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/4_Utils/save_restore_model.ipynb)). Save and Restore a model with TensorFlow 2.0+.\n- **Build Custom Layers & Modules** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/4_Utils/build_custom_layers.ipynb)). Learn how to build your own layers / modules and integrate them into TensorFlow 2.0+ Models.\n- **Tensorboard** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/4_Utils/tensorboard.ipynb)). Track and visualize neural network computation graph, metrics, weights and more using TensorFlow 2.0+ tensorboard.\n\n#### 5 - Data Management\n- **Load and Parse data** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/5_DataManagement/load_data.ipynb)). Build efficient data pipeline with TensorFlow 2.0 (Numpy arrays, Images, CSV files, custom data, ...).\n- **Build and Load TFRecords** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/5_DataManagement/tfrecords.ipynb)). Convert data into TFRecords format, and load them with TensorFlow 2.0+.\n- **Image Transformation (i.e. Image Augmentation)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/5_DataManagement/image_transformation.ipynb)). Apply various image augmentation techniques with TensorFlow 2.0+, to generate distorted images for training.\n\n#### 6 - Hardware\n- **Multi-GPU Training** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/6_Hardware/multigpu_training.ipynb)). Train a convolutional neural network with multiple GPUs on CIFAR-10 dataset.\n\n## TensorFlow v1\n\nThe tutorial index for TF v1 is available here: [TensorFlow v1.15 Examples](tensorflow_v1). Or see below for a list of the examples.\n\n## Dataset\nSome examples require MNIST dataset for training and testing. Don't worry, this dataset will automatically be downloaded when running examples.\nMNIST is a database of handwritten digits, for a quick description of that dataset, you can check [this notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/0_Prerequisite/mnist_dataset_intro.ipynb).\n\nOfficial Website: [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/).\n\n## Installation\n\nTo download all the examples, simply clone this repository:\n```\ngit clone https://github.com/aymericdamien/TensorFlow-Examples\n```\n\nTo run them, you also need the latest version of TensorFlow. To install it:\n```\npip install tensorflow\n```\n\nor (with GPU support):\n```\npip install tensorflow_gpu\n```\n\nFor more details about TensorFlow installation, you can check [TensorFlow Installation Guide](https://www.tensorflow.org/install/)\n\n\n## TensorFlow v1 Examples - Index\n\nThe tutorial index for TF v1 is available here: [TensorFlow v1.15 Examples](tensorflow_v1).\n\n#### 0 - Prerequisite\n- [Introduction to Machine Learning](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/tensorflow_v1/0_Prerequisite/ml_introduction.ipynb).\n- [Introduction to MNIST Dataset](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/tensorflow_v1/0_Prerequisite/mnist_dataset_intro.ipynb).\n\n#### 1 - Introduction\n- **Hello World** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/1_Introduction/helloworld.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/1_Introduction/helloworld.py)). Very simple example to learn how to print \"hello world\" using TensorFlow.\n- **Basic Operations** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/tensorflow_v1/1_Introduction/basic_operations.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-examples/Examples/blob/master/tensorflow_v1/1_Introduction/basic_operations.py)). A simple example that cover TensorFlow basic operations.\n- **TensorFlow Eager API basics** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/1_Introduction/basic_eager_api.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/1_Introduction/basic_eager_api.py)). Get started with TensorFlow's Eager API.\n\n#### 2 - Basic Models\n- **Linear Regression** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/2_BasicModels/linear_regression.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/2_BasicModels/linear_regression.py)). Implement a Linear Regression with TensorFlow.\n- **Linear Regression (eager api)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/2_BasicModels/linear_regression_eager_api.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/2_BasicModels/linear_regression_eager_api.py)). Implement a Linear Regression using TensorFlow's Eager API.\n- **Logistic Regression** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/2_BasicModels/logistic_regression.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/2_BasicModels/logistic_regression.py)). Implement a Logistic Regression with TensorFlow.\n- **Logistic Regression (eager api)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/2_BasicModels/logistic_regression_eager_api.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/2_BasicModels/logistic_regression_eager_api.py)). Implement a Logistic Regression using TensorFlow's Eager API.\n- **Nearest Neighbor** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/2_BasicModels/nearest_neighbor.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/2_BasicModels/nearest_neighbor.py)). Implement Nearest Neighbor algorithm with TensorFlow.\n- **K-Means** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/2_BasicModels/kmeans.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/2_BasicModels/kmeans.py)). Build a K-Means classifier with TensorFlow.\n- **Random Forest** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/2_BasicModels/random_forest.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/2_BasicModels/random_forest.py)). Build a Random Forest classifier with TensorFlow.\n- **Gradient Boosted Decision Tree (GBDT)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/2_BasicModels/gradient_boosted_decision_tree.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/2_BasicModels/gradient_boosted_decision_tree.py)). Build a Gradient Boosted Decision Tree (GBDT) with TensorFlow.\n- **Word2Vec (Word Embedding)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/2_BasicModels/word2vec.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/2_BasicModels/word2vec.py)). Build a Word Embedding Model (Word2Vec) from Wikipedia data, with TensorFlow.\n\n#### 3 - Neural Networks\n##### Supervised\n\n- **Simple Neural Network** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/3_NeuralNetworks/notebooks/neural_network_raw.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/3_NeuralNetworks/neural_network_raw.py)). Build a simple neural network (a.k.a Multi-layer Perceptron) to classify MNIST digits dataset. Raw TensorFlow implementation.\n- **Simple Neural Network (tf.layers/estimator api)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/3_NeuralNetworks/neural_network.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/3_NeuralNetworks/neural_network.py)). Use TensorFlow 'layers' and 'estimator' API to build a simple neural network (a.k.a Multi-layer Perceptron) to classify MNIST digits dataset.\n- **Simple Neural Network (eager api)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/3_NeuralNetworks/neural_network_eager_api.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/3_NeuralNetworks/neural_network_eager_api.py)). Use TensorFlow Eager API to build a simple neural network (a.k.a Multi-layer Perceptron) to classify MNIST digits dataset.\n- **Convolutional Neural Network** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/3_NeuralNetworks/convolutional_network_raw.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/3_NeuralNetworks/convolutional_network_raw.py)). Build a convolutional neural network to classify MNIST digits dataset. Raw TensorFlow implementation.\n- **Convolutional Neural Network (tf.layers/estimator api)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/3_NeuralNetworks/convolutional_network.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/3_NeuralNetworks/convolutional_network.py)). Use TensorFlow 'layers' and 'estimator' API to build a convolutional neural network to classify MNIST digits dataset.\n- **Recurrent Neural Network (LSTM)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/3_NeuralNetworks/recurrent_network.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/3_NeuralNetworks/recurrent_network.py)). Build a recurrent neural network (LSTM) to classify MNIST digits dataset.\n- **Bi-directional Recurrent Neural Network (LSTM)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/3_NeuralNetworks/bidirectional_rnn.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/3_NeuralNetworks/bidirectional_rnn.py)). Build a bi-directional recurrent neural network (LSTM) to classify MNIST digits dataset.\n- **Dynamic Recurrent Neural Network (LSTM)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/3_NeuralNetworks/dynamic_rnn.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/3_NeuralNetworks/dynamic_rnn.py)). Build a recurrent neural network (LSTM) that performs dynamic calculation to classify sequences of different length.\n\n##### Unsupervised\n- **Auto-Encoder** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/3_NeuralNetworks/autoencoder.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/3_NeuralNetworks/autoencoder.py)). Build an auto-encoder to encode an image to a lower dimension and re-construct it.\n- **Variational Auto-Encoder** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/3_NeuralNetworks/variational_autoencoder.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/3_NeuralNetworks/variational_autoencoder.py)). Build a variational auto-encoder (VAE), to encode and generate images from noise.\n- **GAN (Generative Adversarial Networks)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/3_NeuralNetworks/gan.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/3_NeuralNetworks/gan.py)). Build a Generative Adversarial Network (GAN) to generate images from noise.\n- **DCGAN (Deep Convolutional Generative Adversarial Networks)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/3_NeuralNetworks/dcgan.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/3_NeuralNetworks/dcgan.py)). Build a Deep Convolutional Generative Adversarial Network (DCGAN) to generate images from noise.\n\n#### 4 - Utilities\n- **Save and Restore a model** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/4_Utils/save_restore_model.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/4_Utils/save_restore_model.py)). Save and Restore a model with TensorFlow.\n- **Tensorboard - Graph and loss visualization** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/4_Utils/tensorboard_basic.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/4_Utils/tensorboard_basic.py)). Use Tensorboard to visualize the computation Graph and plot the loss.\n- **Tensorboard - Advanced visualization** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/4_Utils/tensorboard_advanced.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/4_Utils/tensorboard_advanced.py)). Going deeper into Tensorboard; visualize the variables, gradients, and more...\n\n#### 5 - Data Management\n- **Build an image dataset** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/5_DataManagement/build_an_image_dataset.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/5_DataManagement/build_an_image_dataset.py)). Build your own images dataset with TensorFlow data queues, from image folders or a dataset file.\n- **TensorFlow Dataset API** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/5_DataManagement/tensorflow_dataset_api.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/5_DataManagement/tensorflow_dataset_api.py)). Introducing TensorFlow Dataset API for optimizing the input data pipeline.\n- **Load and Parse data** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/5_DataManagement/load_data.ipynb)). Build efficient data pipeline (Numpy arrays, Images, CSV files, custom data, ...).\n- **Build and Load TFRecords** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/5_DataManagement/tfrecords.ipynb)). Convert data into TFRecords format, and load them.\n- **Image Transformation (i.e. Image Augmentation)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/5_DataManagement/image_transformation.ipynb)). Apply various image augmentation techniques, to generate distorted images for training.\n\n#### 6 - Multi GPU\n- **Basic Operations on multi-GPU** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/6_MultiGPU/multigpu_basics.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/6_MultiGPU/multigpu_basics.py)). A simple example to introduce multi-GPU in TensorFlow.\n- **Train a Neural Network on multi-GPU** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/6_MultiGPU/multigpu_cnn.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/6_MultiGPU/multigpu_cnn.py)). A clear and simple TensorFlow implementation to train a convolutional neural network on multiple GPUs.\n\n## More Examples\nThe following examples are coming from [TFLearn](https://github.com/tflearn/tflearn), a library that provides a simplified interface for TensorFlow. You can have a look, there are many [examples](https://github.com/tflearn/tflearn/tree/master/examples) and [pre-built operations and layers](http://tflearn.org/doc_index/#api).\n\n### Tutorials\n- [TFLearn Quickstart](https://github.com/tflearn/tflearn/blob/master/tutorials/intro/quickstart.md). Learn the basics of TFLearn through a concrete machine learning task. Build and train a deep neural network classifier.\n\n### Examples\n- [TFLearn Examples](https://github.com/tflearn/tflearn/blob/master/examples). A large collection of examples using TFLearn.\n\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 265612440,
    "name": "TTS",
    "full_name": "coqui-ai/TTS",
    "description": "\ud83d\udc38\ud83d\udcac - a deep learning toolkit for Text-to-Speech, battle-tested in research and production",
    "html_url": "https://github.com/coqui-ai/TTS",
    "clone_url": "https://github.com/coqui-ai/TTS.git",
    "owner_login": "coqui-ai",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/75583352?v=4",
    "stargazers_count": 41833,
    "watchers_count": 41833,
    "forks_count": 5473,
    "open_issues_count": 4,
    "size": 170196,
    "language": "Python",
    "languages": {
      "Python": 2992534,
      "Jupyter Notebook": 244647,
      "HTML": 8448,
      "Shell": 4288,
      "Makefile": 2246,
      "Cython": 1236,
      "Dockerfile": 1015
    },
    "topics": [
      "deep-learning",
      "glow-tts",
      "hifigan",
      "melgan",
      "multi-speaker-tts",
      "python",
      "pytorch",
      "speaker-encoder",
      "speaker-encodings",
      "speech",
      "speech-synthesis",
      "tacotron",
      "text-to-speech",
      "tts",
      "tts-model",
      "vocoder",
      "voice-cloning",
      "voice-conversion",
      "voice-synthesis"
    ],
    "license_name": "Mozilla Public License 2.0",
    "created_at": "2020-05-20T15:45:28+00:00",
    "updated_at": "2025-08-05T22:22:00+00:00",
    "pushed_at": "2024-08-16T12:07:14+00:00",
    "contributors_count": 100,
    "readme_length": 17278,
    "readme_content": "\n## \ud83d\udc38Coqui.ai News\n- \ud83d\udce3 \u24cdTTSv2 is here with 16 languages and better performance across the board.\n- \ud83d\udce3 \u24cdTTS fine-tuning code is out. Check the [example recipes](https://github.com/coqui-ai/TTS/tree/dev/recipes/ljspeech).\n- \ud83d\udce3 \u24cdTTS can now stream with <200ms latency.\n- \ud83d\udce3 \u24cdTTS, our production TTS model that can speak 13 languages, is released [Blog Post](https://coqui.ai/blog/tts/open_xtts), [Demo](https://huggingface.co/spaces/coqui/xtts), [Docs](https://tts.readthedocs.io/en/dev/models/xtts.html)\n- \ud83d\udce3 [\ud83d\udc36Bark](https://github.com/suno-ai/bark) is now available for inference with unconstrained voice cloning. [Docs](https://tts.readthedocs.io/en/dev/models/bark.html)\n- \ud83d\udce3 You can use [~1100 Fairseq models](https://github.com/facebookresearch/fairseq/tree/main/examples/mms) with \ud83d\udc38TTS.\n- \ud83d\udce3 \ud83d\udc38TTS now supports \ud83d\udc22Tortoise with faster inference. [Docs](https://tts.readthedocs.io/en/dev/models/tortoise.html)\n\n<div align=\"center\">\n<img src=\"https://static.scarf.sh/a.png?x-pxid=cf317fe7-2188-4721-bc01-124bb5d5dbb2\" />\n\n## <img src=\"https://raw.githubusercontent.com/coqui-ai/TTS/main/images/coqui-log-green-TTS.png\" height=\"56\"/>\n\n\n**\ud83d\udc38TTS is a library for advanced Text-to-Speech generation.**\n\n\ud83d\ude80 Pretrained models in +1100 languages.\n\n\ud83d\udee0\ufe0f Tools for training new models and fine-tuning existing models in any language.\n\n\ud83d\udcda Utilities for dataset analysis and curation.\n______________________________________________________________________\n\n[![Discord](https://img.shields.io/discord/1037326658807533628?color=%239B59B6&label=chat%20on%20discord)](https://discord.gg/5eXr5seRrv)\n[![License](<https://img.shields.io/badge/License-MPL%202.0-brightgreen.svg>)](https://opensource.org/licenses/MPL-2.0)\n[![PyPI version](https://badge.fury.io/py/TTS.svg)](https://badge.fury.io/py/TTS)\n[![Covenant](https://camo.githubusercontent.com/7d620efaa3eac1c5b060ece5d6aacfcc8b81a74a04d05cd0398689c01c4463bb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6e7472696275746f72253230436f76656e616e742d76322e3025323061646f707465642d6666363962342e737667)](https://github.com/coqui-ai/TTS/blob/master/CODE_OF_CONDUCT.md)\n[![Downloads](https://pepy.tech/badge/tts)](https://pepy.tech/project/tts)\n[![DOI](https://zenodo.org/badge/265612440.svg)](https://zenodo.org/badge/latestdoi/265612440)\n\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/aux_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/data_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/docker.yaml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/inference_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/style_check.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/text_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/tts_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/vocoder_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/zoo_tests0.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/zoo_tests1.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/zoo_tests2.yml/badge.svg)\n[![Docs](<https://readthedocs.org/projects/tts/badge/?version=latest&style=plastic>)](https://tts.readthedocs.io/en/latest/)\n\n</div>\n\n______________________________________________________________________\n\n## \ud83d\udcac Where to ask questions\nPlease use our dedicated channels for questions and discussion. Help is much more valuable if it's shared publicly so that more people can benefit from it.\n\n| Type                            | Platforms                               |\n| ------------------------------- | --------------------------------------- |\n| \ud83d\udea8 **Bug Reports**              | [GitHub Issue Tracker]                  |\n| \ud83c\udf81 **Feature Requests & Ideas** | [GitHub Issue Tracker]                  |\n| \ud83d\udc69\u200d\ud83d\udcbb **Usage Questions**          | [GitHub Discussions]                    |\n| \ud83d\uddef **General Discussion**       | [GitHub Discussions] or [Discord]   |\n\n[github issue tracker]: https://github.com/coqui-ai/tts/issues\n[github discussions]: https://github.com/coqui-ai/TTS/discussions\n[discord]: https://discord.gg/5eXr5seRrv\n[Tutorials and Examples]: https://github.com/coqui-ai/TTS/wiki/TTS-Notebooks-and-Tutorials\n\n\n## \ud83d\udd17 Links and Resources\n| Type                            | Links                               |\n| ------------------------------- | --------------------------------------- |\n| \ud83d\udcbc **Documentation**              | [ReadTheDocs](https://tts.readthedocs.io/en/latest/)\n| \ud83d\udcbe **Installation**               | [TTS/README.md](https://github.com/coqui-ai/TTS/tree/dev#installation)|\n| \ud83d\udc69\u200d\ud83d\udcbb **Contributing**               | [CONTRIBUTING.md](https://github.com/coqui-ai/TTS/blob/main/CONTRIBUTING.md)|\n| \ud83d\udccc **Road Map**                   | [Main Development Plans](https://github.com/coqui-ai/TTS/issues/378)\n| \ud83d\ude80 **Released Models**            | [TTS Releases](https://github.com/coqui-ai/TTS/releases) and [Experimental Models](https://github.com/coqui-ai/TTS/wiki/Experimental-Released-Models)|\n| \ud83d\udcf0 **Papers**                    | [TTS Papers](https://github.com/erogol/TTS-papers)|\n\n\n## \ud83e\udd47 TTS Performance\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/coqui-ai/TTS/main/images/TTS-performance.png\" width=\"800\" /></p>\n\nUnderlined \"TTS*\" and \"Judy*\" are **internal** \ud83d\udc38TTS models that are not released open-source. They are here to show the potential. Models prefixed with a dot (.Jofish .Abe and .Janice) are real human voices.\n\n## Features\n- High-performance Deep Learning models for Text2Speech tasks.\n    - Text2Spec models (Tacotron, Tacotron2, Glow-TTS, SpeedySpeech).\n    - Speaker Encoder to compute speaker embeddings efficiently.\n    - Vocoder models (MelGAN, Multiband-MelGAN, GAN-TTS, ParallelWaveGAN, WaveGrad, WaveRNN)\n- Fast and efficient model training.\n- Detailed training logs on the terminal and Tensorboard.\n- Support for Multi-speaker TTS.\n- Efficient, flexible, lightweight but feature complete `Trainer API`.\n- Released and ready-to-use models.\n- Tools to curate Text2Speech datasets under```dataset_analysis```.\n- Utilities to use and test your models.\n- Modular (but not too much) code base enabling easy implementation of new ideas.\n\n## Model Implementations\n### Spectrogram models\n- Tacotron: [paper](https://arxiv.org/abs/1703.10135)\n- Tacotron2: [paper](https://arxiv.org/abs/1712.05884)\n- Glow-TTS: [paper](https://arxiv.org/abs/2005.11129)\n- Speedy-Speech: [paper](https://arxiv.org/abs/2008.03802)\n- Align-TTS: [paper](https://arxiv.org/abs/2003.01950)\n- FastPitch: [paper](https://arxiv.org/pdf/2006.06873.pdf)\n- FastSpeech: [paper](https://arxiv.org/abs/1905.09263)\n- FastSpeech2: [paper](https://arxiv.org/abs/2006.04558)\n- SC-GlowTTS: [paper](https://arxiv.org/abs/2104.05557)\n- Capacitron: [paper](https://arxiv.org/abs/1906.03402)\n- OverFlow: [paper](https://arxiv.org/abs/2211.06892)\n- Neural HMM TTS: [paper](https://arxiv.org/abs/2108.13320)\n- Delightful TTS: [paper](https://arxiv.org/abs/2110.12612)\n\n### End-to-End Models\n- \u24cdTTS: [blog](https://coqui.ai/blog/tts/open_xtts)\n- VITS: [paper](https://arxiv.org/pdf/2106.06103)\n- \ud83d\udc38 YourTTS: [paper](https://arxiv.org/abs/2112.02418)\n- \ud83d\udc22 Tortoise: [orig. repo](https://github.com/neonbjb/tortoise-tts)\n- \ud83d\udc36 Bark: [orig. repo](https://github.com/suno-ai/bark)\n\n### Attention Methods\n- Guided Attention: [paper](https://arxiv.org/abs/1710.08969)\n- Forward Backward Decoding: [paper](https://arxiv.org/abs/1907.09006)\n- Graves Attention: [paper](https://arxiv.org/abs/1910.10288)\n- Double Decoder Consistency: [blog](https://erogol.com/solving-attention-problems-of-tts-models-with-double-decoder-consistency/)\n- Dynamic Convolutional Attention: [paper](https://arxiv.org/pdf/1910.10288.pdf)\n- Alignment Network: [paper](https://arxiv.org/abs/2108.10447)\n\n### Speaker Encoder\n- GE2E: [paper](https://arxiv.org/abs/1710.10467)\n- Angular Loss: [paper](https://arxiv.org/pdf/2003.11982.pdf)\n\n### Vocoders\n- MelGAN: [paper](https://arxiv.org/abs/1910.06711)\n- MultiBandMelGAN: [paper](https://arxiv.org/abs/2005.05106)\n- ParallelWaveGAN: [paper](https://arxiv.org/abs/1910.11480)\n- GAN-TTS discriminators: [paper](https://arxiv.org/abs/1909.11646)\n- WaveRNN: [origin](https://github.com/fatchord/WaveRNN/)\n- WaveGrad: [paper](https://arxiv.org/abs/2009.00713)\n- HiFiGAN: [paper](https://arxiv.org/abs/2010.05646)\n- UnivNet: [paper](https://arxiv.org/abs/2106.07889)\n\n### Voice Conversion\n- FreeVC: [paper](https://arxiv.org/abs/2210.15418)\n\nYou can also help us implement more models.\n\n## Installation\n\ud83d\udc38TTS is tested on Ubuntu 18.04 with **python >= 3.9, < 3.12.**.\n\nIf you are only interested in [synthesizing speech](https://tts.readthedocs.io/en/latest/inference.html) with the released \ud83d\udc38TTS models, installing from PyPI is the easiest option.\n\n```bash\npip install TTS\n```\n\nIf you plan to code or train models, clone \ud83d\udc38TTS and install it locally.\n\n```bash\ngit clone https://github.com/coqui-ai/TTS\npip install -e .[all,dev,notebooks]  # Select the relevant extras\n```\n\nIf you are on Ubuntu (Debian), you can also run following commands for installation.\n\n```bash\n$ make system-deps  # intended to be used on Ubuntu (Debian). Let us know if you have a different OS.\n$ make install\n```\n\nIf you are on Windows, \ud83d\udc51@GuyPaddock wrote installation instructions [here](https://stackoverflow.com/questions/66726331/how-can-i-run-mozilla-tts-coqui-tts-training-with-cuda-on-a-windows-system).\n\n\n## Docker Image\nYou can also try TTS without install with the docker image.\nSimply run the following command and you will be able to run TTS without installing it.\n\n```bash\ndocker run --rm -it -p 5002:5002 --entrypoint /bin/bash ghcr.io/coqui-ai/tts-cpu\npython3 TTS/server/server.py --list_models #To get the list of available models\npython3 TTS/server/server.py --model_name tts_models/en/vctk/vits # To start a server\n```\n\nYou can then enjoy the TTS server [here](http://[::1]:5002/)\nMore details about the docker images (like GPU support) can be found [here](https://tts.readthedocs.io/en/latest/docker_images.html)\n\n\n## Synthesizing speech by \ud83d\udc38TTS\n\n### \ud83d\udc0d Python API\n\n#### Running a multi-speaker and multi-lingual model\n\n```python\nimport torch\nfrom TTS.api import TTS\n\n# Get device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# List available \ud83d\udc38TTS models\nprint(TTS().list_models())\n\n# Init TTS\ntts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device)\n\n# Run TTS\n# \u2757 Since this model is multi-lingual voice cloning model, we must set the target speaker_wav and language\n# Text to speech list of amplitude values as output\nwav = tts.tts(text=\"Hello world!\", speaker_wav=\"my/cloning/audio.wav\", language=\"en\")\n# Text to speech to a file\ntts.tts_to_file(text=\"Hello world!\", speaker_wav=\"my/cloning/audio.wav\", language=\"en\", file_path=\"output.wav\")\n```\n\n#### Running a single speaker model\n\n```python\n# Init TTS with the target model name\ntts = TTS(model_name=\"tts_models/de/thorsten/tacotron2-DDC\", progress_bar=False).to(device)\n\n# Run TTS\ntts.tts_to_file(text=\"Ich bin eine Testnachricht.\", file_path=OUTPUT_PATH)\n\n# Example voice cloning with YourTTS in English, French and Portuguese\ntts = TTS(model_name=\"tts_models/multilingual/multi-dataset/your_tts\", progress_bar=False).to(device)\ntts.tts_to_file(\"This is voice cloning.\", speaker_wav=\"my/cloning/audio.wav\", language=\"en\", file_path=\"output.wav\")\ntts.tts_to_file(\"C'est le clonage de la voix.\", speaker_wav=\"my/cloning/audio.wav\", language=\"fr-fr\", file_path=\"output.wav\")\ntts.tts_to_file(\"Isso \u00e9 clonagem de voz.\", speaker_wav=\"my/cloning/audio.wav\", language=\"pt-br\", file_path=\"output.wav\")\n```\n\n#### Example voice conversion\n\nConverting the voice in `source_wav` to the voice of `target_wav`\n\n```python\ntts = TTS(model_name=\"voice_conversion_models/multilingual/vctk/freevc24\", progress_bar=False).to(\"cuda\")\ntts.voice_conversion_to_file(source_wav=\"my/source.wav\", target_wav=\"my/target.wav\", file_path=\"output.wav\")\n```\n\n#### Example voice cloning together with the voice conversion model.\nThis way, you can clone voices by using any model in \ud83d\udc38TTS.\n\n```python\n\ntts = TTS(\"tts_models/de/thorsten/tacotron2-DDC\")\ntts.tts_with_vc_to_file(\n    \"Wie sage ich auf Italienisch, dass ich dich liebe?\",\n    speaker_wav=\"target/speaker.wav\",\n    file_path=\"output.wav\"\n)\n```\n\n#### Example text to speech using **Fairseq models in ~1100 languages** \ud83e\udd2f.\nFor Fairseq models, use the following name format: `tts_models/<lang-iso_code>/fairseq/vits`.\nYou can find the language ISO codes [here](https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html)\nand learn about the Fairseq models [here](https://github.com/facebookresearch/fairseq/tree/main/examples/mms).\n\n```python\n# TTS with on the fly voice conversion\napi = TTS(\"tts_models/deu/fairseq/vits\")\napi.tts_with_vc_to_file(\n    \"Wie sage ich auf Italienisch, dass ich dich liebe?\",\n    speaker_wav=\"target/speaker.wav\",\n    file_path=\"output.wav\"\n)\n```\n\n### Command-line `tts`\n\n<!-- begin-tts-readme -->\n\nSynthesize speech on command line.\n\nYou can either use your trained model or choose a model from the provided list.\n\nIf you don't specify any models, then it uses LJSpeech based English model.\n\n#### Single Speaker Models\n\n- List provided models:\n\n  ```\n  $ tts --list_models\n  ```\n\n- Get model info (for both tts_models and vocoder_models):\n\n  - Query by type/name:\n    The model_info_by_name uses the name as it from the --list_models.\n    ```\n    $ tts --model_info_by_name \"<model_type>/<language>/<dataset>/<model_name>\"\n    ```\n    For example:\n    ```\n    $ tts --model_info_by_name tts_models/tr/common-voice/glow-tts\n    $ tts --model_info_by_name vocoder_models/en/ljspeech/hifigan_v2\n    ```\n  - Query by type/idx:\n    The model_query_idx uses the corresponding idx from --list_models.\n\n    ```\n    $ tts --model_info_by_idx \"<model_type>/<model_query_idx>\"\n    ```\n\n    For example:\n\n    ```\n    $ tts --model_info_by_idx tts_models/3\n    ```\n\n  - Query info for model info by full name:\n    ```\n    $ tts --model_info_by_name \"<model_type>/<language>/<dataset>/<model_name>\"\n    ```\n\n- Run TTS with default models:\n\n  ```\n  $ tts --text \"Text for TTS\" --out_path output/path/speech.wav\n  ```\n\n- Run TTS and pipe out the generated TTS wav file data:\n\n  ```\n  $ tts --text \"Text for TTS\" --pipe_out --out_path output/path/speech.wav | aplay\n  ```\n\n- Run a TTS model with its default vocoder model:\n\n  ```\n  $ tts --text \"Text for TTS\" --model_name \"<model_type>/<language>/<dataset>/<model_name>\" --out_path output/path/speech.wav\n  ```\n\n  For example:\n\n  ```\n  $ tts --text \"Text for TTS\" --model_name \"tts_models/en/ljspeech/glow-tts\" --out_path output/path/speech.wav\n  ```\n\n- Run with specific TTS and vocoder models from the list:\n\n  ```\n  $ tts --text \"Text for TTS\" --model_name \"<model_type>/<language>/<dataset>/<model_name>\" --vocoder_name \"<model_type>/<language>/<dataset>/<model_name>\" --out_path output/path/speech.wav\n  ```\n\n  For example:\n\n  ```\n  $ tts --text \"Text for TTS\" --model_name \"tts_models/en/ljspeech/glow-tts\" --vocoder_name \"vocoder_models/en/ljspeech/univnet\" --out_path output/path/speech.wav\n  ```\n\n- Run your own TTS model (Using Griffin-Lim Vocoder):\n\n  ```\n  $ tts --text \"Text for TTS\" --model_path path/to/model.pth --config_path path/to/config.json --out_path output/path/speech.wav\n  ```\n\n- Run your own TTS and Vocoder models:\n\n  ```\n  $ tts --text \"Text for TTS\" --model_path path/to/model.pth --config_path path/to/config.json --out_path output/path/speech.wav\n      --vocoder_path path/to/vocoder.pth --vocoder_config_path path/to/vocoder_config.json\n  ```\n\n#### Multi-speaker Models\n\n- List the available speakers and choose a <speaker_id> among them:\n\n  ```\n  $ tts --model_name \"<language>/<dataset>/<model_name>\"  --list_speaker_idxs\n  ```\n\n- Run the multi-speaker TTS model with the target speaker ID:\n\n  ```\n  $ tts --text \"Text for TTS.\" --out_path output/path/speech.wav --model_name \"<language>/<dataset>/<model_name>\"  --speaker_idx <speaker_id>\n  ```\n\n- Run your own multi-speaker TTS model:\n\n  ```\n  $ tts --text \"Text for TTS\" --out_path output/path/speech.wav --model_path path/to/model.pth --config_path path/to/config.json --speakers_file_path path/to/speaker.json --speaker_idx <speaker_id>\n  ```\n\n### Voice Conversion Models\n\n```\n$ tts --out_path output/path/speech.wav --model_name \"<language>/<dataset>/<model_name>\" --source_wav <path/to/speaker/wav> --target_wav <path/to/reference/wav>\n```\n\n<!-- end-tts-readme -->\n\n## Directory Structure\n```\n|- notebooks/       (Jupyter Notebooks for model evaluation, parameter selection and data analysis.)\n|- utils/           (common utilities.)\n|- TTS\n    |- bin/             (folder for all the executables.)\n      |- train*.py                  (train your target model.)\n      |- ...\n    |- tts/             (text to speech models)\n        |- layers/          (model layer definitions)\n        |- models/          (model definitions)\n        |- utils/           (model specific utilities.)\n    |- speaker_encoder/ (Speaker Encoder models.)\n        |- (same)\n    |- vocoder/         (Vocoder models.)\n        |- (same)\n```\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 156157055,
    "name": "Made-With-ML",
    "full_name": "GokuMohandas/Made-With-ML",
    "description": "Learn how to design, develop, deploy and iterate on production-grade ML applications.",
    "html_url": "https://github.com/GokuMohandas/Made-With-ML",
    "clone_url": "https://github.com/GokuMohandas/Made-With-ML.git",
    "owner_login": "GokuMohandas",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/8000987?v=4",
    "stargazers_count": 41612,
    "watchers_count": 41612,
    "forks_count": 6479,
    "open_issues_count": 20,
    "size": 4001,
    "language": "Jupyter Notebook",
    "languages": {
      "Jupyter Notebook": 2725871,
      "Python": 58624,
      "Shell": 2047,
      "Makefile": 415
    },
    "topics": [
      "data-engineering",
      "data-quality",
      "data-science",
      "deep-learning",
      "distributed-ml",
      "distributed-training",
      "llms",
      "machine-learning",
      "mlops",
      "natural-language-processing",
      "python",
      "pytorch",
      "ray"
    ],
    "license_name": "MIT License",
    "created_at": "2018-11-05T03:44:27+00:00",
    "updated_at": "2025-08-06T01:37:46+00:00",
    "pushed_at": "2024-08-18T04:34:15+00:00",
    "contributors_count": 1,
    "readme_length": 23434,
    "readme_content": "<div align=\"center\">\n<h1><img width=\"30\" src=\"https://madewithml.com/static/images/rounded_logo.png\">&nbsp;<a href=\"https://madewithml.com/\">Made With ML</a></h1>\nDesign \u00b7 Develop \u00b7 Deploy \u00b7 Iterate\n<br>\nJoin 40K+ developers in learning how to responsibly deliver value with ML.\n    <br>\n</div>\n\n<br>\n\n<div align=\"center\">\n    <a target=\"_blank\" href=\"https://madewithml.com/\"><img src=\"https://img.shields.io/badge/Subscribe-40K-brightgreen\"></a>&nbsp;\n    <a target=\"_blank\" href=\"https://github.com/GokuMohandas/Made-With-ML\"><img src=\"https://img.shields.io/github/stars/GokuMohandas/Made-With-ML.svg?style=social&label=Star\"></a>&nbsp;\n    <a target=\"_blank\" href=\"https://www.linkedin.com/in/goku\"><img src=\"https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&logo=linkedin&style=social\"></a>&nbsp;\n    <a target=\"_blank\" href=\"https://twitter.com/GokuMohandas\"><img src=\"https://img.shields.io/twitter/follow/GokuMohandas.svg?label=Follow&style=social\"></a>\n    <br>\n    \ud83d\udd25&nbsp; Among the <a href=\"https://github.com/GokuMohandas/Made-With-ML\" target=\"_blank\">top ML repositories</a> on GitHub\n</div>\n\n<br>\n<hr>\n\n## Lessons\n\nLearn how to combine machine learning with software engineering to design, develop, deploy and iterate on production-grade ML applications.\n\n- Lessons: https://madewithml.com/\n- Code: [GokuMohandas/Made-With-ML](https://github.com/GokuMohandas/Made-With-ML)\n\n<a href=\"https://madewithml.com/#course\">\n  <img src=\"https://madewithml.com/static/images/lessons.png\" alt=\"lessons\">\n</a>\n\n## Overview\n\nIn this course, we'll go from experimentation (design + development) to production (deployment + iteration). We'll do this iteratively by motivating the components that will enable us to build a *reliable* production system.\n\n<blockquote>\n  <img width=20 src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/YouTube_full-color_icon_%282017%29.svg/640px-YouTube_full-color_icon_%282017%29.svg.png\">&nbsp; Be sure to watch the video below for a quick overview of what we'll be building.\n</blockquote>\n\n<div align=\"center\">\n  <a href=\"https://youtu.be/AWgkt8H8yVo\"><img src=\"https://img.youtube.com/vi/AWgkt8H8yVo/0.jpg\" alt=\"Course overview video\"></a>\n</div>\n\n<br>\n\n- **\ud83d\udca1 First principles**: before we jump straight into the code, we develop a first principles understanding for every machine learning concept.\n- **\ud83d\udcbb Best practices**: implement software engineering best practices as we develop and deploy our machine learning models.\n- **\ud83d\udcc8 Scale**: easily scale ML workloads (data, train, tune, serve) in Python without having to learn completely new languages.\n- **\u2699\ufe0f MLOps**: connect MLOps components (tracking, testing, serving, orchestration, etc.) as we build an end-to-end machine learning system.\n- **\ud83d\ude80 Dev to Prod**: learn how to quickly and reliably go from development to production without any changes to our code or infra management.\n- **\ud83d\udc19 CI/CD**: learn how to create mature CI/CD workflows to continuously train and deploy better models in a modular way that integrates with any stack.\n\n## Audience\n\nMachine learning is not a separate industry, instead, it's a powerful way of thinking about data that's not reserved for any one type of person.\n\n- **\ud83d\udc69\u200d\ud83d\udcbb All developers**: whether software/infra engineer or data scientist, ML is increasingly becoming a key part of the products that you'll be developing.\n- **\ud83d\udc69\u200d\ud83c\udf93 College graduates**: learn the practical skills required for industry and bridge gap between the university curriculum and what industry expects.\n- **\ud83d\udc69\u200d\ud83d\udcbc Product/Leadership**: who want to develop a technical foundation so that they can build amazing (and reliable) products powered by machine learning.\n\n## Set up\n\nBe sure to go through the [course](https://madewithml/#course) for a much more detailed walkthrough of the content on this repository. We will have instructions for both local laptop and Anyscale clusters for the sections below, so be sure to toggle the \u25ba dropdown based on what you're using (Anyscale instructions will be toggled on by default). If you do want to run this course with Anyscale, where we'll provide the **structure**, **compute (GPUs)** and **community** to learn everything in one day, join our next upcoming live cohort \u2192 [sign up here](https://4190urw86oh.typeform.com/madewithml)!\n\n### Cluster\n\nWe'll start by setting up our cluster with the environment and compute configurations.\n\n<details>\n  <summary>Local</summary><br>\n  Your personal laptop (single machine) will act as the cluster, where one CPU will be the head node and some of the remaining CPU will be the worker nodes. All of the code in this course will work in any personal laptop though it will be slower than executing the same workloads on a larger cluster.\n</details>\n\n<details open>\n  <summary>Anyscale</summary><br>\n\n  We can create an [Anyscale Workspace](https://docs.anyscale.com/develop/workspaces/get-started) using the [webpage UI](https://console.anyscale.com/o/madewithml/workspaces/add/blank).\n\n  ```md\n  - Workspace name: `madewithml`\n  - Project: `madewithml`\n  - Cluster environment name: `madewithml-cluster-env`\n  # Toggle `Select from saved configurations`\n  - Compute config: `madewithml-cluster-compute-g5.4xlarge`\n  ```\n\n  > Alternatively, we can use the [CLI](https://docs.anyscale.com/reference/anyscale-cli) to create the workspace via `anyscale workspace create ...`\n\n</details>\n\n<details>\n  <summary>Other (cloud platforms, K8s, on-prem)</summary><br>\n\n  If you don't want to do this course locally or via Anyscale, you have the following options:\n\n  - On [AWS and GCP](https://docs.ray.io/en/latest/cluster/vms/index.html#cloud-vm-index). Community-supported Azure and Aliyun integrations also exist.\n  - On [Kubernetes](https://docs.ray.io/en/latest/cluster/kubernetes/index.html#kuberay-index), via the officially supported KubeRay project.\n  - Deploy Ray manually [on-prem](https://docs.ray.io/en/latest/cluster/vms/user-guides/launching-clusters/on-premises.html#on-prem) or onto platforms [not listed here](https://docs.ray.io/en/latest/cluster/vms/user-guides/community/index.html#ref-cluster-setup).\n\n</details>\n\n### Git setup\n\nCreate a repository by following these instructions: [Create a new repository](https://github.com/new) \u2192 name it `Made-With-ML` \u2192 Toggle `Add a README file` (**very important** as this creates a `main` branch) \u2192 Click `Create repository` (scroll down)\n\nNow we're ready to clone the repository that has all of our code:\n\n```bash\ngit clone https://github.com/GokuMohandas/Made-With-ML.git .\n```\n\n### Credentials\n\n```bash\ntouch .env\n```\n```bash\n# Inside .env\nGITHUB_USERNAME=\"CHANGE_THIS_TO_YOUR_USERNAME\"  # \u2190 CHANGE THIS\n```\n```bash\nsource .env\n```\n\n### Virtual environment\n\n<details>\n  <summary>Local</summary><br>\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:$PWD\n  python3 -m venv venv  # recommend using Python 3.10\n  source venv/bin/activate  # on Windows: venv\\Scripts\\activate\n  python3 -m pip install --upgrade pip setuptools wheel\n  python3 -m pip install -r requirements.txt\n  pre-commit install\n  pre-commit autoupdate\n  ```\n\n  > Highly recommend using Python `3.10` and using [pyenv](https://github.com/pyenv/pyenv) (mac) or [pyenv-win](https://github.com/pyenv-win/pyenv-win) (windows).\n\n</details>\n\n<details open>\n  <summary>Anyscale</summary><br>\n\n  Our environment with the appropriate Python version and libraries is already all set for us through the cluster environment we used when setting up our Anyscale Workspace. So we just need to run these commands:\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:$PWD\n  pre-commit install\n  pre-commit autoupdate\n  ```\n\n</details>\n\n## Notebook\n\nStart by exploring the [jupyter notebook](notebooks/madewithml.ipynb) to interactively walkthrough the core machine learning workloads.\n\n<div align=\"center\">\n  <img src=\"https://madewithml.com/static/images/mlops/systems-design/workloads.png\">\n</div>\n\n<details>\n  <summary>Local</summary><br>\n\n  ```bash\n  # Start notebook\n  jupyter lab notebooks/madewithml.ipynb\n```\n\n</details>\n\n<details open>\n  <summary>Anyscale</summary><br>\n\n  Click on the Jupyter icon &nbsp;<img width=15 src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/38/Jupyter_logo.svg/1200px-Jupyter_logo.svg.png\">&nbsp; at the top right corner of our Anyscale Workspace page and this will open up our JupyterLab instance in a new tab. Then navigate to the `notebooks` directory and open up the `madewithml.ipynb` notebook.\n\n</details>\n\n\n## Scripts\n\nNow we'll execute the same workloads using the clean Python scripts following software engineering best practices (testing, documentation, logging, serving, versioning, etc.) The code we've implemented in our notebook will be refactored into the following scripts:\n\n```bash\nmadewithml\n\u251c\u2500\u2500 config.py\n\u251c\u2500\u2500 data.py\n\u251c\u2500\u2500 evaluate.py\n\u251c\u2500\u2500 models.py\n\u251c\u2500\u2500 predict.py\n\u251c\u2500\u2500 serve.py\n\u251c\u2500\u2500 train.py\n\u251c\u2500\u2500 tune.py\n\u2514\u2500\u2500 utils.py\n```\n\n**Note**: Change the `--num-workers`, `--cpu-per-worker`, and `--gpu-per-worker` input argument values below based on your system's resources. For example, if you're on a local laptop, a reasonable configuration would be `--num-workers 6 --cpu-per-worker 1 --gpu-per-worker 0`.\n\n### Training\n```bash\nexport EXPERIMENT_NAME=\"llm\"\nexport DATASET_LOC=\"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/dataset.csv\"\nexport TRAIN_LOOP_CONFIG='{\"dropout_p\": 0.5, \"lr\": 1e-4, \"lr_factor\": 0.8, \"lr_patience\": 3}'\npython madewithml/train.py \\\n    --experiment-name \"$EXPERIMENT_NAME\" \\\n    --dataset-loc \"$DATASET_LOC\" \\\n    --train-loop-config \"$TRAIN_LOOP_CONFIG\" \\\n    --num-workers 1 \\\n    --cpu-per-worker 3 \\\n    --gpu-per-worker 1 \\\n    --num-epochs 10 \\\n    --batch-size 256 \\\n    --results-fp results/training_results.json\n```\n\n### Tuning\n```bash\nexport EXPERIMENT_NAME=\"llm\"\nexport DATASET_LOC=\"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/dataset.csv\"\nexport TRAIN_LOOP_CONFIG='{\"dropout_p\": 0.5, \"lr\": 1e-4, \"lr_factor\": 0.8, \"lr_patience\": 3}'\nexport INITIAL_PARAMS=\"[{\\\"train_loop_config\\\": $TRAIN_LOOP_CONFIG}]\"\npython madewithml/tune.py \\\n    --experiment-name \"$EXPERIMENT_NAME\" \\\n    --dataset-loc \"$DATASET_LOC\" \\\n    --initial-params \"$INITIAL_PARAMS\" \\\n    --num-runs 2 \\\n    --num-workers 1 \\\n    --cpu-per-worker 3 \\\n    --gpu-per-worker 1 \\\n    --num-epochs 10 \\\n    --batch-size 256 \\\n    --results-fp results/tuning_results.json\n```\n\n### Experiment tracking\n\nWe'll use [MLflow](https://mlflow.org/) to track our experiments and store our models and the [MLflow Tracking UI](https://www.mlflow.org/docs/latest/tracking.html#tracking-ui) to view our experiments. We have been saving our experiments to a local directory but note that in an actual production setting, we would have a central location to store all of our experiments. It's easy/inexpensive to spin up your own MLflow server for all of your team members to track their experiments on or use a managed solution like [Weights & Biases](https://wandb.ai/site), [Comet](https://www.comet.ml/), etc.\n\n```bash\nexport MODEL_REGISTRY=$(python -c \"from madewithml import config; print(config.MODEL_REGISTRY)\")\nmlflow server -h 0.0.0.0 -p 8080 --backend-store-uri $MODEL_REGISTRY\n```\n\n<details>\n  <summary>Local</summary><br>\n\n  If you're running this notebook on your local laptop then head on over to <a href=\"http://localhost:8080/\" target=\"_blank\">http://localhost:8080/</a> to view your MLflow dashboard.\n\n</details>\n\n<details open>\n  <summary>Anyscale</summary><br>\n\n  If you're on <a href=\"https://docs.anyscale.com/develop/workspaces/get-started\" target=\"_blank\">Anyscale Workspaces</a>, then we need to first expose the port of the MLflow server. Run the following command on your Anyscale Workspace terminal to generate the public URL to your MLflow server.\n\n  ```bash\n  APP_PORT=8080\n  echo https://$APP_PORT-port-$ANYSCALE_SESSION_DOMAIN\n  ```\n\n</details>\n\n### Evaluation\n```bash\nexport EXPERIMENT_NAME=\"llm\"\nexport RUN_ID=$(python madewithml/predict.py get-best-run-id --experiment-name $EXPERIMENT_NAME --metric val_loss --mode ASC)\nexport HOLDOUT_LOC=\"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/holdout.csv\"\npython madewithml/evaluate.py \\\n    --run-id $RUN_ID \\\n    --dataset-loc $HOLDOUT_LOC \\\n    --results-fp results/evaluation_results.json\n```\n```json\n{\n  \"timestamp\": \"June 09, 2023 09:26:18 AM\",\n  \"run_id\": \"6149e3fec8d24f1492d4a4cabd5c06f6\",\n  \"overall\": {\n    \"precision\": 0.9076136428670714,\n    \"recall\": 0.9057591623036649,\n    \"f1\": 0.9046792827719773,\n    \"num_samples\": 191.0\n  },\n...\n```\n\n### Inference\n```bash\nexport EXPERIMENT_NAME=\"llm\"\nexport RUN_ID=$(python madewithml/predict.py get-best-run-id --experiment-name $EXPERIMENT_NAME --metric val_loss --mode ASC)\npython madewithml/predict.py predict \\\n    --run-id $RUN_ID \\\n    --title \"Transfer learning with transformers\" \\\n    --description \"Using transformers for transfer learning on text classification tasks.\"\n```\n```json\n[{\n  \"prediction\": [\n    \"natural-language-processing\"\n  ],\n  \"probabilities\": {\n    \"computer-vision\": 0.0009767753,\n    \"mlops\": 0.0008223939,\n    \"natural-language-processing\": 0.99762577,\n    \"other\": 0.000575123\n  }\n}]\n```\n\n### Serving\n\n<details>\n  <summary>Local</summary><br>\n\n  ```bash\n  # Start\n  ray start --head\n  ```\n\n  ```bash\n  # Set up\n  export EXPERIMENT_NAME=\"llm\"\n  export RUN_ID=$(python madewithml/predict.py get-best-run-id --experiment-name $EXPERIMENT_NAME --metric val_loss --mode ASC)\n  python madewithml/serve.py --run_id $RUN_ID\n  ```\n\n  Once the application is running, we can use it via cURL, Python, etc.:\n\n  ```python\n  # via Python\n  import json\n  import requests\n  title = \"Transfer learning with transformers\"\n  description = \"Using transformers for transfer learning on text classification tasks.\"\n  json_data = json.dumps({\"title\": title, \"description\": description})\n  requests.post(\"http://127.0.0.1:8000/predict\", data=json_data).json()\n  ```\n\n  ```bash\n  ray stop  # shutdown\n  ```\n\n</details>\n\n<details open>\n  <summary>Anyscale</summary><br>\n\n  In Anyscale Workspaces, Ray is already running so we don't have to manually start/shutdown like we have to do locally.\n\n  ```bash\n  # Set up\n  export EXPERIMENT_NAME=\"llm\"\n  export RUN_ID=$(python madewithml/predict.py get-best-run-id --experiment-name $EXPERIMENT_NAME --metric val_loss --mode ASC)\n  python madewithml/serve.py --run_id $RUN_ID\n  ```\n\n  Once the application is running, we can use it via cURL, Python, etc.:\n\n  ```python\n  # via Python\n  import json\n  import requests\n  title = \"Transfer learning with transformers\"\n  description = \"Using transformers for transfer learning on text classification tasks.\"\n  json_data = json.dumps({\"title\": title, \"description\": description})\n  requests.post(\"http://127.0.0.1:8000/predict\", data=json_data).json()\n  ```\n\n</details>\n\n### Testing\n```bash\n# Code\npython3 -m pytest tests/code --verbose --disable-warnings\n\n# Data\nexport DATASET_LOC=\"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/dataset.csv\"\npytest --dataset-loc=$DATASET_LOC tests/data --verbose --disable-warnings\n\n# Model\nexport EXPERIMENT_NAME=\"llm\"\nexport RUN_ID=$(python madewithml/predict.py get-best-run-id --experiment-name $EXPERIMENT_NAME --metric val_loss --mode ASC)\npytest --run-id=$RUN_ID tests/model --verbose --disable-warnings\n\n# Coverage\npython3 -m pytest tests/code --cov madewithml --cov-report html --disable-warnings  # html report\npython3 -m pytest tests/code --cov madewithml --cov-report term --disable-warnings  # terminal report\n```\n\n## Production\n\nFrom this point onwards, in order to deploy our application into production, we'll need to either be on Anyscale or on a [cloud VM](https://docs.ray.io/en/latest/cluster/vms/index.html#cloud-vm-index) / [on-prem](https://docs.ray.io/en/latest/cluster/vms/user-guides/launching-clusters/on-premises.html#on-prem) cluster you manage yourself (w/ Ray). If not on Anyscale, the commands will be [slightly different](https://docs.ray.io/en/latest/cluster/running-applications/job-submission/index.html) but the concepts will be the same.\n\n> If you don't want to set up all of this yourself, we highly recommend joining our [upcoming live cohort](https://4190urw86oh.typeform.com/madewithml){:target=\"_blank\"} where we'll provide an environment with all of this infrastructure already set up for you so that you just focused on the machine learning.\n\n<div align=\"center\">\n  <img src=\"https://madewithml.com/static/images/mlops/jobs_and_services/manual.png\">\n</div>\n\n### Authentication\n\nThese credentials below are **automatically** set for us if we're using Anyscale Workspaces. We **do not** need to set these credentials explicitly on Workspaces but we do if we're running this locally or on a cluster outside of where our Anyscale Jobs and Services are configured to run.\n\n``` bash\nexport ANYSCALE_HOST=https://console.anyscale.com\nexport ANYSCALE_CLI_TOKEN=$YOUR_CLI_TOKEN  # retrieved from Anyscale credentials page\n```\n\n### Cluster environment\n\nThe cluster environment determines **where** our workloads will be executed (OS, dependencies, etc.) We've already created this [cluster environment](./deploy/cluster_env.yaml) for us but this is how we can create/update one ourselves.\n\n```bash\nexport CLUSTER_ENV_NAME=\"madewithml-cluster-env\"\nanyscale cluster-env build deploy/cluster_env.yaml --name $CLUSTER_ENV_NAME\n```\n\n### Compute configuration\n\nThe compute configuration determines **what** resources our workloads will be executes on. We've already created this [compute configuration](./deploy/cluster_compute.yaml) for us but this is how we can create it ourselves.\n\n```bash\nexport CLUSTER_COMPUTE_NAME=\"madewithml-cluster-compute-g5.4xlarge\"\nanyscale cluster-compute create deploy/cluster_compute.yaml --name $CLUSTER_COMPUTE_NAME\n```\n\n### Anyscale jobs\n\nNow we're ready to execute our ML workloads. We've decided to combine them all together into one [job](./deploy/jobs/workloads.yaml) but we could have also created separate jobs for each workload (train, evaluate, etc.) We'll start by editing the `$GITHUB_USERNAME` slots inside our [`workloads.yaml`](./deploy/jobs/workloads.yaml) file:\n```yaml\nruntime_env:\n  working_dir: .\n  upload_path: s3://madewithml/$GITHUB_USERNAME/jobs  # <--- CHANGE USERNAME (case-sensitive)\n  env_vars:\n    GITHUB_USERNAME: $GITHUB_USERNAME  # <--- CHANGE USERNAME (case-sensitive)\n```\n\nThe `runtime_env` here specifies that we should upload our current `working_dir` to an S3 bucket so that all of our workers when we execute an Anyscale Job have access to the code to use. The `GITHUB_USERNAME` is used later to save results from our workloads to S3 so that we can retrieve them later (ex. for serving).\n\nNow we're ready to submit our job to execute our ML workloads:\n```bash\nanyscale job submit deploy/jobs/workloads.yaml\n```\n\n### Anyscale Services\n\nAnd after our ML workloads have been executed, we're ready to launch our serve our model to production. Similar to our Anyscale Jobs configs, be sure to change the `$GITHUB_USERNAME` in [`serve_model.yaml`](./deploy/services/serve_model.yaml).\n\n```yaml\nray_serve_config:\n  import_path: deploy.services.serve_model:entrypoint\n  runtime_env:\n    working_dir: .\n    upload_path: s3://madewithml/$GITHUB_USERNAME/services  # <--- CHANGE USERNAME (case-sensitive)\n    env_vars:\n      GITHUB_USERNAME: $GITHUB_USERNAME  # <--- CHANGE USERNAME (case-sensitive)\n```\n\nNow we're ready to launch our service:\n```bash\n# Rollout service\nanyscale service rollout -f deploy/services/serve_model.yaml\n\n# Query\ncurl -X POST -H \"Content-Type: application/json\" -H \"Authorization: Bearer $SECRET_TOKEN\" -d '{\n  \"title\": \"Transfer learning with transformers\",\n  \"description\": \"Using transformers for transfer learning on text classification tasks.\"\n}' $SERVICE_ENDPOINT/predict/\n\n# Rollback (to previous version of the Service)\nanyscale service rollback -f $SERVICE_CONFIG --name $SERVICE_NAME\n\n# Terminate\nanyscale service terminate --name $SERVICE_NAME\n```\n\n### CI/CD\n\nWe're not going to manually deploy our application every time we make a change. Instead, we'll automate this process using GitHub Actions!\n\n<div align=\"center\">\n  <img src=\"https://madewithml.com/static/images/mlops/cicd/cicd.png\">\n</div>\n\n1. Create a new github branch to save our changes to and execute CI/CD workloads:\n```bash\ngit remote set-url origin https://github.com/$GITHUB_USERNAME/Made-With-ML.git  # <-- CHANGE THIS to your username\ngit checkout -b dev\n```\n\n2. We'll start by adding the necessary credentials to the [`/settings/secrets/actions`](https://github.com/GokuMohandas/Made-With-ML/settings/secrets/actions) page of our GitHub repository.\n\n``` bash\nexport ANYSCALE_HOST=https://console.anyscale.com\nexport ANYSCALE_CLI_TOKEN=$YOUR_CLI_TOKEN  # retrieved from https://console.anyscale.com/o/madewithml/credentials\n```\n\n3. Now we can make changes to our code (not on `main` branch) and push them to GitHub. But in order to push our code to GitHub, we'll need to first authenticate with our credentials before pushing to our repository:\n\n```bash\ngit config --global user.name $GITHUB_USERNAME  # <-- CHANGE THIS to your username\ngit config --global user.email you@example.com  # <-- CHANGE THIS to your email\ngit add .\ngit commit -m \"\"  # <-- CHANGE THIS to your message\ngit push origin dev\n```\n\nNow you will be prompted to enter your username and password (personal access token). Follow these steps to get personal access token: [New GitHub personal access token](https://github.com/settings/tokens/new) \u2192 Add a name \u2192 Toggle `repo` and `workflow` \u2192 Click `Generate token` (scroll down) \u2192 Copy the token and paste it when prompted for your password.\n\n4. Now we can start a PR from this branch to our `main` branch and this will trigger the [workloads workflow](/.github/workflows/workloads.yaml). If the workflow (Anyscale Jobs) succeeds, this will produce comments with the training and evaluation results directly on the PR.\n\n<div align=\"center\">\n  <img src=\"https://madewithml.com/static/images/mlops/cicd/comments.png\">\n</div>\n\n5. If we like the results, we can merge the PR into the `main` branch. This will trigger the [serve workflow](/.github/workflows/serve.yaml) which will rollout our new service to production!\n\n### Continual learning\n\nWith our CI/CD workflow in place to deploy our application, we can now focus on continually improving our model. It becomes really easy to extend on this foundation to connect to scheduled runs (cron), [data pipelines](https://madewithml.com/courses/mlops/data-engineering/), drift detected through [monitoring](https://madewithml.com/courses/mlops/monitoring/), [online evaluation](https://madewithml.com/courses/mlops/evaluation/#online-evaluation), etc. And we can easily add additional context such as comparing any experiment with what's currently in production (directly in the PR even), etc.\n\n<div align=\"center\">\n  <img src=\"https://madewithml.com/static/images/mlops/cicd/continual.png\">\n</div>\n\n## FAQ\n\n### Jupyter notebook kernels\n\nIssues with configuring the notebooks with jupyter? By default, jupyter will use the kernel with our virtual environment but we can also manually add it to jupyter:\n```bash\npython3 -m ipykernel install --user --name=venv\n```\nNow we can open up a notebook \u2192 Kernel (top menu bar) \u2192 Change Kernel \u2192 `venv`. To ever delete this kernel, we can do the following:\n```bash\njupyter kernelspec list\njupyter kernelspec uninstall venv\n```\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 422274596,
    "name": "ColossalAI",
    "full_name": "hpcaitech/ColossalAI",
    "description": "Making large AI models cheaper, faster and more accessible",
    "html_url": "https://github.com/hpcaitech/ColossalAI",
    "clone_url": "https://github.com/hpcaitech/ColossalAI.git",
    "owner_login": "hpcaitech",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/88699314?v=4",
    "stargazers_count": 41065,
    "watchers_count": 41065,
    "forks_count": 4526,
    "open_issues_count": 484,
    "size": 66354,
    "language": "Python",
    "languages": {
      "Python": 9538711,
      "Cuda": 261454,
      "HTML": 202976,
      "C++": 131264,
      "Shell": 91611,
      "C": 22075,
      "Dockerfile": 3615
    },
    "topics": [
      "ai",
      "big-model",
      "data-parallelism",
      "deep-learning",
      "distributed-computing",
      "foundation-models",
      "heterogeneous-training",
      "hpc",
      "inference",
      "large-scale",
      "model-parallelism",
      "pipeline-parallelism"
    ],
    "license_name": "Apache License 2.0",
    "created_at": "2021-10-28T16:19:44+00:00",
    "updated_at": "2025-08-06T02:13:16+00:00",
    "pushed_at": "2025-08-05T06:42:03+00:00",
    "contributors_count": 100,
    "readme_length": 32018,
    "readme_content": "# Colossal-AI\n<div id=\"top\" align=\"center\">\n\n   [![logo](https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/colossal-ai_logo_vertical.png)](https://www.colossalai.org/)\n\n   Colossal-AI: Making large AI models cheaper, faster, and more accessible\n\n   <h3> <a href=\"https://arxiv.org/abs/2110.14883\"> Paper </a> |\n   <a href=\"https://www.colossalai.org/\"> Documentation </a> |\n   <a href=\"https://github.com/hpcaitech/ColossalAI/tree/main/examples\"> Examples </a> |\n   <a href=\"https://github.com/hpcaitech/ColossalAI/discussions\"> Forum </a> |\n   <a href=\"https://colossalai.org/zh-Hans/docs/get_started/bonus/\">GPU Cloud Playground </a> |\n   <a href=\"https://hpc-ai.com/blog\"> Blog </a></h3>\n\n   [![GitHub Repo stars](https://img.shields.io/github/stars/hpcaitech/ColossalAI?style=social)](https://github.com/hpcaitech/ColossalAI/stargazers)\n   [![Build](https://github.com/hpcaitech/ColossalAI/actions/workflows/build_on_schedule.yml/badge.svg)](https://github.com/hpcaitech/ColossalAI/actions/workflows/build_on_schedule.yml)\n   [![Documentation](https://readthedocs.org/projects/colossalai/badge/?version=latest)](https://colossalai.readthedocs.io/en/latest/?badge=latest)\n   [![CodeFactor](https://www.codefactor.io/repository/github/hpcaitech/colossalai/badge)](https://www.codefactor.io/repository/github/hpcaitech/colossalai)\n   [![HuggingFace badge](https://img.shields.io/badge/%F0%9F%A4%97HuggingFace-Join-yellow)](https://huggingface.co/hpcai-tech)\n   [![slack badge](https://img.shields.io/badge/Slack-join-blueviolet?logo=slack&amp)](https://github.com/hpcaitech/public_assets/tree/main/colossalai/contact/slack)\n   [![WeChat badge](https://img.shields.io/badge/\u5fae\u4fe1-\u52a0\u5165-green?logo=wechat&amp)](https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/WeChat.png)\n\n\n   | [English](README.md) | [\u4e2d\u6587](docs/README-zh-Hans.md) |\n\n</div>\n\n## Get Started with Colossal-AI Without Setup\n\nAccess high-end, on-demand compute for your research instantly\u2014no setup needed.\n\nSign up now and get $10 in credits!\n\nLimited Academic Bonuses:\n\n* Top up $1,000 and receive 300 credits\n* Top up $500 and receive 100 credits\n\n<div align=\"center\">\n   <a href=\"https://hpc-ai.com/?utm_source=github&utm_medium=social&utm_campaign=promotion-colossalai\">\n   <img src=\"https://github.com/hpcaitech/public_assets/blob/main/colossalai/img/2-2.gif\" width=\"850\" />\n   </a>\n</div>\n\n\n## Latest News\n* [2025/02] [DeepSeek 671B Fine-Tuning Guide Revealed\u2014Unlock the Upgraded DeepSeek Suite with One Click, AI Players Ecstatic!](https://company.hpc-ai.com/blog/shocking-release-deepseek-671b-fine-tuning-guide-revealed-unlock-the-upgraded-deepseek-suite-with-one-click-ai-players-ecstatic)\n* [2024/12] [The development cost of video generation models has saved by 50%! Open-source solutions are now available with H200 GPU vouchers](https://company.hpc-ai.com/blog/the-development-cost-of-video-generation-models-has-saved-by-50-open-source-solutions-are-now-available-with-h200-gpu-vouchers) [[code]](https://github.com/hpcaitech/Open-Sora/blob/main/scripts/train.py) [[vouchers]](https://colossalai.org/zh-Hans/docs/get_started/bonus/)\n* [2024/10] [How to build a low-cost Sora-like app? Solutions for you](https://company.hpc-ai.com/blog/how-to-build-a-low-cost-sora-like-app-solutions-for-you)\n* [2024/09] [Singapore Startup HPC-AI Tech Secures 50 Million USD in Series A Funding to Build the Video Generation AI Model and GPU Platform](https://company.hpc-ai.com/blog/singapore-startup-hpc-ai-tech-secures-50-million-usd-in-series-a-funding-to-build-the-video-generation-ai-model-and-gpu-platform)\n* [2024/09] [Reducing AI Large Model Training Costs by 30% Requires Just a Single Line of Code From FP8 Mixed Precision Training Upgrades](https://company.hpc-ai.com/blog/reducing-ai-large-model-training-costs-by-30-requires-just-a-single-line-of-code-from-fp8-mixed-precision-training-upgrades)\n* [2024/06] [Open-Sora Continues Open Source: Generate Any 16-Second 720p HD Video with One Click, Model Weights Ready to Use](https://hpc-ai.com/blog/open-sora-from-hpc-ai-tech-team-continues-open-source-generate-any-16-second-720p-hd-video-with-one-click-model-weights-ready-to-use)\n* [2024/05] [Large AI Models Inference Speed Doubled, Colossal-Inference Open Source Release](https://hpc-ai.com/blog/colossal-inference)\n* [2024/04] [Open-Sora Unveils Major Upgrade: Embracing Open Source with Single-Shot 16-Second Video Generation and 720p Resolution](https://hpc-ai.com/blog/open-soras-comprehensive-upgrade-unveiled-embracing-16-second-video-generation-and-720p-resolution-in-open-source)\n* [2024/04] [Most cost-effective solutions for inference, fine-tuning and pretraining, tailored to LLaMA3 series](https://hpc-ai.com/blog/most-cost-effective-solutions-for-inference-fine-tuning-and-pretraining-tailored-to-llama3-series)\n\n## Table of Contents\n<ul>\n <li><a href=\"#Why-Colossal-AI\">Why Colossal-AI</a> </li>\n <li><a href=\"#Features\">Features</a> </li>\n <li>\n   <a href=\"#Colossal-AI-in-the-Real-World\">Colossal-AI for Real World Applications</a>\n   <ul>\n     <li><a href=\"#Open-Sora\">Open-Sora: Revealing Complete Model Parameters, Training Details, and Everything for Sora-like Video Generation Models</a></li>\n     <li><a href=\"#Colossal-LLaMA-2\">Colossal-LLaMA-2: One Half-Day of Training Using a Few Hundred Dollars Yields Similar Results to Mainstream Large Models, Open-Source and Commercial-Free Domain-Specific Llm Solution</a></li>\n     <li><a href=\"#ColossalChat\">ColossalChat: An Open-Source Solution for Cloning ChatGPT With a Complete RLHF Pipeline</a></li>\n     <li><a href=\"#AIGC\">AIGC: Acceleration of Stable Diffusion</a></li>\n     <li><a href=\"#Biomedicine\">Biomedicine: Acceleration of AlphaFold Protein Structure</a></li>\n   </ul>\n </li>\n <li>\n   <a href=\"#Parallel-Training-Demo\">Parallel Training Demo</a>\n   <ul>\n     <li><a href=\"#LLaMA3\">LLaMA 1/2/3 </a></li>\n     <li><a href=\"#MoE\">MoE</a></li>\n     <li><a href=\"#GPT-3\">GPT-3</a></li>\n     <li><a href=\"#GPT-2\">GPT-2</a></li>\n     <li><a href=\"#BERT\">BERT</a></li>\n     <li><a href=\"#PaLM\">PaLM</a></li>\n     <li><a href=\"#OPT\">OPT</a></li>\n     <li><a href=\"#ViT\">ViT</a></li>\n     <li><a href=\"#Recommendation-System-Models\">Recommendation System Models</a></li>\n   </ul>\n </li>\n <li>\n   <a href=\"#Single-GPU-Training-Demo\">Single GPU Training Demo</a>\n   <ul>\n     <li><a href=\"#GPT-2-Single\">GPT-2</a></li>\n     <li><a href=\"#PaLM-Single\">PaLM</a></li>\n   </ul>\n </li>\n <li>\n   <a href=\"#Inference\">Inference</a>\n   <ul>\n     <li><a href=\"#Colossal-Inference\">Colossal-Inference: Large AI  Models Inference Speed Doubled</a></li>\n     <li><a href=\"#Grok-1\">Grok-1: 314B model of PyTorch + HuggingFace Inference</a></li>\n     <li><a href=\"#SwiftInfer\">SwiftInfer:Breaks the Length Limit of LLM for Multi-Round Conversations with 46% Acceleration</a></li>\n   </ul>\n </li>\n <li>\n   <a href=\"#Installation\">Installation</a>\n   <ul>\n     <li><a href=\"#PyPI\">PyPI</a></li>\n     <li><a href=\"#Install-From-Source\">Install From Source</a></li>\n   </ul>\n </li>\n <li><a href=\"#Use-Docker\">Use Docker</a></li>\n <li><a href=\"#Community\">Community</a></li>\n <li><a href=\"#Contributing\">Contributing</a></li>\n <li><a href=\"#Cite-Us\">Cite Us</a></li>\n</ul>\n\n## Why Colossal-AI\n<div align=\"center\">\n   <a href=\"https://youtu.be/KnXSfjqkKN0\">\n   <img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/JamesDemmel_Colossal-AI.png\" width=\"600\" />\n   </a>\n\n   Prof. James Demmel (UC Berkeley): Colossal-AI makes training AI models efficient, easy, and scalable.\n</div>\n\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n\n## Features\n\nColossal-AI provides a collection of parallel components for you. We aim to support you to write your\ndistributed deep learning models just like how you write your model on your laptop. We provide user-friendly tools to kickstart\ndistributed training and inference in a few lines.\n\n- Parallelism strategies\n  - Data Parallelism\n  - Pipeline Parallelism\n  - 1D, [2D](https://arxiv.org/abs/2104.05343), [2.5D](https://arxiv.org/abs/2105.14500), [3D](https://arxiv.org/abs/2105.14450) Tensor Parallelism\n  - [Sequence Parallelism](https://arxiv.org/abs/2105.13120)\n  - [Zero Redundancy Optimizer (ZeRO)](https://arxiv.org/abs/1910.02054)\n  - [Auto-Parallelism](https://arxiv.org/abs/2302.02599)\n\n- Heterogeneous Memory Management\n  - [PatrickStar](https://arxiv.org/abs/2108.05818)\n\n- Friendly Usage\n  - Parallelism based on the configuration file\n\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n\n## Colossal-AI in the Real World\n### Open-Sora\n\n[Open-Sora](https://github.com/hpcaitech/Open-Sora)\uff1aRevealing Complete Model Parameters, Training Details, and Everything for Sora-like Video Generation Models\n[[code]](https://github.com/hpcaitech/Open-Sora)\n[[blog]](https://hpc-ai.com/blog/open-sora-from-hpc-ai-tech-team-continues-open-source-generate-any-16-second-720p-hd-video-with-one-click-model-weights-ready-to-use)\n[[Model weights]](https://github.com/hpcaitech/Open-Sora?tab=readme-ov-file#model-weights)\n[[Demo]](https://github.com/hpcaitech/Open-Sora?tab=readme-ov-file#-latest-demo)\n[[GPU Cloud Playground]](https://cloud.luchentech.com/)\n[[OpenSora Image]](https://cloud.luchentech.com/doc/docs/image/open-sora/)\n\n<div align=\"center\">\n   <a href=\"https://youtu.be/ilMQpU71ddI?si=J4JSPzZ03ycYmlki\">\n   <img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/sora/opensora-v1.2.png\" width=\"700\" />\n   </a>\n</div>\n\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n\n### Colossal-LLaMA-2\n\n[[GPU Cloud Playground]](https://cloud.luchentech.com/)\n[[LLaMA3 Image]](https://cloud.luchentech.com/doc/docs/image/llama)\n\n- 7B: One half-day of training using a few hundred dollars yields similar results to mainstream large models, open-source and commercial-free domain-specific LLM solution.\n[[code]](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Colossal-LLaMA-2)\n[[blog]](https://www.hpc-ai.tech/blog/one-half-day-of-training-using-a-few-hundred-dollars-yields-similar-results-to-mainstream-large-models-open-source-and-commercial-free-domain-specific-llm-solution)\n[[HuggingFace model weights]](https://huggingface.co/hpcai-tech/Colossal-LLaMA-2-7b-base)\n[[Modelscope model weights]](https://www.modelscope.cn/models/colossalai/Colossal-LLaMA-2-7b-base/summary)\n\n- 13B: Construct refined 13B private model with just $5000 USD.\n[[code]](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Colossal-LLaMA-2)\n[[blog]](https://hpc-ai.com/blog/colossal-llama-2-13b)\n[[HuggingFace model weights]](https://huggingface.co/hpcai-tech/Colossal-LLaMA-2-13b-base)\n[[Modelscope model weights]](https://www.modelscope.cn/models/colossalai/Colossal-LLaMA-2-13b-base/summary)\n\n|              Model              |  Backbone  | Tokens Consumed |     MMLU (5-shot)    | CMMLU (5-shot)| AGIEval (5-shot) | GAOKAO (0-shot) | CEval (5-shot)  |\n| :-----------------------------: | :--------: | :-------------: | :------------------: | :-----------: | :--------------: | :-------------: | :-------------: |\n|          Baichuan-7B            |     -      |      1.2T       |    42.32 (42.30)     | 44.53 (44.02) |        38.72     |       36.74     |       42.80     |\n|       Baichuan-13B-Base         |     -      |      1.4T       |    50.51 (51.60)     | 55.73 (55.30) |        47.20     |       51.41     |       53.60     |\n|       Baichuan2-7B-Base         |     -      |      2.6T       |    46.97 (54.16)     | 57.67 (57.07) |        45.76     |       52.60     |       54.00     |\n|       Baichuan2-13B-Base        |     -      |      2.6T       |    54.84 (59.17)     | 62.62 (61.97) |        52.08     |       58.25     |       58.10     |\n|           ChatGLM-6B            |     -      |      1.0T       |    39.67 (40.63)     |   41.17 (-)   |        40.10     |       36.53     |       38.90     |\n|          ChatGLM2-6B            |     -      |      1.4T       |    44.74 (45.46)     |   49.40 (-)   |        46.36     |       45.49     |       51.70     |\n|          InternLM-7B            |     -      |      1.6T       |    46.70 (51.00)     |   52.00 (-)   |        44.77     |       61.64     |       52.80     |\n|            Qwen-7B              |     -      |      2.2T       |    54.29 (56.70)     | 56.03 (58.80) |        52.47     |       56.42     |       59.60     |\n|           Llama-2-7B            |     -      |      2.0T       |    44.47 (45.30)     |   32.97 (-)   |        32.60     |       25.46     |         -       |\n| Linly-AI/Chinese-LLaMA-2-7B-hf  | Llama-2-7B |      1.0T       |        37.43         |     29.92     |        32.00     |       27.57     |         -       |\n| wenge-research/yayi-7b-llama2   | Llama-2-7B |        -        |        38.56         |     31.52     |        30.99     |       25.95     |         -       |\n| ziqingyang/chinese-llama-2-7b   | Llama-2-7B |        -        |        33.86         |     34.69     |        34.52     |       25.18     |        34.2     |\n| TigerResearch/tigerbot-7b-base  | Llama-2-7B |      0.3T       |        43.73         |     42.04     |        37.64     |       30.61     |         -       |\n|  LinkSoul/Chinese-Llama-2-7b    | Llama-2-7B |        -        |        48.41         |     38.31     |        38.45     |       27.72     |         -       |\n|       FlagAlpha/Atom-7B         | Llama-2-7B |      0.1T       |        49.96         |     41.10     |        39.83     |       33.00     |         -       |\n| IDEA-CCNL/Ziya-LLaMA-13B-v1.1   | Llama-13B  |      0.11T      |        50.25         |     40.99     |        40.04     |       30.54     |         -       |\n|  **Colossal-LLaMA-2-7b-base**   | Llama-2-7B |   **0.0085T**   |        53.06         |     49.89     |        51.48     |       58.82     |        50.2     |\n|  **Colossal-LLaMA-2-13b-base**  | Llama-2-13B |   **0.025T**    |        56.42         |     61.80     |        54.69     |       69.53     |        60.3     |\n\n\n### ColossalChat\n\n<div align=\"center\">\n   <a href=\"https://www.youtube.com/watch?v=HcTiHzApHm0\">\n   <img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chat/ColossalChat%20YouTube.png\" width=\"700\" />\n   </a>\n</div>\n\n[ColossalChat](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat): An open-source solution for cloning [ChatGPT](https://openai.com/blog/chatgpt/) with a complete RLHF pipeline.\n[[code]](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat)\n[[blog]](https://medium.com/@yangyou_berkeley/colossalchat-an-open-source-solution-for-cloning-chatgpt-with-a-complete-rlhf-pipeline-5edf08fb538b)\n[[demo]](https://www.youtube.com/watch?v=HcTiHzApHm0)\n[[tutorial]](https://www.youtube.com/watch?v=-qFBZFmOJfg)\n\n<p id=\"ColossalChat-Speed\" align=\"center\">\n<img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chat/ColossalChat%20Speed.jpg\" width=450/>\n</p>\n\n- Up to 10 times faster for RLHF PPO Stage3 Training\n\n<p id=\"ColossalChat_scaling\" align=\"center\">\n<img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/ChatGPT%20scaling.png\" width=800/>\n</p>\n\n- Up to 7.73 times faster for single server training and 1.42 times faster for single-GPU inference\n\n<p id=\"ColossalChat-1GPU\" align=\"center\">\n<img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/ChatGPT-1GPU.jpg\" width=450/>\n</p>\n\n- Up to 10.3x growth in model capacity on one GPU\n- A mini demo training process requires only 1.62GB of GPU memory (any consumer-grade GPU)\n\n<p id=\"ColossalChat-LoRA\" align=\"center\">\n<img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/LoRA%20data.jpg\" width=600/>\n</p>\n\n- Increase the capacity of the fine-tuning model by up to 3.7 times on a single GPU\n- Keep at a sufficiently high running speed\n\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n\n\n### AIGC\nAcceleration of AIGC (AI-Generated Content) models such as [Stable Diffusion v1](https://github.com/CompVis/stable-diffusion) and [Stable Diffusion v2](https://github.com/Stability-AI/stablediffusion).\n<p id=\"diffusion_train\" align=\"center\">\n<img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/Stable%20Diffusion%20v2.png\" width=800/>\n</p>\n\n- [Training](https://github.com/hpcaitech/ColossalAI/tree/main/examples/images/diffusion): Reduce Stable Diffusion memory consumption by up to 5.6x and hardware cost by up to 46x (from A100 to RTX3060).\n\n<p id=\"diffusion_demo\" align=\"center\">\n<img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/DreamBooth.png\" width=800/>\n</p>\n\n- [DreamBooth Fine-tuning](https://github.com/hpcaitech/ColossalAI/tree/main/examples/images/dreambooth): Personalize your model using just 3-5 images of the desired subject.\n\n<p id=\"inference-sd\" align=\"center\">\n<img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/Stable%20Diffusion%20Inference.jpg\" width=800/>\n</p>\n\n- [Inference](https://github.com/hpcaitech/ColossalAI/tree/main/examples/images/diffusion): Reduce inference GPU memory consumption by 2.5x.\n\n\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n\n### Biomedicine\nAcceleration of [AlphaFold Protein Structure](https://alphafold.ebi.ac.uk/)\n\n<p id=\"FastFold\" align=\"center\">\n<img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/FastFold.jpg\" width=800/>\n</p>\n\n- [FastFold](https://github.com/hpcaitech/FastFold): Accelerating training and inference on GPU Clusters, faster data processing, inference sequence containing more than 10000 residues.\n\n<p id=\"FastFold-Intel\" align=\"center\">\n<img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/data%20preprocessing%20with%20Intel.jpg\" width=600/>\n</p>\n\n- [FastFold with Intel](https://github.com/hpcaitech/FastFold): 3x inference acceleration and 39% cost reduce.\n\n<p id=\"xTrimoMultimer\" align=\"center\">\n<img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/xTrimoMultimer_Table.jpg\" width=800/>\n</p>\n\n- [xTrimoMultimer](https://github.com/biomap-research/xTrimoMultimer): accelerating structure prediction of protein monomers and multimer by 11x.\n\n\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n\n## Parallel Training Demo\n### LLaMA3\n<p align=\"center\">\n<img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/images/LLaMA3-70B-H100.png\" width=600/>\n</p>\n\n- 70 billion parameter LLaMA3 model training accelerated by 18%\n[[code]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/llama)\n[[GPU Cloud Playground]](https://cloud.luchentech.com/)\n[[LLaMA3 Image]](https://cloud.luchentech.com/doc/docs/image/llama)\n\n### LLaMA2\n<p align=\"center\">\n<img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/llama2_pretraining.png\" width=600/>\n</p>\n\n- 70 billion parameter LLaMA2 model training accelerated by 195%\n[[code]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/llama)\n[[blog]](https://www.hpc-ai.tech/blog/70b-llama2-training)\n\n### LLaMA1\n<p align=\"center\">\n<img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/images/LLaMA_pretraining.png\" width=600/>\n</p>\n\n- 65-billion-parameter large model pretraining accelerated by 38%\n[[code]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/llama)\n[[blog]](https://www.hpc-ai.tech/blog/large-model-pretraining)\n\n### MoE\n<p align=\"center\">\n<img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/images/MOE_training.png\" width=800/>\n</p>\n\n- Enhanced MoE parallelism, Open-source MoE model training can be 9 times more efficient\n[[code]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/openmoe)\n[[blog]](https://www.hpc-ai.tech/blog/enhanced-moe-parallelism-open-source-moe-model-training-can-be-9-times-more-efficient)\n\n### GPT-3\n<p align=\"center\">\n<img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/GPT3-v5.png\" width=700/>\n</p>\n\n- Save 50% GPU resources and 10.7% acceleration\n\n### GPT-2\n<img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/GPT2.png\" width=800/>\n\n- 11x lower GPU memory consumption, and superlinear scaling efficiency with Tensor Parallelism\n\n<img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/(updated)GPT-2.png\" width=800>\n\n- 24x larger model size on the same hardware\n- over 3x acceleration\n### BERT\n<img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/BERT.png\" width=800/>\n\n- 2x faster training, or 50% longer sequence length\n\n### PaLM\n- [PaLM-colossalai](https://github.com/hpcaitech/PaLM-colossalai): Scalable implementation of Google's Pathways Language Model ([PaLM](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html)).\n\n### OPT\n<img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/OPT_update.png\" width=800/>\n\n- [Open Pretrained Transformer (OPT)](https://github.com/facebookresearch/metaseq), a 175-Billion parameter AI language model released by Meta, which stimulates AI programmers to perform various downstream tasks and application deployments because of public pre-trained model weights.\n- 45% speedup fine-tuning OPT at low cost in lines. [[Example]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/opt) [[Online Serving]](https://colossalai.org/docs/advanced_tutorials/opt_service)\n\nPlease visit our [documentation](https://www.colossalai.org/) and [examples](https://github.com/hpcaitech/ColossalAI/tree/main/examples) for more details.\n\n### ViT\n<p align=\"center\">\n<img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/ViT.png\" width=\"450\" />\n</p>\n\n- 14x larger batch size, and 5x faster training for Tensor Parallelism = 64\n\n### Recommendation System Models\n- [Cached Embedding](https://github.com/hpcaitech/CachedEmbedding), utilize software cache to train larger embedding tables with a smaller GPU memory budget.\n\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n\n## Single GPU Training Demo\n\n### GPT-2\n<p id=\"GPT-2-Single\" align=\"center\">\n<img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/GPT2-GPU1.png\" width=450/>\n</p>\n\n- 20x larger model size on the same hardware\n\n<p id=\"GPT-2-NVME\" align=\"center\">\n<img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/GPT2-NVME.png\" width=800/>\n</p>\n\n- 120x larger model size on the same hardware (RTX 3080)\n\n### PaLM\n<p id=\"PaLM-Single\" align=\"center\">\n<img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/PaLM-GPU1.png\" width=450/>\n</p>\n\n- 34x larger model size on the same hardware\n\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n\n\n## Inference\n### Colossal-Inference\n<p align=\"center\">\n<img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/inference/colossal-inference-v1-1.png\" width=1000/>\n</p>\n\n<p align=\"center\">\n<img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/inference/colossal-inference-v1-2.png\" width=1000/>\n</p>\n\n - Large AI models inference speed doubled, compared to the offline inference performance of vLLM in some cases.\n[[code]](https://github.com/hpcaitech/ColossalAI/tree/main/colossalai/inference)\n[[blog]](https://hpc-ai.com/blog/colossal-inference)\n[[GPU Cloud Playground]](https://cloud.luchentech.com/)\n[[LLaMA3 Image]](https://cloud.luchentech.com/doc/docs/image/llama)\n\n### Grok-1\n<p id=\"Grok-1\" align=\"center\">\n<img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/images/grok-1-inference.jpg\" width=600/>\n</p>\n\n - 314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, an easy-to-use Python + PyTorch + HuggingFace version for Inference.\n\n[[code]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/grok-1)\n[[blog]](https://hpc-ai.com/blog/314-billion-parameter-grok-1-inference-accelerated-by-3.8x-efficient-and-easy-to-use-pytorchhuggingface-version-is-here)\n[[HuggingFace Grok-1 PyTorch model weights]](https://huggingface.co/hpcai-tech/grok-1)\n[[ModelScope Grok-1 PyTorch model weights]](https://www.modelscope.cn/models/colossalai/grok-1-pytorch/summary)\n\n### SwiftInfer\n<p id=\"SwiftInfer\" align=\"center\">\n<img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/SwiftInfer.jpg\" width=800/>\n</p>\n\n- [SwiftInfer](https://github.com/hpcaitech/SwiftInfer): Inference performance improved by 46%, open source solution breaks the length limit of LLM for multi-round conversations\n\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n\n## Installation\n\nRequirements:\n- PyTorch >= 2.2\n- Python >= 3.7\n- CUDA >= 11.0\n- [NVIDIA GPU Compute Capability](https://developer.nvidia.com/cuda-gpus) >= 7.0 (V100/RTX20 and higher)\n- Linux OS\n\nIf you encounter any problem with installation, you may want to raise an [issue](https://github.com/hpcaitech/ColossalAI/issues/new/choose) in this repository.\n\n### Install from PyPI\n\nYou can easily install Colossal-AI with the following command. **By default, we do not build PyTorch extensions during installation.**\n\n```bash\npip install colossalai\n```\n\n**Note: only Linux is supported for now.**\n\nHowever, if you want to build the PyTorch extensions during installation, you can set `BUILD_EXT=1`.\n\n```bash\nBUILD_EXT=1 pip install colossalai\n```\n\n**Otherwise, CUDA kernels will be built during runtime when you actually need them.**\n\nWe also keep releasing the nightly version to PyPI every week. This allows you to access the unreleased features and bug fixes in the main branch.\nInstallation can be made via\n\n```bash\npip install colossalai-nightly\n```\n\n### Download From Source\n\n> The version of Colossal-AI will be in line with the main branch of the repository. Feel free to raise an issue if you encounter any problems. :)\n\n```shell\ngit clone https://github.com/hpcaitech/ColossalAI.git\ncd ColossalAI\n\n# install colossalai\npip install .\n```\n\nBy default, we do not compile CUDA/C++ kernels. ColossalAI will build them during runtime.\nIf you want to install and enable CUDA kernel fusion (compulsory installation when using fused optimizer):\n\n```shell\nBUILD_EXT=1 pip install .\n```\n\nFor Users with CUDA 10.2, you can still build ColossalAI from source. However, you need to manually download the cub library and copy it to the corresponding directory.\n\n```bash\n# clone the repository\ngit clone https://github.com/hpcaitech/ColossalAI.git\ncd ColossalAI\n\n# download the cub library\nwget https://github.com/NVIDIA/cub/archive/refs/tags/1.8.0.zip\nunzip 1.8.0.zip\ncp -r cub-1.8.0/cub/ colossalai/kernel/cuda_native/csrc/kernels/include/\n\n# install\nBUILD_EXT=1 pip install .\n```\n\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n\n## Use Docker\n\n### Pull from DockerHub\n\nYou can directly pull the docker image from our [DockerHub page](https://hub.docker.com/r/hpcaitech/colossalai). The image is automatically uploaded upon release.\n\n\n### Build On Your Own\n\nRun the following command to build a docker image from Dockerfile provided.\n\n> Building Colossal-AI from scratch requires GPU support, you need to use Nvidia Docker Runtime as the default when doing `docker build`. More details can be found [here](https://stackoverflow.com/questions/59691207/docker-build-with-nvidia-runtime).\n> We recommend you install Colossal-AI from our [project page](https://www.colossalai.org) directly.\n\n\n```bash\ncd ColossalAI\ndocker build -t colossalai ./docker\n```\n\nRun the following command to start the docker container in interactive mode.\n\n```bash\ndocker run -ti --gpus all --rm --ipc=host colossalai bash\n```\n\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n\n## Community\n\nJoin the Colossal-AI community on [Forum](https://github.com/hpcaitech/ColossalAI/discussions),\n[Slack](https://join.slack.com/t/colossalaiworkspace/shared_invite/zt-z7b26eeb-CBp7jouvu~r0~lcFzX832w),\nand [WeChat(\u5fae\u4fe1)](https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/WeChat.png \"qrcode\") to share your suggestions, feedback, and questions with our engineering team.\n\n## Contributing\nReferring to the successful attempts of [BLOOM](https://bigscience.huggingface.co/) and [Stable Diffusion](https://en.wikipedia.org/wiki/Stable_Diffusion), any and all developers and partners with computing powers, datasets, models are welcome to join and build the Colossal-AI community, making efforts towards the era of big AI models!\n\nYou may contact us or participate in the following ways:\n1. [Leaving a Star \u2b50](https://github.com/hpcaitech/ColossalAI/stargazers) to show your like and support. Thanks!\n2. Posting an [issue](https://github.com/hpcaitech/ColossalAI/issues/new/choose), or submitting a PR on GitHub follow the guideline in [Contributing](https://github.com/hpcaitech/ColossalAI/blob/main/CONTRIBUTING.md)\n3. Send your official proposal to email contact@hpcaitech.com\n\nThanks so much to all of our amazing contributors!\n\n<a href=\"https://github.com/hpcaitech/ColossalAI/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=hpcaitech/ColossalAI\"  width=\"800px\"/>\n</a>\n\n\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n\n\n## CI/CD\n\nWe leverage the power of [GitHub Actions](https://github.com/features/actions) to automate our development, release and deployment workflows. Please check out this [documentation](.github/workflows/README.md) on how the automated workflows are operated.\n\n\n## Cite Us\n\nThis project is inspired by some related projects (some by our team and some by other organizations). We would like to credit these amazing projects as listed in the [Reference List](./docs/REFERENCE.md).\n\nTo cite this project, you can use the following BibTeX citation.\n\n```\n@inproceedings{10.1145/3605573.3605613,\nauthor = {Li, Shenggui and Liu, Hongxin and Bian, Zhengda and Fang, Jiarui and Huang, Haichen and Liu, Yuliang and Wang, Boxiang and You, Yang},\ntitle = {Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training},\nyear = {2023},\nisbn = {9798400708435},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3605573.3605613},\ndoi = {10.1145/3605573.3605613},\nabstract = {The success of Transformer models has pushed the deep learning model scale to billions of parameters, but the memory limitation of a single GPU has led to an urgent need for training on multi-GPU clusters. However, the best practice for choosing the optimal parallel strategy is still lacking, as it requires domain expertise in both deep learning and parallel computing. The Colossal-AI system addressed the above challenge by introducing a unified interface to scale your sequential code of model training to distributed environments. It supports parallel training methods such as data, pipeline, tensor, and sequence parallelism and is integrated with heterogeneous training and zero redundancy optimizer. Compared to the baseline system, Colossal-AI can achieve up to 2.76 times training speedup on large-scale models.},\nbooktitle = {Proceedings of the 52nd International Conference on Parallel Processing},\npages = {766\u2013775},\nnumpages = {10},\nkeywords = {datasets, gaze detection, text tagging, neural networks},\nlocation = {Salt Lake City, UT, USA},\nseries = {ICPP '23}\n}\n```\n\nColossal-AI has been accepted as official tutorial by top conferences [NeurIPS](https://nips.cc/), [SC](https://sc22.supercomputing.org/), [AAAI](https://aaai.org/Conferences/AAAI-23/),\n[PPoPP](https://ppopp23.sigplan.org/), [CVPR](https://cvpr2023.thecvf.com/), [ISC](https://www.isc-hpc.com/), [NVIDIA GTC](https://www.nvidia.com/en-us/on-demand/session/gtcspring23-S51482/) ,etc.\n\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 70905478,
    "name": "Deep-Learning-Papers-Reading-Roadmap",
    "full_name": "floodsung/Deep-Learning-Papers-Reading-Roadmap",
    "description": "Deep Learning papers reading roadmap for anyone who are eager to learn this amazing tech!",
    "html_url": "https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap",
    "clone_url": "https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap.git",
    "owner_login": "floodsung",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/3880963?v=4",
    "stargazers_count": 39203,
    "watchers_count": 39203,
    "forks_count": 7350,
    "open_issues_count": 91,
    "size": 3638,
    "language": "Python",
    "languages": {
      "Python": 4593
    },
    "topics": [
      "deep-learning"
    ],
    "license_name": null,
    "created_at": "2016-10-14T11:49:48+00:00",
    "updated_at": "2025-08-05T13:19:03+00:00",
    "pushed_at": "2022-11-27T13:18:32+00:00",
    "contributors_count": 25,
    "readme_length": 35282,
    "readme_content": "# Deep Learning Papers Reading Roadmap\n\n>If you are a newcomer to the Deep Learning area, the first question you may have is \"Which paper should I start reading from?\"\n\n>Here is a reading roadmap of Deep Learning papers!\n\nThe roadmap is constructed in accordance with the following four guidelines:\n\n- From outline to detail\n- From old to state-of-the-art\n- from generic to specific areas\n- focus on state-of-the-art\n\nYou will find many papers that are quite new but really worth reading.\n\nI would continue adding papers to this roadmap.\n\n\n---------------------------------------\n\n# 1 Deep Learning History and Basics\n\n## 1.0 Book\n\n**[0]** Bengio, Yoshua, Ian J. Goodfellow, and Aaron Courville. \"**Deep learning**.\" An MIT Press book. (2015). [[html]](http://www.deeplearningbook.org/) **(Deep Learning Bible, you can read this book while reading following papers.)** :star::star::star::star::star:\n\n## 1.1 Survey\n\n**[1]** LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. \"**Deep learning**.\" Nature 521.7553 (2015): 436-444. [[pdf]](http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf) **(Three Giants' Survey)** :star::star::star::star::star:\n\n## 1.2 Deep Belief Network(DBN)(Milestone of Deep Learning Eve)\n\n**[2]** Hinton, Geoffrey E., Simon Osindero, and Yee-Whye Teh. \"**A fast learning algorithm for deep belief nets**.\" Neural computation 18.7 (2006): 1527-1554. [[pdf]](http://www.cs.toronto.edu/~hinton/absps/ncfast.pdf)**(Deep Learning Eve)** :star::star::star:\n\n**[3]** Hinton, Geoffrey E., and Ruslan R. Salakhutdinov. \"**Reducing the dimensionality of data with neural networks**.\" Science 313.5786 (2006): 504-507. [[pdf]](http://www.cs.toronto.edu/~hinton/science.pdf) **(Milestone, Show the promise of deep learning)** :star::star::star:\n\n## 1.3 ImageNet Evolution\uff08Deep Learning broke out from here\uff09\n\n**[4]** Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. \"**Imagenet classification with deep convolutional neural networks**.\" Advances in neural information processing systems. 2012. [[pdf]](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) **(AlexNet, Deep Learning Breakthrough)** :star::star::star::star::star:\n\n**[5]** Simonyan, Karen, and Andrew Zisserman. \"**Very deep convolutional networks for large-scale image recognition**.\" arXiv preprint arXiv:1409.1556 (2014). [[pdf]](https://arxiv.org/pdf/1409.1556.pdf) **(VGGNet,Neural Networks become very deep!)** :star::star::star:\n\n**[6]** Szegedy, Christian, et al. \"**Going deeper with convolutions**.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015. [[pdf]](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf) **(GoogLeNet)** :star::star::star:\n\n**[7]** He, Kaiming, et al. \"**Deep residual learning for image recognition**.\" arXiv preprint arXiv:1512.03385 (2015). [[pdf]](https://arxiv.org/pdf/1512.03385.pdf) **(ResNet,Very very deep networks, CVPR best paper)** :star::star::star::star::star:\n\n## 1.4 Speech Recognition Evolution\n\n**[8]** Hinton, Geoffrey, et al. \"**Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups**.\" IEEE Signal Processing Magazine 29.6 (2012): 82-97. [[pdf]](http://cs224d.stanford.edu/papers/maas_paper.pdf) **(Breakthrough in speech recognition)**:star::star::star::star:\n\n**[9]** Graves, Alex, Abdel-rahman Mohamed, and Geoffrey Hinton. \"**Speech recognition with deep recurrent neural networks**.\" 2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013. [[pdf]](http://arxiv.org/pdf/1303.5778.pdf) **(RNN)**:star::star::star:\n\n**[10]** Graves, Alex, and Navdeep Jaitly. \"**Towards End-To-End Speech Recognition with Recurrent Neural Networks**.\" ICML. Vol. 14. 2014. [[pdf]](http://www.jmlr.org/proceedings/papers/v32/graves14.pdf):star::star::star:\n\n**[11]** Sak, Ha\u015fim, et al. \"**Fast and accurate recurrent neural network acoustic models for speech recognition**.\" arXiv preprint arXiv:1507.06947 (2015). [[pdf]](http://arxiv.org/pdf/1507.06947) **(Google Speech Recognition System)** :star::star::star:\n\n**[12]** Amodei, Dario, et al. \"**Deep speech 2: End-to-end speech recognition in english and mandarin**.\" arXiv preprint arXiv:1512.02595 (2015). [[pdf]](https://arxiv.org/pdf/1512.02595.pdf) **(Baidu Speech Recognition System)** :star::star::star::star:\n\n**[13]** W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, G. Zweig \"**Achieving Human Parity in Conversational Speech Recognition**.\" arXiv preprint arXiv:1610.05256 (2016). [[pdf]](https://arxiv.org/pdf/1610.05256v1) **(State-of-the-art in speech recognition, Microsoft)** :star::star::star::star:\n\n>After reading above papers, you will have a basic understanding of the Deep Learning history, the basic architectures of Deep Learning model(including CNN, RNN, LSTM) and how deep learning can be applied to image and speech recognition issues. The following papers will take you in-depth understanding of the Deep Learning method, Deep Learning in different areas of application and the frontiers. I suggest that you can choose the following papers based on your interests and research direction.\n\n#2 Deep Learning Method\n\n## 2.1 Model\n\n**[14]** Hinton, Geoffrey E., et al. \"**Improving neural networks by preventing co-adaptation of feature detectors**.\" arXiv preprint arXiv:1207.0580 (2012). [[pdf]](https://arxiv.org/pdf/1207.0580.pdf) **(Dropout)** :star::star::star:\n\n**[15]** Srivastava, Nitish, et al. \"**Dropout: a simple way to prevent neural networks from overfitting**.\" Journal of Machine Learning Research 15.1 (2014): 1929-1958. [[pdf]](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf) :star::star::star:\n\n**[16]** Ioffe, Sergey, and Christian Szegedy. \"**Batch normalization: Accelerating deep network training by reducing internal covariate shift**.\" arXiv preprint arXiv:1502.03167 (2015). [[pdf]](http://arxiv.org/pdf/1502.03167) **(An outstanding Work in 2015)** :star::star::star::star:\n\n**[17]** Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. \"**Layer normalization**.\" arXiv preprint arXiv:1607.06450 (2016). [[pdf]](https://arxiv.org/pdf/1607.06450.pdf?utm_source=sciontist.com&utm_medium=refer&utm_campaign=promote) **(Update of Batch Normalization)** :star::star::star::star:\n\n**[18]** Courbariaux, Matthieu, et al. \"**Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to+ 1 or\u22121**.\" [[pdf]](https://pdfs.semanticscholar.org/f832/b16cb367802609d91d400085eb87d630212a.pdf) **(New Model,Fast)**  :star::star::star:\n\n**[19]** Jaderberg, Max, et al. \"**Decoupled neural interfaces using synthetic gradients**.\" arXiv preprint arXiv:1608.05343 (2016). [[pdf]](https://arxiv.org/pdf/1608.05343) **(Innovation of Training Method,Amazing Work)** :star::star::star::star::star:\n\n**[20]** Chen, Tianqi, Ian Goodfellow, and Jonathon Shlens. \"Net2net: Accelerating learning via knowledge transfer.\" arXiv preprint arXiv:1511.05641 (2015). [[pdf]](https://arxiv.org/abs/1511.05641) **(Modify previously trained network to reduce training epochs)** :star::star::star:\n\n**[21]** Wei, Tao, et al. \"Network Morphism.\" arXiv preprint arXiv:1603.01670 (2016). [[pdf]](https://arxiv.org/abs/1603.01670) **(Modify previously trained network to reduce training epochs)** :star::star::star:\n\n## 2.2 Optimization\n\n**[22]** Sutskever, Ilya, et al. \"**On the importance of initialization and momentum in deep learning**.\" ICML (3) 28 (2013): 1139-1147. [[pdf]](http://www.jmlr.org/proceedings/papers/v28/sutskever13.pdf) **(Momentum optimizer)** :star::star:\n\n**[23]** Kingma, Diederik, and Jimmy Ba. \"**Adam: A method for stochastic optimization**.\" arXiv preprint arXiv:1412.6980 (2014). [[pdf]](http://arxiv.org/pdf/1412.6980) **(Maybe used most often currently)** :star::star::star:\n\n**[24]** Andrychowicz, Marcin, et al. \"**Learning to learn by gradient descent by gradient descent**.\" arXiv preprint arXiv:1606.04474 (2016). [[pdf]](https://arxiv.org/pdf/1606.04474) **(Neural Optimizer,Amazing Work)** :star::star::star::star::star:\n\n**[25]** Han, Song, Huizi Mao, and William J. Dally. \"**Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding**.\" CoRR, abs/1510.00149 2 (2015). [[pdf]](https://pdfs.semanticscholar.org/5b6c/9dda1d88095fa4aac1507348e498a1f2e863.pdf) **(ICLR best paper, new direction to make NN running fast,DeePhi Tech Startup)** :star::star::star::star::star:\n\n**[26]** Iandola, Forrest N., et al. \"**SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 1MB model size**.\" arXiv preprint arXiv:1602.07360 (2016). [[pdf]](http://arxiv.org/pdf/1602.07360) **(Also a new direction to optimize NN,DeePhi Tech Startup)** :star::star::star::star:\n\n**[27]** Glorat Xavier, Bengio Yoshua, et al. \"**Understanding the difficulty of training deep forward neural networks**.\" Proceedings of the thirteenth International Conference on Artificial Intelligence and Statistics, PMLR 9:249-256,2010. [[pdf]](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) :star::star::star::star:\n\n## 2.3 Unsupervised Learning / Deep Generative Model\n\n**[28]** Le, Quoc V. \"**Building high-level features using large scale unsupervised learning**.\" 2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013. [[pdf]](http://arxiv.org/pdf/1112.6209.pdf&embed) **(Milestone, Andrew Ng, Google Brain Project, Cat)** :star::star::star::star:\n\n\n**[29]** Kingma, Diederik P., and Max Welling. \"**Auto-encoding variational bayes**.\" arXiv preprint arXiv:1312.6114 (2013). [[pdf]](http://arxiv.org/pdf/1312.6114) **(VAE)** :star::star::star::star:\n\n**[30]** Goodfellow, Ian, et al. \"**Generative adversarial nets**.\" Advances in Neural Information Processing Systems. 2014. [[pdf]](http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf) **(GAN,super cool idea)** :star::star::star::star::star:\n\n**[31]** Radford, Alec, Luke Metz, and Soumith Chintala. \"**Unsupervised representation learning with deep convolutional generative adversarial networks**.\" arXiv preprint arXiv:1511.06434 (2015). [[pdf]](http://arxiv.org/pdf/1511.06434) **(DCGAN)** :star::star::star::star:\n\n**[32]** Gregor, Karol, et al. \"**DRAW: A recurrent neural network for image generation**.\" arXiv preprint arXiv:1502.04623 (2015). [[pdf]](http://jmlr.org/proceedings/papers/v37/gregor15.pdf) **(VAE with attention, outstanding work)** :star::star::star::star::star:\n\n**[33]** Oord, Aaron van den, Nal Kalchbrenner, and Koray Kavukcuoglu. \"**Pixel recurrent neural networks**.\" arXiv preprint arXiv:1601.06759 (2016). [[pdf]](http://arxiv.org/pdf/1601.06759) **(PixelRNN)** :star::star::star::star:\n\n**[34]** Oord, Aaron van den, et al. \"Conditional image generation with PixelCNN decoders.\" arXiv preprint arXiv:1606.05328 (2016). [[pdf]](https://arxiv.org/pdf/1606.05328) **(PixelCNN)** :star::star::star::star:\n\n**[34]** S. Mehri et al., \"**SampleRNN: An Unconditional End-to-End Neural Audio Generation Model**.\" arXiv preprint \tarXiv:1612.07837 (2016). [[pdf]](https://arxiv.org/pdf/1612.07837.pdf) :star::star::star::star::star:\n\n## 2.4 RNN / Sequence-to-Sequence Model\n\n**[35]** Graves, Alex. \"**Generating sequences with recurrent neural networks**.\" arXiv preprint arXiv:1308.0850 (2013). [[pdf]](http://arxiv.org/pdf/1308.0850) **(LSTM, very nice generating result, show the power of RNN)** :star::star::star::star:\n\n**[36]** Cho, Kyunghyun, et al. \"**Learning phrase representations using RNN encoder-decoder for statistical machine translation**.\" arXiv preprint arXiv:1406.1078 (2014). [[pdf]](http://arxiv.org/pdf/1406.1078) **(First Seq-to-Seq Paper)** :star::star::star::star:\n\n**[37]** Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. \"**Sequence to sequence learning with neural networks**.\" Advances in neural information processing systems. 2014. [[pdf]](https://arxiv.org/pdf/1409.3215.pdf) **(Outstanding Work)** :star::star::star::star::star:\n\n**[38]** Bahdanau, Dzmitry, KyungHyun Cho, and Yoshua Bengio. \"**Neural Machine Translation by Jointly Learning to Align and Translate**.\" arXiv preprint arXiv:1409.0473 (2014). [[pdf]](https://arxiv.org/pdf/1409.0473v7.pdf) :star::star::star::star:\n\n**[39]** Vinyals, Oriol, and Quoc Le. \"**A neural conversational model**.\" arXiv preprint arXiv:1506.05869 (2015). [[pdf]](http://arxiv.org/pdf/1506.05869.pdf%20(http://arxiv.org/pdf/1506.05869.pdf)) **(Seq-to-Seq on Chatbot)** :star::star::star:\n\n## 2.5 Neural Turing Machine\n\n**[40]** Graves, Alex, Greg Wayne, and Ivo Danihelka. \"**Neural turing machines**.\" arXiv preprint arXiv:1410.5401 (2014). [[pdf]](http://arxiv.org/pdf/1410.5401.pdf) **(Basic Prototype of Future Computer)** :star::star::star::star::star:\n\n**[41]** Zaremba, Wojciech, and Ilya Sutskever. \"**Reinforcement learning neural Turing machines**.\" arXiv preprint arXiv:1505.00521 362 (2015). [[pdf]](https://pdfs.semanticscholar.org/f10e/071292d593fef939e6ef4a59baf0bb3a6c2b.pdf) :star::star::star:\n\n**[42]** Weston, Jason, Sumit Chopra, and Antoine Bordes. \"**Memory networks**.\" arXiv preprint arXiv:1410.3916 (2014). [[pdf]](http://arxiv.org/pdf/1410.3916) :star::star::star:\n\n\n**[43]** Sukhbaatar, Sainbayar, Jason Weston, and Rob Fergus. \"**End-to-end memory networks**.\" Advances in neural information processing systems. 2015. [[pdf]](http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf) :star::star::star::star:\n\n**[44]** Vinyals, Oriol, Meire Fortunato, and Navdeep Jaitly. \"**Pointer networks**.\" Advances in Neural Information Processing Systems. 2015. [[pdf]](http://papers.nips.cc/paper/5866-pointer-networks.pdf) :star::star::star::star:\n\n**[45]** Graves, Alex, et al. \"**Hybrid computing using a neural network with dynamic external memory**.\" Nature (2016). [[pdf]](https://www.dropbox.com/s/0a40xi702grx3dq/2016-graves.pdf) **(Milestone,combine above papers' ideas)** :star::star::star::star::star:\n\n## 2.6 Deep Reinforcement Learning\n\n**[46]** Mnih, Volodymyr, et al. \"**Playing atari with deep reinforcement learning**.\" arXiv preprint arXiv:1312.5602 (2013). [[pdf]](http://arxiv.org/pdf/1312.5602.pdf)) **(First Paper named deep reinforcement learning)** :star::star::star::star:\n\n**[47]** Mnih, Volodymyr, et al. \"**Human-level control through deep reinforcement learning**.\" Nature 518.7540 (2015): 529-533. [[pdf]](https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf) **(Milestone)** :star::star::star::star::star:\n\n**[48]** Wang, Ziyu, Nando de Freitas, and Marc Lanctot. \"**Dueling network architectures for deep reinforcement learning**.\" arXiv preprint arXiv:1511.06581 (2015). [[pdf]](http://arxiv.org/pdf/1511.06581) **(ICLR best paper,great idea)**  :star::star::star::star:\n\n**[49]** Mnih, Volodymyr, et al. \"**Asynchronous methods for deep reinforcement learning**.\" arXiv preprint arXiv:1602.01783 (2016). [[pdf]](http://arxiv.org/pdf/1602.01783) **(State-of-the-art method)** :star::star::star::star::star:\n\n**[50]** Lillicrap, Timothy P., et al. \"**Continuous control with deep reinforcement learning**.\" arXiv preprint arXiv:1509.02971 (2015). [[pdf]](http://arxiv.org/pdf/1509.02971) **(DDPG)** :star::star::star::star:\n\n**[51]** Gu, Shixiang, et al. \"**Continuous Deep Q-Learning with Model-based Acceleration**.\" arXiv preprint arXiv:1603.00748 (2016). [[pdf]](http://arxiv.org/pdf/1603.00748) **(NAF)** :star::star::star::star:\n\n**[52]** Schulman, John, et al. \"**Trust region policy optimization**.\" CoRR, abs/1502.05477 (2015). [[pdf]](http://www.jmlr.org/proceedings/papers/v37/schulman15.pdf) **(TRPO)** :star::star::star::star:\n\n**[53]** Silver, David, et al. \"**Mastering the game of Go with deep neural networks and tree search**.\" Nature 529.7587 (2016): 484-489. [[pdf]](http://willamette.edu/~levenick/cs448/goNature.pdf) **(AlphaGo)** :star::star::star::star::star:\n\n## 2.7 Deep Transfer Learning / Lifelong Learning / especially for RL\n\n**[54]** Bengio, Yoshua. \"**Deep Learning of Representations for Unsupervised and Transfer Learning**.\" ICML Unsupervised and Transfer Learning 27 (2012): 17-36. [[pdf]](http://www.jmlr.org/proceedings/papers/v27/bengio12a/bengio12a.pdf) **(A Tutorial)** :star::star::star:\n\n**[55]** Silver, Daniel L., Qiang Yang, and Lianghao Li. \"**Lifelong Machine Learning Systems: Beyond Learning Algorithms**.\" AAAI Spring Symposium: Lifelong Machine Learning. 2013. [[pdf]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.696.7800&rep=rep1&type=pdf) **(A brief discussion about lifelong learning)**  :star::star::star:\n\n**[56]** Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. \"**Distilling the knowledge in a neural network**.\" arXiv preprint arXiv:1503.02531 (2015). [[pdf]](http://arxiv.org/pdf/1503.02531) **(Godfather's Work)** :star::star::star::star:\n\n**[57]** Rusu, Andrei A., et al. \"**Policy distillation**.\" arXiv preprint arXiv:1511.06295 (2015). [[pdf]](http://arxiv.org/pdf/1511.06295) **(RL domain)** :star::star::star:\n\n**[58]** Parisotto, Emilio, Jimmy Lei Ba, and Ruslan Salakhutdinov. \"**Actor-mimic: Deep multitask and transfer reinforcement learning**.\" arXiv preprint arXiv:1511.06342 (2015). [[pdf]](http://arxiv.org/pdf/1511.06342) **(RL domain)** :star::star::star:\n\n**[59]** Rusu, Andrei A., et al. \"**Progressive neural networks**.\" arXiv preprint arXiv:1606.04671 (2016). [[pdf]](https://arxiv.org/pdf/1606.04671) **(Outstanding Work, A novel idea)** :star::star::star::star::star:\n\n\n## 2.8 One Shot Deep Learning\n\n**[60]** Lake, Brenden M., Ruslan Salakhutdinov, and Joshua B. Tenenbaum. \"**Human-level concept learning through probabilistic program induction**.\" Science 350.6266 (2015): 1332-1338. [[pdf]](http://clm.utexas.edu/compjclub/wp-content/uploads/2016/02/lake2015.pdf) **(No Deep Learning,but worth reading)** :star::star::star::star::star:\n\n**[61]** Koch, Gregory, Richard Zemel, and Ruslan Salakhutdinov. \"**Siamese Neural Networks for One-shot Image Recognition**.\"(2015) [[pdf]](http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf) :star::star::star:\n\n**[62]** Santoro, Adam, et al. \"**One-shot Learning with Memory-Augmented Neural Networks**.\" arXiv preprint arXiv:1605.06065 (2016). [[pdf]](http://arxiv.org/pdf/1605.06065) **(A basic step to one shot learning)** :star::star::star::star:\n\n**[63]** Vinyals, Oriol, et al. \"**Matching Networks for One Shot Learning**.\" arXiv preprint arXiv:1606.04080 (2016). [[pdf]](https://arxiv.org/pdf/1606.04080) :star::star::star:\n\n**[64]** Hariharan, Bharath, and Ross Girshick. \"**Low-shot visual object recognition**.\" arXiv preprint arXiv:1606.02819 (2016). [[pdf]](http://arxiv.org/pdf/1606.02819) **(A step to large data)** :star::star::star::star:\n\n\n# 3 Applications\n\n## 3.1 NLP(Natural Language Processing)\n\n**[1]** Antoine Bordes, et al. \"**Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing**.\" AISTATS(2012) [[pdf]](https://www.hds.utc.fr/~bordesan/dokuwiki/lib/exe/fetch.php?id=en%3Apubli&cache=cache&media=en:bordes12aistats.pdf) :star::star::star::star:\n\n**[2]** Mikolov, et al. \"**Distributed representations of words and phrases and their compositionality**.\" ANIPS(2013): 3111-3119 [[pdf]](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) **(word2vec)** :star::star::star:\n\n**[3]** Sutskever, et al. \"**\u201cSequence to sequence learning with neural networks**.\" ANIPS(2014) [[pdf]](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) :star::star::star:\n\n**[4]** Ankit Kumar, et al. \"**\u201cAsk Me Anything: Dynamic Memory Networks for Natural Language Processing**.\" arXiv preprint arXiv:1506.07285(2015) [[pdf]](https://arxiv.org/abs/1506.07285) :star::star::star::star:\n\n**[5]** Yoon Kim, et al. \"**Character-Aware Neural Language Models**.\" NIPS(2015) arXiv preprint arXiv:1508.06615(2015) [[pdf]](https://arxiv.org/abs/1508.06615) :star::star::star::star:\n\n**[6]** Jason Weston, et al. \"**Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks**.\" arXiv preprint arXiv:1502.05698(2015) [[pdf]](https://arxiv.org/abs/1502.05698) **(bAbI tasks)** :star::star::star:\n\n**[7]** Karl Moritz Hermann, et al. \"**Teaching Machines to Read and Comprehend**.\" arXiv preprint arXiv:1506.03340(2015) [[pdf]](https://arxiv.org/abs/1506.03340) **(CNN/DailyMail cloze style questions)** :star::star:\n\n**[8]** Alexis Conneau, et al. \"**Very Deep Convolutional Networks for Natural Language Processing**.\" arXiv preprint arXiv:1606.01781(2016) [[pdf]](https://arxiv.org/abs/1606.01781) **(state-of-the-art in text classification)** :star::star::star:\n\n**[9]** Armand Joulin, et al. \"**Bag of Tricks for Efficient Text Classification**.\" arXiv preprint arXiv:1607.01759(2016) [[pdf]](https://arxiv.org/abs/1607.01759) **(slightly worse than state-of-the-art, but a lot faster)** :star::star::star:\n\n## 3.2 Object Detection\n\n**[1]** Szegedy, Christian, Alexander Toshev, and Dumitru Erhan. \"**Deep neural networks for object detection**.\" Advances in Neural Information Processing Systems. 2013. [[pdf]](http://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection.pdf) :star::star::star:\n\n**[2]** Girshick, Ross, et al. \"**Rich feature hierarchies for accurate object detection and semantic segmentation**.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2014. [[pdf]](http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf) **(RCNN)** :star::star::star::star::star:\n\n**[3]** He, Kaiming, et al. \"**Spatial pyramid pooling in deep convolutional networks for visual recognition**.\" European Conference on Computer Vision. Springer International Publishing, 2014. [[pdf]](http://arxiv.org/pdf/1406.4729) **(SPPNet)** :star::star::star::star:\n\n**[4]** Girshick, Ross. \"**Fast r-cnn**.\" Proceedings of the IEEE International Conference on Computer Vision. 2015. [[pdf]](https://pdfs.semanticscholar.org/8f67/64a59f0d17081f2a2a9d06f4ed1cdea1a0ad.pdf) :star::star::star::star:\n\n**[5]** Ren, Shaoqing, et al. \"**Faster R-CNN: Towards real-time object detection with region proposal networks**.\" Advances in neural information processing systems. 2015. [[pdf]](https://arxiv.org/pdf/1506.01497.pdf) :star::star::star::star:\n\n**[6]** Redmon, Joseph, et al. \"**You only look once: Unified, real-time object detection**.\" arXiv preprint arXiv:1506.02640 (2015). [[pdf]](http://homes.cs.washington.edu/~ali/papers/YOLO.pdf) **(YOLO,Oustanding Work, really practical)** :star::star::star::star::star:\n\n**[7]** Liu, Wei, et al. \"**SSD: Single Shot MultiBox Detector**.\" arXiv preprint arXiv:1512.02325 (2015). [[pdf]](http://arxiv.org/pdf/1512.02325) :star::star::star:\n\n**[8]** Dai, Jifeng, et al. \"**R-FCN: Object Detection via\nRegion-based Fully Convolutional Networks**.\" arXiv preprint arXiv:1605.06409 (2016). [[pdf]](https://arxiv.org/abs/1605.06409) :star::star::star::star:\n\n**[9]** He, Gkioxari, et al. \"**Mask R-CNN**\" arXiv preprint arXiv:1703.06870 (2017). [[pdf]](https://arxiv.org/abs/1703.06870) :star::star::star::star:\n\n**[10]** Bochkovskiy, Alexey, et al. \"**YOLOv4: Optimal Speed and Accuracy of Object Detection.**\"  arXiv preprint arXiv:2004.10934 (2020). [[pdf]](https://arxiv.org/pdf/2004.10934) :star::star::star::star:\n\n\n**[11]** Tan, Mingxing, et al. \u201c**EfficientDet: Scalable and Efficient Object Detection.**\" arXiv preprint arXiv:1911.09070 (2019). [[pdf]](https://arxiv.org/pdf/1911.09070) :star::star::star::star::star:\n\n\n## 3.3 Visual Tracking\n\n**[1]** Wang, Naiyan, and Dit-Yan Yeung. \"**Learning a deep compact image representation for visual tracking**.\" Advances in neural information processing systems. 2013. [[pdf]](http://papers.nips.cc/paper/5192-learning-a-deep-compact-image-representation-for-visual-tracking.pdf) **(First Paper to do visual tracking using Deep Learning,DLT Tracker)** :star::star::star:\n\n**[2]** Wang, Naiyan, et al. \"**Transferring rich feature hierarchies for robust visual tracking**.\" arXiv preprint arXiv:1501.04587 (2015). [[pdf]](http://arxiv.org/pdf/1501.04587) **(SO-DLT)** :star::star::star::star:\n\n**[3]** Wang, Lijun, et al. \"**Visual tracking with fully convolutional networks**.\" Proceedings of the IEEE International Conference on Computer Vision. 2015. [[pdf]](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Wang_Visual_Tracking_With_ICCV_2015_paper.pdf) **(FCNT)** :star::star::star::star:\n\n**[4]** Held, David, Sebastian Thrun, and Silvio Savarese. \"**Learning to Track at 100 FPS with Deep Regression Networks**.\" arXiv preprint arXiv:1604.01802 (2016). [[pdf]](http://arxiv.org/pdf/1604.01802) **(GOTURN,Really fast as a deep learning method,but still far behind un-deep-learning methods)** :star::star::star::star:\n\n**[5]** Bertinetto, Luca, et al. \"**Fully-Convolutional Siamese Networks for Object Tracking**.\" arXiv preprint arXiv:1606.09549 (2016). [[pdf]](https://arxiv.org/pdf/1606.09549) **(SiameseFC,New state-of-the-art for real-time object tracking)** :star::star::star::star:\n\n**[6]** Martin Danelljan, Andreas Robinson, Fahad Khan, Michael Felsberg. \"**Beyond Correlation Filters: Learning Continuous Convolution Operators for Visual Tracking**.\" ECCV (2016) [[pdf]](http://www.cvl.isy.liu.se/research/objrec/visualtracking/conttrack/C-COT_ECCV16.pdf) **(C-COT)** :star::star::star::star:\n\n**[7]** Nam, Hyeonseob, Mooyeol Baek, and Bohyung Han. \"**Modeling and Propagating CNNs in a Tree Structure for Visual Tracking**.\" arXiv preprint arXiv:1608.07242 (2016). [[pdf]](https://arxiv.org/pdf/1608.07242) **(VOT2016 Winner,TCNN)** :star::star::star::star:\n\n## 3.4 Image Caption\n**[1]** Farhadi,Ali,etal. \"**Every picture tells a story: Generating sentences from images**\". In Computer VisionECCV 2010. Springer Berlin Heidelberg:15-29, 2010. [[pdf]](https://www.cs.cmu.edu/~afarhadi/papers/sentence.pdf) :star::star::star:\n\n**[2]** Kulkarni, Girish, et al. \"**Baby talk: Understanding and generating image descriptions**\". In Proceedings of the 24th CVPR, 2011. [[pdf]](http://tamaraberg.com/papers/generation_cvpr11.pdf):star::star::star::star:\n\n**[3]** Vinyals, Oriol, et al. \"**Show and tell: A neural image caption generator**\". In arXiv preprint arXiv:1411.4555, 2014. [[pdf]](https://arxiv.org/pdf/1411.4555.pdf):star::star::star:\n\n**[4]** Donahue, Jeff, et al. \"**Long-term recurrent convolutional networks for visual recognition and description**\". In arXiv preprint arXiv:1411.4389 ,2014. [[pdf]](https://arxiv.org/pdf/1411.4389.pdf)\n\n**[5]** Karpathy, Andrej, and Li Fei-Fei. \"**Deep visual-semantic alignments for generating image descriptions**\". In arXiv preprint arXiv:1412.2306, 2014. [[pdf]](https://cs.stanford.edu/people/karpathy/cvpr2015.pdf):star::star::star::star::star:\n\n**[6]** Karpathy, Andrej, Armand Joulin, and Fei Fei F. Li. \"**Deep fragment embeddings for bidirectional image sentence mapping**\". In Advances in neural information processing systems, 2014. [[pdf]](https://arxiv.org/pdf/1406.5679v1.pdf):star::star::star::star:\n\n**[7]** Fang, Hao, et al. \"**From captions to visual concepts and back**\". In arXiv preprint arXiv:1411.4952, 2014. [[pdf]](https://arxiv.org/pdf/1411.4952v3.pdf):star::star::star::star::star:\n\n**[8]** Chen, Xinlei, and C. Lawrence Zitnick. \"**Learning a recurrent visual representation for image caption generation**\". In arXiv preprint arXiv:1411.5654, 2014. [[pdf]](https://arxiv.org/pdf/1411.5654v1.pdf):star::star::star::star:\n\n**[9]** Mao, Junhua, et al. \"**Deep captioning with multimodal recurrent neural networks (m-rnn)**\". In arXiv preprint arXiv:1412.6632, 2014. [[pdf]](https://arxiv.org/pdf/1412.6632v5.pdf):star::star::star:\n\n**[10]** Xu, Kelvin, et al. \"**Show, attend and tell: Neural image caption generation with visual attention**\". In arXiv preprint arXiv:1502.03044, 2015. [[pdf]](https://arxiv.org/pdf/1502.03044v3.pdf):star::star::star::star::star:\n\n## 3.5 Machine Translation\n\n> Some milestone papers are listed in RNN / Seq-to-Seq topic.\n\n**[1]** Luong, Minh-Thang, et al. \"**Addressing the rare word problem in neural machine translation**.\" arXiv preprint arXiv:1410.8206 (2014). [[pdf]](http://arxiv.org/pdf/1410.8206) :star::star::star::star:\n\n\n**[2]** Sennrich, et al. \"**Neural Machine Translation of Rare Words with Subword Units**\". In arXiv preprint arXiv:1508.07909, 2015. [[pdf]](https://arxiv.org/pdf/1508.07909.pdf):star::star::star:\n\n**[3]** Luong, Minh-Thang, Hieu Pham, and Christopher D. Manning. \"**Effective approaches to attention-based neural machine translation**.\" arXiv preprint arXiv:1508.04025 (2015). [[pdf]](http://arxiv.org/pdf/1508.04025) :star::star::star::star:\n\n**[4]** Chung, et al. \"**A Character-Level Decoder without Explicit Segmentation for Neural Machine Translation**\". In arXiv preprint arXiv:1603.06147, 2016. [[pdf]](https://arxiv.org/pdf/1603.06147.pdf):star::star:\n\n**[5]** Lee, et al. \"**Fully Character-Level Neural Machine Translation without Explicit Segmentation**\". In arXiv preprint arXiv:1610.03017, 2016. [[pdf]](https://arxiv.org/pdf/1610.03017.pdf):star::star::star::star::star:\n\n**[6]** Wu, Schuster, Chen, Le, et al. \"**Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation**\". In arXiv preprint arXiv:1609.08144v2, 2016. [[pdf]](https://arxiv.org/pdf/1609.08144v2.pdf) **(Milestone)** :star::star::star::star:\n\n## 3.6 Robotics\n\n**[1]** Koutn\u00edk, Jan, et al. \"**Evolving large-scale neural networks for vision-based reinforcement learning**.\" Proceedings of the 15th annual conference on Genetic and evolutionary computation. ACM, 2013. [[pdf]](http://repository.supsi.ch/4550/1/koutnik2013gecco.pdf) :star::star::star:\n\n**[2]** Levine, Sergey, et al. \"**End-to-end training of deep visuomotor policies**.\" Journal of Machine Learning Research 17.39 (2016): 1-40. [[pdf]](http://www.jmlr.org/papers/volume17/15-522/15-522.pdf) :star::star::star::star::star:\n\n**[3]** Pinto, Lerrel, and Abhinav Gupta. \"**Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours**.\" arXiv preprint arXiv:1509.06825 (2015). [[pdf]](http://arxiv.org/pdf/1509.06825) :star::star::star:\n\n**[4]** Levine, Sergey, et al. \"**Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection**.\" arXiv preprint arXiv:1603.02199 (2016). [[pdf]](http://arxiv.org/pdf/1603.02199) :star::star::star::star:\n\n**[5]** Zhu, Yuke, et al. \"**Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning**.\" arXiv preprint arXiv:1609.05143 (2016). [[pdf]](https://arxiv.org/pdf/1609.05143) :star::star::star::star:\n\n**[6]** Yahya, Ali, et al. \"**Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search**.\" arXiv preprint arXiv:1610.00673 (2016). [[pdf]](https://arxiv.org/pdf/1610.00673) :star::star::star::star:\n\n**[7]** Gu, Shixiang, et al. \"**Deep Reinforcement Learning for Robotic Manipulation**.\" arXiv preprint arXiv:1610.00633 (2016). [[pdf]](https://arxiv.org/pdf/1610.00633) :star::star::star::star:\n\n**[8]** A Rusu, M Vecerik, Thomas Roth\u00f6rl, N Heess, R Pascanu, R Hadsell.\"**Sim-to-Real Robot Learning from Pixels with Progressive Nets**.\" arXiv preprint arXiv:1610.04286 (2016). [[pdf]](https://arxiv.org/pdf/1610.04286.pdf) :star::star::star::star:\n\n**[9]** Mirowski, Piotr, et al. \"**Learning to navigate in complex environments**.\" arXiv preprint arXiv:1611.03673 (2016). [[pdf]](https://arxiv.org/pdf/1611.03673) :star::star::star::star:\n\n## 3.7 Art\n\n**[1]** Mordvintsev, Alexander; Olah, Christopher; Tyka, Mike (2015). \"**Inceptionism: Going Deeper into Neural Networks**\". Google Research. [[html]](https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html) **(Deep Dream)**\n:star::star::star::star:\n\n**[2]** Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. \"**A neural algorithm of artistic style**.\" arXiv preprint arXiv:1508.06576 (2015). [[pdf]](http://arxiv.org/pdf/1508.06576) **(Outstanding Work, most successful method currently)** :star::star::star::star::star:\n\n**[3]** Zhu, Jun-Yan, et al. \"**Generative Visual Manipulation on the Natural Image Manifold**.\" European Conference on Computer Vision. Springer International Publishing, 2016. [[pdf]](https://arxiv.org/pdf/1609.03552) **(iGAN)** :star::star::star::star:\n\n**[4]** Champandard, Alex J. \"**Semantic Style Transfer and Turning Two-Bit Doodles into Fine Artworks**.\" arXiv preprint arXiv:1603.01768 (2016). [[pdf]](http://arxiv.org/pdf/1603.01768) **(Neural Doodle)** :star::star::star::star:\n\n**[5]** Zhang, Richard, Phillip Isola, and Alexei A. Efros. \"**Colorful Image Colorization**.\" arXiv preprint arXiv:1603.08511 (2016). [[pdf]](http://arxiv.org/pdf/1603.08511) :star::star::star::star:\n\n**[6]** Johnson, Justin, Alexandre Alahi, and Li Fei-Fei. \"**Perceptual losses for real-time style transfer and super-resolution**.\" arXiv preprint arXiv:1603.08155 (2016). [[pdf]](https://arxiv.org/pdf/1603.08155.pdf) :star::star::star::star:\n\n**[7]** Vincent Dumoulin, Jonathon Shlens and Manjunath Kudlur. \"**A learned representation for artistic style**.\" arXiv preprint arXiv:1610.07629 (2016). [[pdf]](https://arxiv.org/pdf/1610.07629v1.pdf) :star::star::star::star:\n\n**[8]** Gatys, Leon and Ecker, et al.\"**Controlling Perceptual Factors in Neural Style Transfer**.\" arXiv preprint arXiv:1611.07865 (2016). [[pdf]](https://arxiv.org/pdf/1611.07865.pdf) **(control style transfer over spatial location,colour information and across spatial scale)**:star::star::star::star:\n\n**[9]** Ulyanov, Dmitry and Lebedev, Vadim, et al. \"**Texture Networks: Feed-forward Synthesis of Textures and Stylized Images**.\" arXiv preprint arXiv:1603.03417(2016). [[pdf]](http://arxiv.org/abs/1603.03417) **(texture generation and style transfer)** :star::star::star::star:\n\n**[10]** Yijun Li, Ming-Yu Liu ,Xueting Li, Ming-Hsuan Yang,Jan Kautz (NVIDIA). \"**A Closed-form Solution to Photorealistic Image Stylization**.\" arXiv preprint arXiv:1802.06474(2018). [[pdf]](https://arxiv.org/pdf/1802.06474.pdf) **(Very fast and ultra realistic style transfer)** :star::star::star::star:\n\n## 3.8 Object Segmentation\n\n**[1]** J. Long, E. Shelhamer, and T. Darrell, \u201c**Fully convolutional networks for semantic segmentation**.\u201d in CVPR, 2015. [[pdf]](https://arxiv.org/pdf/1411.4038v2.pdf) :star::star::star::star::star:\n\n**[2]** L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. \"**Semantic image segmentation with deep convolutional nets and fully connected crfs**.\" In ICLR, 2015. [[pdf]](https://arxiv.org/pdf/1606.00915v1.pdf) :star::star::star::star::star:\n\n**[3]** Pinheiro, P.O., Collobert, R., Dollar, P. \"**Learning to segment object candidates.**\" In: NIPS. 2015. [[pdf]](https://arxiv.org/pdf/1506.06204v2.pdf) :star::star::star::star:\n\n**[4]** Dai, J., He, K., Sun, J. \"**Instance-aware semantic segmentation via multi-task network cascades**.\" in CVPR. 2016 [[pdf]](https://arxiv.org/pdf/1512.04412v1.pdf) :star::star::star:\n\n**[5]** Dai, J., He, K., Sun, J. \"**Instance-sensitive Fully Convolutional Networks**.\" arXiv preprint arXiv:1603.08678 (2016). [[pdf]](https://arxiv.org/pdf/1603.08678v1.pdf) :star::star::star:\n\n\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 349321229,
    "name": "GFPGAN",
    "full_name": "TencentARC/GFPGAN",
    "description": "GFPGAN aims at developing Practical Algorithms for Real-world Face Restoration.",
    "html_url": "https://github.com/TencentARC/GFPGAN",
    "clone_url": "https://github.com/TencentARC/GFPGAN.git",
    "owner_login": "TencentARC",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/83739826?v=4",
    "stargazers_count": 36967,
    "watchers_count": 36967,
    "forks_count": 6159,
    "open_issues_count": 392,
    "size": 5467,
    "language": "Python",
    "languages": {
      "Python": 208553
    },
    "topics": [
      "deep-learning",
      "face-restoration",
      "gan",
      "gfpgan",
      "image-restoration",
      "pytorch",
      "super-resolution"
    ],
    "license_name": "Other",
    "created_at": "2021-03-19T06:18:20+00:00",
    "updated_at": "2025-08-05T18:48:27+00:00",
    "pushed_at": "2024-07-26T18:44:02+00:00",
    "contributors_count": 11,
    "readme_length": 11710,
    "readme_content": "<p align=\"center\">\n  <img src=\"assets/gfpgan_logo.png\" height=130>\n</p>\n\n## <div align=\"center\"><b><a href=\"README.md\">English</a> | <a href=\"README_CN.md\">\u7b80\u4f53\u4e2d\u6587</a></b></div>\n\n<div align=\"center\">\n<!-- <a href=\"https://twitter.com/_Xintao_\" style=\"text-decoration:none;\">\n    <img src=\"https://user-images.githubusercontent.com/17445847/187162058-c764ced6-952f-404b-ac85-ba95cce18e7b.png\" width=\"4%\" alt=\"\" />\n</a> -->\n\n[![download](https://img.shields.io/github/downloads/TencentARC/GFPGAN/total.svg)](https://github.com/TencentARC/GFPGAN/releases)\n[![PyPI](https://img.shields.io/pypi/v/gfpgan)](https://pypi.org/project/gfpgan/)\n[![Open issue](https://img.shields.io/github/issues/TencentARC/GFPGAN)](https://github.com/TencentARC/GFPGAN/issues)\n[![Closed issue](https://img.shields.io/github/issues-closed/TencentARC/GFPGAN)](https://github.com/TencentARC/GFPGAN/issues)\n[![LICENSE](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/TencentARC/GFPGAN/blob/master/LICENSE)\n[![python lint](https://github.com/TencentARC/GFPGAN/actions/workflows/pylint.yml/badge.svg)](https://github.com/TencentARC/GFPGAN/blob/master/.github/workflows/pylint.yml)\n[![Publish-pip](https://github.com/TencentARC/GFPGAN/actions/workflows/publish-pip.yml/badge.svg)](https://github.com/TencentARC/GFPGAN/blob/master/.github/workflows/publish-pip.yml)\n</div>\n\n1. :boom: **Updated** online demo: [![Replicate](https://img.shields.io/static/v1?label=Demo&message=Replicate&color=blue)](https://replicate.com/tencentarc/gfpgan). Here is the [backup](https://replicate.com/xinntao/gfpgan).\n1. :boom: **Updated** online demo: [![Huggingface Gradio](https://img.shields.io/static/v1?label=Demo&message=Huggingface%20Gradio&color=orange)](https://huggingface.co/spaces/Xintao/GFPGAN)\n1. [Colab Demo](https://colab.research.google.com/drive/1sVsoBd9AjckIXThgtZhGrHRfFI6UUYOo) for GFPGAN <a href=\"https://colab.research.google.com/drive/1sVsoBd9AjckIXThgtZhGrHRfFI6UUYOo\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"google colab logo\"></a>; (Another [Colab Demo](https://colab.research.google.com/drive/1Oa1WwKB4M4l1GmR7CtswDVgOCOeSLChA?usp=sharing) for the original paper model)\n\n<!-- 3. Online demo: [Replicate.ai](https://replicate.com/xinntao/gfpgan) (may need to sign in, return the whole image)\n4. Online demo: [Baseten.co](https://app.baseten.co/applications/Q04Lz0d/operator_views/8qZG6Bg) (backed by GPU, returns the whole image)\n5. We provide a *clean* version of GFPGAN, which can run without CUDA extensions. So that it can run in **Windows** or on **CPU mode**. -->\n\n> :rocket: **Thanks for your interest in our work. You may also want to check our new updates on the *tiny models* for *anime images and videos* in [Real-ESRGAN](https://github.com/xinntao/Real-ESRGAN/blob/master/docs/anime_video_model.md)** :blush:\n\nGFPGAN aims at developing a **Practical Algorithm for Real-world Face Restoration**.<br>\nIt leverages rich and diverse priors encapsulated in a pretrained face GAN (*e.g.*, StyleGAN2) for blind face restoration.\n\n:question: Frequently Asked Questions can be found in [FAQ.md](FAQ.md).\n\n:triangular_flag_on_post: **Updates**\n\n- :white_check_mark: Add [RestoreFormer](https://github.com/wzhouxiff/RestoreFormer) inference codes.\n- :white_check_mark: Add [V1.4 model](https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.4.pth), which produces slightly more details and better identity than V1.3.\n- :white_check_mark: Add **[V1.3 model](https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth)**, which produces **more natural** restoration results, and better results on *very low-quality* / *high-quality* inputs. See more in [Model zoo](#european_castle-model-zoo), [Comparisons.md](Comparisons.md)\n- :white_check_mark: Integrated to [Huggingface Spaces](https://huggingface.co/spaces) with [Gradio](https://github.com/gradio-app/gradio). See [Gradio Web Demo](https://huggingface.co/spaces/akhaliq/GFPGAN).\n- :white_check_mark: Support enhancing non-face regions (background) with [Real-ESRGAN](https://github.com/xinntao/Real-ESRGAN).\n- :white_check_mark: We provide a *clean* version of GFPGAN, which does not require CUDA extensions.\n- :white_check_mark: We provide an updated model without colorizing faces.\n\n---\n\nIf GFPGAN is helpful in your photos/projects, please help to :star: this repo or recommend it to your friends. Thanks:blush:\nOther recommended projects:<br>\n:arrow_forward: [Real-ESRGAN](https://github.com/xinntao/Real-ESRGAN): A practical algorithm for general image restoration<br>\n:arrow_forward: [BasicSR](https://github.com/xinntao/BasicSR): An open-source image and video restoration toolbox<br>\n:arrow_forward: [facexlib](https://github.com/xinntao/facexlib): A collection that provides useful face-relation functions<br>\n:arrow_forward: [HandyView](https://github.com/xinntao/HandyView): A PyQt5-based image viewer that is handy for view and comparison<br>\n\n---\n\n### :book: GFP-GAN: Towards Real-World Blind Face Restoration with Generative Facial Prior\n\n> [[Paper](https://arxiv.org/abs/2101.04061)] &emsp; [[Project Page](https://xinntao.github.io/projects/gfpgan)] &emsp; [Demo] <br>\n> [Xintao Wang](https://xinntao.github.io/), [Yu Li](https://yu-li.github.io/), [Honglun Zhang](https://scholar.google.com/citations?hl=en&user=KjQLROoAAAAJ), [Ying Shan](https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en) <br>\n> Applied Research Center (ARC), Tencent PCG\n\n<p align=\"center\">\n  <img src=\"https://xinntao.github.io/projects/GFPGAN_src/gfpgan_teaser.jpg\">\n</p>\n\n---\n\n## :wrench: Dependencies and Installation\n\n- Python >= 3.7 (Recommend to use [Anaconda](https://www.anaconda.com/download/#linux) or [Miniconda](https://docs.conda.io/en/latest/miniconda.html))\n- [PyTorch >= 1.7](https://pytorch.org/)\n- Option: NVIDIA GPU + [CUDA](https://developer.nvidia.com/cuda-downloads)\n- Option: Linux\n\n### Installation\n\nWe now provide a *clean* version of GFPGAN, which does not require customized CUDA extensions. <br>\nIf you want to use the original model in our paper, please see [PaperModel.md](PaperModel.md) for installation.\n\n1. Clone repo\n\n    ```bash\n    git clone https://github.com/TencentARC/GFPGAN.git\n    cd GFPGAN\n    ```\n\n1. Install dependent packages\n\n    ```bash\n    # Install basicsr - https://github.com/xinntao/BasicSR\n    # We use BasicSR for both training and inference\n    pip install basicsr\n\n    # Install facexlib - https://github.com/xinntao/facexlib\n    # We use face detection and face restoration helper in the facexlib package\n    pip install facexlib\n\n    pip install -r requirements.txt\n    python setup.py develop\n\n    # If you want to enhance the background (non-face) regions with Real-ESRGAN,\n    # you also need to install the realesrgan package\n    pip install realesrgan\n    ```\n\n## :zap: Quick Inference\n\nWe take the v1.3 version for an example. More models can be found [here](#european_castle-model-zoo).\n\nDownload pre-trained models: [GFPGANv1.3.pth](https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth)\n\n```bash\nwget https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth -P experiments/pretrained_models\n```\n\n**Inference!**\n\n```bash\npython inference_gfpgan.py -i inputs/whole_imgs -o results -v 1.3 -s 2\n```\n\n```console\nUsage: python inference_gfpgan.py -i inputs/whole_imgs -o results -v 1.3 -s 2 [options]...\n\n  -h                   show this help\n  -i input             Input image or folder. Default: inputs/whole_imgs\n  -o output            Output folder. Default: results\n  -v version           GFPGAN model version. Option: 1 | 1.2 | 1.3. Default: 1.3\n  -s upscale           The final upsampling scale of the image. Default: 2\n  -bg_upsampler        background upsampler. Default: realesrgan\n  -bg_tile             Tile size for background sampler, 0 for no tile during testing. Default: 400\n  -suffix              Suffix of the restored faces\n  -only_center_face    Only restore the center face\n  -aligned             Input are aligned faces\n  -ext                 Image extension. Options: auto | jpg | png, auto means using the same extension as inputs. Default: auto\n```\n\nIf you want to use the original model in our paper, please see [PaperModel.md](PaperModel.md) for installation and inference.\n\n## :european_castle: Model Zoo\n\n| Version | Model Name  | Description |\n| :---: | :---:        |     :---:      |\n| V1.3 | [GFPGANv1.3.pth](https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth) | Based on V1.2; **more natural** restoration results; better results on very low-quality / high-quality inputs. |\n| V1.2 | [GFPGANCleanv1-NoCE-C2.pth](https://github.com/TencentARC/GFPGAN/releases/download/v0.2.0/GFPGANCleanv1-NoCE-C2.pth) | No colorization; no CUDA extensions are required. Trained with more data with pre-processing. |\n| V1 | [GFPGANv1.pth](https://github.com/TencentARC/GFPGAN/releases/download/v0.1.0/GFPGANv1.pth) | The paper model, with colorization. |\n\nThe comparisons are in [Comparisons.md](Comparisons.md).\n\nNote that V1.3 is not always better than V1.2. You may need to select different models based on your purpose and inputs.\n\n| Version | Strengths  | Weaknesses |\n| :---: | :---:        |     :---:      |\n|V1.3 |  \u2713 natural outputs<br> \u2713better results on very low-quality inputs <br> \u2713 work on relatively high-quality inputs <br>\u2713 can have repeated (twice) restorations | \u2717 not very sharp <br> \u2717 have a slight change on identity |\n|V1.2 |  \u2713 sharper output <br> \u2713 with beauty makeup | \u2717 some outputs are unnatural |\n\nYou can find **more models (such as the discriminators)** here: [[Google Drive](https://drive.google.com/drive/folders/17rLiFzcUMoQuhLnptDsKolegHWwJOnHu?usp=sharing)], OR [[Tencent Cloud \u817e\u8baf\u5fae\u4e91](https://share.weiyun.com/ShYoCCoc)]\n\n## :computer: Training\n\nWe provide the training codes for GFPGAN (used in our paper). <br>\nYou could improve it according to your own needs.\n\n**Tips**\n\n1. More high quality faces can improve the restoration quality.\n2. You may need to perform some pre-processing, such as beauty makeup.\n\n**Procedures**\n\n(You can try a simple version ( `options/train_gfpgan_v1_simple.yml`) that does not require face component landmarks.)\n\n1. Dataset preparation: [FFHQ](https://github.com/NVlabs/ffhq-dataset)\n\n1. Download pre-trained models and other data. Put them in the `experiments/pretrained_models` folder.\n    1. [Pre-trained StyleGAN2 model: StyleGAN2_512_Cmul1_FFHQ_B12G4_scratch_800k.pth](https://github.com/TencentARC/GFPGAN/releases/download/v0.1.0/StyleGAN2_512_Cmul1_FFHQ_B12G4_scratch_800k.pth)\n    1. [Component locations of FFHQ: FFHQ_eye_mouth_landmarks_512.pth](https://github.com/TencentARC/GFPGAN/releases/download/v0.1.0/FFHQ_eye_mouth_landmarks_512.pth)\n    1. [A simple ArcFace model: arcface_resnet18.pth](https://github.com/TencentARC/GFPGAN/releases/download/v0.1.0/arcface_resnet18.pth)\n\n1. Modify the configuration file `options/train_gfpgan_v1.yml` accordingly.\n\n1. Training\n\n> python -m torch.distributed.launch --nproc_per_node=4 --master_port=22021 gfpgan/train.py -opt options/train_gfpgan_v1.yml --launcher pytorch\n\n## :scroll: License and Acknowledgement\n\nGFPGAN is released under Apache License Version 2.0.\n\n## BibTeX\n\n    @InProceedings{wang2021gfpgan,\n        author = {Xintao Wang and Yu Li and Honglun Zhang and Ying Shan},\n        title = {Towards Real-World Blind Face Restoration with Generative Facial Prior},\n        booktitle={The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n        year = {2021}\n    }\n\n## :e-mail: Contact\n\nIf you have any question, please email `xintao.wang@outlook.com` or `xintaowang@tencent.com`.\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 393571599,
    "name": "MockingBird",
    "full_name": "babysor/MockingBird",
    "description": "\ud83d\ude80AI\u62df\u58f0: 5\u79d2\u5185\u514b\u9686\u60a8\u7684\u58f0\u97f3\u5e76\u751f\u6210\u4efb\u610f\u8bed\u97f3\u5185\u5bb9 Clone a voice in 5 seconds to generate arbitrary speech in real-time",
    "html_url": "https://github.com/babysor/MockingBird",
    "clone_url": "https://github.com/babysor/MockingBird.git",
    "owner_login": "babysor",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/7423248?v=4",
    "stargazers_count": 36515,
    "watchers_count": 36515,
    "forks_count": 5262,
    "open_issues_count": 481,
    "size": 130682,
    "language": "Python",
    "languages": {
      "Python": 769956,
      "Shell": 1868,
      "Cython": 1148,
      "Dockerfile": 533
    },
    "topics": [
      "ai",
      "deep-learning",
      "pytorch",
      "speech",
      "text-to-speech",
      "tts"
    ],
    "license_name": "Other",
    "created_at": "2021-08-07T03:53:39+00:00",
    "updated_at": "2025-08-05T08:43:37+00:00",
    "pushed_at": "2024-11-15T05:00:29+00:00",
    "contributors_count": 37,
    "readme_length": 15092,
    "readme_content": "> \ud83d\udea7 While I no longer actively update this repo, you can find me continuously pushing this tech forward to good side and open-source. I'm also building an optimized and cloud hosted version: https://noiz.ai/ and it's free but not ready for commersial use now.\n>\n![mockingbird](https://user-images.githubusercontent.com/12797292/131216767-6eb251d6-14fc-4951-8324-2722f0cd4c63.jpg)\n\n\n[![MIT License](https://img.shields.io/badge/license-MIT-blue.svg?style=flat)](http://choosealicense.com/licenses/mit/)\n\n> English | [\u4e2d\u6587](README-CN.md)| [\u4e2d\u6587Linux](README-LINUX-CN.md)\n\n## Features\n\ud83c\udf0d **Chinese** supported mandarin and tested with multiple datasets: aidatatang_200zh, magicdata, aishell3, data_aishell, and etc.\n\n\ud83e\udd29 **PyTorch** worked for pytorch, tested in version of 1.9.0(latest in August 2021), with GPU Tesla T4 and GTX 2060\n\n\ud83c\udf0d **Windows + Linux** run in both Windows OS and linux OS (even in M1 MACOS)\n\n\ud83e\udd29 **Easy & Awesome** effect with only newly-trained synthesizer, by reusing the pretrained encoder/vocoder\n\n\ud83c\udf0d **Webserver Ready** to serve your result with remote calling\n\n### [DEMO VIDEO](https://www.bilibili.com/video/BV17Q4y1B7mY/)\n\n## Quick Start\n\n### 1. Install Requirements\n#### 1.1 General Setup\n> Follow the original repo to test if you got all environment ready.\n**Python 3.7 or higher ** is needed to run the toolbox.\n\n* Install [PyTorch](https://pytorch.org/get-started/locally/).\n> If you get an `ERROR: Could not find a version that satisfies the requirement torch==1.9.0+cu102 (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2 )` This error is probably due to a low version of python, try using 3.9 and it will install successfully\n* Install [ffmpeg](https://ffmpeg.org/download.html#get-packages).\n* Run `pip install -r requirements.txt` to install the remaining necessary packages.\n> The recommended environment here is `Repo Tag 0.0.1` `Pytorch1.9.0 with Torchvision0.10.0 and cudatoolkit10.2` `requirements.txt` `webrtcvad-wheels` because `requirements. txt` was exported a few months ago, so it doesn't work with newer versions\n* Install webrtcvad `pip install webrtcvad-wheels`(If you need)\n\nor\n- install dependencies with\u00a0`conda`\u00a0or\u00a0`mamba`\n\n  ```conda env create -n env_name -f env.yml```\n\n  ```mamba env create -n env_name -f env.yml```\n\n  will create a virtual environment where necessary dependencies are installed. Switch to the new environment by\u00a0`conda activate env_name`\u00a0and enjoy it.\n  > env.yml only includes the necessary dependencies to run the project\uff0ctemporarily without monotonic-align. You can check the official website to install the GPU version of pytorch.\n\n#### 1.2 Setup with a M1 Mac\n> The following steps are a workaround to directly use the original `demo_toolbox.py`without the changing of codes.\n>\n  >  Since the major issue comes with the PyQt5 packages used in `demo_toolbox.py` not compatible with M1 chips, were one to attempt on training models with the M1 chip, either that person can forgo `demo_toolbox.py`, or one can try the `web.py` in the project.\n\n##### 1.2.1 Install `PyQt5`, with [ref](https://stackoverflow.com/a/68038451/20455983) here.\n  * Create and open a Rosetta Terminal, with [ref](https://dev.to/courier/tips-and-tricks-to-setup-your-apple-m1-for-development-547g) here.\n  * Use system Python to create a virtual environment for the project\n    ```\n    /usr/bin/python3 -m venv /PathToMockingBird/venv\n    source /PathToMockingBird/venv/bin/activate\n    ```\n  * Upgrade pip and install `PyQt5`\n    ```\n    pip install --upgrade pip\n    pip install pyqt5\n    ```\n##### 1.2.2 Install `pyworld` and `ctc-segmentation`\n\n> Both packages seem to be unique to this project and are not seen in the original [Real-Time Voice Cloning](https://github.com/CorentinJ/Real-Time-Voice-Cloning) project. When installing with `pip install`, both packages lack wheels so the program tries to directly compile from c code and could not find `Python.h`.\n\n  * Install `pyworld`\n      * `brew install python` `Python.h` can come with Python installed by brew\n      * `export CPLUS_INCLUDE_PATH=/opt/homebrew/Frameworks/Python.framework/Headers` The filepath of brew-installed `Python.h` is unique to M1 MacOS and listed above. One needs to manually add the path to the environment variables.\n      * `pip install pyworld` that should do.\n\n\n  * Install`ctc-segmentation`\n    > Same method does not apply to `ctc-segmentation`, and one needs to compile it from the source code on [github](https://github.com/lumaku/ctc-segmentation).\n    * `git clone https://github.com/lumaku/ctc-segmentation.git`\n    * `cd ctc-segmentation`\n    * `source /PathToMockingBird/venv/bin/activate` If the virtual environment hasn't been deployed, activate it.\n    * `cythonize -3 ctc_segmentation/ctc_segmentation_dyn.pyx`\n    * `/usr/bin/arch -x86_64 python setup.py build` Build with x86 architecture.\n    * `/usr/bin/arch -x86_64 python setup.py install --optimize=1 --skip-build`Install with x86 architecture.\n\n##### 1.2.3 Other dependencies\n  * `/usr/bin/arch -x86_64 pip install torch torchvision torchaudio` Pip installing `PyTorch` as an example, articulate that it's installed with x86 architecture\n  * `pip install ffmpeg`  Install ffmpeg\n  * `pip install -r requirements.txt` Install other requirements.\n\n##### 1.2.4 Run the Inference Time (with Toolbox)\n  > To run the project on x86 architecture. [ref](https://youtrack.jetbrains.com/issue/PY-46290/Allow-running-Python-under-Rosetta-2-in-PyCharm-for-Apple-Silicon).\n  * `vim /PathToMockingBird/venv/bin/pythonM1` Create an executable file `pythonM1` to condition python interpreter at `/PathToMockingBird/venv/bin`.\n  * Write in the following content:\n    ```\n    #!/usr/bin/env zsh\n    mydir=${0:a:h}\n    /usr/bin/arch -x86_64 $mydir/python \"$@\"\n    ```\n  * `chmod +x pythonM1` Set the file as executable.\n  * If using PyCharm IDE, configure project interpreter to `pythonM1`([steps here](https://www.jetbrains.com/help/pycharm/configuring-python-interpreter.html#add-existing-interpreter)), if using command line python, run `/PathToMockingBird/venv/bin/pythonM1 demo_toolbox.py`\n\n\n### 2. Prepare your models\n> Note that we are using the pretrained encoder/vocoder but not synthesizer, since the original model is incompatible with the Chinese symbols. It means the demo_cli is not working at this moment, so additional synthesizer models are required.\n\nYou can either train your models or use existing ones:\n\n#### 2.1 Train encoder with your dataset (Optional)\n\n* Preprocess with the audios and the mel spectrograms:\n`python encoder_preprocess.py <datasets_root>` Allowing parameter `--dataset {dataset}` to support the datasets you want to preprocess. Only the train set of these datasets will be used. Possible names: librispeech_other, voxceleb1, voxceleb2. Use comma to sperate multiple datasets.\n\n* Train the encoder: `python encoder_train.py my_run <datasets_root>/SV2TTS/encoder`\n> For training, the encoder uses visdom. You can disable it with `--no_visdom`, but it's nice to have. Run \"visdom\" in a separate CLI/process to start your visdom server.\n\n#### 2.2 Train synthesizer with your dataset\n* Download dataset and unzip: make sure you can access all .wav in folder\n* Preprocess with the audios and the mel spectrograms:\n`python pre.py <datasets_root>`\nAllowing parameter `--dataset {dataset}` to support aidatatang_200zh, magicdata, aishell3, data_aishell, etc.If this parameter is not passed, the default dataset will be aidatatang_200zh.\n\n* Train the synthesizer:\n`python train.py --type=synth mandarin <datasets_root>/SV2TTS/synthesizer`\n\n* Go to next step when you see attention line show and loss meet your need in training folder *synthesizer/saved_models/*.\n\n#### 2.3 Use pretrained model of synthesizer\n> Thanks to the community, some models will be shared:\n\n| author | Download link | Preview Video | Info |\n| --- | ----------- | ----- |----- |\n| @author | https://pan.baidu.com/s/1iONvRxmkI-t1nHqxKytY3g  [Baidu](https://pan.baidu.com/s/1iONvRxmkI-t1nHqxKytY3g) 4j5d  |  | 75k steps trained by multiple datasets\n| @author | https://pan.baidu.com/s/1fMh9IlgKJlL2PIiRTYDUvw  [Baidu](https://pan.baidu.com/s/1fMh9IlgKJlL2PIiRTYDUvw) code\uff1aom7f  |  | 25k steps trained by multiple datasets, only works under version 0.0.1\n|@FawenYo | https://yisiou-my.sharepoint.com/:u:/g/personal/lawrence_cheng_fawenyo_onmicrosoft_com/EWFWDHzee-NNg9TWdKckCc4BC7bK2j9cCbOWn0-_tK0nOg?e=n0gGgC  | [input](https://github.com/babysor/MockingBird/wiki/audio/self_test.mp3) [output](https://github.com/babysor/MockingBird/wiki/audio/export.wav) | 200k steps with local accent of Taiwan, only works under version 0.0.1\n|@miven| https://pan.baidu.com/s/1PI-hM3sn5wbeChRryX-RCQ code: 2021 https://www.aliyundrive.com/s/AwPsbo8mcSP code: z2m0 | https://www.bilibili.com/video/BV1uh411B7AD/ | only works under version 0.0.1\n\n#### 2.4 Train vocoder (Optional)\n> note: vocoder has little difference in effect, so you may not need to train a new one.\n* Preprocess the data:\n`python vocoder_preprocess.py <datasets_root> -m <synthesizer_model_path>`\n> `<datasets_root>` replace with your dataset root\uff0c`<synthesizer_model_path>`replace with directory of your best trained models of sythensizer, e.g. *sythensizer\\saved_mode\\xxx*\n\n* Train the wavernn vocoder:\n`python vocoder_train.py mandarin <datasets_root>`\n\n* Train the hifigan vocoder\n`python vocoder_train.py mandarin <datasets_root> hifigan`\n\n### 3. Launch\n#### 3.1 Using the web server\nYou can then try to run:`python web.py` and open it in browser, default as `http://localhost:8080`\n\n#### 3.2 Using the Toolbox\nYou can then try the toolbox:\n`python demo_toolbox.py -d <datasets_root>`\n\n#### 3.3 Using the command line\nYou can then try the command:\n`python gen_voice.py <text_file.txt> your_wav_file.wav`\nyou may need to install cn2an by \"pip install cn2an\" for better digital number result.\n\n## Reference\n> This repository is forked from [Real-Time-Voice-Cloning](https://github.com/CorentinJ/Real-Time-Voice-Cloning) which only support English.\n\n| URL | Designation | Title | Implementation source |\n| --- | ----------- | ----- | --------------------- |\n| [1803.09017](https://arxiv.org/abs/1803.09017) | GlobalStyleToken (synthesizer)| Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis | This repo |\n| [2010.05646](https://arxiv.org/abs/2010.05646) | HiFi-GAN (vocoder)| Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis | This repo |\n| [2106.02297](https://arxiv.org/abs/2106.02297) | Fre-GAN (vocoder)| Fre-GAN: Adversarial Frequency-consistent Audio Synthesis | This repo |\n|[**1806.04558**](https://arxiv.org/pdf/1806.04558.pdf) | **SV2TTS** | **Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis** | This repo |\n|[1802.08435](https://arxiv.org/pdf/1802.08435.pdf) | WaveRNN (vocoder) | Efficient Neural Audio Synthesis | [fatchord/WaveRNN](https://github.com/fatchord/WaveRNN) |\n|[1703.10135](https://arxiv.org/pdf/1703.10135.pdf) | Tacotron (synthesizer) | Tacotron: Towards End-to-End Speech Synthesis | [fatchord/WaveRNN](https://github.com/fatchord/WaveRNN)\n|[1710.10467](https://arxiv.org/pdf/1710.10467.pdf) | GE2E (encoder)| Generalized End-To-End Loss for Speaker Verification | This repo |\n\n## F Q&A\n#### 1.Where can I download the dataset?\n| Dataset | Original Source | Alternative Sources |\n| --- | ----------- | ---------------|\n| aidatatang_200zh | [OpenSLR](http://www.openslr.org/62/) | [Google Drive](https://drive.google.com/file/d/110A11KZoVe7vy6kXlLb6zVPLb_J91I_t/view?usp=sharing) |\n| magicdata | [OpenSLR](http://www.openslr.org/68/) | [Google Drive (Dev set)](https://drive.google.com/file/d/1g5bWRUSNH68ycC6eNvtwh07nX3QhOOlo/view?usp=sharing) |\n| aishell3 | [OpenSLR](https://www.openslr.org/93/) | [Google Drive](https://drive.google.com/file/d/1shYp_o4Z0X0cZSKQDtFirct2luFUwKzZ/view?usp=sharing) |\n| data_aishell | [OpenSLR](https://www.openslr.org/33/) |  |\n> After unzip aidatatang_200zh, you need to unzip all the files under `aidatatang_200zh\\corpus\\train`\n\n#### 2.What is`<datasets_root>`?\nIf the dataset path is `D:\\data\\aidatatang_200zh`,then `<datasets_root>` is`D:\\data`\n\n#### 3.Not enough VRAM\nTrain the synthesizer\uff1aadjust the batch_size in `synthesizer/hparams.py`\n```\n//Before\ntts_schedule = [(2,  1e-3,  20_000,  12),   # Progressive training schedule\n                (2,  5e-4,  40_000,  12),   # (r, lr, step, batch_size)\n                (2,  2e-4,  80_000,  12),   #\n                (2,  1e-4, 160_000,  12),   # r = reduction factor (# of mel frames\n                (2,  3e-5, 320_000,  12),   #     synthesized for each decoder iteration)\n                (2,  1e-5, 640_000,  12)],  # lr = learning rate\n//After\ntts_schedule = [(2,  1e-3,  20_000,  8),   # Progressive training schedule\n                (2,  5e-4,  40_000,  8),   # (r, lr, step, batch_size)\n                (2,  2e-4,  80_000,  8),   #\n                (2,  1e-4, 160_000,  8),   # r = reduction factor (# of mel frames\n                (2,  3e-5, 320_000,  8),   #     synthesized for each decoder iteration)\n                (2,  1e-5, 640_000,  8)],  # lr = learning rate\n```\n\nTrain Vocoder-Preprocess the data\uff1aadjust the batch_size in `synthesizer/hparams.py`\n```\n//Before\n### Data Preprocessing\n        max_mel_frames = 900,\n        rescale = True,\n        rescaling_max = 0.9,\n        synthesis_batch_size = 16,                  # For vocoder preprocessing and inference.\n//After\n### Data Preprocessing\n        max_mel_frames = 900,\n        rescale = True,\n        rescaling_max = 0.9,\n        synthesis_batch_size = 8,                  # For vocoder preprocessing and inference.\n```\n\nTrain Vocoder-Train the vocoder\uff1aadjust the batch_size in `vocoder/wavernn/hparams.py`\n```\n//Before\n# Training\nvoc_batch_size = 100\nvoc_lr = 1e-4\nvoc_gen_at_checkpoint = 5\nvoc_pad = 2\n\n//After\n# Training\nvoc_batch_size = 6\nvoc_lr = 1e-4\nvoc_gen_at_checkpoint = 5\nvoc_pad =2\n```\n\n#### 4.If it happens `RuntimeError: Error(s) in loading state_dict for Tacotron: size mismatch for encoder.embedding.weight: copying a param with shape torch.Size([70, 512]) from checkpoint, the shape in current model is torch.Size([75, 512]).`\nPlease refer to issue [#37](https://github.com/babysor/MockingBird/issues/37)\n\n#### 5. How to improve CPU and GPU occupancy rate?\nAdjust the batch_size as appropriate to improve\n\n\n#### 6. What if it happens `the page file is too small to complete the operation`\nPlease refer to this [video](https://www.youtube.com/watch?v=Oh6dga-Oy10&ab_channel=CodeProf) and change the virtual memory to 100G (102400), for example : When the file is placed in the D disk, the virtual memory of the D disk is changed.\n\n#### 7. When should I stop during training?\nFYI, my attention came after 18k steps and loss became lower than 0.4 after 50k steps.\n![attention_step_20500_sample_1](https://user-images.githubusercontent.com/7423248/128587252-f669f05a-f411-4811-8784-222156ea5e9d.png)\n![step-135500-mel-spectrogram_sample_1](https://user-images.githubusercontent.com/7423248/128587255-4945faa0-5517-46ea-b173-928eff999330.png)\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 658928958,
    "name": "ollama",
    "full_name": "ollama/ollama",
    "description": "Get up and running with OpenAI gpt-oss, DeepSeek-R1, Gemma 3 and other models.",
    "html_url": "https://github.com/ollama/ollama",
    "clone_url": "https://github.com/ollama/ollama.git",
    "owner_login": "ollama",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/151674099?v=4",
    "stargazers_count": 148895,
    "watchers_count": 148895,
    "forks_count": 12679,
    "open_issues_count": 2024,
    "size": 48118,
    "language": "Go",
    "languages": {
      "Go": 2159422,
      "C": 46582,
      "Shell": 21285,
      "TypeScript": 17893,
      "PowerShell": 12447,
      "Inno Setup": 7568,
      "CMake": 6044,
      "Dockerfile": 5330,
      "Objective-C": 1285,
      "CSS": 518,
      "JavaScript": 248,
      "HTML": 123
    },
    "topics": [
      "deepseek",
      "gemma",
      "gemma3",
      "gemma3n",
      "go",
      "golang",
      "llama",
      "llama2",
      "llama3",
      "llava",
      "llm",
      "llms",
      "mistral",
      "ollama",
      "phi4",
      "qwen"
    ],
    "license_name": "MIT License",
    "created_at": "2023-06-26T19:39:32+00:00",
    "updated_at": "2025-08-06T02:10:20+00:00",
    "pushed_at": "2025-08-06T00:29:10+00:00",
    "contributors_count": 100,
    "readme_length": 39567,
    "readme_content": "<div align=\"center\">\n\u00a0 <a href=\"https://ollama.com\">\n    <img alt=\"ollama\" width=\"240\" src=\"https://github.com/ollama/ollama/assets/3325447/0d0b44e2-8f4a-4e99-9b52-a5c1c741c8f7\">\n  </a>\n</div>\n\n# Ollama\n\nGet up and running with large language models.\n\n### macOS\n\n[Download](https://ollama.com/download/Ollama.dmg)\n\n### Windows\n\n[Download](https://ollama.com/download/OllamaSetup.exe)\n\n### Linux\n\n```shell\ncurl -fsSL https://ollama.com/install.sh | sh\n```\n\n[Manual install instructions](https://github.com/ollama/ollama/blob/main/docs/linux.md)\n\n### Docker\n\nThe official [Ollama Docker image](https://hub.docker.com/r/ollama/ollama) `ollama/ollama` is available on Docker Hub.\n\n### Libraries\n\n- [ollama-python](https://github.com/ollama/ollama-python)\n- [ollama-js](https://github.com/ollama/ollama-js)\n\n### Community\n\n- [Discord](https://discord.gg/ollama)\n- [Reddit](https://reddit.com/r/ollama)\n\n## Quickstart\n\nTo run and chat with [Gemma 3](https://ollama.com/library/gemma3):\n\n```shell\nollama run gemma3\n```\n\n## Model library\n\nOllama supports a list of models available on [ollama.com/library](https://ollama.com/library 'ollama model library')\n\nHere are some example models that can be downloaded:\n\n| Model              | Parameters | Size  | Download                         |\n| ------------------ | ---------- | ----- | -------------------------------- |\n| Gemma 3            | 1B         | 815MB | `ollama run gemma3:1b`           |\n| Gemma 3            | 4B         | 3.3GB | `ollama run gemma3`              |\n| Gemma 3            | 12B        | 8.1GB | `ollama run gemma3:12b`          |\n| Gemma 3            | 27B        | 17GB  | `ollama run gemma3:27b`          |\n| QwQ                | 32B        | 20GB  | `ollama run qwq`                 |\n| DeepSeek-R1        | 7B         | 4.7GB | `ollama run deepseek-r1`         |\n| DeepSeek-R1        | 671B       | 404GB | `ollama run deepseek-r1:671b`    |\n| Llama 4            | 109B       | 67GB  | `ollama run llama4:scout`        |\n| Llama 4            | 400B       | 245GB | `ollama run llama4:maverick`     |\n| Llama 3.3          | 70B        | 43GB  | `ollama run llama3.3`            |\n| Llama 3.2          | 3B         | 2.0GB | `ollama run llama3.2`            |\n| Llama 3.2          | 1B         | 1.3GB | `ollama run llama3.2:1b`         |\n| Llama 3.2 Vision   | 11B        | 7.9GB | `ollama run llama3.2-vision`     |\n| Llama 3.2 Vision   | 90B        | 55GB  | `ollama run llama3.2-vision:90b` |\n| Llama 3.1          | 8B         | 4.7GB | `ollama run llama3.1`            |\n| Llama 3.1          | 405B       | 231GB | `ollama run llama3.1:405b`       |\n| Phi 4              | 14B        | 9.1GB | `ollama run phi4`                |\n| Phi 4 Mini         | 3.8B       | 2.5GB | `ollama run phi4-mini`           |\n| Mistral            | 7B         | 4.1GB | `ollama run mistral`             |\n| Moondream 2        | 1.4B       | 829MB | `ollama run moondream`           |\n| Neural Chat        | 7B         | 4.1GB | `ollama run neural-chat`         |\n| Starling           | 7B         | 4.1GB | `ollama run starling-lm`         |\n| Code Llama         | 7B         | 3.8GB | `ollama run codellama`           |\n| Llama 2 Uncensored | 7B         | 3.8GB | `ollama run llama2-uncensored`   |\n| LLaVA              | 7B         | 4.5GB | `ollama run llava`               |\n| Granite-3.3         | 8B         | 4.9GB | `ollama run granite3.3`          |\n\n> [!NOTE]\n> You should have at least 8 GB of RAM available to run the 7B models, 16 GB to run the 13B models, and 32 GB to run the 33B models.\n\n## Customize a model\n\n### Import from GGUF\n\nOllama supports importing GGUF models in the Modelfile:\n\n1. Create a file named `Modelfile`, with a `FROM` instruction with the local filepath to the model you want to import.\n\n   ```\n   FROM ./vicuna-33b.Q4_0.gguf\n   ```\n\n2. Create the model in Ollama\n\n   ```shell\n   ollama create example -f Modelfile\n   ```\n\n3. Run the model\n\n   ```shell\n   ollama run example\n   ```\n\n### Import from Safetensors\n\nSee the [guide](docs/import.md) on importing models for more information.\n\n### Customize a prompt\n\nModels from the Ollama library can be customized with a prompt. For example, to customize the `llama3.2` model:\n\n```shell\nollama pull llama3.2\n```\n\nCreate a `Modelfile`:\n\n```\nFROM llama3.2\n\n# set the temperature to 1 [higher is more creative, lower is more coherent]\nPARAMETER temperature 1\n\n# set the system message\nSYSTEM \"\"\"\nYou are Mario from Super Mario Bros. Answer as Mario, the assistant, only.\n\"\"\"\n```\n\nNext, create and run the model:\n\n```\nollama create mario -f ./Modelfile\nollama run mario\n>>> hi\nHello! It's your friend Mario.\n```\n\nFor more information on working with a Modelfile, see the [Modelfile](docs/modelfile.md) documentation.\n\n## CLI Reference\n\n### Create a model\n\n`ollama create` is used to create a model from a Modelfile.\n\n```shell\nollama create mymodel -f ./Modelfile\n```\n\n### Pull a model\n\n```shell\nollama pull llama3.2\n```\n\n> This command can also be used to update a local model. Only the diff will be pulled.\n\n### Remove a model\n\n```shell\nollama rm llama3.2\n```\n\n### Copy a model\n\n```shell\nollama cp llama3.2 my-model\n```\n\n### Multiline input\n\nFor multiline input, you can wrap text with `\"\"\"`:\n\n```\n>>> \"\"\"Hello,\n... world!\n... \"\"\"\nI'm a basic program that prints the famous \"Hello, world!\" message to the console.\n```\n\n### Multimodal models\n\n```\nollama run llava \"What's in this image? /Users/jmorgan/Desktop/smile.png\"\n```\n\n> **Output**: The image features a yellow smiley face, which is likely the central focus of the picture.\n\n### Pass the prompt as an argument\n\n```shell\nollama run llama3.2 \"Summarize this file: $(cat README.md)\"\n```\n\n> **Output**: Ollama is a lightweight, extensible framework for building and running language models on the local machine. It provides a simple API for creating, running, and managing models, as well as a library of pre-built models that can be easily used in a variety of applications.\n\n### Show model information\n\n```shell\nollama show llama3.2\n```\n\n### List models on your computer\n\n```shell\nollama list\n```\n\n### List which models are currently loaded\n\n```shell\nollama ps\n```\n\n### Stop a model which is currently running\n\n```shell\nollama stop llama3.2\n```\n\n### Start Ollama\n\n`ollama serve` is used when you want to start ollama without running the desktop application.\n\n## Building\n\nSee the [developer guide](https://github.com/ollama/ollama/blob/main/docs/development.md)\n\n### Running local builds\n\nNext, start the server:\n\n```shell\n./ollama serve\n```\n\nFinally, in a separate shell, run a model:\n\n```shell\n./ollama run llama3.2\n```\n\n## REST API\n\nOllama has a REST API for running and managing models.\n\n### Generate a response\n\n```shell\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.2\",\n  \"prompt\":\"Why is the sky blue?\"\n}'\n```\n\n### Chat with a model\n\n```shell\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2\",\n  \"messages\": [\n    { \"role\": \"user\", \"content\": \"why is the sky blue?\" }\n  ]\n}'\n```\n\nSee the [API documentation](./docs/api.md) for all endpoints.\n\n## Community Integrations\n\n### Web & Desktop\n\n- [Open WebUI](https://github.com/open-webui/open-webui)\n- [SwiftChat (macOS with ReactNative)](https://github.com/aws-samples/swift-chat)\n- [Enchanted (macOS native)](https://github.com/AugustDev/enchanted)\n- [Hollama](https://github.com/fmaclen/hollama)\n- [Lollms-Webui](https://github.com/ParisNeo/lollms-webui)\n- [LibreChat](https://github.com/danny-avila/LibreChat)\n- [Bionic GPT](https://github.com/bionic-gpt/bionic-gpt)\n- [HTML UI](https://github.com/rtcfirefly/ollama-ui)\n- [Saddle](https://github.com/jikkuatwork/saddle)\n- [TagSpaces](https://www.tagspaces.org) (A platform for file-based apps, [utilizing Ollama](https://docs.tagspaces.org/ai/) for the generation of tags and descriptions)\n- [Chatbot UI](https://github.com/ivanfioravanti/chatbot-ollama)\n- [Chatbot UI v2](https://github.com/mckaywrigley/chatbot-ui)\n- [Typescript UI](https://github.com/ollama-interface/Ollama-Gui?tab=readme-ov-file)\n- [Minimalistic React UI for Ollama Models](https://github.com/richawo/minimal-llm-ui)\n- [Ollamac](https://github.com/kevinhermawan/Ollamac)\n- [big-AGI](https://github.com/enricoros/big-AGI)\n- [Cheshire Cat assistant framework](https://github.com/cheshire-cat-ai/core)\n- [Amica](https://github.com/semperai/amica)\n- [chatd](https://github.com/BruceMacD/chatd)\n- [Ollama-SwiftUI](https://github.com/kghandour/Ollama-SwiftUI)\n- [Dify.AI](https://github.com/langgenius/dify)\n- [MindMac](https://mindmac.app)\n- [NextJS Web Interface for Ollama](https://github.com/jakobhoeg/nextjs-ollama-llm-ui)\n- [Msty](https://msty.app)\n- [Chatbox](https://github.com/Bin-Huang/Chatbox)\n- [WinForm Ollama Copilot](https://github.com/tgraupmann/WinForm_Ollama_Copilot)\n- [NextChat](https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web) with [Get Started Doc](https://docs.nextchat.dev/models/ollama)\n- [Alpaca WebUI](https://github.com/mmo80/alpaca-webui)\n- [OllamaGUI](https://github.com/enoch1118/ollamaGUI)\n- [OpenAOE](https://github.com/InternLM/OpenAOE)\n- [Odin Runes](https://github.com/leonid20000/OdinRunes)\n- [LLM-X](https://github.com/mrdjohnson/llm-x) (Progressive Web App)\n- [AnythingLLM (Docker + MacOs/Windows/Linux native app)](https://github.com/Mintplex-Labs/anything-llm)\n- [Ollama Basic Chat: Uses HyperDiv Reactive UI](https://github.com/rapidarchitect/ollama_basic_chat)\n- [Ollama-chats RPG](https://github.com/drazdra/ollama-chats)\n- [IntelliBar](https://intellibar.app/) (AI-powered assistant for macOS)\n- [Jirapt](https://github.com/AliAhmedNada/jirapt) (Jira Integration to generate issues, tasks, epics)\n- [ojira](https://github.com/AliAhmedNada/ojira) (Jira chrome plugin to easily generate descriptions for tasks)\n- [QA-Pilot](https://github.com/reid41/QA-Pilot) (Interactive chat tool that can leverage Ollama models for rapid understanding and navigation of GitHub code repositories)\n- [ChatOllama](https://github.com/sugarforever/chat-ollama) (Open Source Chatbot based on Ollama with Knowledge Bases)\n- [CRAG Ollama Chat](https://github.com/Nagi-ovo/CRAG-Ollama-Chat) (Simple Web Search with Corrective RAG)\n- [RAGFlow](https://github.com/infiniflow/ragflow) (Open-source Retrieval-Augmented Generation engine based on deep document understanding)\n- [StreamDeploy](https://github.com/StreamDeploy-DevRel/streamdeploy-llm-app-scaffold) (LLM Application Scaffold)\n- [chat](https://github.com/swuecho/chat) (chat web app for teams)\n- [Lobe Chat](https://github.com/lobehub/lobe-chat) with [Integrating Doc](https://lobehub.com/docs/self-hosting/examples/ollama)\n- [Ollama RAG Chatbot](https://github.com/datvodinh/rag-chatbot.git) (Local Chat with multiple PDFs using Ollama and RAG)\n- [BrainSoup](https://www.nurgo-software.com/products/brainsoup) (Flexible native client with RAG & multi-agent automation)\n- [macai](https://github.com/Renset/macai) (macOS client for Ollama, ChatGPT, and other compatible API back-ends)\n- [RWKV-Runner](https://github.com/josStorer/RWKV-Runner) (RWKV offline LLM deployment tool, also usable as a client for ChatGPT and Ollama)\n- [Ollama Grid Search](https://github.com/dezoito/ollama-grid-search) (app to evaluate and compare models)\n- [Olpaka](https://github.com/Otacon/olpaka) (User-friendly Flutter Web App for Ollama)\n- [Casibase](https://casibase.org) (An open source AI knowledge base and dialogue system combining the latest RAG, SSO, ollama support, and multiple large language models.)\n- [OllamaSpring](https://github.com/CrazyNeil/OllamaSpring) (Ollama Client for macOS)\n- [LLocal.in](https://github.com/kartikm7/llocal) (Easy to use Electron Desktop Client for Ollama)\n- [Shinkai Desktop](https://github.com/dcSpark/shinkai-apps) (Two click install Local AI using Ollama + Files + RAG)\n- [AiLama](https://github.com/zeyoyt/ailama) (A Discord User App that allows you to interact with Ollama anywhere in Discord)\n- [Ollama with Google Mesop](https://github.com/rapidarchitect/ollama_mesop/) (Mesop Chat Client implementation with Ollama)\n- [R2R](https://github.com/SciPhi-AI/R2R) (Open-source RAG engine)\n- [Ollama-Kis](https://github.com/elearningshow/ollama-kis) (A simple easy-to-use GUI with sample custom LLM for Drivers Education)\n- [OpenGPA](https://opengpa.org) (Open-source offline-first Enterprise Agentic Application)\n- [Painting Droid](https://github.com/mateuszmigas/painting-droid) (Painting app with AI integrations)\n- [Kerlig AI](https://www.kerlig.com/) (AI writing assistant for macOS)\n- [AI Studio](https://github.com/MindWorkAI/AI-Studio)\n- [Sidellama](https://github.com/gyopak/sidellama) (browser-based LLM client)\n- [LLMStack](https://github.com/trypromptly/LLMStack) (No-code multi-agent framework to build LLM agents and workflows)\n- [BoltAI for Mac](https://boltai.com) (AI Chat Client for Mac)\n- [Harbor](https://github.com/av/harbor) (Containerized LLM Toolkit with Ollama as default backend)\n- [PyGPT](https://github.com/szczyglis-dev/py-gpt) (AI desktop assistant for Linux, Windows, and Mac)\n- [Alpaca](https://github.com/Jeffser/Alpaca) (An Ollama client application for Linux and macOS made with GTK4 and Adwaita)\n- [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT/blob/master/docs/content/platform/ollama.md) (AutoGPT Ollama integration)\n- [Go-CREW](https://www.jonathanhecl.com/go-crew/) (Powerful Offline RAG in Golang)\n- [PartCAD](https://github.com/openvmp/partcad/) (CAD model generation with OpenSCAD and CadQuery)\n- [Ollama4j Web UI](https://github.com/ollama4j/ollama4j-web-ui) - Java-based Web UI for Ollama built with Vaadin, Spring Boot, and Ollama4j\n- [PyOllaMx](https://github.com/kspviswa/pyOllaMx) - macOS application capable of chatting with both Ollama and Apple MLX models.\n- [Cline](https://github.com/cline/cline) - Formerly known as Claude Dev is a VSCode extension for multi-file/whole-repo coding\n- [Cherry Studio](https://github.com/kangfenmao/cherry-studio) (Desktop client with Ollama support)\n- [ConfiChat](https://github.com/1runeberg/confichat) (Lightweight, standalone, multi-platform, and privacy-focused LLM chat interface with optional encryption)\n- [Archyve](https://github.com/nickthecook/archyve) (RAG-enabling document library)\n- [crewAI with Mesop](https://github.com/rapidarchitect/ollama-crew-mesop) (Mesop Web Interface to run crewAI with Ollama)\n- [Tkinter-based client](https://github.com/chyok/ollama-gui) (Python tkinter-based Client for Ollama)\n- [LLMChat](https://github.com/trendy-design/llmchat) (Privacy focused, 100% local, intuitive all-in-one chat interface)\n- [Local Multimodal AI Chat](https://github.com/Leon-Sander/Local-Multimodal-AI-Chat) (Ollama-based LLM Chat with support for multiple features, including PDF RAG, voice chat, image-based interactions, and integration with OpenAI.)\n- [ARGO](https://github.com/xark-argo/argo) (Locally download and run Ollama and Huggingface models with RAG and deep research on Mac/Windows/Linux)\n- [OrionChat](https://github.com/EliasPereirah/OrionChat) - OrionChat is a web interface for chatting with different AI providers\n- [G1](https://github.com/bklieger-groq/g1) (Prototype of using prompting strategies to improve the LLM's reasoning through o1-like reasoning chains.)\n- [Web management](https://github.com/lemonit-eric-mao/ollama-web-management) (Web management page)\n- [Promptery](https://github.com/promptery/promptery) (desktop client for Ollama.)\n- [Ollama App](https://github.com/JHubi1/ollama-app) (Modern and easy-to-use multi-platform client for Ollama)\n- [chat-ollama](https://github.com/annilq/chat-ollama) (a React Native client for Ollama)\n- [SpaceLlama](https://github.com/tcsenpai/spacellama) (Firefox and Chrome extension to quickly summarize web pages with ollama in a sidebar)\n- [YouLama](https://github.com/tcsenpai/youlama) (Webapp to quickly summarize any YouTube video, supporting Invidious as well)\n- [DualMind](https://github.com/tcsenpai/dualmind) (Experimental app allowing two models to talk to each other in the terminal or in a web interface)\n- [ollamarama-matrix](https://github.com/h1ddenpr0cess20/ollamarama-matrix) (Ollama chatbot for the Matrix chat protocol)\n- [ollama-chat-app](https://github.com/anan1213095357/ollama-chat-app) (Flutter-based chat app)\n- [Perfect Memory AI](https://www.perfectmemory.ai/) (Productivity AI assists personalized by what you have seen on your screen, heard, and said in the meetings)\n- [Hexabot](https://github.com/hexastack/hexabot) (A conversational AI builder)\n- [Reddit Rate](https://github.com/rapidarchitect/reddit_analyzer) (Search and Rate Reddit topics with a weighted summation)\n- [OpenTalkGpt](https://github.com/adarshM84/OpenTalkGpt) (Chrome Extension to manage open-source models supported by Ollama, create custom models, and chat with models from a user-friendly UI)\n- [VT](https://github.com/vinhnx/vt.ai) (A minimal multimodal AI chat app, with dynamic conversation routing. Supports local models via Ollama)\n- [Nosia](https://github.com/nosia-ai/nosia) (Easy to install and use RAG platform based on Ollama)\n- [Witsy](https://github.com/nbonamy/witsy) (An AI Desktop application available for Mac/Windows/Linux)\n- [Abbey](https://github.com/US-Artificial-Intelligence/abbey) (A configurable AI interface server with notebooks, document storage, and YouTube support)\n- [Minima](https://github.com/dmayboroda/minima) (RAG with on-premises or fully local workflow)\n- [aidful-ollama-model-delete](https://github.com/AidfulAI/aidful-ollama-model-delete) (User interface for simplified model cleanup)\n- [Perplexica](https://github.com/ItzCrazyKns/Perplexica) (An AI-powered search engine & an open-source alternative to Perplexity AI)\n- [Ollama Chat WebUI for Docker ](https://github.com/oslook/ollama-webui) (Support for local docker deployment, lightweight ollama webui)\n- [AI Toolkit for Visual Studio Code](https://aka.ms/ai-tooklit/ollama-docs) (Microsoft-official VSCode extension to chat, test, evaluate models with Ollama support, and use them in your AI applications.)\n- [MinimalNextOllamaChat](https://github.com/anilkay/MinimalNextOllamaChat) (Minimal Web UI for Chat and Model Control)\n- [Chipper](https://github.com/TilmanGriesel/chipper) AI interface for tinkerers (Ollama, Haystack RAG, Python)\n- [ChibiChat](https://github.com/CosmicEventHorizon/ChibiChat) (Kotlin-based Android app to chat with Ollama and Koboldcpp API endpoints)\n- [LocalLLM](https://github.com/qusaismael/localllm) (Minimal Web-App to run ollama models on it with a GUI)\n- [Ollamazing](https://github.com/buiducnhat/ollamazing) (Web extension to run Ollama models)\n- [OpenDeepResearcher-via-searxng](https://github.com/benhaotang/OpenDeepResearcher-via-searxng) (A Deep Research equivalent endpoint with Ollama support for running locally)\n- [AntSK](https://github.com/AIDotNet/AntSK) (Out-of-the-box & Adaptable RAG Chatbot)\n- [MaxKB](https://github.com/1Panel-dev/MaxKB/) (Ready-to-use & flexible RAG Chatbot)\n- [yla](https://github.com/danielekp/yla) (Web interface to freely interact with your customized models)\n- [LangBot](https://github.com/RockChinQ/LangBot) (LLM-based instant messaging bots platform, with Agents, RAG features, supports multiple platforms)\n- [1Panel](https://github.com/1Panel-dev/1Panel/) (Web-based Linux Server Management Tool)\n- [AstrBot](https://github.com/Soulter/AstrBot/) (User-friendly LLM-based multi-platform chatbot with a WebUI, supporting RAG, LLM agents, and plugins integration)\n- [Reins](https://github.com/ibrahimcetin/reins) (Easily tweak parameters, customize system prompts per chat, and enhance your AI experiments with reasoning model support.)\n- [Flufy](https://github.com/Aharon-Bensadoun/Flufy) (A beautiful chat interface for interacting with Ollama's API. Built with React, TypeScript, and Material-UI.)\n- [Ellama](https://github.com/zeozeozeo/ellama) (Friendly native app to chat with an Ollama instance)\n- [screenpipe](https://github.com/mediar-ai/screenpipe) Build agents powered by your screen history\n- [Ollamb](https://github.com/hengkysteen/ollamb) (Simple yet rich in features, cross-platform built with Flutter and designed for Ollama. Try the [web demo](https://hengkysteen.github.io/demo/ollamb/).)\n- [Writeopia](https://github.com/Writeopia/Writeopia) (Text editor with integration with Ollama)\n- [AppFlowy](https://github.com/AppFlowy-IO/AppFlowy) (AI collaborative workspace with Ollama, cross-platform and self-hostable)\n- [Lumina](https://github.com/cushydigit/lumina.git) (A lightweight, minimal React.js frontend for interacting with Ollama servers)\n- [Tiny Notepad](https://pypi.org/project/tiny-notepad) (A lightweight, notepad-like interface to chat with ollama available on PyPI)\n- [macLlama (macOS native)](https://github.com/hellotunamayo/macLlama) (A native macOS GUI application for interacting with Ollama models, featuring a chat interface.) \n- [GPTranslate](https://github.com/philberndt/GPTranslate) (A fast and lightweight, AI powered desktop translation application written with Rust and Tauri. Features real-time translation with OpenAI/Azure/Ollama.)\n- [ollama launcher](https://github.com/NGC13009/ollama-launcher) (A launcher for Ollama, aiming to provide users with convenient functions such as ollama server launching, management, or configuration.)\n- [ai-hub](https://github.com/Aj-Seven/ai-hub) (AI Hub supports multiple models via API keys and Chat support via Ollama API.)\n- [Mayan EDMS](https://gitlab.com/mayan-edms/mayan-edms) (Open source document management system to organize, tag, search, and automate your files with powerful Ollama driven workflows.)\n\n### Cloud\n\n- [Google Cloud](https://cloud.google.com/run/docs/tutorials/gpu-gemma2-with-ollama)\n- [Fly.io](https://fly.io/docs/python/do-more/add-ollama/)\n- [Koyeb](https://www.koyeb.com/deploy/ollama)\n\n### Terminal\n\n- [oterm](https://github.com/ggozad/oterm)\n- [Ellama Emacs client](https://github.com/s-kostyaev/ellama)\n- [Emacs client](https://github.com/zweifisch/ollama)\n- [neollama](https://github.com/paradoxical-dev/neollama) UI client for interacting with models from within Neovim\n- [gen.nvim](https://github.com/David-Kunz/gen.nvim)\n- [ollama.nvim](https://github.com/nomnivore/ollama.nvim)\n- [ollero.nvim](https://github.com/marco-souza/ollero.nvim)\n- [ollama-chat.nvim](https://github.com/gerazov/ollama-chat.nvim)\n- [ogpt.nvim](https://github.com/huynle/ogpt.nvim)\n- [gptel Emacs client](https://github.com/karthink/gptel)\n- [Oatmeal](https://github.com/dustinblackman/oatmeal)\n- [cmdh](https://github.com/pgibler/cmdh)\n- [ooo](https://github.com/npahlfer/ooo)\n- [shell-pilot](https://github.com/reid41/shell-pilot)(Interact with models via pure shell scripts on Linux or macOS)\n- [tenere](https://github.com/pythops/tenere)\n- [llm-ollama](https://github.com/taketwo/llm-ollama) for [Datasette's LLM CLI](https://llm.datasette.io/en/stable/).\n- [typechat-cli](https://github.com/anaisbetts/typechat-cli)\n- [ShellOracle](https://github.com/djcopley/ShellOracle)\n- [tlm](https://github.com/yusufcanb/tlm)\n- [podman-ollama](https://github.com/ericcurtin/podman-ollama)\n- [gollama](https://github.com/sammcj/gollama)\n- [ParLlama](https://github.com/paulrobello/parllama)\n- [Ollama eBook Summary](https://github.com/cognitivetech/ollama-ebook-summary/)\n- [Ollama Mixture of Experts (MOE) in 50 lines of code](https://github.com/rapidarchitect/ollama_moe)\n- [vim-intelligence-bridge](https://github.com/pepo-ec/vim-intelligence-bridge) Simple interaction of \"Ollama\" with the Vim editor\n- [x-cmd ollama](https://x-cmd.com/mod/ollama)\n- [bb7](https://github.com/drunkwcodes/bb7)\n- [SwollamaCLI](https://github.com/marcusziade/Swollama) bundled with the Swollama Swift package. [Demo](https://github.com/marcusziade/Swollama?tab=readme-ov-file#cli-usage)\n- [aichat](https://github.com/sigoden/aichat) All-in-one LLM CLI tool featuring Shell Assistant, Chat-REPL, RAG, AI tools & agents, with access to OpenAI, Claude, Gemini, Ollama, Groq, and more.\n- [PowershAI](https://github.com/rrg92/powershai) PowerShell module that brings AI to terminal on Windows, including support for Ollama\n- [DeepShell](https://github.com/Abyss-c0re/deepshell) Your self-hosted AI assistant. Interactive Shell, Files and Folders analysis.\n- [orbiton](https://github.com/xyproto/orbiton) Configuration-free text editor and IDE with support for tab completion with Ollama.\n- [orca-cli](https://github.com/molbal/orca-cli) Ollama Registry CLI Application - Browse, pull, and download models from Ollama Registry in your terminal.\n- [GGUF-to-Ollama](https://github.com/jonathanhecl/gguf-to-ollama) - Importing GGUF to Ollama made easy (multiplatform)\n- [AWS-Strands-With-Ollama](https://github.com/rapidarchitect/ollama_strands) - AWS Strands Agents with Ollama Examples\n- [ollama-multirun](https://github.com/attogram/ollama-multirun) - A bash shell script to run a single prompt against any or all of your locally installed ollama models, saving the output and performance statistics as easily navigable web pages. ([Demo](https://attogram.github.io/ai_test_zone/))\n- [ollama-bash-toolshed](https://github.com/attogram/ollama-bash-toolshed) - Bash scripts to chat with tool using models. Add new tools to your shed with ease. Runs on Ollama.\n\n### Apple Vision Pro\n\n- [SwiftChat](https://github.com/aws-samples/swift-chat) (Cross-platform AI chat app supporting Apple Vision Pro via \"Designed for iPad\")\n- [Enchanted](https://github.com/AugustDev/enchanted)\n\n### Database\n\n- [pgai](https://github.com/timescale/pgai) - PostgreSQL as a vector database (Create and search embeddings from Ollama models using pgvector)\n   - [Get started guide](https://github.com/timescale/pgai/blob/main/docs/vectorizer-quick-start.md)\n- [MindsDB](https://github.com/mindsdb/mindsdb/blob/staging/mindsdb/integrations/handlers/ollama_handler/README.md) (Connects Ollama models with nearly 200 data platforms and apps)\n- [chromem-go](https://github.com/philippgille/chromem-go/blob/v0.5.0/embed_ollama.go) with [example](https://github.com/philippgille/chromem-go/tree/v0.5.0/examples/rag-wikipedia-ollama)\n- [Kangaroo](https://github.com/dbkangaroo/kangaroo) (AI-powered SQL client and admin tool for popular databases)\n\n### Package managers\n\n- [Pacman](https://archlinux.org/packages/extra/x86_64/ollama/)\n- [Gentoo](https://github.com/gentoo/guru/tree/master/app-misc/ollama)\n- [Homebrew](https://formulae.brew.sh/formula/ollama)\n- [Helm Chart](https://artifacthub.io/packages/helm/ollama-helm/ollama)\n- [Guix channel](https://codeberg.org/tusharhero/ollama-guix)\n- [Nix package](https://search.nixos.org/packages?show=ollama&from=0&size=50&sort=relevance&type=packages&query=ollama)\n- [Flox](https://flox.dev/blog/ollama-part-one)\n\n### Libraries\n\n- [LangChain](https://python.langchain.com/docs/integrations/chat/ollama/) and [LangChain.js](https://js.langchain.com/docs/integrations/chat/ollama/) with [example](https://js.langchain.com/docs/tutorials/local_rag/)\n- [Firebase Genkit](https://firebase.google.com/docs/genkit/plugins/ollama)\n- [crewAI](https://github.com/crewAIInc/crewAI)\n- [Yacana](https://remembersoftwares.github.io/yacana/) (User-friendly multi-agent framework for brainstorming and executing predetermined flows with built-in tool integration)\n- [Spring AI](https://github.com/spring-projects/spring-ai) with [reference](https://docs.spring.io/spring-ai/reference/api/chat/ollama-chat.html) and [example](https://github.com/tzolov/ollama-tools)\n- [LangChainGo](https://github.com/tmc/langchaingo/) with [example](https://github.com/tmc/langchaingo/tree/main/examples/ollama-completion-example)\n- [LangChain4j](https://github.com/langchain4j/langchain4j) with [example](https://github.com/langchain4j/langchain4j-examples/tree/main/ollama-examples/src/main/java)\n- [LangChainRust](https://github.com/Abraxas-365/langchain-rust) with [example](https://github.com/Abraxas-365/langchain-rust/blob/main/examples/llm_ollama.rs)\n- [LangChain for .NET](https://github.com/tryAGI/LangChain) with [example](https://github.com/tryAGI/LangChain/blob/main/examples/LangChain.Samples.OpenAI/Program.cs)\n- [LLPhant](https://github.com/theodo-group/LLPhant?tab=readme-ov-file#ollama)\n- [LlamaIndex](https://docs.llamaindex.ai/en/stable/examples/llm/ollama/) and [LlamaIndexTS](https://ts.llamaindex.ai/modules/llms/available_llms/ollama)\n- [LiteLLM](https://github.com/BerriAI/litellm)\n- [OllamaFarm for Go](https://github.com/presbrey/ollamafarm)\n- [OllamaSharp for .NET](https://github.com/awaescher/OllamaSharp)\n- [Ollama for Ruby](https://github.com/gbaptista/ollama-ai)\n- [Ollama-rs for Rust](https://github.com/pepperoni21/ollama-rs)\n- [Ollama-hpp for C++](https://github.com/jmont-dev/ollama-hpp)\n- [Ollama4j for Java](https://github.com/ollama4j/ollama4j)\n- [ModelFusion Typescript Library](https://modelfusion.dev/integration/model-provider/ollama)\n- [OllamaKit for Swift](https://github.com/kevinhermawan/OllamaKit)\n- [Ollama for Dart](https://github.com/breitburg/dart-ollama)\n- [Ollama for Laravel](https://github.com/cloudstudio/ollama-laravel)\n- [LangChainDart](https://github.com/davidmigloz/langchain_dart)\n- [Semantic Kernel - Python](https://github.com/microsoft/semantic-kernel/tree/main/python/semantic_kernel/connectors/ai/ollama)\n- [Haystack](https://github.com/deepset-ai/haystack-integrations/blob/main/integrations/ollama.md)\n- [Elixir LangChain](https://github.com/brainlid/langchain)\n- [Ollama for R - rollama](https://github.com/JBGruber/rollama)\n- [Ollama for R - ollama-r](https://github.com/hauselin/ollama-r)\n- [Ollama-ex for Elixir](https://github.com/lebrunel/ollama-ex)\n- [Ollama Connector for SAP ABAP](https://github.com/b-tocs/abap_btocs_ollama)\n- [Testcontainers](https://testcontainers.com/modules/ollama/)\n- [Portkey](https://portkey.ai/docs/welcome/integration-guides/ollama)\n- [PromptingTools.jl](https://github.com/svilupp/PromptingTools.jl) with an [example](https://svilupp.github.io/PromptingTools.jl/dev/examples/working_with_ollama)\n- [LlamaScript](https://github.com/Project-Llama/llamascript)\n- [llm-axe](https://github.com/emirsahin1/llm-axe) (Python Toolkit for Building LLM Powered Apps)\n- [Gollm](https://docs.gollm.co/examples/ollama-example)\n- [Gollama for Golang](https://github.com/jonathanhecl/gollama)\n- [Ollamaclient for Golang](https://github.com/xyproto/ollamaclient)\n- [High-level function abstraction in Go](https://gitlab.com/tozd/go/fun)\n- [Ollama PHP](https://github.com/ArdaGnsrn/ollama-php)\n- [Agents-Flex for Java](https://github.com/agents-flex/agents-flex) with [example](https://github.com/agents-flex/agents-flex/tree/main/agents-flex-llm/agents-flex-llm-ollama/src/test/java/com/agentsflex/llm/ollama)\n- [Parakeet](https://github.com/parakeet-nest/parakeet) is a GoLang library, made to simplify the development of small generative AI applications with Ollama.\n- [Haverscript](https://github.com/andygill/haverscript) with [examples](https://github.com/andygill/haverscript/tree/main/examples)\n- [Ollama for Swift](https://github.com/mattt/ollama-swift)\n- [Swollama for Swift](https://github.com/marcusziade/Swollama) with [DocC](https://marcusziade.github.io/Swollama/documentation/swollama/)\n- [GoLamify](https://github.com/prasad89/golamify)\n- [Ollama for Haskell](https://github.com/tusharad/ollama-haskell)\n- [multi-llm-ts](https://github.com/nbonamy/multi-llm-ts) (A Typescript/JavaScript library allowing access to different LLM in a unified API)\n- [LlmTornado](https://github.com/lofcz/llmtornado) (C# library providing a unified interface for major FOSS & Commercial inference APIs)\n- [Ollama for Zig](https://github.com/dravenk/ollama-zig)\n- [Abso](https://github.com/lunary-ai/abso) (OpenAI-compatible TypeScript SDK for any LLM provider)\n- [Nichey](https://github.com/goodreasonai/nichey) is a Python package for generating custom wikis for your research topic\n- [Ollama for D](https://github.com/kassane/ollama-d)\n- [OllamaPlusPlus](https://github.com/HardCodeDev777/OllamaPlusPlus) (Very simple C++ library for Ollama)\n\n### Mobile\n\n- [SwiftChat](https://github.com/aws-samples/swift-chat) (Lightning-fast Cross-platform AI chat app with native UI for Android, iOS, and iPad)\n- [Enchanted](https://github.com/AugustDev/enchanted)\n- [Maid](https://github.com/Mobile-Artificial-Intelligence/maid)\n- [Ollama App](https://github.com/JHubi1/ollama-app) (Modern and easy-to-use multi-platform client for Ollama)\n- [ConfiChat](https://github.com/1runeberg/confichat) (Lightweight, standalone, multi-platform, and privacy-focused LLM chat interface with optional encryption)\n- [Ollama Android Chat](https://github.com/sunshine0523/OllamaServer) (No need for Termux, start the Ollama service with one click on an Android device)\n- [Reins](https://github.com/ibrahimcetin/reins) (Easily tweak parameters, customize system prompts per chat, and enhance your AI experiments with reasoning model support.)\n\n### Extensions & Plugins\n\n- [Raycast extension](https://github.com/MassimilianoPasquini97/raycast_ollama)\n- [Discollama](https://github.com/mxyng/discollama) (Discord bot inside the Ollama discord channel)\n- [Continue](https://github.com/continuedev/continue)\n- [Vibe](https://github.com/thewh1teagle/vibe) (Transcribe and analyze meetings with Ollama)\n- [Obsidian Ollama plugin](https://github.com/hinterdupfinger/obsidian-ollama)\n- [Logseq Ollama plugin](https://github.com/omagdy7/ollama-logseq)\n- [NotesOllama](https://github.com/andersrex/notesollama) (Apple Notes Ollama plugin)\n- [Dagger Chatbot](https://github.com/samalba/dagger-chatbot)\n- [Discord AI Bot](https://github.com/mekb-turtle/discord-ai-bot)\n- [Ollama Telegram Bot](https://github.com/ruecat/ollama-telegram)\n- [Hass Ollama Conversation](https://github.com/ej52/hass-ollama-conversation)\n- [Rivet plugin](https://github.com/abrenneke/rivet-plugin-ollama)\n- [Obsidian BMO Chatbot plugin](https://github.com/longy2k/obsidian-bmo-chatbot)\n- [Cliobot](https://github.com/herval/cliobot) (Telegram bot with Ollama support)\n- [Copilot for Obsidian plugin](https://github.com/logancyang/obsidian-copilot)\n- [Obsidian Local GPT plugin](https://github.com/pfrankov/obsidian-local-gpt)\n- [Open Interpreter](https://docs.openinterpreter.com/language-model-setup/local-models/ollama)\n- [Llama Coder](https://github.com/ex3ndr/llama-coder) (Copilot alternative using Ollama)\n- [Ollama Copilot](https://github.com/bernardo-bruning/ollama-copilot) (Proxy that allows you to use Ollama as a copilot like GitHub Copilot)\n- [twinny](https://github.com/rjmacarthy/twinny) (Copilot and Copilot chat alternative using Ollama)\n- [Wingman-AI](https://github.com/RussellCanfield/wingman-ai) (Copilot code and chat alternative using Ollama and Hugging Face)\n- [Page Assist](https://github.com/n4ze3m/page-assist) (Chrome Extension)\n- [Plasmoid Ollama Control](https://github.com/imoize/plasmoid-ollamacontrol) (KDE Plasma extension that allows you to quickly manage/control Ollama model)\n- [AI Telegram Bot](https://github.com/tusharhero/aitelegrambot) (Telegram bot using Ollama in backend)\n- [AI ST Completion](https://github.com/yaroslavyaroslav/OpenAI-sublime-text) (Sublime Text 4 AI assistant plugin with Ollama support)\n- [Discord-Ollama Chat Bot](https://github.com/kevinthedang/discord-ollama) (Generalized TypeScript Discord Bot w/ Tuning Documentation)\n- [ChatGPTBox: All in one browser extension](https://github.com/josStorer/chatGPTBox) with [Integrating Tutorial](https://github.com/josStorer/chatGPTBox/issues/616#issuecomment-1975186467)\n- [Discord AI chat/moderation bot](https://github.com/rapmd73/Companion) Chat/moderation bot written in python. Uses Ollama to create personalities.\n- [Headless Ollama](https://github.com/nischalj10/headless-ollama) (Scripts to automatically install ollama client & models on any OS for apps that depend on ollama server)\n- [Terraform AWS Ollama & Open WebUI](https://github.com/xuyangbocn/terraform-aws-self-host-llm) (A Terraform module to deploy on AWS a ready-to-use Ollama service, together with its front-end Open WebUI service.)\n- [node-red-contrib-ollama](https://github.com/jakubburkiewicz/node-red-contrib-ollama)\n- [Local AI Helper](https://github.com/ivostoykov/localAI) (Chrome and Firefox extensions that enable interactions with the active tab and customisable API endpoints. Includes secure storage for user prompts.)\n- [vnc-lm](https://github.com/jake83741/vnc-lm) (Discord bot for messaging with LLMs through Ollama and LiteLLM. Seamlessly move between local and flagship models.)\n- [LSP-AI](https://github.com/SilasMarvin/lsp-ai) (Open-source language server for AI-powered functionality)\n- [QodeAssist](https://github.com/Palm1r/QodeAssist) (AI-powered coding assistant plugin for Qt Creator)\n- [Obsidian Quiz Generator plugin](https://github.com/ECuiDev/obsidian-quiz-generator)\n- [AI Summmary Helper plugin](https://github.com/philffm/ai-summary-helper)\n- [TextCraft](https://github.com/suncloudsmoon/TextCraft) (Copilot in Word alternative using Ollama)\n- [Alfred Ollama](https://github.com/zeitlings/alfred-ollama) (Alfred Workflow)\n- [TextLLaMA](https://github.com/adarshM84/TextLLaMA) A Chrome Extension that helps you write emails, correct grammar, and translate into any language\n- [Simple-Discord-AI](https://github.com/zyphixor/simple-discord-ai)\n- [LLM Telegram Bot](https://github.com/innightwolfsleep/llm_telegram_bot) (telegram bot, primary for RP. Oobabooga-like buttons, [A1111](https://github.com/AUTOMATIC1111/stable-diffusion-webui) API integration e.t.c)\n- [mcp-llm](https://github.com/sammcj/mcp-llm) (MCP Server to allow LLMs to call other LLMs)\n- [SimpleOllamaUnity](https://github.com/HardCodeDev777/SimpleOllamaUnity) (Unity Engine extension for communicating with Ollama in a few lines of code. Also works at runtime)\n- [UnityCodeLama](https://github.com/HardCodeDev777/UnityCodeLama) (Unity Edtior tool to analyze scripts via Ollama)\n- [NativeMind](https://github.com/NativeMindBrowser/NativeMindExtension) (Private, on-device AI Assistant, no cloud dependencies)\n- [GMAI - Gradle Managed AI](https://gmai.premex.se/) (Gradle plugin for automated Ollama lifecycle management during build phases)\n\n### Supported backends\n\n- [llama.cpp](https://github.com/ggml-org/llama.cpp) project founded by Georgi Gerganov.\n\n### Observability\n- [Opik](https://www.comet.com/docs/opik/cookbook/ollama) is an open-source platform to debug, evaluate, and monitor your LLM applications, RAG systems, and agentic workflows with comprehensive tracing, automated evaluations, and production-ready dashboards. Opik supports native intergration to Ollama.\n- [Lunary](https://lunary.ai/docs/integrations/ollama) is the leading open-source LLM observability platform. It provides a variety of enterprise-grade features such as real-time analytics, prompt templates management, PII masking, and comprehensive agent tracing.\n- [OpenLIT](https://github.com/openlit/openlit) is an OpenTelemetry-native tool for monitoring Ollama Applications & GPUs using traces and metrics.\n- [HoneyHive](https://docs.honeyhive.ai/integrations/ollama) is an AI observability and evaluation platform for AI agents. Use HoneyHive to evaluate agent performance, interrogate failures, and monitor quality in production.\n- [Langfuse](https://langfuse.com/docs/integrations/ollama) is an open source LLM observability platform that enables teams to collaboratively monitor, evaluate and debug AI applications.\n- [MLflow Tracing](https://mlflow.org/docs/latest/llms/tracing/index.html#automatic-tracing) is an open source LLM observability tool with a convenient API to log and visualize traces, making it easy to debug and evaluate GenAI applications.\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 552661142,
    "name": "langchain",
    "full_name": "langchain-ai/langchain",
    "description": "\ud83e\udd9c\ud83d\udd17 Build context-aware reasoning applications",
    "html_url": "https://github.com/langchain-ai/langchain",
    "clone_url": "https://github.com/langchain-ai/langchain.git",
    "owner_login": "langchain-ai",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/126733545?v=4",
    "stargazers_count": 112938,
    "watchers_count": 112938,
    "forks_count": 18478,
    "open_issues_count": 329,
    "size": 463642,
    "language": "Jupyter Notebook",
    "languages": {
      "Jupyter Notebook": 31946753,
      "Python": 8543273,
      "Makefile": 59313,
      "MDX": 56541,
      "Shell": 12533,
      "XSLT": 7380,
      "HTML": 1900,
      "Dockerfile": 1788,
      "JavaScript": 157
    },
    "topics": [
      "ai",
      "anthropic",
      "gemini",
      "langchain",
      "llm",
      "openai",
      "python"
    ],
    "license_name": "MIT License",
    "created_at": "2022-10-17T02:58:36+00:00",
    "updated_at": "2025-08-06T01:48:56+00:00",
    "pushed_at": "2025-08-06T01:54:29+00:00",
    "contributors_count": 100,
    "readme_length": 5325,
    "readme_content": "<picture>\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"docs/static/img/logo-dark.svg\">\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/static/img/logo-light.svg\">\n  <img alt=\"LangChain Logo\" src=\"docs/static/img/logo-dark.svg\" width=\"80%\">\n</picture>\n\n<div>\n<br>\n</div>\n\n[![Release Notes](https://img.shields.io/github/release/langchain-ai/langchain?style=flat-square)](https://github.com/langchain-ai/langchain/releases)\n[![CI](https://github.com/langchain-ai/langchain/actions/workflows/check_diffs.yml/badge.svg)](https://github.com/langchain-ai/langchain/actions/workflows/check_diffs.yml)\n[![PyPI - License](https://img.shields.io/pypi/l/langchain-core?style=flat-square)](https://opensource.org/licenses/MIT)\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-core?style=flat-square)](https://pypistats.org/packages/langchain-core)\n[![GitHub star chart](https://img.shields.io/github/stars/langchain-ai/langchain?style=flat-square)](https://star-history.com/#langchain-ai/langchain)\n[![Open Issues](https://img.shields.io/github/issues-raw/langchain-ai/langchain?style=flat-square)](https://github.com/langchain-ai/langchain/issues)\n[![Open in Dev Containers](https://img.shields.io/static/v1?label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode&style=flat-square)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/langchain-ai/langchain)\n[<img src=\"https://github.com/codespaces/badge.svg\" alt=\"Open in Github Codespace\" title=\"Open in Github Codespace\" width=\"150\" height=\"20\">](https://codespaces.new/langchain-ai/langchain)\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&label=Follow%20%40LangChainAI)](https://twitter.com/langchainai)\n[![CodSpeed Badge](https://img.shields.io/endpoint?url=https://codspeed.io/badge.json)](https://codspeed.io/langchain-ai/langchain)\n\n> [!NOTE]\n> Looking for the JS/TS library? Check out [LangChain.js](https://github.com/langchain-ai/langchainjs).\n\nLangChain is a framework for building LLM-powered applications. It helps you chain\ntogether interoperable components and third-party integrations to simplify AI\napplication development \u2014  all while future-proofing decisions as the underlying\ntechnology evolves.\n\n```bash\npip install -U langchain\n```\n\nTo learn more about LangChain, check out\n[the docs](https://python.langchain.com/docs/introduction/). If you\u2019re looking for more\nadvanced customization or agent orchestration, check out\n[LangGraph](https://langchain-ai.github.io/langgraph/), our framework for building\ncontrollable agent workflows.\n\n## Why use LangChain?\n\nLangChain helps developers build applications powered by LLMs through a standard\ninterface for models, embeddings, vector stores, and more.\n\nUse LangChain for:\n\n- **Real-time data augmentation**. Easily connect LLMs to diverse data sources and\nexternal / internal systems, drawing from LangChain\u2019s vast library of integrations with\nmodel providers, tools, vector stores, retrievers, and more.\n- **Model interoperability**. Swap models in and out as your engineering team\nexperiments to find the best choice for your application\u2019s needs. As the industry\nfrontier evolves, adapt quickly \u2014 LangChain\u2019s abstractions keep you moving without\nlosing momentum.\n\n## LangChain\u2019s ecosystem\n\nWhile the LangChain framework can be used standalone, it also integrates seamlessly\nwith any LangChain product, giving developers a full suite of tools when building LLM\napplications.\n\nTo improve your LLM application development, pair LangChain with:\n\n- [LangSmith](http://www.langchain.com/langsmith) - Helpful for agent evals and\nobservability. Debug poor-performing LLM app runs, evaluate agent trajectories, gain\nvisibility in production, and improve performance over time.\n- [LangGraph](https://langchain-ai.github.io/langgraph/) - Build agents that can\nreliably handle complex tasks with LangGraph, our low-level agent orchestration\nframework. LangGraph offers customizable architecture, long-term memory, and\nhuman-in-the-loop workflows \u2014 and is trusted in production by companies like LinkedIn,\nUber, Klarna, and GitLab.\n- [LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/) - Deploy\nand scale agents effortlessly with a purpose-built deployment platform for long\nrunning, stateful workflows. Discover, reuse, configure, and share agents across\nteams \u2014 and iterate quickly with visual prototyping in\n[LangGraph Studio](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/).\n\n## Additional resources\n\n- [Tutorials](https://python.langchain.com/docs/tutorials/): Simple walkthroughs with\nguided examples on getting started with LangChain.\n- [How-to Guides](https://python.langchain.com/docs/how_to/): Quick, actionable code\nsnippets for topics such as tool calling, RAG use cases, and more.\n- [Conceptual Guides](https://python.langchain.com/docs/concepts/): Explanations of key\nconcepts behind the LangChain framework.\n- [LangChain Forum](https://forum.langchain.com/): Connect with the community and share all of your technical questions, ideas, and feedback.\n- [API Reference](https://python.langchain.com/api_reference/): Detailed reference on\nnavigating base packages and integrations for LangChain.\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 626805178,
    "name": "dify",
    "full_name": "langgenius/dify",
    "description": "Production-ready platform for agentic workflow development.",
    "html_url": "https://github.com/langgenius/dify",
    "clone_url": "https://github.com/langgenius/dify.git",
    "owner_login": "langgenius",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/127165244?v=4",
    "stargazers_count": 109866,
    "watchers_count": 109866,
    "forks_count": 16738,
    "open_issues_count": 768,
    "size": 108475,
    "language": "TypeScript",
    "languages": {
      "TypeScript": 12272626,
      "Python": 7601173,
      "JavaScript": 1447838,
      "MDX": 963691,
      "CSS": 176893,
      "HTML": 102368,
      "SCSS": 21945,
      "Shell": 20252,
      "PHP": 6106,
      "Dockerfile": 4322,
      "Makefile": 1304,
      "Mako": 518
    },
    "topics": [
      "agent",
      "agentic-ai",
      "agentic-framework",
      "agentic-workflow",
      "ai",
      "automation",
      "gemini",
      "genai",
      "gpt",
      "gpt-4",
      "llm",
      "low-code",
      "mcp",
      "nextjs",
      "no-code",
      "openai",
      "orchestration",
      "python",
      "rag",
      "workflow"
    ],
    "license_name": "Other",
    "created_at": "2023-04-12T07:40:24+00:00",
    "updated_at": "2025-08-06T02:05:48+00:00",
    "pushed_at": "2025-08-06T01:50:44+00:00",
    "contributors_count": 100,
    "readme_length": 14591,
    "readme_content": "![cover-v5-optimized](./images/GitHub_README_if.png)\n\n<p align=\"center\">\n  \ud83d\udccc <a href=\"https://dify.ai/blog/introducing-dify-workflow-file-upload-a-demo-on-ai-podcast\">Introducing Dify Workflow File Upload: Recreate Google NotebookLM Podcast</a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://cloud.dify.ai\">Dify Cloud</a> \u00b7\n  <a href=\"https://docs.dify.ai/getting-started/install-self-hosted\">Self-hosting</a> \u00b7\n  <a href=\"https://docs.dify.ai\">Documentation</a> \u00b7\n  <a href=\"https://dify.ai/pricing\">Dify edition overview</a>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://dify.ai\" target=\"_blank\">\n        <img alt=\"Static Badge\" src=\"https://img.shields.io/badge/Product-F04438\"></a>\n    <a href=\"https://dify.ai/pricing\" target=\"_blank\">\n        <img alt=\"Static Badge\" src=\"https://img.shields.io/badge/free-pricing?logo=free&color=%20%23155EEF&label=pricing&labelColor=%20%23528bff\"></a>\n    <a href=\"https://discord.gg/FngNHpbcY7\" target=\"_blank\">\n        <img src=\"https://img.shields.io/discord/1082486657678311454?logo=discord&labelColor=%20%235462eb&logoColor=%20%23f5f5f5&color=%20%235462eb\"\n            alt=\"chat on Discord\"></a>\n    <a href=\"https://reddit.com/r/difyai\" target=\"_blank\">  \n        <img src=\"https://img.shields.io/reddit/subreddit-subscribers/difyai?style=plastic&logo=reddit&label=r%2Fdifyai&labelColor=white\"\n            alt=\"join Reddit\"></a>\n    <a href=\"https://twitter.com/intent/follow?screen_name=dify_ai\" target=\"_blank\">\n        <img src=\"https://img.shields.io/twitter/follow/dify_ai?logo=X&color=%20%23f5f5f5\"\n            alt=\"follow on X(Twitter)\"></a>\n    <a href=\"https://www.linkedin.com/company/langgenius/\" target=\"_blank\">\n        <img src=\"https://custom-icon-badges.demolab.com/badge/LinkedIn-0A66C2?logo=linkedin-white&logoColor=fff\"\n            alt=\"follow on LinkedIn\"></a>\n    <a href=\"https://hub.docker.com/u/langgenius\" target=\"_blank\">\n        <img alt=\"Docker Pulls\" src=\"https://img.shields.io/docker/pulls/langgenius/dify-web?labelColor=%20%23FDB062&color=%20%23f79009\"></a>\n    <a href=\"https://github.com/langgenius/dify/graphs/commit-activity\" target=\"_blank\">\n        <img alt=\"Commits last month\" src=\"https://img.shields.io/github/commit-activity/m/langgenius/dify?labelColor=%20%2332b583&color=%20%2312b76a\"></a>\n    <a href=\"https://github.com/langgenius/dify/\" target=\"_blank\">\n        <img alt=\"Issues closed\" src=\"https://img.shields.io/github/issues-search?query=repo%3Alanggenius%2Fdify%20is%3Aclosed&label=issues%20closed&labelColor=%20%237d89b0&color=%20%235d6b98\"></a>\n    <a href=\"https://github.com/langgenius/dify/discussions/\" target=\"_blank\">\n        <img alt=\"Discussion posts\" src=\"https://img.shields.io/github/discussions/langgenius/dify?labelColor=%20%239b8afb&color=%20%237a5af8\"></a>\n</p>\n\n<p align=\"center\">\n  <a href=\"./README.md\"><img alt=\"README in English\" src=\"https://img.shields.io/badge/English-d9d9d9\"></a>\n  <a href=\"./README_TW.md\"><img alt=\"\u7e41\u9ad4\u4e2d\u6587\u6587\u4ef6\" src=\"https://img.shields.io/badge/\u7e41\u9ad4\u4e2d\u6587-d9d9d9\"></a>\n  <a href=\"./README_CN.md\"><img alt=\"\u7b80\u4f53\u4e2d\u6587\u7248\u81ea\u8ff0\u6587\u4ef6\" src=\"https://img.shields.io/badge/\u7b80\u4f53\u4e2d\u6587-d9d9d9\"></a>\n  <a href=\"./README_JA.md\"><img alt=\"\u65e5\u672c\u8a9e\u306eREADME\" src=\"https://img.shields.io/badge/\u65e5\u672c\u8a9e-d9d9d9\"></a>\n  <a href=\"./README_ES.md\"><img alt=\"README en Espa\u00f1ol\" src=\"https://img.shields.io/badge/Espa\u00f1ol-d9d9d9\"></a>\n  <a href=\"./README_FR.md\"><img alt=\"README en Fran\u00e7ais\" src=\"https://img.shields.io/badge/Fran\u00e7ais-d9d9d9\"></a>\n  <a href=\"./README_KL.md\"><img alt=\"README tlhIngan Hol\" src=\"https://img.shields.io/badge/Klingon-d9d9d9\"></a>\n  <a href=\"./README_KR.md\"><img alt=\"README in Korean\" src=\"https://img.shields.io/badge/\ud55c\uad6d\uc5b4-d9d9d9\"></a>\n  <a href=\"./README_AR.md\"><img alt=\"README \u0628\u0627\u0644\u0639\u0631\u0628\u064a\u0629\" src=\"https://img.shields.io/badge/\u0627\u0644\u0639\u0631\u0628\u064a\u0629-d9d9d9\"></a>\n  <a href=\"./README_TR.md\"><img alt=\"T\u00fcrk\u00e7e README\" src=\"https://img.shields.io/badge/T\u00fcrk\u00e7e-d9d9d9\"></a>\n  <a href=\"./README_VI.md\"><img alt=\"README Ti\u1ebfng Vi\u1ec7t\" src=\"https://img.shields.io/badge/Ti%E1%BA%BFng%20Vi%E1%BB%87t-d9d9d9\"></a>\n  <a href=\"./README_DE.md\"><img alt=\"README in Deutsch\" src=\"https://img.shields.io/badge/German-d9d9d9\"></a>\n  <a href=\"./README_BN.md\"><img alt=\"README in \u09ac\u09be\u0982\u09b2\u09be\" src=\"https://img.shields.io/badge/\u09ac\u09be\u0982\u09b2\u09be-d9d9d9\"></a>\n</p>\n\nDify is an open-source platform for developing LLM applications. Its intuitive interface combines agentic AI workflows, RAG pipelines, agent capabilities, model management, observability features, and more\u2014allowing you to quickly move from prototype to production.\n\n## Quick start\n\n> Before installing Dify, make sure your machine meets the following minimum system requirements:\n>\n> - CPU >= 2 Core\n> - RAM >= 4 GiB\n\n</br>\n\nThe easiest way to start the Dify server is through [Docker Compose](docker/docker-compose.yaml). Before running Dify with the following commands, make sure that [Docker](https://docs.docker.com/get-docker/) and [Docker Compose](https://docs.docker.com/compose/install/) are installed on your machine:\n\n```bash\ncd dify\ncd docker\ncp .env.example .env\ndocker compose up -d\n```\n\nAfter running, you can access the Dify dashboard in your browser at [http://localhost/install](http://localhost/install) and start the initialization process.\n\n#### Seeking help\n\nPlease refer to our [FAQ](https://docs.dify.ai/getting-started/install-self-hosted/faqs) if you encounter problems setting up Dify. Reach out to [the community and us](#community--contact) if you are still having issues.\n\n> If you'd like to contribute to Dify or do additional development, refer to our [guide to deploying from source code](https://docs.dify.ai/getting-started/install-self-hosted/local-source-code)\n\n## Key features\n\n**1. Workflow**:\nBuild and test powerful AI workflows on a visual canvas, leveraging all the following features and beyond.\n\n**2. Comprehensive model support**:\nSeamless integration with hundreds of proprietary / open-source LLMs from dozens of inference providers and self-hosted solutions, covering GPT, Mistral, Llama3, and any OpenAI API-compatible models. A full list of supported model providers can be found [here](https://docs.dify.ai/getting-started/readme/model-providers).\n\n![providers-v5](https://github.com/langgenius/dify/assets/13230914/5a17bdbe-097a-4100-8363-40255b70f6e3)\n\n**3. Prompt IDE**:\nIntuitive interface for crafting prompts, comparing model performance, and adding additional features such as text-to-speech to a chat-based app.\n\n**4. RAG Pipeline**:\nExtensive RAG capabilities that cover everything from document ingestion to retrieval, with out-of-box support for text extraction from PDFs, PPTs, and other common document formats.\n\n**5. Agent capabilities**:\nYou can define agents based on LLM Function Calling or ReAct, and add pre-built or custom tools for the agent. Dify provides 50+ built-in tools for AI agents, such as Google Search, DALL\u00b7E, Stable Diffusion and WolframAlpha.\n\n**6. LLMOps**:\nMonitor and analyze application logs and performance over time. You could continuously improve prompts, datasets, and models based on production data and annotations.\n\n**7. Backend-as-a-Service**:\nAll of Dify's offerings come with corresponding APIs, so you could effortlessly integrate Dify into your own business logic.\n\n## Feature Comparison\n\n<table style=\"width: 100%;\">\n  <tr>\n    <th align=\"center\">Feature</th>\n    <th align=\"center\">Dify.AI</th>\n    <th align=\"center\">LangChain</th>\n    <th align=\"center\">Flowise</th>\n    <th align=\"center\">OpenAI Assistants API</th>\n  </tr>\n  <tr>\n    <td align=\"center\">Programming Approach</td>\n    <td align=\"center\">API + App-oriented</td>\n    <td align=\"center\">Python Code</td>\n    <td align=\"center\">App-oriented</td>\n    <td align=\"center\">API-oriented</td>\n  </tr>\n  <tr>\n    <td align=\"center\">Supported LLMs</td>\n    <td align=\"center\">Rich Variety</td>\n    <td align=\"center\">Rich Variety</td>\n    <td align=\"center\">Rich Variety</td>\n    <td align=\"center\">OpenAI-only</td>\n  </tr>\n  <tr>\n    <td align=\"center\">RAG Engine</td>\n    <td align=\"center\">\u2705</td>\n    <td align=\"center\">\u2705</td>\n    <td align=\"center\">\u2705</td>\n    <td align=\"center\">\u2705</td>\n  </tr>\n  <tr>\n    <td align=\"center\">Agent</td>\n    <td align=\"center\">\u2705</td>\n    <td align=\"center\">\u2705</td>\n    <td align=\"center\">\u274c</td>\n    <td align=\"center\">\u2705</td>\n  </tr>\n  <tr>\n    <td align=\"center\">Workflow</td>\n    <td align=\"center\">\u2705</td>\n    <td align=\"center\">\u274c</td>\n    <td align=\"center\">\u2705</td>\n    <td align=\"center\">\u274c</td>\n  </tr>\n  <tr>\n    <td align=\"center\">Observability</td>\n    <td align=\"center\">\u2705</td>\n    <td align=\"center\">\u2705</td>\n    <td align=\"center\">\u274c</td>\n    <td align=\"center\">\u274c</td>\n  </tr>\n  <tr>\n    <td align=\"center\">Enterprise Feature (SSO/Access control)</td>\n    <td align=\"center\">\u2705</td>\n    <td align=\"center\">\u274c</td>\n    <td align=\"center\">\u274c</td>\n    <td align=\"center\">\u274c</td>\n  </tr>\n  <tr>\n    <td align=\"center\">Local Deployment</td>\n    <td align=\"center\">\u2705</td>\n    <td align=\"center\">\u2705</td>\n    <td align=\"center\">\u2705</td>\n    <td align=\"center\">\u274c</td>\n  </tr>\n</table>\n\n## Using Dify\n\n- **Cloud </br>**\n  We host a [Dify Cloud](https://dify.ai) service for anyone to try with zero setup. It provides all the capabilities of the self-deployed version, and includes 200 free GPT-4 calls in the sandbox plan.\n\n- **Self-hosting Dify Community Edition</br>**\n  Quickly get Dify running in your environment with this [starter guide](#quick-start).\n  Use our [documentation](https://docs.dify.ai) for further references and more in-depth instructions.\n\n- **Dify for enterprise / organizations</br>**\n  We provide additional enterprise-centric features. [Log your questions for us through this chatbot](https://udify.app/chat/22L1zSxg6yW1cWQg) or [send us an email](mailto:business@dify.ai?subject=[GitHub]Business%20License%20Inquiry) to discuss enterprise needs. </br>\n  > For startups and small businesses using AWS, check out [Dify Premium on AWS Marketplace](https://aws.amazon.com/marketplace/pp/prodview-t22mebxzwjhu6) and deploy it to your own AWS VPC with one click. It's an affordable AMI offering with the option to create apps with custom logo and branding.\n\n## Staying ahead\n\nStar Dify on GitHub and be instantly notified of new releases.\n\n![star-us](https://github.com/langgenius/dify/assets/13230914/b823edc1-6388-4e25-ad45-2f6b187adbb4)\n\n## Advanced Setup\n\nIf you need to customize the configuration, please refer to the comments in our [.env.example](docker/.env.example) file and update the corresponding values in your `.env` file. Additionally, you might need to make adjustments to the `docker-compose.yaml` file itself, such as changing image versions, port mappings, or volume mounts, based on your specific deployment environment and requirements. After making any changes, please re-run `docker-compose up -d`. You can find the full list of available environment variables [here](https://docs.dify.ai/getting-started/install-self-hosted/environments).\n\nIf you'd like to configure a highly-available setup, there are community-contributed [Helm Charts](https://helm.sh/) and YAML files which allow Dify to be deployed on Kubernetes.\n\n- [Helm Chart by @LeoQuote](https://github.com/douban/charts/tree/master/charts/dify)\n- [Helm Chart by @BorisPolonsky](https://github.com/BorisPolonsky/dify-helm)\n- [Helm Chart by @magicsong](https://github.com/magicsong/ai-charts)\n- [YAML file by @Winson-030](https://github.com/Winson-030/dify-kubernetes)\n- [YAML file by @wyy-holding](https://github.com/wyy-holding/dify-k8s)\n- [\ud83d\ude80 NEW! YAML files (Supports Dify v1.6.0) by @Zhoneym](https://github.com/Zhoneym/DifyAI-Kubernetes)\n\n#### Using Terraform for Deployment\n\nDeploy Dify to Cloud Platform with a single click using [terraform](https://www.terraform.io/)\n\n##### Azure Global\n\n- [Azure Terraform by @nikawang](https://github.com/nikawang/dify-azure-terraform)\n\n##### Google Cloud\n\n- [Google Cloud Terraform by @sotazum](https://github.com/DeNA/dify-google-cloud-terraform)\n\n#### Using AWS CDK for Deployment\n\nDeploy Dify to AWS with [CDK](https://aws.amazon.com/cdk/)\n\n##### AWS\n\n- [AWS CDK by @KevinZhao](https://github.com/aws-samples/solution-for-deploying-dify-on-aws)\n\n#### Using Alibaba Cloud Computing Nest\n\nQuickly deploy Dify to Alibaba cloud with [Alibaba Cloud Computing Nest](https://computenest.console.aliyun.com/service/instance/create/default?type=user&ServiceName=Dify%E7%A4%BE%E5%8C%BA%E7%89%88) \n\n#### Using Alibaba Cloud Data Management\n\nOne-Click deploy Dify to Alibaba Cloud with [Alibaba Cloud Data Management](https://www.alibabacloud.com/help/en/dms/dify-in-invitational-preview/) \n\n#### Deploy to AKS with Azure Devops Pipeline\n\nOne-Click deploy Dify to AKS with [Azure Devops Pipeline Helm Chart by @LeoZhang](https://github.com/Ruiruiz30/Dify-helm-chart-AKS) \n\n\n## Contributing\n\nFor those who'd like to contribute code, see our [Contribution Guide](https://github.com/langgenius/dify/blob/main/CONTRIBUTING.md).\nAt the same time, please consider supporting Dify by sharing it on social media and at events and conferences.\n\n> We are looking for contributors to help translate Dify into languages other than Mandarin or English. If you are interested in helping, please see the [i18n README](https://github.com/langgenius/dify/blob/main/web/i18n-config/README.md) for more information, and leave us a comment in the `global-users` channel of our [Discord Community Server](https://discord.gg/8Tpq4AcN9c).\n\n## Community & contact\n\n- [GitHub Discussion](https://github.com/langgenius/dify/discussions). Best for: sharing feedback and asking questions.\n- [GitHub Issues](https://github.com/langgenius/dify/issues). Best for: bugs you encounter using Dify.AI, and feature proposals. See our [Contribution Guide](https://github.com/langgenius/dify/blob/main/CONTRIBUTING.md).\n- [Discord](https://discord.gg/FngNHpbcY7). Best for: sharing your applications and hanging out with the community.\n- [X(Twitter)](https://twitter.com/dify_ai). Best for: sharing your applications and hanging out with the community.\n\n**Contributors**\n\n<a href=\"https://github.com/langgenius/dify/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=langgenius/dify\" />\n</a>\n\n## Star history\n\n[![Star History Chart](https://api.star-history.com/svg?repos=langgenius/dify&type=Date)](https://star-history.com/#langgenius/dify&Date)\n\n## Security disclosure\n\nTo protect your privacy, please avoid posting security issues on GitHub. Instead, report issues to security@dify.ai, and our team will respond with detailed answer.\n\n## License\n\nThis repository is licensed under the [Dify Open Source License](LICENSE), based on Apache 2.0 with additional conditions.\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 701547123,
    "name": "open-webui",
    "full_name": "open-webui/open-webui",
    "description": "User-friendly AI Interface (Supports Ollama, OpenAI API, ...)",
    "html_url": "https://github.com/open-webui/open-webui",
    "clone_url": "https://github.com/open-webui/open-webui.git",
    "owner_login": "open-webui",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/158137808?v=4",
    "stargazers_count": 105305,
    "watchers_count": 105305,
    "forks_count": 14170,
    "open_issues_count": 266,
    "size": 250068,
    "language": "JavaScript",
    "languages": {
      "JavaScript": 2258245,
      "Svelte": 2003600,
      "Python": 1715604,
      "TypeScript": 319977,
      "CSS": 206882,
      "Shell": 12185,
      "Dockerfile": 6815,
      "HTML": 6133,
      "Batchfile": 1875,
      "Mako": 665,
      "Makefile": 643
    },
    "topics": [
      "ai",
      "llm",
      "llm-ui",
      "llm-webui",
      "llms",
      "mcp",
      "ollama",
      "ollama-webui",
      "open-webui",
      "openai",
      "openapi",
      "rag",
      "self-hosted",
      "ui",
      "webui"
    ],
    "license_name": "Other",
    "created_at": "2023-10-06T22:08:27+00:00",
    "updated_at": "2025-08-06T02:09:47+00:00",
    "pushed_at": "2025-08-05T21:44:54+00:00",
    "contributors_count": 100,
    "readme_length": 15410,
    "readme_content": "# Open WebUI \ud83d\udc4b\n\n![GitHub stars](https://img.shields.io/github/stars/open-webui/open-webui?style=social)\n![GitHub forks](https://img.shields.io/github/forks/open-webui/open-webui?style=social)\n![GitHub watchers](https://img.shields.io/github/watchers/open-webui/open-webui?style=social)\n![GitHub repo size](https://img.shields.io/github/repo-size/open-webui/open-webui)\n![GitHub language count](https://img.shields.io/github/languages/count/open-webui/open-webui)\n![GitHub top language](https://img.shields.io/github/languages/top/open-webui/open-webui)\n![GitHub last commit](https://img.shields.io/github/last-commit/open-webui/open-webui?color=red)\n[![Discord](https://img.shields.io/badge/Discord-Open_WebUI-blue?logo=discord&logoColor=white)](https://discord.gg/5rJgQTnV4s)\n[![](https://img.shields.io/static/v1?label=Sponsor&message=%E2%9D%A4&logo=GitHub&color=%23fe8e86)](https://github.com/sponsors/tjbck)\n\n**Open WebUI is an [extensible](https://docs.openwebui.com/features/plugin/), feature-rich, and user-friendly self-hosted AI platform designed to operate entirely offline.** It supports various LLM runners like **Ollama** and **OpenAI-compatible APIs**, with **built-in inference engine** for RAG, making it a **powerful AI deployment solution**.\n\nPassionate about open-source AI? [Join our team \u2192](https://careers.openwebui.com/)\n\n![Open WebUI Demo](./demo.gif)\n\n> [!TIP]  \n> **Looking for an [Enterprise Plan](https://docs.openwebui.com/enterprise)?** \u2013 **[Speak with Our Sales Team Today!](mailto:sales@openwebui.com)**\n>\n> Get **enhanced capabilities**, including **custom theming and branding**, **Service Level Agreement (SLA) support**, **Long-Term Support (LTS) versions**, and **more!**\n\nFor more information, be sure to check out our [Open WebUI Documentation](https://docs.openwebui.com/).\n\n## Key Features of Open WebUI \u2b50\n\n- \ud83d\ude80 **Effortless Setup**: Install seamlessly using Docker or Kubernetes (kubectl, kustomize or helm) for a hassle-free experience with support for both `:ollama` and `:cuda` tagged images.\n\n- \ud83e\udd1d **Ollama/OpenAI API Integration**: Effortlessly integrate OpenAI-compatible APIs for versatile conversations alongside Ollama models. Customize the OpenAI API URL to link with **LMStudio, GroqCloud, Mistral, OpenRouter, and more**.\n\n- \ud83d\udee1\ufe0f **Granular Permissions and User Groups**: By allowing administrators to create detailed user roles and permissions, we ensure a secure user environment. This granularity not only enhances security but also allows for customized user experiences, fostering a sense of ownership and responsibility amongst users.\n\n- \ud83d\udcf1 **Responsive Design**: Enjoy a seamless experience across Desktop PC, Laptop, and Mobile devices.\n\n- \ud83d\udcf1 **Progressive Web App (PWA) for Mobile**: Enjoy a native app-like experience on your mobile device with our PWA, providing offline access on localhost and a seamless user interface.\n\n- \u2712\ufe0f\ud83d\udd22 **Full Markdown and LaTeX Support**: Elevate your LLM experience with comprehensive Markdown and LaTeX capabilities for enriched interaction.\n\n- \ud83c\udfa4\ud83d\udcf9 **Hands-Free Voice/Video Call**: Experience seamless communication with integrated hands-free voice and video call features, allowing for a more dynamic and interactive chat environment.\n\n- \ud83d\udee0\ufe0f **Model Builder**: Easily create Ollama models via the Web UI. Create and add custom characters/agents, customize chat elements, and import models effortlessly through [Open WebUI Community](https://openwebui.com/) integration.\n\n- \ud83d\udc0d **Native Python Function Calling Tool**: Enhance your LLMs with built-in code editor support in the tools workspace. Bring Your Own Function (BYOF) by simply adding your pure Python functions, enabling seamless integration with LLMs.\n\n- \ud83d\udcda **Local RAG Integration**: Dive into the future of chat interactions with groundbreaking Retrieval Augmented Generation (RAG) support. This feature seamlessly integrates document interactions into your chat experience. You can load documents directly into the chat or add files to your document library, effortlessly accessing them using the `#` command before a query.\n\n- \ud83d\udd0d **Web Search for RAG**: Perform web searches using providers like `SearXNG`, `Google PSE`, `Brave Search`, `serpstack`, `serper`, `Serply`, `DuckDuckGo`, `TavilySearch`, `SearchApi` and `Bing` and inject the results directly into your chat experience.\n\n- \ud83c\udf10 **Web Browsing Capability**: Seamlessly integrate websites into your chat experience using the `#` command followed by a URL. This feature allows you to incorporate web content directly into your conversations, enhancing the richness and depth of your interactions.\n\n- \ud83c\udfa8 **Image Generation Integration**: Seamlessly incorporate image generation capabilities using options such as AUTOMATIC1111 API or ComfyUI (local), and OpenAI's DALL-E (external), enriching your chat experience with dynamic visual content.\n\n- \u2699\ufe0f **Many Models Conversations**: Effortlessly engage with various models simultaneously, harnessing their unique strengths for optimal responses. Enhance your experience by leveraging a diverse set of models in parallel.\n\n- \ud83d\udd10 **Role-Based Access Control (RBAC)**: Ensure secure access with restricted permissions; only authorized individuals can access your Ollama, and exclusive model creation/pulling rights are reserved for administrators.\n\n- \ud83c\udf10\ud83c\udf0d **Multilingual Support**: Experience Open WebUI in your preferred language with our internationalization (i18n) support. Join us in expanding our supported languages! We're actively seeking contributors!\n\n- \ud83e\udde9 **Pipelines, Open WebUI Plugin Support**: Seamlessly integrate custom logic and Python libraries into Open WebUI using [Pipelines Plugin Framework](https://github.com/open-webui/pipelines). Launch your Pipelines instance, set the OpenAI URL to the Pipelines URL, and explore endless possibilities. [Examples](https://github.com/open-webui/pipelines/tree/main/examples) include **Function Calling**, User **Rate Limiting** to control access, **Usage Monitoring** with tools like Langfuse, **Live Translation with LibreTranslate** for multilingual support, **Toxic Message Filtering** and much more.\n\n- \ud83c\udf1f **Continuous Updates**: We are committed to improving Open WebUI with regular updates, fixes, and new features.\n\nWant to learn more about Open WebUI's features? Check out our [Open WebUI documentation](https://docs.openwebui.com/features) for a comprehensive overview!\n\n## Sponsors \ud83d\ude4c\n\n#### Emerald\n\n<table>\n  <tr>\n    <td>\n      <a href=\"https://n8n.io/\" target=\"_blank\">\n        <img src=\"https://docs.openwebui.com/sponsors/logos/n8n.png\" alt=\"n8n\" style=\"width: 8rem; height: 8rem; border-radius: .75rem;\" />\n      </a>\n    </td>\n    <td>\n      <a href=\"https://n8n.io/\">n8n</a> \u2022 Does your interface have a backend yet?<br>Try <a href=\"https://n8n.io/\">n8n</a>\n    </td>\n  </tr>\n  <tr>\n    <td>\n      <a href=\"https://tailscale.com/blog/self-host-a-local-ai-stack/?utm_source=OpenWebUI&utm_medium=paid-ad-placement&utm_campaign=OpenWebUI-Docs\" target=\"_blank\">\n        <img src=\"https://docs.openwebui.com/sponsors/logos/tailscale.png\" alt=\"Tailscale\" style=\"width: 8rem; height: 8rem; border-radius: .75rem;\" />\n      </a>\n    </td>\n    <td>\n      <a href=\"https://tailscale.com/blog/self-host-a-local-ai-stack/?utm_source=OpenWebUI&utm_medium=paid-ad-placement&utm_campaign=OpenWebUI-Docs\">Tailscale</a> \u2022 Connect self-hosted AI to any device with Tailscale\n    </td>\n  </tr>\n</table>\n\n---\n\nWe are incredibly grateful for the generous support of our sponsors. Their contributions help us to maintain and improve our project, ensuring we can continue to deliver quality work to our community. Thank you!\n\n## How to Install \ud83d\ude80\n\n### Installation via Python pip \ud83d\udc0d\n\nOpen WebUI can be installed using pip, the Python package installer. Before proceeding, ensure you're using **Python 3.11** to avoid compatibility issues.\n\n1. **Install Open WebUI**:\n   Open your terminal and run the following command to install Open WebUI:\n\n   ```bash\n   pip install open-webui\n   ```\n\n2. **Running Open WebUI**:\n   After installation, you can start Open WebUI by executing:\n\n   ```bash\n   open-webui serve\n   ```\n\nThis will start the Open WebUI server, which you can access at [http://localhost:8080](http://localhost:8080)\n\n### Quick Start with Docker \ud83d\udc33\n\n> [!NOTE]  \n> Please note that for certain Docker environments, additional configurations might be needed. If you encounter any connection issues, our detailed guide on [Open WebUI Documentation](https://docs.openwebui.com/) is ready to assist you.\n\n> [!WARNING]\n> When using Docker to install Open WebUI, make sure to include the `-v open-webui:/app/backend/data` in your Docker command. This step is crucial as it ensures your database is properly mounted and prevents any loss of data.\n\n> [!TIP]  \n> If you wish to utilize Open WebUI with Ollama included or CUDA acceleration, we recommend utilizing our official images tagged with either `:cuda` or `:ollama`. To enable CUDA, you must install the [Nvidia CUDA container toolkit](https://docs.nvidia.com/dgx/nvidia-container-runtime-upgrade/) on your Linux/WSL system.\n\n### Installation with Default Configuration\n\n- **If Ollama is on your computer**, use this command:\n\n  ```bash\n  docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main\n  ```\n\n- **If Ollama is on a Different Server**, use this command:\n\n  To connect to Ollama on another server, change the `OLLAMA_BASE_URL` to the server's URL:\n\n  ```bash\n  docker run -d -p 3000:8080 -e OLLAMA_BASE_URL=https://example.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main\n  ```\n\n- **To run Open WebUI with Nvidia GPU support**, use this command:\n\n  ```bash\n  docker run -d -p 3000:8080 --gpus all --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:cuda\n  ```\n\n### Installation for OpenAI API Usage Only\n\n- **If you're only using OpenAI API**, use this command:\n\n  ```bash\n  docker run -d -p 3000:8080 -e OPENAI_API_KEY=your_secret_key -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main\n  ```\n\n### Installing Open WebUI with Bundled Ollama Support\n\nThis installation method uses a single container image that bundles Open WebUI with Ollama, allowing for a streamlined setup via a single command. Choose the appropriate command based on your hardware setup:\n\n- **With GPU Support**:\n  Utilize GPU resources by running the following command:\n\n  ```bash\n  docker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama\n  ```\n\n- **For CPU Only**:\n  If you're not using a GPU, use this command instead:\n\n  ```bash\n  docker run -d -p 3000:8080 -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama\n  ```\n\nBoth commands facilitate a built-in, hassle-free installation of both Open WebUI and Ollama, ensuring that you can get everything up and running swiftly.\n\nAfter installation, you can access Open WebUI at [http://localhost:3000](http://localhost:3000). Enjoy! \ud83d\ude04\n\n### Other Installation Methods\n\nWe offer various installation alternatives, including non-Docker native installation methods, Docker Compose, Kustomize, and Helm. Visit our [Open WebUI Documentation](https://docs.openwebui.com/getting-started/) or join our [Discord community](https://discord.gg/5rJgQTnV4s) for comprehensive guidance.\n\nLook at the [Local Development Guide](https://docs.openwebui.com/getting-started/advanced-topics/development) for instructions on setting up a local development environment.\n\n### Troubleshooting\n\nEncountering connection issues? Our [Open WebUI Documentation](https://docs.openwebui.com/troubleshooting/) has got you covered. For further assistance and to join our vibrant community, visit the [Open WebUI Discord](https://discord.gg/5rJgQTnV4s).\n\n#### Open WebUI: Server Connection Error\n\nIf you're experiencing connection issues, it\u2019s often due to the WebUI docker container not being able to reach the Ollama server at 127.0.0.1:11434 (host.docker.internal:11434) inside the container . Use the `--network=host` flag in your docker command to resolve this. Note that the port changes from 3000 to 8080, resulting in the link: `http://localhost:8080`.\n\n**Example Docker Command**:\n\n```bash\ndocker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main\n```\n\n### Keeping Your Docker Installation Up-to-Date\n\nIn case you want to update your local Docker installation to the latest version, you can do it with [Watchtower](https://containrrr.dev/watchtower/):\n\n```bash\ndocker run --rm --volume /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --run-once open-webui\n```\n\nIn the last part of the command, replace `open-webui` with your container name if it is different.\n\nCheck our Updating Guide available in our [Open WebUI Documentation](https://docs.openwebui.com/getting-started/updating).\n\n### Using the Dev Branch \ud83c\udf19\n\n> [!WARNING]\n> The `:dev` branch contains the latest unstable features and changes. Use it at your own risk as it may have bugs or incomplete features.\n\nIf you want to try out the latest bleeding-edge features and are okay with occasional instability, you can use the `:dev` tag like this:\n\n```bash\ndocker run -d -p 3000:8080 -v open-webui:/app/backend/data --name open-webui --add-host=host.docker.internal:host-gateway --restart always ghcr.io/open-webui/open-webui:dev\n```\n\n### Offline Mode\n\nIf you are running Open WebUI in an offline environment, you can set the `HF_HUB_OFFLINE` environment variable to `1` to prevent attempts to download models from the internet.\n\n```bash\nexport HF_HUB_OFFLINE=1\n```\n\n## What's Next? \ud83c\udf1f\n\nDiscover upcoming features on our roadmap in the [Open WebUI Documentation](https://docs.openwebui.com/roadmap/).\n\n## License \ud83d\udcdc\n\nThis project is licensed under the [Open WebUI License](LICENSE), a revised BSD-3-Clause license. You receive all the same rights as the classic BSD-3 license: you can use, modify, and distribute the software, including in proprietary and commercial products, with minimal restrictions. The only additional requirement is to preserve the \"Open WebUI\" branding, as detailed in the LICENSE file. For full terms, see the [LICENSE](LICENSE) document. \ud83d\udcc4\n\n## Support \ud83d\udcac\n\nIf you have any questions, suggestions, or need assistance, please open an issue or join our\n[Open WebUI Discord community](https://discord.gg/5rJgQTnV4s) to connect with us! \ud83e\udd1d\n\n## Star History\n\n<a href=\"https://star-history.com/#open-webui/open-webui&Date\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=open-webui/open-webui&type=Date&theme=dark\" />\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=open-webui/open-webui&type=Date\" />\n    <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=open-webui/open-webui&type=Date\" />\n  </picture>\n</a>\n\n---\n\nCreated by [Timothy Jaeryang Baek](https://github.com/tjbck) - Let's make Open WebUI even more amazing together! \ud83d\udcaa\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 881458615,
    "name": "browser-use",
    "full_name": "browser-use/browser-use",
    "description": "\ud83c\udf10 Make websites accessible for AI agents. Automate tasks online with ease.",
    "html_url": "https://github.com/browser-use/browser-use",
    "clone_url": "https://github.com/browser-use/browser-use.git",
    "owner_login": "browser-use",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/192012301?v=4",
    "stargazers_count": 67033,
    "watchers_count": 67033,
    "forks_count": 7730,
    "open_issues_count": 580,
    "size": 16370,
    "language": "Python",
    "languages": {
      "Python": 1567892,
      "JavaScript": 49642,
      "Dockerfile": 12417,
      "Shell": 4001
    },
    "topics": [
      "ai-agents",
      "ai-tools",
      "browser-automation",
      "browser-use",
      "llm",
      "playwright",
      "python"
    ],
    "license_name": "MIT License",
    "created_at": "2024-10-31T16:00:56+00:00",
    "updated_at": "2025-08-06T01:57:50+00:00",
    "pushed_at": "2025-08-06T01:30:56+00:00",
    "contributors_count": 100,
    "readme_length": 9872,
    "readme_content": "<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"./static/browser-use-dark.png\">\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"./static/browser-use.png\">\n  <img alt=\"Shows a black Browser Use Logo in light color mode and a white one in dark color mode.\" src=\"./static/browser-use.png\"  width=\"full\">\n</picture>\n\n<h1 align=\"center\">Enable AI to control your browser \ud83e\udd16</h1>\n\n[![GitHub stars](https://img.shields.io/github/stars/gregpr07/browser-use?style=social)](https://github.com/gregpr07/browser-use/stargazers)\n[![Discord](https://img.shields.io/discord/1303749220842340412?color=7289DA&label=Discord&logo=discord&logoColor=white)](https://link.browser-use.com/discord)\n[![Cloud](https://img.shields.io/badge/Cloud-\u2601\ufe0f-blue)](https://cloud.browser-use.com)\n[![Documentation](https://img.shields.io/badge/Documentation-\ud83d\udcd5-blue)](https://docs.browser-use.com)\n[![Twitter Follow](https://img.shields.io/twitter/follow/Gregor?style=social)](https://x.com/intent/user?screen_name=gregpr07)\n[![Twitter Follow](https://img.shields.io/twitter/follow/Magnus?style=social)](https://x.com/intent/user?screen_name=mamagnus00)\n[![Weave Badge](https://img.shields.io/endpoint?url=https%3A%2F%2Fapp.workweave.ai%2Fapi%2Frepository%2Fbadge%2Forg_T5Pvn3UBswTHIsN1dWS3voPg%2F881458615&labelColor=#EC6341)](https://app.workweave.ai/reports/repository/org_T5Pvn3UBswTHIsN1dWS3voPg/881458615)\n\n\ud83c\udf10 Browser-use is the easiest way to connect your AI agents with the browser.\n\n\ud83d\udca1 See what others are building and share your projects in our [Discord](https://link.browser-use.com/discord)! Want Swag? Check out our [Merch store](https://browsermerch.com).\n\n\ud83c\udf24\ufe0f Skip the setup - try our <b>hosted version</b> for instant browser automation! <b>[Try the cloud \u2601\ufe0e](https://cloud.browser-use.com)</b>.\n\n# Quick start\n\nWith pip (Python>=3.11):\n\n```bash\npip install browser-use\n```\n\nInstall the browser:\n\n```bash\nplaywright install chromium --with-deps --no-shell\n```\n\nSpin up your agent:\n\n```python\nimport asyncio\nfrom dotenv import load_dotenv\nload_dotenv()\nfrom browser_use import Agent\nfrom browser_use.llm import ChatOpenAI\n\nasync def main():\n    agent = Agent(\n        task=\"Compare the price of gpt-4o and DeepSeek-V3\",\n        llm=ChatOpenAI(model=\"o4-mini\", temperature=1.0),\n    )\n    await agent.run()\n\nasyncio.run(main())\n```\n\nAdd your API keys for the provider you want to use to your `.env` file.\n\n```bash\nOPENAI_API_KEY=\nANTHROPIC_API_KEY=\nAZURE_OPENAI_ENDPOINT=\nAZURE_OPENAI_KEY=\nGOOGLE_API_KEY=\nDEEPSEEK_API_KEY=\nGROK_API_KEY=\nNOVITA_API_KEY=\n```\n\nFor other settings, models, and more, check out the [documentation \ud83d\udcd5](https://docs.browser-use.com).\n\n### Test with UI\n\nYou can test browser-use using its [Web UI](https://github.com/browser-use/web-ui) or [Desktop App](https://github.com/browser-use/desktop).\n\n### Test with an interactive CLI\n\nYou can also use our `browser-use` interactive CLI (similar to `claude` code):\n\n```bash\npip install \"browser-use[cli]\"\nbrowser-use\n```\n\n## MCP Integration\n\nBrowser-use supports the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/), enabling integration with Claude Desktop and other MCP-compatible clients.\n\n### Use as MCP Server with Claude Desktop\n\nAdd browser-use to your Claude Desktop configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"browser-use\": {\n      \"command\": \"uvx\",\n      \"args\": [\"browser-use[cli]\", \"--mcp\"],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"sk-...\"\n      }\n    }\n  }\n}\n```\n\nThis gives Claude Desktop access to browser automation tools for web scraping, form filling, and more.\n\n### Connect External MCP Servers to Browser-Use Agent\n\nBrowser-use agents can connect to multiple external MCP servers to extend their capabilities:\n\n```python\nimport asyncio\nfrom browser_use import Agent, Controller\nfrom browser_use.mcp.client import MCPClient\nfrom browser_use.llm import ChatOpenAI\n\nasync def main():\n    # Initialize controller\n    controller = Controller()\n    \n    # Connect to multiple MCP servers\n    filesystem_client = MCPClient(\n        server_name=\"filesystem\",\n        command=\"npx\",\n        args=[\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/Users/me/documents\"]\n    )\n    \n    github_client = MCPClient(\n        server_name=\"github\", \n        command=\"npx\",\n        args=[\"-y\", \"@modelcontextprotocol/server-github\"],\n        env={\"GITHUB_TOKEN\": \"your-github-token\"}\n    )\n    \n    # Connect and register tools from both servers\n    await filesystem_client.connect()\n    await filesystem_client.register_to_controller(controller)\n    \n    await github_client.connect()\n    await github_client.register_to_controller(controller)\n    \n    # Create agent with MCP-enabled controller\n    agent = Agent(\n        task=\"Find the latest report.pdf in my documents and create a GitHub issue about it\",\n        llm=ChatOpenAI(model=\"gpt-4o\"),\n        controller=controller  # Controller has tools from both MCP servers\n    )\n    \n    # Run the agent\n    await agent.run()\n    \n    # Cleanup\n    await filesystem_client.disconnect()\n    await github_client.disconnect()\n\nasyncio.run(main())\n```\n\nSee the [MCP documentation](https://docs.browser-use.com/customize/mcp-server) for more details.\n\n# Demos\n\n<br/><br/>\n\n[Task](https://github.com/browser-use/browser-use/blob/main/examples/use-cases/shopping.py): Add grocery items to cart, and checkout.\n\n[![AI Did My Groceries](https://github.com/user-attachments/assets/a0ffd23d-9a11-4368-8893-b092703abc14)](https://www.youtube.com/watch?v=L2Ya9PYNns8)\n\n<br/><br/>\n\nPrompt: Add my latest LinkedIn follower to my leads in Salesforce.\n\n![LinkedIn to Salesforce](https://github.com/user-attachments/assets/50d6e691-b66b-4077-a46c-49e9d4707e07)\n\n<br/><br/>\n\n[Prompt](https://github.com/browser-use/browser-use/blob/main/examples/use-cases/find_and_apply_to_jobs.py): Read my CV & find ML jobs, save them to a file, and then start applying for them in new tabs, if you need help, ask me.'\n\nhttps://github.com/user-attachments/assets/171fb4d6-0355-46f2-863e-edb04a828d04\n\n<br/><br/>\n\n[Prompt](https://github.com/browser-use/browser-use/blob/main/examples/browser/real_browser.py): Write a letter in Google Docs to my Papa, thanking him for everything, and save the document as a PDF.\n\n![Letter to Papa](https://github.com/user-attachments/assets/242ade3e-15bc-41c2-988f-cbc5415a66aa)\n\n<br/><br/>\n\n[Prompt](https://github.com/browser-use/browser-use/blob/main/examples/custom-functions/save_to_file_hugging_face.py): Look up models with a license of cc-by-sa-4.0 and sort by most likes on Hugging face, save top 5 to file.\n\nhttps://github.com/user-attachments/assets/de73ee39-432c-4b97-b4e8-939fd7f323b3\n\n<br/><br/>\n\n## More examples\n\nFor more examples see the [examples](examples) folder or join the [Discord](https://link.browser-use.com/discord) and show off your project. You can also see our [`awesome-prompts`](https://github.com/browser-use/awesome-prompts) repo for prompting inspiration.\n\n# Vision\n\nTell your computer what to do, and it gets it done.\n\n## Roadmap\n\n### Agent\n\n- [ ] Improve agent memory to handle +100 steps\n- [ ] Enhance planning capabilities (load website specific context)\n- [ ] Reduce token consumption (system prompt, DOM state)\n\n### DOM Extraction\n\n- [ ] Enable detection for all possible UI elements\n- [ ] Improve state representation for UI elements so that all LLMs can understand what's on the page\n\n### Workflows\n\n- [ ] Let user record a workflow - which we can rerun with browser-use as a fallback\n- [ ] Make rerunning of workflows work, even if pages change\n\n### User Experience\n\n- [ ] Create various templates for tutorial execution, job application, QA testing, social media, etc. which users can just copy & paste.\n- [ ] Improve docs\n- [ ] Make it faster\n\n### Parallelization\n\n- [ ] Human work is sequential. The real power of a browser agent comes into reality if we can parallelize similar tasks. For example, if you want to find contact information for 100 companies, this can all be done in parallel and reported back to a main agent, which processes the results and kicks off parallel subtasks again.\n\n## Contributing\n\nWe love contributions! Feel free to open issues for bugs or feature requests. To contribute to the docs, check out the `/docs` folder.\n\n## \ud83e\uddea How to make your agents robust?\n\nWe offer to run your tasks in our CI\u2014automatically, on every update!\n\n- **Add your task:** Add a YAML file in `tests/agent_tasks/` (see the [`README there`](tests/agent_tasks/README.md) for details).\n- **Automatic validation:** Every time we push updates, your task will be run by the agent and evaluated using your criteria.\n\n## Local Setup\n\nTo learn more about the library, check out the [local setup \ud83d\udcd5](https://docs.browser-use.com/development/local-setup).\n\n`main` is the primary development branch with frequent changes. For production use, install a stable [versioned release](https://github.com/browser-use/browser-use/releases) instead.\n\n---\n\n## Swag\n\nWant to show off your Browser-use swag? Check out our [Merch store](https://browsermerch.com). Good contributors will receive swag for free \ud83d\udc40.\n\n## Citation\n\nIf you use Browser Use in your research or project, please cite:\n\n```bibtex\n@software{browser_use2024,\n  author = {M\u00fcller, Magnus and \u017duni\u010d, Gregor},\n  title = {Browser Use: Enable AI to control your browser},\n  year = {2024},\n  publisher = {GitHub},\n  url = {https://github.com/browser-use/browser-use}\n}\n```\n\n <div align=\"center\"> <img src=\"https://github.com/user-attachments/assets/06fa3078-8461-4560-b434-445510c1766f\" width=\"400\"/> \n \n[![Twitter Follow](https://img.shields.io/twitter/follow/Gregor?style=social)](https://x.com/intent/user?screen_name=gregpr07)\n[![Twitter Follow](https://img.shields.io/twitter/follow/Magnus?style=social)](https://x.com/intent/user?screen_name=mamagnus00)\n \n </div>\n\n<div align=\"center\">\nMade with \u2764\ufe0f in Zurich and San Francisco\n </div>\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 660551251,
    "name": "MetaGPT",
    "full_name": "FoundationAgents/MetaGPT",
    "description": "\ud83c\udf1f The Multi-Agent Framework: First AI Software Company, Towards Natural Language Programming",
    "html_url": "https://github.com/FoundationAgents/MetaGPT",
    "clone_url": "https://github.com/FoundationAgents/MetaGPT.git",
    "owner_login": "FoundationAgents",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/198047230?v=4",
    "stargazers_count": 57700,
    "watchers_count": 57700,
    "forks_count": 6935,
    "open_issues_count": 61,
    "size": 184056,
    "language": "Python",
    "languages": {
      "Python": 3214321,
      "JavaScript": 32579,
      "TypeScript": 25281,
      "Shell": 23970,
      "Dockerfile": 982
    },
    "topics": [
      "agent",
      "gpt",
      "llm",
      "metagpt",
      "multi-agent"
    ],
    "license_name": "MIT License",
    "created_at": "2023-06-30T09:04:55+00:00",
    "updated_at": "2025-08-06T01:42:34+00:00",
    "pushed_at": "2025-06-30T11:45:55+00:00",
    "contributors_count": 100,
    "readme_length": 8138,
    "readme_content": "\n# MetaGPT: The Multi-Agent Framework\n\n<p align=\"center\">\n<a href=\"\"><img src=\"docs/resources/MetaGPT-new-log.png\" alt=\"MetaGPT logo: Enable GPT to work in a software company, collaborating to tackle more complex tasks.\" width=\"150px\"></a>\n</p>\n\n<p align=\"center\">\n[ <b>En</b> |\n<a href=\"docs/README_CN.md\">\u4e2d</a> |\n<a href=\"docs/README_FR.md\">Fr</a> |\n<a href=\"docs/README_JA.md\">\u65e5</a> ]\n<b>Assign different roles to GPTs to form a collaborative entity for complex tasks.</b>\n</p>\n\n<p align=\"center\">\n<a href=\"https://opensource.org/licenses/MIT\"><img src=\"https://img.shields.io/badge/License-MIT-blue.svg\" alt=\"License: MIT\"></a>\n<a href=\"https://discord.gg/DYn29wFk9z\"><img src=\"https://dcbadge.vercel.app/api/server/DYn29wFk9z?style=flat\" alt=\"Discord Follow\"></a>\n<a href=\"https://twitter.com/MetaGPT_\"><img src=\"https://img.shields.io/twitter/follow/MetaGPT?style=social\" alt=\"Twitter Follow\"></a>\n</p>\n\n<h4 align=\"center\">\n    \n</h4>\n\n## News\n\n\ud83d\ude80 Mar. 10, 2025: \ud83c\udf89 [mgx.dev](https://mgx.dev/) is the #1 Product of the Week on @ProductHunt! \ud83c\udfc6\n\n\ud83d\ude80 Mar. &nbsp; 4, 2025: \ud83c\udf89 [mgx.dev](https://mgx.dev/) is the #1 Product of the Day on @ProductHunt! \ud83c\udfc6\n\n\ud83d\ude80 Feb. 19, 2025: Today we are officially launching our natural language programming product: [MGX (MetaGPT X)](https://mgx.dev/) - the world's first AI agent development team. More details on [Twitter](https://x.com/MetaGPT_/status/1892199535130329356).\n\n\ud83d\ude80 Feb. 17, 2025: We introduced two papers: [SPO](https://arxiv.org/pdf/2502.06855) and [AOT](https://arxiv.org/pdf/2502.12018), check the [code](examples)!\n\n\ud83d\ude80 Jan. 22, 2025: Our paper [AFlow: Automating Agentic Workflow Generation](https://openreview.net/forum?id=z5uVAKwmjf) accepted for **oral presentation (top 1.8%)** at ICLR 2025, **ranking #2** in the LLM-based Agent category.\n\n\ud83d\udc49\ud83d\udc49 [Earlier news](docs/NEWS.md) \n\n## Software Company as Multi-Agent System\n\n1. MetaGPT takes a **one line requirement** as input and outputs **user stories / competitive analysis / requirements / data structures / APIs / documents, etc.**\n2. Internally, MetaGPT includes **product managers / architects / project managers / engineers.** It provides the entire process of a **software company along with carefully orchestrated SOPs.**\n   1. `Code = SOP(Team)` is the core philosophy. We materialize SOP and apply it to teams composed of LLMs.\n\n![A software company consists of LLM-based roles](docs/resources/software_company_cd.jpeg)\n\n<p align=\"center\">Software Company Multi-Agent Schematic (Gradually Implementing)</p>\n\n## Get Started\n\n### Installation\n\n> Ensure that Python 3.9 or later, but less than 3.12, is installed on your system. You can check this by using: `python --version`.  \n> You can use conda like this: `conda create -n metagpt python=3.9 && conda activate metagpt`\n\n```bash\npip install --upgrade metagpt\n# or `pip install --upgrade git+https://github.com/geekan/MetaGPT.git`\n# or `git clone https://github.com/geekan/MetaGPT && cd MetaGPT && pip install --upgrade -e .`\n```\n\n**Install [node](https://nodejs.org/en/download) and [pnpm](https://pnpm.io/installation#using-npm) before actual use.**\n\nFor detailed installation guidance, please refer to [cli_install](https://docs.deepwisdom.ai/main/en/guide/get_started/installation.html#install-stable-version)\n or [docker_install](https://docs.deepwisdom.ai/main/en/guide/get_started/installation.html#install-with-docker)\n\n### Configuration\n\nYou can init the config of MetaGPT by running the following command, or manually create `~/.metagpt/config2.yaml` file:\n```bash\n# Check https://docs.deepwisdom.ai/main/en/guide/get_started/configuration.html for more details\nmetagpt --init-config  # it will create ~/.metagpt/config2.yaml, just modify it to your needs\n```\n\nYou can configure `~/.metagpt/config2.yaml` according to the [example](https://github.com/geekan/MetaGPT/blob/main/config/config2.example.yaml) and [doc](https://docs.deepwisdom.ai/main/en/guide/get_started/configuration.html):\n\n```yaml\nllm:\n  api_type: \"openai\"  # or azure / ollama / groq etc. Check LLMType for more options\n  model: \"gpt-4-turbo\"  # or gpt-3.5-turbo\n  base_url: \"https://api.openai.com/v1\"  # or forward url / other llm url\n  api_key: \"YOUR_API_KEY\"\n```\n\n### Usage\n\nAfter installation, you can use MetaGPT at CLI\n\n```bash\nmetagpt \"Create a 2048 game\"  # this will create a repo in ./workspace\n```\n\nor use it as library\n\n```python\nfrom metagpt.software_company import generate_repo\nfrom metagpt.utils.project_repo import ProjectRepo\n\nrepo: ProjectRepo = generate_repo(\"Create a 2048 game\")  # or ProjectRepo(\"<path>\")\nprint(repo)  # it will print the repo structure with files\n```\n\nYou can also use [Data Interpreter](https://github.com/geekan/MetaGPT/tree/main/examples/di) to write code:\n\n```python\nimport asyncio\nfrom metagpt.roles.di.data_interpreter import DataInterpreter\n\nasync def main():\n    di = DataInterpreter()\n    await di.run(\"Run data analysis on sklearn Iris dataset, include a plot\")\n\nasyncio.run(main())  # or await main() in a jupyter notebook setting\n```\n\n\n### QuickStart & Demo Video\n- Try it on [MetaGPT Huggingface Space](https://huggingface.co/spaces/deepwisdom/MetaGPT-SoftwareCompany)\n- [Matthew Berman: How To Install MetaGPT - Build A Startup With One Prompt!!](https://youtu.be/uT75J_KG_aY)\n- [Official Demo Video](https://github.com/geekan/MetaGPT/assets/2707039/5e8c1062-8c35-440f-bb20-2b0320f8d27d)\n\nhttps://github.com/user-attachments/assets/888cb169-78c3-4a42-9d62-9d90ed3928c9\n\n## Tutorial\n\n- \ud83d\uddd2 [Online Document](https://docs.deepwisdom.ai/main/en/)\n- \ud83d\udcbb [Usage](https://docs.deepwisdom.ai/main/en/guide/get_started/quickstart.html)  \n- \ud83d\udd0e [What can MetaGPT do?](https://docs.deepwisdom.ai/main/en/guide/get_started/introduction.html)\n- \ud83d\udee0 How to build your own agents? \n  - [MetaGPT Usage & Development Guide | Agent 101](https://docs.deepwisdom.ai/main/en/guide/tutorials/agent_101.html)\n  - [MetaGPT Usage & Development Guide | MultiAgent 101](https://docs.deepwisdom.ai/main/en/guide/tutorials/multi_agent_101.html)\n- \ud83e\uddd1\u200d\ud83d\udcbb Contribution\n  - [Develop Roadmap](docs/ROADMAP.md)\n- \ud83d\udd16 Use Cases\n  - [Data Interpreter](https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/interpreter/intro.html)\n  - [Debate](https://docs.deepwisdom.ai/main/en/guide/use_cases/multi_agent/debate.html)\n  - [Researcher](https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/researcher.html)\n  - [Receipt Assistant](https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/receipt_assistant.html)\n- \u2753 [FAQs](https://docs.deepwisdom.ai/main/en/guide/faq.html)\n\n## Support\n\n### Discord Join US\n\n\ud83d\udce2 Join Our [Discord Channel](https://discord.gg/ZRHeExS6xv)! Looking forward to seeing you there! \ud83c\udf89\n\n### Contributor form\n\n\ud83d\udcdd [Fill out the form](https://airtable.com/appInfdG0eJ9J4NNL/pagK3Fh1sGclBvVkV/form) to become a contributor. We are looking forward to your participation!\n\n### Contact Information\n\nIf you have any questions or feedback about this project, please feel free to contact us. We highly appreciate your suggestions!\n\n- **Email:** alexanderwu@deepwisdom.ai\n- **GitHub Issues:** For more technical inquiries, you can also create a new issue in our [GitHub repository](https://github.com/geekan/metagpt/issues).\n\nWe will respond to all questions within 2-3 business days.\n\n## Citation\n\nTo stay updated with the latest research and development, follow [@MetaGPT_](https://twitter.com/MetaGPT_) on Twitter. \n\nTo cite [MetaGPT](https://openreview.net/forum?id=VtmBAGCN7o) in publications, please use the following BibTeX entries.   \n\n```bibtex\n@inproceedings{hong2024metagpt,\n      title={Meta{GPT}: Meta Programming for A Multi-Agent Collaborative Framework},\n      author={Sirui Hong and Mingchen Zhuge and Jonathan Chen and Xiawu Zheng and Yuheng Cheng and Jinlin Wang and Ceyao Zhang and Zili Wang and Steven Ka Shing Yau and Zijuan Lin and Liyang Zhou and Chenyu Ran and Lingfeng Xiao and Chenglin Wu and J{\\\"u}rgen Schmidhuber},\n      booktitle={The Twelfth International Conference on Learning Representations},\n      year={2024},\n      url={https://openreview.net/forum?id=VtmBAGCN7o}\n}\n```\n\nFor more work, please refer to [Academic Work](docs/ACADEMIC_WORK.md).\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 646410686,
    "name": "LLaMA-Factory",
    "full_name": "hiyouga/LLaMA-Factory",
    "description": "Unified Efficient Fine-Tuning of 100+ LLMs & VLMs (ACL 2024)",
    "html_url": "https://github.com/hiyouga/LLaMA-Factory",
    "clone_url": "https://github.com/hiyouga/LLaMA-Factory.git",
    "owner_login": "hiyouga",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/16256802?v=4",
    "stargazers_count": 55610,
    "watchers_count": 55610,
    "forks_count": 6827,
    "open_issues_count": 555,
    "size": 54553,
    "language": "Python",
    "languages": {
      "Python": 1278466,
      "Dockerfile": 5439,
      "Makefile": 457
    },
    "topics": [
      "agent",
      "ai",
      "deepseek",
      "fine-tuning",
      "gemma",
      "gpt",
      "instruction-tuning",
      "large-language-models",
      "llama",
      "llama3",
      "llm",
      "lora",
      "moe",
      "nlp",
      "peft",
      "qlora",
      "quantization",
      "qwen",
      "rlhf",
      "transformers"
    ],
    "license_name": "Apache License 2.0",
    "created_at": "2023-05-28T10:09:12+00:00",
    "updated_at": "2025-08-06T02:12:27+00:00",
    "pushed_at": "2025-08-05T22:58:11+00:00",
    "contributors_count": 100,
    "readme_length": 68801,
    "readme_content": "![# LLaMA Factory](assets/logo.png)\n\n[![GitHub Repo stars](https://img.shields.io/github/stars/hiyouga/LLaMA-Factory?style=social)](https://github.com/hiyouga/LLaMA-Factory/stargazers)\n[![GitHub last commit](https://img.shields.io/github/last-commit/hiyouga/LLaMA-Factory)](https://github.com/hiyouga/LLaMA-Factory/commits/main)\n[![GitHub contributors](https://img.shields.io/github/contributors/hiyouga/LLaMA-Factory?color=orange)](https://github.com/hiyouga/LLaMA-Factory/graphs/contributors)\n[![GitHub workflow](https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml/badge.svg)](https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml)\n[![PyPI](https://img.shields.io/pypi/v/llamafactory)](https://pypi.org/project/llamafactory/)\n[![Citation](https://img.shields.io/badge/citation-760-green)](https://scholar.google.com/scholar?cites=12620864006390196564)\n[![Docker Pulls](https://img.shields.io/docker/pulls/hiyouga/llamafactory)](https://hub.docker.com/r/hiyouga/llamafactory/tags)\n\n[![Twitter](https://img.shields.io/twitter/follow/llamafactory_ai)](https://twitter.com/llamafactory_ai)\n[![Discord](https://dcbadge.vercel.app/api/server/rKfvV9r9FK?compact=true&style=flat)](https://discord.gg/rKfvV9r9FK)\n[![GitCode](https://gitcode.com/zhengyaowei/LLaMA-Factory/star/badge.svg)](https://gitcode.com/zhengyaowei/LLaMA-Factory)\n\n[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing)\n[![Open in DSW](https://gallery.pai-ml.com/assets/open-in-dsw.svg)](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory)\n[![Open in Alaya](assets/alaya_new.svg)](https://docs.alayanew.com/docs/documents/newActivities/llamafactory/?utm_source=LLaMA-Factory)\n[![Open in Spaces](https://img.shields.io/badge/\ud83e\udd17-Open%20in%20Spaces-blue)](https://huggingface.co/spaces/hiyouga/LLaMA-Board)\n[![Open in Studios](https://img.shields.io/badge/ModelScope-Open%20in%20Studios-blue)](https://modelscope.cn/studios/hiyouga/LLaMA-Board)\n[![Open in Novita](https://img.shields.io/badge/Novita-Deploy%20Template-blue)](https://novita.ai/templates-library/105981?sharer=88115474-394e-4bda-968e-b88e123d0c47)\n\n### Used by [Amazon](https://aws.amazon.com/cn/blogs/machine-learning/how-apoidea-group-enhances-visual-information-extraction-from-banking-documents-with-multimodal-models-using-llama-factory-on-amazon-sagemaker-hyperpod/), [NVIDIA](https://developer.nvidia.com/rtx/ai-toolkit), [Aliyun](https://help.aliyun.com/zh/pai/use-cases/fine-tune-a-llama-3-model-with-llama-factory), etc.\n\n<div align=\"center\" markdown=\"1\">\n\n### Supporters \u2764\ufe0f\n\n| <div style=\"text-align: center;\"><a href=\"https://warp.dev/llama-factory\"><img alt=\"Warp sponsorship\" width=\"400\" src=\"assets/warp.jpg\"></a><br><a href=\"https://warp.dev/llama-factory\" style=\"font-size:larger;\">Warp, the agentic terminal for developers</a><br><a href=\"https://warp.dev/llama-factory\">Available for MacOS, Linux, & Windows</a> | <a href=\"https://serpapi.com\"><img alt=\"SerpAPI sponsorship\" width=\"250\" src=\"assets/serpapi.svg\"> </a> |\n| ---- | ---- |\n\n----\n\n### Easily fine-tune 100+ large language models with zero-code [CLI](#quickstart) and [Web UI](#fine-tuning-with-llama-board-gui-powered-by-gradio)\n\n![GitHub Trend](https://trendshift.io/api/badge/repositories/4535)\n\n</div>\n\n\ud83d\udc4b Join our [WeChat group](assets/wechat.jpg), [NPU user group](assets/wechat_npu.jpg) or [Alaya NeW user group](assets/wechat_alaya.png).\n\n\\[ English | [\u4e2d\u6587](README_zh.md) \\]\n\n**Fine-tuning a large language model can be easy as...**\n\nhttps://github.com/user-attachments/assets/3991a3a8-4276-4d30-9cab-4cb0c4b9b99e\n\nChoose your path:\n\n- **Documentation (WIP)**: https://llamafactory.readthedocs.io/en/latest/\n- **Documentation (AMD GPU)**: https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/fine_tune/llama_factory_llama3.html\n- **Colab (free)**: https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing\n- **Local machine**: Please refer to [usage](#getting-started)\n- **PAI-DSW (free trial)**: https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory\n- **Alaya NeW (cloud GPU deal)**: https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory\n\n> [!NOTE]\n> Except for the above links, all other websites are unauthorized third-party websites. Please carefully use them.\n\n## Table of Contents\n\n- [Features](#features)\n- [Blogs](#blogs)\n- [Changelog](#changelog)\n- [Supported Models](#supported-models)\n- [Supported Training Approaches](#supported-training-approaches)\n- [Provided Datasets](#provided-datasets)\n- [Requirement](#requirement)\n- [Getting Started](#getting-started)\n  - [Installation](#installation)\n  - [Data Preparation](#data-preparation)\n  - [Quickstart](#quickstart)\n  - [Fine-Tuning with LLaMA Board GUI](#fine-tuning-with-llama-board-gui-powered-by-gradio)\n  - [Build Docker](#build-docker)\n  - [Deploy with OpenAI-style API and vLLM](#deploy-with-openai-style-api-and-vllm)\n  - [Download from ModelScope Hub](#download-from-modelscope-hub)\n  - [Download from Modelers Hub](#download-from-modelers-hub)\n  - [Use W&B Logger](#use-wb-logger)\n  - [Use SwanLab Logger](#use-swanlab-logger)\n- [Projects using LLaMA Factory](#projects-using-llama-factory)\n- [License](#license)\n- [Citation](#citation)\n- [Acknowledgement](#acknowledgement)\n\n## Features\n\n- **Various models**: LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, Qwen2-VL, DeepSeek, Yi, Gemma, ChatGLM, Phi, etc.\n- **Integrated methods**: (Continuous) pre-training, (multimodal) supervised fine-tuning, reward modeling, PPO, DPO, KTO, ORPO, etc.\n- **Scalable resources**: 16-bit full-tuning, freeze-tuning, LoRA and 2/3/4/5/6/8-bit QLoRA via AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ.\n- **Advanced algorithms**: [GaLore](https://github.com/jiaweizzhao/GaLore), [BAdam](https://github.com/Ledzy/BAdam), [APOLLO](https://github.com/zhuhanqing/APOLLO), [Adam-mini](https://github.com/zyushun/Adam-mini), [Muon](https://github.com/KellerJordan/Muon), DoRA, LongLoRA, LLaMA Pro, Mixture-of-Depths, LoRA+, LoftQ and PiSSA.\n- **Practical tricks**: [FlashAttention-2](https://github.com/Dao-AILab/flash-attention), [Unsloth](https://github.com/unslothai/unsloth), [Liger Kernel](https://github.com/linkedin/Liger-Kernel), RoPE scaling, NEFTune and rsLoRA.\n- **Wide tasks**: Multi-turn dialogue, tool using, image understanding, visual grounding, video recognition, audio understanding, etc.\n- **Experiment monitors**: LlamaBoard, TensorBoard, Wandb, MLflow, [SwanLab](https://github.com/SwanHubX/SwanLab), etc.\n- **Faster inference**: OpenAI-style API, Gradio UI and CLI with [vLLM worker](https://github.com/vllm-project/vllm) or [SGLang worker](https://github.com/sgl-project/sglang).\n\n### Day-N Support for Fine-Tuning Cutting-Edge Models\n\n| Support Date | Model Name                                                           |\n| ------------ | -------------------------------------------------------------------- |\n| Day 0        | Qwen3 / Qwen2.5-VL / Gemma 3 / GLM-4.1V / InternLM 3 / MiniCPM-o-2.6 |\n| Day 1        | Llama 3 / GLM-4 / Mistral Small / PaliGemma2 / Llama 4               |\n\n## Blogs\n\n- [Fine-tune Llama3.1-70B for Medical Diagnosis using LLaMA-Factory](https://docs.alayanew.com/docs/documents/bestPractice/bigModel/llama70B/?utm_source=LLaMA-Factory) (Chinese)\n- [A One-Stop Code-Free Model Reinforcement Learning and Deployment Platform based on LLaMA-Factory and EasyR1](https://aws.amazon.com/cn/blogs/china/building-llm-model-hub-based-on-llamafactory-and-easyr1/) (Chinese)\n- [How Apoidea Group enhances visual information extraction from banking documents with multimodal models using LLaMA-Factory on Amazon SageMaker HyperPod](https://aws.amazon.com/cn/blogs/machine-learning/how-apoidea-group-enhances-visual-information-extraction-from-banking-documents-with-multimodal-models-using-llama-factory-on-amazon-sagemaker-hyperpod/) (English)\n- [Easy Dataset \u00d7 LLaMA Factory: Enabling LLMs to Efficiently Learn Domain Knowledge](https://buaa-act.feishu.cn/wiki/GVzlwYcRFiR8OLkHbL6cQpYin7g) (English)\n\n<details><summary>All Blogs</summary>\n\n- [Fine-tune Qwen2.5-VL for Autonomous Driving using LLaMA-Factory](https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory) (Chinese)\n- [LLaMA Factory: Fine-tuning the DeepSeek-R1-Distill-Qwen-7B Model for News Classifier](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_deepseek_r1_distill_7b) (Chinese)\n- [A One-Stop Code-Free Model Fine-Tuning \\& Deployment Platform based on SageMaker and LLaMA-Factory](https://aws.amazon.com/cn/blogs/china/a-one-stop-code-free-model-fine-tuning-deployment-platform-based-on-sagemaker-and-llama-factory/) (Chinese)\n- [LLaMA Factory Multi-Modal Fine-Tuning Practice: Fine-Tuning Qwen2-VL for Personal Tourist Guide](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_qwen2vl) (Chinese)\n- [LLaMA Factory: Fine-tuning Llama3 for Role-Playing](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory) (Chinese)\n\n</details>\n\n## Changelog\n\n[25/08/06] We supported fine-tuning the **[GPT-OSS](https://github.com/openai/gpt-oss)** models. See [PR #8826](https://github.com/hiyouga/LLaMA-Factory/pull/8826) to get started.\n\n[25/07/02] We supported fine-tuning the **[GLM-4.1V-9B-Thinking](https://github.com/THUDM/GLM-4.1V-Thinking)** model.\n\n[25/04/28] We supported fine-tuning the **[Qwen3](https://qwenlm.github.io/blog/qwen3/)** model family.\n\n<details><summary>Full Changelog</summary>\n\n[25/04/21] We supported the **[Muon](https://github.com/KellerJordan/Muon)** optimizer. See [examples](examples/README.md) for usage. Thank [@tianshijing](https://github.com/tianshijing)'s PR.\n\n[25/04/16] We supported fine-tuning the **[InternVL3](https://huggingface.co/OpenGVLab/InternVL3-8B)** model. See [PR #7258](https://github.com/hiyouga/LLaMA-Factory/pull/7258) to get started.\n\n[25/04/14] We supported fine-tuning the **[GLM-Z1](https://huggingface.co/THUDM/GLM-Z1-9B-0414)** and **[Kimi-VL](https://huggingface.co/moonshotai/Kimi-VL-A3B-Instruct)** models.\n\n[25/04/06] We supported fine-tuning the **[Llama 4](https://ai.meta.com/blog/llama-4-multimodal-intelligence/)** model. See [PR #7611](https://github.com/hiyouga/LLaMA-Factory/pull/7611) to get started.\n\n[25/03/31] We supported fine-tuning the **[Qwen2.5 Omni](https://qwenlm.github.io/blog/qwen2.5-omni/)** model. See [PR #7537](https://github.com/hiyouga/LLaMA-Factory/pull/7537) to get started.\n\n[25/03/15] We supported **[SGLang](https://github.com/sgl-project/sglang)** as inference backend. Try `infer_backend: sglang` to accelerate inference.\n\n[25/03/12] We supported fine-tuning the **[Gemma 3](https://huggingface.co/blog/gemma3)** model.\n\n[25/02/24] Announcing **[EasyR1](https://github.com/hiyouga/EasyR1)**, an efficient, scalable and multi-modality RL training framework for efficient GRPO training.\n\n[25/02/11] We supported saving the **[Ollama](https://github.com/ollama/ollama)** modelfile when exporting the model checkpoints. See [examples](examples/README.md) for usage.\n\n[25/02/05] We supported fine-tuning the **[Qwen2-Audio](Qwen/Qwen2-Audio-7B-Instruct)** and **[MiniCPM-o-2.6](https://huggingface.co/openbmb/MiniCPM-o-2_6)** on audio understanding tasks.\n\n[25/01/31] We supported fine-tuning the **[DeepSeek-R1](https://huggingface.co/deepseek-ai/DeepSeek-R1)** and **[Qwen2.5-VL](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct)** models.\n\n[25/01/15] We supported **[APOLLO](https://arxiv.org/abs/2412.05270)** optimizer. See [examples](examples/README.md) for usage.\n\n[25/01/14] We supported fine-tuning the **[MiniCPM-o-2.6](https://huggingface.co/openbmb/MiniCPM-o-2_6)** and **[MiniCPM-V-2.6](https://huggingface.co/openbmb/MiniCPM-V-2_6)** models. Thank [@BUAADreamer](https://github.com/BUAADreamer)'s PR.\n\n[25/01/14] We supported fine-tuning the **[InternLM 3](https://huggingface.co/collections/internlm/)** models. Thank [@hhaAndroid](https://github.com/hhaAndroid)'s PR.\n\n[25/01/10] We supported fine-tuning the **[Phi-4](https://huggingface.co/microsoft/phi-4)** model.\n\n[24/12/21] We supported using **[SwanLab](https://github.com/SwanHubX/SwanLab)** for experiment tracking and visualization. See [this section](#use-swanlab-logger) for details.\n\n[24/11/27] We supported fine-tuning the **[Skywork-o1](https://huggingface.co/Skywork/Skywork-o1-Open-Llama-3.1-8B)** model and the **[OpenO1](https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT)** dataset.\n\n[24/10/09] We supported downloading pre-trained models and datasets from the **[Modelers Hub](https://modelers.cn/models)**. See [this tutorial](#download-from-modelers-hub) for usage.\n\n[24/09/19] We supported fine-tuning the **[Qwen2.5](https://qwenlm.github.io/blog/qwen2.5/)** models.\n\n[24/08/30] We supported fine-tuning the **[Qwen2-VL](https://qwenlm.github.io/blog/qwen2-vl/)** models. Thank [@simonJJJ](https://github.com/simonJJJ)'s PR.\n\n[24/08/27] We supported **[Liger Kernel](https://github.com/linkedin/Liger-Kernel)**. Try `enable_liger_kernel: true` for efficient training.\n\n[24/08/09] We supported **[Adam-mini](https://github.com/zyushun/Adam-mini)** optimizer. See [examples](examples/README.md) for usage. Thank [@relic-yuexi](https://github.com/relic-yuexi)'s PR.\n\n[24/07/04] We supported [contamination-free packed training](https://github.com/MeetKai/functionary/tree/main/functionary/train/packing). Use `neat_packing: true` to activate it. Thank [@chuan298](https://github.com/chuan298)'s PR.\n\n[24/06/16] We supported **[PiSSA](https://arxiv.org/abs/2404.02948)** algorithm. See [examples](examples/README.md) for usage.\n\n[24/06/07] We supported fine-tuning the **[Qwen2](https://qwenlm.github.io/blog/qwen2/)** and **[GLM-4](https://github.com/THUDM/GLM-4)** models.\n\n[24/05/26] We supported **[SimPO](https://arxiv.org/abs/2405.14734)** algorithm for preference learning. See [examples](examples/README.md) for usage.\n\n[24/05/20] We supported fine-tuning the **PaliGemma** series models. Note that the PaliGemma models are pre-trained models, you need to fine-tune them with `paligemma` template for chat completion.\n\n[24/05/18] We supported **[KTO](https://arxiv.org/abs/2402.01306)** algorithm for preference learning. See [examples](examples/README.md) for usage.\n\n[24/05/14] We supported training and inference on the Ascend NPU devices. Check [installation](#installation) section for details.\n\n[24/04/26] We supported fine-tuning the **LLaVA-1.5** multimodal LLMs. See [examples](examples/README.md) for usage.\n\n[24/04/22] We provided a **[Colab notebook](https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing)** for fine-tuning the Llama-3 model on a free T4 GPU. Two Llama-3-derived models fine-tuned using LLaMA Factory are available at Hugging Face, check [Llama3-8B-Chinese-Chat](https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat) and [Llama3-Chinese](https://huggingface.co/zhichen/Llama3-Chinese) for details.\n\n[24/04/21] We supported **[Mixture-of-Depths](https://arxiv.org/abs/2404.02258)** according to [AstraMindAI's implementation](https://github.com/astramind-ai/Mixture-of-depths). See [examples](examples/README.md) for usage.\n\n[24/04/16] We supported **[BAdam](https://arxiv.org/abs/2404.02827)** optimizer. See [examples](examples/README.md) for usage.\n\n[24/04/16] We supported **[unsloth](https://github.com/unslothai/unsloth)**'s long-sequence training (Llama-2-7B-56k within 24GB). It achieves **117%** speed and **50%** memory compared with FlashAttention-2, more benchmarks can be found in [this page](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison).\n\n[24/03/31] We supported **[ORPO](https://arxiv.org/abs/2403.07691)**. See [examples](examples/README.md) for usage.\n\n[24/03/21] Our paper \"[LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models](https://arxiv.org/abs/2403.13372)\" is available at arXiv!\n\n[24/03/20] We supported **FSDP+QLoRA** that fine-tunes a 70B model on 2x24GB GPUs. See [examples](examples/README.md) for usage.\n\n[24/03/13] We supported **[LoRA+](https://arxiv.org/abs/2402.12354)**. See [examples](examples/README.md) for usage.\n\n[24/03/07] We supported **[GaLore](https://arxiv.org/abs/2403.03507)** optimizer. See [examples](examples/README.md) for usage.\n\n[24/03/07] We integrated **[vLLM](https://github.com/vllm-project/vllm)** for faster and concurrent inference. Try `infer_backend: vllm` to enjoy **270%** inference speed.\n\n[24/02/28] We supported weight-decomposed LoRA (**[DoRA](https://arxiv.org/abs/2402.09353)**). Try `use_dora: true` to activate DoRA training.\n\n[24/02/15] We supported **block expansion** proposed by [LLaMA Pro](https://github.com/TencentARC/LLaMA-Pro). See [examples](examples/README.md) for usage.\n\n[24/02/05] Qwen1.5 (Qwen2 beta version) series models are supported in LLaMA-Factory. Check this [blog post](https://qwenlm.github.io/blog/qwen1.5/) for details.\n\n[24/01/18] We supported **agent tuning** for most models, equipping model with tool using abilities by fine-tuning with `dataset: glaive_toolcall_en`.\n\n[23/12/23] We supported **[unsloth](https://github.com/unslothai/unsloth)**'s implementation to boost LoRA tuning for the LLaMA, Mistral and Yi models. Try `use_unsloth: true` argument to activate unsloth patch. It achieves **170%** speed in our benchmark, check [this page](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison) for details.\n\n[23/12/12] We supported fine-tuning the latest MoE model **[Mixtral 8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)** in our framework. See hardware requirement [here](#hardware-requirement).\n\n[23/12/01] We supported downloading pre-trained models and datasets from the **[ModelScope Hub](https://modelscope.cn/models)**. See [this tutorial](#download-from-modelscope-hub) for usage.\n\n[23/10/21] We supported **[NEFTune](https://arxiv.org/abs/2310.05914)** trick for fine-tuning. Try `neftune_noise_alpha: 5` argument to activate NEFTune.\n\n[23/09/27] We supported **$S^2$-Attn** proposed by [LongLoRA](https://github.com/dvlab-research/LongLoRA) for the LLaMA models. Try `shift_attn: true` argument to enable shift short attention.\n\n[23/09/23] We integrated MMLU, C-Eval and CMMLU benchmarks in this repo. See [examples](examples/README.md) for usage.\n\n[23/09/10] We supported **[FlashAttention-2](https://github.com/Dao-AILab/flash-attention)**. Try `flash_attn: fa2` argument to enable FlashAttention-2 if you are using RTX4090, A100 or H100 GPUs.\n\n[23/08/12] We supported **RoPE scaling** to extend the context length of the LLaMA models. Try `rope_scaling: linear` argument in training and `rope_scaling: dynamic` argument at inference to extrapolate the position embeddings.\n\n[23/08/11] We supported **[DPO training](https://arxiv.org/abs/2305.18290)** for instruction-tuned models. See [examples](examples/README.md) for usage.\n\n[23/07/31] We supported **dataset streaming**. Try `streaming: true` and `max_steps: 10000` arguments to load your dataset in streaming mode.\n\n[23/07/29] We released two instruction-tuned 13B models at Hugging Face. See these Hugging Face Repos ([LLaMA-2](https://huggingface.co/hiyouga/Llama-2-Chinese-13b-chat) / [Baichuan](https://huggingface.co/hiyouga/Baichuan-13B-sft)) for details.\n\n[23/07/18] We developed an **all-in-one Web UI** for training, evaluation and inference. Try `train_web.py` to fine-tune models in your Web browser. Thank [@KanadeSiina](https://github.com/KanadeSiina) and [@codemayq](https://github.com/codemayq) for their efforts in the development.\n\n[23/07/09] We released **[FastEdit](https://github.com/hiyouga/FastEdit)** \u26a1\ud83e\ude79, an easy-to-use package for editing the factual knowledge of large language models efficiently. Please follow [FastEdit](https://github.com/hiyouga/FastEdit) if you are interested.\n\n[23/06/29] We provided a **reproducible example** of training a chat model using instruction-following datasets, see [Baichuan-7B-sft](https://huggingface.co/hiyouga/Baichuan-7B-sft) for details.\n\n[23/06/22] We aligned the [demo API](src/api_demo.py) with the [OpenAI's](https://platform.openai.com/docs/api-reference/chat) format where you can insert the fine-tuned model in **arbitrary ChatGPT-based applications**.\n\n[23/06/03] We supported quantized training and inference (aka **[QLoRA](https://github.com/artidoro/qlora)**). See [examples](examples/README.md) for usage.\n\n</details>\n\n> [!TIP]\n> If you cannot use the latest feature, please pull the latest code and install LLaMA-Factory again.\n\n## Supported Models\n\n| Model                                                             | Model size                       | Template            |\n| ----------------------------------------------------------------- | -------------------------------- | ------------------- |\n| [Baichuan 2](https://huggingface.co/baichuan-inc)                 | 7B/13B                           | baichuan2           |\n| [BLOOM/BLOOMZ](https://huggingface.co/bigscience)                 | 560M/1.1B/1.7B/3B/7.1B/176B      | -                   |\n| [ChatGLM3](https://huggingface.co/THUDM)                          | 6B                               | chatglm3            |\n| [Command R](https://huggingface.co/CohereForAI)                   | 35B/104B                         | cohere              |\n| [DeepSeek (Code/MoE)](https://huggingface.co/deepseek-ai)         | 7B/16B/67B/236B                  | deepseek            |\n| [DeepSeek 2.5/3](https://huggingface.co/deepseek-ai)              | 236B/671B                        | deepseek3           |\n| [DeepSeek R1 (Distill)](https://huggingface.co/deepseek-ai)       | 1.5B/7B/8B/14B/32B/70B/671B      | deepseekr1          |\n| [Falcon](https://huggingface.co/tiiuae)                           | 7B/11B/40B/180B                  | falcon              |\n| [Falcon-H1](https://huggingface.co/tiiuae)                        | 0.5B/1.5B/3B/7B/34B              | falcon_h1           |\n| [Gemma/Gemma 2/CodeGemma](https://huggingface.co/google)          | 2B/7B/9B/27B                     | gemma/gemma2        |\n| [Gemma 3/Gemma 3n](https://huggingface.co/google)                 | 1B/4B/6B/8B/12B/27B              | gemma3/gemma3n      |\n| [GLM-4/GLM-4-0414/GLM-Z1](https://huggingface.co/zai-org)         | 9B/32B                           | glm4/glmz1          |\n| [GLM-4.1V](https://huggingface.co/zai-org)*                       | 9B                               | glm4v               |\n| [GLM-4.5](https://huggingface.co/zai-org)*                        | 106B/355B                        | glm4_moe            |\n| [GPT-2](https://huggingface.co/openai-community)                  | 0.1B/0.4B/0.8B/1.5B              | -                   |\n| [GPT-OSS](https://huggingface.co/openai)                          | 20B/120B                         | gpt                 |\n| [Granite 3.0-3.3](https://huggingface.co/ibm-granite)             | 1B/2B/3B/8B                      | granite3            |\n| [Granite 4](https://huggingface.co/ibm-granite)                   | 7B                               | granite4            |\n| [Hunyuan](https://huggingface.co/tencent/)                        | 7B                               | hunyuan             |\n| [Index](https://huggingface.co/IndexTeam)                         | 1.9B                             | index               |\n| [InternLM 2-3](https://huggingface.co/internlm)                   | 7B/8B/20B                        | intern2             |\n| [InternVL 2.5-3](https://huggingface.co/OpenGVLab)                | 1B/2B/8B/14B/38B/78B             | intern_vl           |\n| [Kimi-VL](https://huggingface.co/moonshotai)                      | 16B                              | kimi_vl             |\n| [Llama](https://github.com/facebookresearch/llama)                | 7B/13B/33B/65B                   | -                   |\n| [Llama 2](https://huggingface.co/meta-llama)                      | 7B/13B/70B                       | llama2              |\n| [Llama 3-3.3](https://huggingface.co/meta-llama)                  | 1B/3B/8B/70B                     | llama3              |\n| [Llama 4](https://huggingface.co/meta-llama)                      | 109B/402B                        | llama4              |\n| [Llama 3.2 Vision](https://huggingface.co/meta-llama)             | 11B/90B                          | mllama              |\n| [LLaVA-1.5](https://huggingface.co/llava-hf)                      | 7B/13B                           | llava               |\n| [LLaVA-NeXT](https://huggingface.co/llava-hf)                     | 7B/8B/13B/34B/72B/110B           | llava_next          |\n| [LLaVA-NeXT-Video](https://huggingface.co/llava-hf)               | 7B/34B                           | llava_next_video    |\n| [MiMo](https://huggingface.co/XiaomiMiMo)                         | 7B                               | mimo                |\n| [MiniCPM](https://huggingface.co/openbmb)                         | 0.5B/1B/2B/4B/8B                 | cpm/cpm3/cpm4       |\n| [MiniCPM-o-2.6/MiniCPM-V-2.6](https://huggingface.co/openbmb)     | 8B                               | minicpm_o/minicpm_v |\n| [Ministral/Mistral-Nemo](https://huggingface.co/mistralai)        | 8B/12B                           | ministral           |\n| [Mistral/Mixtral](https://huggingface.co/mistralai)               | 7B/8x7B/8x22B                    | mistral             |\n| [Mistral Small](https://huggingface.co/mistralai)                 | 24B                              | mistral_small       |\n| [OLMo](https://huggingface.co/allenai)                            | 1B/7B                            | -                   |\n| [PaliGemma/PaliGemma2](https://huggingface.co/google)             | 3B/10B/28B                       | paligemma           |\n| [Phi-1.5/Phi-2](https://huggingface.co/microsoft)                 | 1.3B/2.7B                        | -                   |\n| [Phi-3/Phi-3.5](https://huggingface.co/microsoft)                 | 4B/14B                           | phi                 |\n| [Phi-3-small](https://huggingface.co/microsoft)                   | 7B                               | phi_small           |\n| [Phi-4](https://huggingface.co/microsoft)                         | 14B                              | phi4                |\n| [Pixtral](https://huggingface.co/mistralai)                       | 12B                              | pixtral             |\n| [Qwen (1-2.5) (Code/Math/MoE/QwQ)](https://huggingface.co/Qwen)   | 0.5B/1.5B/3B/7B/14B/32B/72B/110B | qwen                |\n| [Qwen3 (MoE)](https://huggingface.co/Qwen)                        | 0.6B/1.7B/4B/8B/14B/32B/235B     | qwen3               |\n| [Qwen2-Audio](https://huggingface.co/Qwen)                        | 7B                               | qwen2_audio         |\n| [Qwen2.5-Omni](https://huggingface.co/Qwen)                       | 3B/7B                            | qwen2_omni          |\n| [Qwen2-VL/Qwen2.5-VL/QVQ](https://huggingface.co/Qwen)            | 2B/3B/7B/32B/72B                 | qwen2_vl            |\n| [Seed Coder](https://huggingface.co/ByteDance-Seed)               | 8B                               | seed_coder          |\n| [Skywork o1](https://huggingface.co/Skywork)                      | 8B                               | skywork_o1          |\n| [StarCoder 2](https://huggingface.co/bigcode)                     | 3B/7B/15B                        | -                   |\n| [TeleChat2](https://huggingface.co/Tele-AI)                       | 3B/7B/35B/115B                   | telechat2           |\n| [XVERSE](https://huggingface.co/xverse)                           | 7B/13B/65B                       | xverse              |\n| [Yi/Yi-1.5 (Code)](https://huggingface.co/01-ai)                  | 1.5B/6B/9B/34B                   | yi                  |\n| [Yi-VL](https://huggingface.co/01-ai)                             | 6B/34B                           | yi_vl               |\n| [Yuan 2](https://huggingface.co/IEITYuan)                         | 2B/51B/102B                      | yuan                |\n\n> [!NOTE]\n> For the \"base\" models, the `template` argument can be chosen from `default`, `alpaca`, `vicuna` etc. But make sure to use the **corresponding template** for the \"instruct/chat\" models.\n>\n> Remember to use the **SAME** template in training and inference.\n>\n> \\*: You should install the `transformers` from main branch and use `DISABLE_VERSION_CHECK=1` to skip version check.\n>\n> \\*\\*: You need to install a specific version of `transformers` to use the corresponding model.\n\nPlease refer to [constants.py](src/llamafactory/extras/constants.py) for a full list of models we supported.\n\nYou also can add a custom chat template to [template.py](src/llamafactory/data/template.py).\n\n## Supported Training Approaches\n\n| Approach               |     Full-tuning    |    Freeze-tuning   |       LoRA         |       QLoRA        |\n| ---------------------- | ------------------ | ------------------ | ------------------ | ------------------ |\n| Pre-Training           | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| Supervised Fine-Tuning | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| Reward Modeling        | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| PPO Training           | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| DPO Training           | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| KTO Training           | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| ORPO Training          | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| SimPO Training         | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n\n> [!TIP]\n> The implementation details of PPO can be found in [this blog](https://newfacade.github.io/notes-on-reinforcement-learning/17-ppo-trl.html).\n\n## Provided Datasets\n\n<details><summary>Pre-training datasets</summary>\n\n- [Wiki Demo (en)](data/wiki_demo.txt)\n- [RefinedWeb (en)](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)\n- [RedPajama V2 (en)](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2)\n- [Wikipedia (en)](https://huggingface.co/datasets/olm/olm-wikipedia-20221220)\n- [Wikipedia (zh)](https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered)\n- [Pile (en)](https://huggingface.co/datasets/EleutherAI/pile)\n- [SkyPile (zh)](https://huggingface.co/datasets/Skywork/SkyPile-150B)\n- [FineWeb (en)](https://huggingface.co/datasets/HuggingFaceFW/fineweb)\n- [FineWeb-Edu (en)](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu)\n- [The Stack (en)](https://huggingface.co/datasets/bigcode/the-stack)\n- [StarCoder (en)](https://huggingface.co/datasets/bigcode/starcoderdata)\n\n</details>\n\n<details><summary>Supervised fine-tuning datasets</summary>\n\n- [Identity (en&zh)](data/identity.json)\n- [Stanford Alpaca (en)](https://github.com/tatsu-lab/stanford_alpaca)\n- [Stanford Alpaca (zh)](https://github.com/ymcui/Chinese-LLaMA-Alpaca-3)\n- [Alpaca GPT4 (en&zh)](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)\n- [Glaive Function Calling V2 (en&zh)](https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2)\n- [LIMA (en)](https://huggingface.co/datasets/GAIR/lima)\n- [Guanaco Dataset (multilingual)](https://huggingface.co/datasets/JosephusCheung/GuanacoDataset)\n- [BELLE 2M (zh)](https://huggingface.co/datasets/BelleGroup/train_2M_CN)\n- [BELLE 1M (zh)](https://huggingface.co/datasets/BelleGroup/train_1M_CN)\n- [BELLE 0.5M (zh)](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN)\n- [BELLE Dialogue 0.4M (zh)](https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M)\n- [BELLE School Math 0.25M (zh)](https://huggingface.co/datasets/BelleGroup/school_math_0.25M)\n- [BELLE Multiturn Chat 0.8M (zh)](https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M)\n- [UltraChat (en)](https://github.com/thunlp/UltraChat)\n- [OpenPlatypus (en)](https://huggingface.co/datasets/garage-bAInd/Open-Platypus)\n- [CodeAlpaca 20k (en)](https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k)\n- [Alpaca CoT (multilingual)](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT)\n- [OpenOrca (en)](https://huggingface.co/datasets/Open-Orca/OpenOrca)\n- [SlimOrca (en)](https://huggingface.co/datasets/Open-Orca/SlimOrca)\n- [MathInstruct (en)](https://huggingface.co/datasets/TIGER-Lab/MathInstruct)\n- [Firefly 1.1M (zh)](https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M)\n- [Wiki QA (en)](https://huggingface.co/datasets/wiki_qa)\n- [Web QA (zh)](https://huggingface.co/datasets/suolyer/webqa)\n- [WebNovel (zh)](https://huggingface.co/datasets/zxbsmk/webnovel_cn)\n- [Nectar (en)](https://huggingface.co/datasets/berkeley-nest/Nectar)\n- [deepctrl (en&zh)](https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data)\n- [Advertise Generating (zh)](https://huggingface.co/datasets/HasturOfficial/adgen)\n- [ShareGPT Hyperfiltered (en)](https://huggingface.co/datasets/totally-not-an-llm/sharegpt-hyperfiltered-3k)\n- [ShareGPT4 (en&zh)](https://huggingface.co/datasets/shibing624/sharegpt_gpt4)\n- [UltraChat 200k (en)](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k)\n- [AgentInstruct (en)](https://huggingface.co/datasets/THUDM/AgentInstruct)\n- [LMSYS Chat 1M (en)](https://huggingface.co/datasets/lmsys/lmsys-chat-1m)\n- [Evol Instruct V2 (en)](https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k)\n- [Cosmopedia (en)](https://huggingface.co/datasets/HuggingFaceTB/cosmopedia)\n- [STEM (zh)](https://huggingface.co/datasets/hfl/stem_zh_instruction)\n- [Ruozhiba (zh)](https://huggingface.co/datasets/hfl/ruozhiba_gpt4_turbo)\n- [Neo-sft (zh)](https://huggingface.co/datasets/m-a-p/neo_sft_phase2)\n- [Magpie-Pro-300K-Filtered (en)](https://huggingface.co/datasets/Magpie-Align/Magpie-Pro-300K-Filtered)\n- [Magpie-ultra-v0.1 (en)](https://huggingface.co/datasets/argilla/magpie-ultra-v0.1)\n- [WebInstructSub (en)](https://huggingface.co/datasets/TIGER-Lab/WebInstructSub)\n- [OpenO1-SFT (en&zh)](https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT)\n- [Open-Thoughts (en)](https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k)\n- [Open-R1-Math (en)](https://huggingface.co/datasets/open-r1/OpenR1-Math-220k)\n- [Chinese-DeepSeek-R1-Distill (zh)](https://huggingface.co/datasets/Congliu/Chinese-DeepSeek-R1-Distill-data-110k-SFT)\n- [LLaVA mixed (en&zh)](https://huggingface.co/datasets/BUAADreamer/llava-en-zh-300k)\n- [Pokemon-gpt4o-captions (en&zh)](https://huggingface.co/datasets/jugg1024/pokemon-gpt4o-captions)\n- [Open Assistant (de)](https://huggingface.co/datasets/mayflowergmbh/oasst_de)\n- [Dolly 15k (de)](https://huggingface.co/datasets/mayflowergmbh/dolly-15k_de)\n- [Alpaca GPT4 (de)](https://huggingface.co/datasets/mayflowergmbh/alpaca-gpt4_de)\n- [OpenSchnabeltier (de)](https://huggingface.co/datasets/mayflowergmbh/openschnabeltier_de)\n- [Evol Instruct (de)](https://huggingface.co/datasets/mayflowergmbh/evol-instruct_de)\n- [Dolphin (de)](https://huggingface.co/datasets/mayflowergmbh/dolphin_de)\n- [Booksum (de)](https://huggingface.co/datasets/mayflowergmbh/booksum_de)\n- [Airoboros (de)](https://huggingface.co/datasets/mayflowergmbh/airoboros-3.0_de)\n- [Ultrachat (de)](https://huggingface.co/datasets/mayflowergmbh/ultra-chat_de)\n\n</details>\n\n<details><summary>Preference datasets</summary>\n\n- [DPO mixed (en&zh)](https://huggingface.co/datasets/hiyouga/DPO-En-Zh-20k)\n- [UltraFeedback (en)](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized)\n- [COIG-P (zh)](https://huggingface.co/datasets/m-a-p/COIG-P)\n- [RLHF-V (en)](https://huggingface.co/datasets/openbmb/RLHF-V-Dataset)\n- [VLFeedback (en)](https://huggingface.co/datasets/Zhihui/VLFeedback)\n- [RLAIF-V (en)](https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset)\n- [Orca DPO Pairs (en)](https://huggingface.co/datasets/Intel/orca_dpo_pairs)\n- [HH-RLHF (en)](https://huggingface.co/datasets/Anthropic/hh-rlhf)\n- [Nectar (en)](https://huggingface.co/datasets/berkeley-nest/Nectar)\n- [Orca DPO (de)](https://huggingface.co/datasets/mayflowergmbh/intel_orca_dpo_pairs_de)\n- [KTO mixed (en)](https://huggingface.co/datasets/argilla/kto-mix-15k)\n\n</details>\n\nSome datasets require confirmation before using them, so we recommend logging in with your Hugging Face account using these commands.\n\n```bash\npip install --upgrade huggingface_hub\nhuggingface-cli login\n```\n\n## Requirement\n\n| Mandatory    | Minimum | Recommend |\n| ------------ | ------- | --------- |\n| python       | 3.9     | 3.10      |\n| torch        | 2.0.0   | 2.6.0     |\n| torchvision  | 0.15.0  | 0.21.0    |\n| transformers | 4.49.0  | 4.50.0    |\n| datasets     | 2.16.0  | 3.2.0     |\n| accelerate   | 0.34.0  | 1.2.1     |\n| peft         | 0.14.0  | 0.15.1    |\n| trl          | 0.8.6   | 0.9.6     |\n\n| Optional     | Minimum | Recommend |\n| ------------ | ------- | --------- |\n| CUDA         | 11.6    | 12.2      |\n| deepspeed    | 0.10.0  | 0.16.4    |\n| bitsandbytes | 0.39.0  | 0.43.1    |\n| vllm         | 0.4.3   | 0.8.2     |\n| flash-attn   | 2.5.6   | 2.7.2     |\n\n### Hardware Requirement\n\n\\* *estimated*\n\n| Method                          | Bits |   7B  |  14B  |  30B  |   70B  |   `x`B  |\n| ------------------------------- | ---- | ----- | ----- | ----- | ------ | ------- |\n| Full (`bf16` or `fp16`)         |  32  | 120GB | 240GB | 600GB | 1200GB | `18x`GB |\n| Full (`pure_bf16`)              |  16  |  60GB | 120GB | 300GB |  600GB |  `8x`GB |\n| Freeze/LoRA/GaLore/APOLLO/BAdam |  16  |  16GB |  32GB |  64GB |  160GB |  `2x`GB |\n| QLoRA                           |   8  |  10GB |  20GB |  40GB |   80GB |   `x`GB |\n| QLoRA                           |   4  |   6GB |  12GB |  24GB |   48GB | `x/2`GB |\n| QLoRA                           |   2  |   4GB |   8GB |  16GB |   24GB | `x/4`GB |\n\n## Getting Started\n\n### Installation\n\n> [!IMPORTANT]\n> Installation is mandatory.\n\n#### Install from Source\n\n```bash\ngit clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\ncd LLaMA-Factory\npip install -e \".[torch,metrics]\" --no-build-isolation\n```\n\nExtra dependencies available: torch, torch-npu, metrics, deepspeed, liger-kernel, bitsandbytes, hqq, eetq, gptq, aqlm, vllm, sglang, galore, apollo, badam, adam-mini, qwen, minicpm_v, openmind, swanlab, dev\n\n#### Install from Docker Image\n\n```bash\ndocker run -it --rm --gpus=all --ipc=host hiyouga/llamafactory:latest\n```\n\nThis image is built on Ubuntu 22.04 (x86\\_64), CUDA 12.4, Python 3.11, PyTorch 2.6.0, and Flash-attn 2.7.4.\n\nFind the pre-built images: https://hub.docker.com/r/hiyouga/llamafactory/tags\n\nPlease refer to [build docker](#build-docker) to build the image yourself.\n\n<details><summary>Setting up a virtual environment with <b>uv</b></summary>\n\nCreate an isolated Python environment with [uv](https://github.com/astral-sh/uv):\n\n```bash\nuv sync --extra torch --extra metrics --prerelease=allow\n```\n\nRun LLaMA-Factory in the isolated environment:\n\n```bash\nuv run --prerelease=allow llamafactory-cli train examples/train_lora/llama3_lora_pretrain.yaml\n```\n\n</details>\n\n<details><summary>For Windows users</summary>\n\n#### Install PyTorch\n\nYou need to manually install the GPU version of PyTorch on the Windows platform. Please refer to the [official website](https://pytorch.org/get-started/locally/) and the following command to install PyTorch with CUDA support:\n\n```bash\npip uninstall torch torchvision torchaudio\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\npython -c \"import torch; print(torch.cuda.is_available())\"\n```\n\nIf you see `True` then you have successfully installed PyTorch with CUDA support.\n\nTry `dataloader_num_workers: 0` if you encounter `Can't pickle local object` error.\n\n#### Install BitsAndBytes\n\nIf you want to enable the quantized LoRA (QLoRA) on the Windows platform, you need to install a pre-built version of `bitsandbytes` library, which supports CUDA 11.1 to 12.2, please select the appropriate [release version](https://github.com/jllllll/bitsandbytes-windows-webui/releases/tag/wheels) based on your CUDA version.\n\n```bash\npip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl\n```\n\n#### Install Flash Attention-2\n\nTo enable FlashAttention-2 on the Windows platform, please use the script from [flash-attention-windows-wheel](https://huggingface.co/lldacing/flash-attention-windows-wheel) to compile and install it by yourself.\n\n</details>\n\n<details><summary>For Ascend NPU users</summary>\n\nTo install LLaMA Factory on Ascend NPU devices, please upgrade Python to version 3.10 or higher and specify extra dependencies: `pip install -e \".[torch-npu,metrics]\"`. Additionally, you need to install the **[Ascend CANN Toolkit and Kernels](https://www.hiascend.com/developer/download/community/result?module=cann)**. Please follow the [installation tutorial](https://www.hiascend.com/document/detail/en/CANNCommunityEdition/600alphaX/softwareinstall/instg/atlasdeploy_03_0031.html) or use the following commands:\n\n```bash\n# replace the url according to your CANN version and devices\n# install CANN Toolkit\nwget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-toolkit_8.0.0.alpha002_linux-\"$(uname -i)\".run\nbash Ascend-cann-toolkit_8.0.0.alpha002_linux-\"$(uname -i)\".run --install\n\n# install CANN Kernels\nwget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-kernels-910b_8.0.0.alpha002_linux-\"$(uname -i)\".run\nbash Ascend-cann-kernels-910b_8.0.0.alpha002_linux-\"$(uname -i)\".run --install\n\n# set env variables\nsource /usr/local/Ascend/ascend-toolkit/set_env.sh\n```\n\n| Requirement  | Minimum | Recommend      |\n| ------------ | ------- | -------------- |\n| CANN         | 8.0.RC1 | 8.0.0.alpha002 |\n| torch        | 2.1.0   | 2.4.0          |\n| torch-npu    | 2.1.0   | 2.4.0.post2    |\n| deepspeed    | 0.13.2  | 0.13.2         |\n| vllm-ascend  | -       | 0.7.3          |\n\nRemember to use `ASCEND_RT_VISIBLE_DEVICES` instead of `CUDA_VISIBLE_DEVICES` to specify the device to use.\n\nIf you cannot infer model on NPU devices, try setting `do_sample: false` in the configurations.\n\nDownload the pre-built Docker images: [32GB](http://mirrors.cn-central-221.ovaijisuan.com/detail/130.html) | [64GB](http://mirrors.cn-central-221.ovaijisuan.com/detail/131.html)\n\n#### Install BitsAndBytes\n\nTo use QLoRA based on bitsandbytes on Ascend NPU, please follow these 3 steps:\n\n1. Manually compile bitsandbytes: Refer to [the installation documentation](https://huggingface.co/docs/bitsandbytes/installation?backend=Ascend+NPU&platform=Ascend+NPU) for the NPU version of bitsandbytes to complete the compilation and installation. The compilation requires a cmake version of at least 3.22.1 and a g++ version of at least 12.x.\n\n```bash\n# Install bitsandbytes from source\n# Clone bitsandbytes repo, Ascend NPU backend is currently enabled on multi-backend-refactor branch\ngit clone -b multi-backend-refactor https://github.com/bitsandbytes-foundation/bitsandbytes.git\ncd bitsandbytes/\n\n# Install dependencies\npip install -r requirements-dev.txt\n\n# Install the dependencies for the compilation tools. Note that the commands for this step may vary depending on the operating system. The following are provided for reference\napt-get install -y build-essential cmake\n\n# Compile & install  \ncmake -DCOMPUTE_BACKEND=npu -S .\nmake\npip install .\n```\n\n2. Install transformers from the main branch.\n\n```bash\ngit clone -b main https://github.com/huggingface/transformers.git\ncd transformers\npip install .\n```\n\n3. Set `double_quantization: false` in the configuration. You can refer to the [example](examples/train_qlora/llama3_lora_sft_bnb_npu.yaml).\n\n</details>\n\n### Data Preparation\n\nPlease refer to [data/README.md](data/README.md) for checking the details about the format of dataset files. You can use datasets on HuggingFace / ModelScope / Modelers hub, load the dataset in local disk, or specify a path to s3/gcs cloud storage.\n\n> [!NOTE]\n> Please update `data/dataset_info.json` to use your custom dataset.\n\nYou can also use **[Easy Dataset](https://github.com/ConardLi/easy-dataset)**, **[DataFlow](https://github.com/OpenDCAI/DataFlow)** and **[GraphGen](https://github.com/open-sciencelab/GraphGen)** to create synthetic data for fine-tuning.\n\n### Quickstart\n\nUse the following 3 commands to run LoRA **fine-tuning**, **inference** and **merging** of the Llama3-8B-Instruct model, respectively.\n\n```bash\nllamafactory-cli train examples/train_lora/llama3_lora_sft.yaml\nllamafactory-cli chat examples/inference/llama3_lora_sft.yaml\nllamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml\n```\n\nSee [examples/README.md](examples/README.md) for advanced usage (including distributed training).\n\n> [!TIP]\n> Use `llamafactory-cli help` to show help information.\n>\n> Read [FAQs](https://github.com/hiyouga/LLaMA-Factory/issues/4614) first if you encounter any problems.\n\n### Fine-Tuning with LLaMA Board GUI (powered by [Gradio](https://github.com/gradio-app/gradio))\n\n```bash\nllamafactory-cli webui\n```\n\n### Build Docker\n\nFor CUDA users:\n\n```bash\ncd docker/docker-cuda/\ndocker compose up -d\ndocker compose exec llamafactory bash\n```\n\nFor Ascend NPU users:\n\n```bash\ncd docker/docker-npu/\ndocker compose up -d\ndocker compose exec llamafactory bash\n```\n\nFor AMD ROCm users:\n\n```bash\ncd docker/docker-rocm/\ndocker compose up -d\ndocker compose exec llamafactory bash\n```\n\n<details><summary>Build without Docker Compose</summary>\n\nFor CUDA users:\n\n```bash\ndocker build -f ./docker/docker-cuda/Dockerfile \\\n    --build-arg PIP_INDEX=https://pypi.org/simple \\\n    --build-arg EXTRAS=metrics \\\n    -t llamafactory:latest .\n\ndocker run -dit --ipc=host --gpus=all \\\n    -p 7860:7860 \\\n    -p 8000:8000 \\\n    --name llamafactory \\\n    llamafactory:latest\n\ndocker exec -it llamafactory bash\n```\n\nFor Ascend NPU users:\n\n```bash\ndocker build -f ./docker/docker-npu/Dockerfile \\\n    --build-arg PIP_INDEX=https://pypi.org/simple \\\n    --build-arg EXTRAS=torch-npu,metrics \\\n    -t llamafactory:latest .\n\ndocker run -dit --ipc=host \\\n    -v /usr/local/dcmi:/usr/local/dcmi \\\n    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \\\n    -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \\\n    -v /etc/ascend_install.info:/etc/ascend_install.info \\\n    -p 7860:7860 \\\n    -p 8000:8000 \\\n    --device /dev/davinci0 \\\n    --device /dev/davinci_manager \\\n    --device /dev/devmm_svm \\\n    --device /dev/hisi_hdc \\\n    --name llamafactory \\\n    llamafactory:latest\n\ndocker exec -it llamafactory bash\n```\n\nFor AMD ROCm users:\n\n```bash\ndocker build -f ./docker/docker-rocm/Dockerfile \\\n    --build-arg PIP_INDEX=https://pypi.org/simple \\\n    --build-arg EXTRAS=metrics \\\n    -t llamafactory:latest .\n\ndocker run -dit --ipc=host \\\n    -p 7860:7860 \\\n    -p 8000:8000 \\\n    --device /dev/kfd \\\n    --device /dev/dri \\\n    --name llamafactory \\\n    llamafactory:latest\n\ndocker exec -it llamafactory bash\n```\n\n</details>\n\n<details><summary>Use Docker volumes</summary>\n\nYou can uncomment `VOLUME [ \"/root/.cache/huggingface\", \"/app/shared_data\", \"/app/output\" ]` in the Dockerfile to use data volumes.\n\nWhen building the Docker image, use `-v ./hf_cache:/root/.cache/huggingface` argument to mount the local directory to the container. The following data volumes are available.\n\n- `hf_cache`: Utilize Hugging Face cache on the host machine.\n- `shared_data`: The directionary to store datasets on the host machine.\n- `output`: Set export dir to this location so that the merged result can be accessed directly on the host machine.\n\n</details>\n\n### Deploy with OpenAI-style API and vLLM\n\n```bash\nAPI_PORT=8000 llamafactory-cli api examples/inference/llama3.yaml infer_backend=vllm vllm_enforce_eager=true\n```\n\n> [!TIP]\n> Visit [this page](https://platform.openai.com/docs/api-reference/chat/create) for API document.\n>\n> Examples: [Image understanding](scripts/api_example/test_image.py) | [Function calling](scripts/api_example/test_toolcall.py)\n\n### Download from ModelScope Hub\n\nIf you have trouble with downloading models and datasets from Hugging Face, you can use ModelScope.\n\n```bash\nexport USE_MODELSCOPE_HUB=1 # `set USE_MODELSCOPE_HUB=1` for Windows\n```\n\nTrain the model by specifying a model ID of the ModelScope Hub as the `model_name_or_path`. You can find a full list of model IDs at [ModelScope Hub](https://modelscope.cn/models), e.g., `LLM-Research/Meta-Llama-3-8B-Instruct`.\n\n### Download from Modelers Hub\n\nYou can also use Modelers Hub to download models and datasets.\n\n```bash\nexport USE_OPENMIND_HUB=1 # `set USE_OPENMIND_HUB=1` for Windows\n```\n\nTrain the model by specifying a model ID of the Modelers Hub as the `model_name_or_path`. You can find a full list of model IDs at [Modelers Hub](https://modelers.cn/models), e.g., `TeleAI/TeleChat-7B-pt`.\n\n### Use W&B Logger\n\nTo use [Weights & Biases](https://wandb.ai) for logging experimental results, you need to add the following arguments to yaml files.\n\n```yaml\nreport_to: wandb\nrun_name: test_run # optional\n```\n\nSet `WANDB_API_KEY` to [your key](https://wandb.ai/authorize) when launching training tasks to log in with your W&B account.\n\n### Use SwanLab Logger\n\nTo use [SwanLab](https://github.com/SwanHubX/SwanLab) for logging experimental results, you need to add the following arguments to yaml files.\n\n```yaml\nuse_swanlab: true\nswanlab_run_name: test_run # optional\n```\n\nWhen launching training tasks, you can log in to SwanLab in three ways:\n\n1. Add `swanlab_api_key=<your_api_key>` to the yaml file, and set it to your [API key](https://swanlab.cn/settings).\n2. Set the environment variable `SWANLAB_API_KEY` to your [API key](https://swanlab.cn/settings).\n3. Use the `swanlab login` command to complete the login.\n\n## Projects using LLaMA Factory\n\nIf you have a project that should be incorporated, please contact via email or create a pull request.\n\n<details><summary>Click to show</summary>\n\n1. Wang et al. ESRL: Efficient Sampling-based Reinforcement Learning for Sequence Generation. 2023. [[arxiv]](https://arxiv.org/abs/2308.02223)\n1. Yu et al. Open, Closed, or Small Language Models for Text Classification? 2023. [[arxiv]](https://arxiv.org/abs/2308.10092)\n1. Wang et al. UbiPhysio: Support Daily Functioning, Fitness, and Rehabilitation with Action Understanding and Feedback in Natural Language. 2023. [[arxiv]](https://arxiv.org/abs/2308.10526)\n1. Luceri et al. Leveraging Large Language Models to Detect Influence Campaigns in Social Media. 2023. [[arxiv]](https://arxiv.org/abs/2311.07816)\n1. Zhang et al. Alleviating Hallucinations of Large Language Models through Induced Hallucinations. 2023. [[arxiv]](https://arxiv.org/abs/2312.15710)\n1. Wang et al. Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs. KDD 2024. [[arxiv]](https://arxiv.org/abs/2401.04319)\n1. Wang et al. CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning. ACL 2024. [[arxiv]](https://arxiv.org/abs/2401.07286)\n1. Choi et al. FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs. 2024. [[arxiv]](https://arxiv.org/abs/2402.05904)\n1. Zhang et al. AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts. 2024. [[arxiv]](https://arxiv.org/abs/2402.07625)\n1. Lyu et al. KnowTuning: Knowledge-aware Fine-tuning for Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.11176)\n1. Yang et al. LaCo: Large Language Model Pruning via Layer Collaps. 2024. [[arxiv]](https://arxiv.org/abs/2402.11187)\n1. Bhardwaj et al. Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic. 2024. [[arxiv]](https://arxiv.org/abs/2402.11746)\n1. Yang et al. Enhancing Empathetic Response Generation by Augmenting LLMs with Small-scale Empathetic Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.11801)\n1. Yi et al. Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding. ACL 2024 Findings. [[arxiv]](https://arxiv.org/abs/2402.11809)\n1. Cao et al. Head-wise Shareable Attention for Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.11819)\n1. Zhang et al. Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages. 2024. [[arxiv]](https://arxiv.org/abs/2402.12204)\n1. Kim et al. Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.14714)\n1. Yu et al. KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models. ACL 2024. [[arxiv]](https://arxiv.org/abs/2402.15043)\n1. Huang et al. Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning. 2024. [[arxiv]](https://arxiv.org/abs/2403.02333)\n1. Duan et al. Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization. 2024. [[arxiv]](https://arxiv.org/abs/2403.03419)\n1. Xie and Schwertfeger. Empowering Robotics with Large Language Models: osmAG Map Comprehension with LLMs. 2024. [[arxiv]](https://arxiv.org/abs/2403.08228)\n1. Wu et al. Large Language Models are Parallel Multilingual Learners. 2024. [[arxiv]](https://arxiv.org/abs/2403.09073)\n1. Zhang et al. EDT: Improving Large Language Models' Generation by Entropy-based Dynamic Temperature Sampling. 2024. [[arxiv]](https://arxiv.org/abs/2403.14541)\n1. Weller et al. FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions. 2024. [[arxiv]](https://arxiv.org/abs/2403.15246)\n1. Hongbin Na. CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering. COLING 2024. [[arxiv]](https://arxiv.org/abs/2403.16008)\n1. Zan et al. CodeS: Natural Language to Code Repository via Multi-Layer Sketch. 2024. [[arxiv]](https://arxiv.org/abs/2403.16443)\n1. Liu et al. Extensive Self-Contrast Enables Feedback-Free Language Model Alignment. 2024. [[arxiv]](https://arxiv.org/abs/2404.00604)\n1. Luo et al. BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.02827)\n1. Du et al. Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model. 2024. [[arxiv]](https://arxiv.org/abs/2404.04167)\n1. Ma et al. Parameter Efficient Quasi-Orthogonal Fine-Tuning via Givens Rotation. ICML 2024. [[arxiv]](https://arxiv.org/abs/2404.04316)\n1. Liu et al. Dynamic Generation of Personalities with Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.07084)\n1. Shang et al. How Far Have We Gone in Stripped Binary Code Understanding Using Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.09836)\n1. Huang et al. LLMTune: Accelerate Database Knob Tuning with Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.11581)\n1. Deng et al. Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction. 2024. [[arxiv]](https://arxiv.org/abs/2404.14215)\n1. Acikgoz et al. Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare. 2024. [[arxiv]](https://arxiv.org/abs/2404.16621)\n1. Zhang et al. Small Language Models Need Strong Verifiers to Self-Correct Reasoning. ACL 2024 Findings. [[arxiv]](https://arxiv.org/abs/2404.17140)\n1. Zhou et al. FREB-TQA: A Fine-Grained Robustness Evaluation Benchmark for Table Question Answering. NAACL 2024. [[arxiv]](https://arxiv.org/abs/2404.18585)\n1. Xu et al. Large Language Models for Cyber Security: A Systematic Literature Review. 2024. [[arxiv]](https://arxiv.org/abs/2405.04760)\n1. Dammu et al. \"They are uncultured\": Unveiling Covert Harms and Social Threats in LLM Generated Conversations. 2024. [[arxiv]](https://arxiv.org/abs/2405.05378)\n1. Yi et al. A safety realignment framework via subspace-oriented model fusion for large language models. 2024. [[arxiv]](https://arxiv.org/abs/2405.09055)\n1. Lou et al. SPO: Multi-Dimensional Preference Sequential Alignment With Implicit Reward Modeling. 2024. [[arxiv]](https://arxiv.org/abs/2405.12739)\n1. Zhang et al. Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners. 2024. [[arxiv]](https://arxiv.org/abs/2405.13816)\n1. Zhang et al. TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2405.20215)\n1. Zihong Chen. Sentence Segmentation and Sentence Punctuation Based on XunziALLM. 2024. [[paper]](https://aclanthology.org/2024.lt4hala-1.30)\n1. Gao et al. The Best of Both Worlds: Toward an Honest and Helpful Large Language Model. 2024. [[arxiv]](https://arxiv.org/abs/2406.00380)\n1. Wang and Song. MARS: Benchmarking the Metaphysical Reasoning Abilities of Language Models with a Multi-task Evaluation Dataset. 2024. [[arxiv]](https://arxiv.org/abs/2406.02106)\n1. Hu et al. Computational Limits of Low-Rank Adaptation (LoRA) for Transformer-Based Models. 2024. [[arxiv]](https://arxiv.org/abs/2406.03136)\n1. Ge et al. Time Sensitive Knowledge Editing through Efficient Finetuning. ACL 2024. [[arxiv]](https://arxiv.org/abs/2406.04496)\n1. Tan et al. Peer Review as A Multi-Turn and Long-Context Dialogue with Role-Based Interactions. 2024. [[arxiv]](https://arxiv.org/abs/2406.05688)\n1. Song et al. Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters. 2024. [[arxiv]](https://arxiv.org/abs/2406.05955)\n1. Gu et al. RWKV-CLIP: A Robust Vision-Language Representation Learner. 2024. [[arxiv]](https://arxiv.org/abs/2406.06973)\n1. Chen et al. Advancing Tool-Augmented Large Language Models: Integrating Insights from Errors in Inference Trees. 2024. [[arxiv]](https://arxiv.org/abs/2406.07115)\n1. Zhu et al. Are Large Language Models Good Statisticians?. 2024. [[arxiv]](https://arxiv.org/abs/2406.07815)\n1. Li et al. Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction Tuning. 2024. [[arxiv]](https://arxiv.org/abs/2406.10099)\n1. Ding et al. IntentionQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Language Models in E-commerce. 2024. [[arxiv]](https://arxiv.org/abs/2406.10173)\n1. He et al. COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities. 2024. [[arxiv]](https://arxiv.org/abs/2406.12074)\n1. Lin et al. FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving. 2024. [[arxiv]](https://arxiv.org/abs/2406.14408)\n1. Treutlein et al. Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data. 2024. [[arxiv]](https://arxiv.org/abs/2406.14546)\n1. Feng et al. SS-Bench: A Benchmark for Social Story Generation and Evaluation. 2024. [[arxiv]](https://arxiv.org/abs/2406.15695)\n1. Feng et al. Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement. 2024. [[arxiv]](https://arxiv.org/abs/2406.17233)\n1. Liu et al. Large Language Models for Cuffless Blood Pressure Measurement From Wearable Biosignals. 2024. [[arxiv]](https://arxiv.org/abs/2406.18069)\n1. Iyer et al. Exploring Very Low-Resource Translation with LLMs: The University of Edinburgh's Submission to AmericasNLP 2024 Translation Task. AmericasNLP 2024. [[paper]](https://aclanthology.org/2024.americasnlp-1.25)\n1. Li et al. Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring. 2024. [[arxiv]](https://arxiv.org/abs/2406.19949)\n1. Yang et al. Financial Knowledge Large Language Model. 2024. [[arxiv]](https://arxiv.org/abs/2407.00365)\n1. Lin et al. DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging. 2024. [[arxiv]](https://arxiv.org/abs/2407.01470)\n1. Bako et al. Evaluating the Semantic Profiling Abilities of LLMs for Natural Language Utterances in Data Visualization. 2024. [[arxiv]](https://arxiv.org/abs/2407.06129)\n1. Huang et al. RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization. 2024. [[arxiv]](https://arxiv.org/abs/2407.08044)\n1. Jiang et al. LLM-Collaboration on Automatic Science Journalism for the General Audience. 2024. [[arxiv]](https://arxiv.org/abs/2407.09756)\n1. Inouye et al. Applied Auto-tuning on LoRA Hyperparameters. 2024. [[paper]](https://scholarcommons.scu.edu/cseng_senior/272/)\n1. Qi et al. Research on Tibetan Tourism Viewpoints information generation system based on LLM. 2024. [[arxiv]](https://arxiv.org/abs/2407.13561)\n1. Xu et al. Course-Correction: Safety Alignment Using Synthetic Preferences. 2024. [[arxiv]](https://arxiv.org/abs/2407.16637)\n1. Sun et al. LAMBDA: A Large Model Based Data Agent. 2024. [[arxiv]](https://arxiv.org/abs/2407.17535)\n1. Zhu et al. CollectiveSFT: Scaling Large Language Models for Chinese Medical Benchmark with Collective Instructions in Healthcare. 2024. [[arxiv]](https://arxiv.org/abs/2407.19705)\n1. Yu et al. Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment. 2024. [[arxiv]](https://arxiv.org/abs/2408.00137)\n1. Xie et al. The Power of Personalized Datasets: Advancing Chinese Composition Writing for Elementary School through Targeted Model Fine-Tuning. IALP 2024. [[paper]](https://www.asianlp.sg/conferences/ialp2024/proceedings/papers/IALP2024_P055.pdf)\n1. Liu et al. Instruct-Code-Llama: Improving Capabilities of Language Model in Competition Level Code Generation by Online Judge Feedback. ICIC 2024. [[paper]](https://link.springer.com/chapter/10.1007/978-981-97-5669-8_11)\n1. Wang et al. Cybernetic Sentinels: Unveiling the Impact of Safety Data Selection on Model Security in Supervised Fine-Tuning. ICIC 2024. [[paper]](https://link.springer.com/chapter/10.1007/978-981-97-5669-8_23)\n1. Xia et al. Understanding the Performance and Estimating the Cost of LLM Fine-Tuning. 2024. [[arxiv]](https://arxiv.org/abs/2408.04693)\n1. Zeng et al. Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City Navigation without Instructions. 2024. [[arxiv]](https://arxiv.org/abs/2408.04168)\n1. Xia et al. Using Pre-trained Language Model for Accurate ESG Prediction. FinNLP 2024. [[paper]](https://aclanthology.org/2024.finnlp-2.1/)\n1. Liang et al. I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative Self-Enhancement Paradigm. 2024. [[arxiv]](https://arxiv.org/abs/2408.08072)\n1. Bai et al. Aligning Large Language Model with Direct Multi-Preference Optimization for Recommendation. CIKM 2024. [[paper]](https://dl.acm.org/doi/10.1145/3627673.3679611)\n1. Zhang et al. CPsyCoun: A Report-based Multi-turn Dialogue Reconstruction and Evaluation Framework for Chinese Psychological Counseling. ACL 2024. [[paper]](https://aclanthology.org/2024.findings-acl.830.pdf)\n1. **[StarWhisper](https://github.com/Yu-Yang-Li/StarWhisper)**: A large language model for Astronomy, based on ChatGLM2-6B and Qwen-14B.\n1. **[DISC-LawLLM](https://github.com/FudanDISC/DISC-LawLLM)**: A large language model specialized in Chinese legal domain, based on Baichuan-13B, is capable of retrieving and reasoning on legal knowledge.\n1. **[Sunsimiao](https://github.com/X-D-Lab/Sunsimiao)**: A large language model specialized in Chinese medical domain, based on Baichuan-7B and ChatGLM-6B.\n1. **[CareGPT](https://github.com/WangRongsheng/CareGPT)**: A series of large language models for Chinese medical domain, based on LLaMA2-7B and Baichuan-13B.\n1. **[MachineMindset](https://github.com/PKU-YuanGroup/Machine-Mindset/)**: A series of MBTI Personality large language models, capable of giving any LLM 16 different personality types based on different datasets and training methods.\n1. **[Luminia-13B-v3](https://huggingface.co/Nekochu/Luminia-13B-v3)**: A large language model specialized in generate metadata for stable diffusion. [[demo]](https://huggingface.co/spaces/Nekochu/Luminia-13B_SD_Prompt)\n1. **[Chinese-LLaVA-Med](https://github.com/BUAADreamer/Chinese-LLaVA-Med)**: A multimodal large language model specialized in Chinese medical domain, based on LLaVA-1.5-7B.\n1. **[AutoRE](https://github.com/THUDM/AutoRE)**: A document-level relation extraction system based on large language models.\n1. **[NVIDIA RTX AI Toolkit](https://github.com/NVIDIA/RTX-AI-Toolkit)**: SDKs for fine-tuning LLMs on Windows PC for NVIDIA RTX.\n1. **[LazyLLM](https://github.com/LazyAGI/LazyLLM)**: An easy and lazy way for building multi-agent LLMs applications and supports model fine-tuning via LLaMA Factory.\n1. **[RAG-Retrieval](https://github.com/NLPJCL/RAG-Retrieval)**: A full pipeline for RAG retrieval model fine-tuning, inference, and distillation. [[blog]](https://zhuanlan.zhihu.com/p/987727357)\n1. **[360-LLaMA-Factory](https://github.com/Qihoo360/360-LLaMA-Factory)**: A modified library that supports long sequence SFT & DPO using ring attention.\n1. **[Sky-T1](https://novasky-ai.github.io/posts/sky-t1/)**: An o1-like model fine-tuned by NovaSky AI with very small cost.\n1. **[WeClone](https://github.com/xming521/WeClone)**: One-stop solution for creating your digital avatar from chat logs.\n1. **[EmoLLM](https://github.com/SmartFlowAI/EmoLLM)**: A project about large language models (LLMs) and mental health.\n</details>\n\n## License\n\nThis repository is licensed under the [Apache-2.0 License](LICENSE).\n\nPlease follow the model licenses to use the corresponding model weights: [Baichuan 2](https://huggingface.co/baichuan-inc/Baichuan2-7B-Base/blob/main/Community%20License%20for%20Baichuan%202%20Model.pdf) / [BLOOM](https://huggingface.co/spaces/bigscience/license) / [ChatGLM3](https://github.com/THUDM/ChatGLM3/blob/main/MODEL_LICENSE) / [Command R](https://cohere.com/c4ai-cc-by-nc-license) / [DeepSeek](https://github.com/deepseek-ai/DeepSeek-LLM/blob/main/LICENSE-MODEL) / [Falcon](https://huggingface.co/tiiuae/falcon-180B/blob/main/LICENSE.txt) / [Gemma](https://ai.google.dev/gemma/terms) / [GLM-4](https://huggingface.co/THUDM/glm-4-9b/blob/main/LICENSE) / [GPT-2](https://github.com/openai/gpt-2/blob/master/LICENSE) / [Granite](LICENSE) / [Index](https://huggingface.co/IndexTeam/Index-1.9B/blob/main/LICENSE) / [InternLM](https://github.com/InternLM/InternLM#license) / [Llama](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md) / [Llama 2](https://ai.meta.com/llama/license/) / [Llama 3](https://llama.meta.com/llama3/license/) / [Llama 4](https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE) / [MiniCPM](https://github.com/OpenBMB/MiniCPM/blob/main/MiniCPM%20Model%20License.md) / [Mistral/Mixtral/Pixtral](LICENSE) / [OLMo](LICENSE) / [Phi-1.5/Phi-2](https://huggingface.co/microsoft/phi-1_5/resolve/main/Research%20License.docx) / [Phi-3/Phi-4](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/LICENSE) / [Qwen](https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT) / [Skywork](https://huggingface.co/Skywork/Skywork-13B-base/blob/main/Skywork%20Community%20License.pdf) / [StarCoder 2](https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement) / [TeleChat2](https://huggingface.co/Tele-AI/telechat-7B/blob/main/TeleChat%E6%A8%A1%E5%9E%8B%E7%A4%BE%E5%8C%BA%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf) / [XVERSE](https://github.com/xverse-ai/XVERSE-13B/blob/main/MODEL_LICENSE.pdf) / [Yi](https://huggingface.co/01-ai/Yi-6B/blob/main/LICENSE) / [Yi-1.5](LICENSE) / [Yuan 2](https://github.com/IEIT-Yuan/Yuan-2.0/blob/main/LICENSE-Yuan)\n\n## Citation\n\nIf this work is helpful, please kindly cite as:\n\n```bibtex\n@inproceedings{zheng2024llamafactory,\n  title={LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models},\n  author={Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Zhangchi Feng and Yongqiang Ma},\n  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)},\n  address={Bangkok, Thailand},\n  publisher={Association for Computational Linguistics},\n  year={2024},\n  url={http://arxiv.org/abs/2403.13372}\n}\n```\n\n## Acknowledgement\n\nThis repo benefits from [PEFT](https://github.com/huggingface/peft), [TRL](https://github.com/huggingface/trl), [QLoRA](https://github.com/artidoro/qlora) and [FastChat](https://github.com/lm-sys/FastChat). Thanks for their wonderful works.\n\n## Star History\n\n![Star History Chart](https://api.star-history.com/svg?repos=hiyouga/LLaMA-Factory&type=Date)\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 599547518,
    "name": "vllm",
    "full_name": "vllm-project/vllm",
    "description": "A high-throughput and memory-efficient inference and serving engine for LLMs",
    "html_url": "https://github.com/vllm-project/vllm",
    "clone_url": "https://github.com/vllm-project/vllm.git",
    "owner_login": "vllm-project",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/136984999?v=4",
    "stargazers_count": 54065,
    "watchers_count": 54065,
    "forks_count": 9139,
    "open_issues_count": 2740,
    "size": 70764,
    "language": "Python",
    "languages": {
      "Python": 17601811,
      "Cuda": 1819291,
      "C++": 989217,
      "Shell": 152368,
      "C": 93110,
      "CMake": 70119,
      "Dockerfile": 23839,
      "Jinja": 1650
    },
    "topics": [
      "amd",
      "cuda",
      "deepseek",
      "gpt",
      "hpu",
      "inference",
      "inferentia",
      "llama",
      "llm",
      "llm-serving",
      "llmops",
      "mlops",
      "model-serving",
      "pytorch",
      "qwen",
      "rocm",
      "tpu",
      "trainium",
      "transformer",
      "xpu"
    ],
    "license_name": "Apache License 2.0",
    "created_at": "2023-02-09T11:23:20+00:00",
    "updated_at": "2025-08-06T02:10:44+00:00",
    "pushed_at": "2025-08-06T00:05:40+00:00",
    "contributors_count": 100,
    "readme_length": 11089,
    "readme_content": "<!-- markdownlint-disable MD001 MD041 -->\n<p align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/vllm-project/vllm/main/docs/assets/logos/vllm-logo-text-dark.png\">\n    <img alt=\"vLLM\" src=\"https://raw.githubusercontent.com/vllm-project/vllm/main/docs/assets/logos/vllm-logo-text-light.png\" width=55%>\n  </picture>\n</p>\n\n<h3 align=\"center\">\nEasy, fast, and cheap LLM serving for everyone\n</h3>\n\n<p align=\"center\">\n| <a href=\"https://docs.vllm.ai\"><b>Documentation</b></a> | <a href=\"https://blog.vllm.ai/\"><b>Blog</b></a> | <a href=\"https://arxiv.org/abs/2309.06180\"><b>Paper</b></a> | <a href=\"https://x.com/vllm_project\"><b>Twitter/X</b></a> | <a href=\"https://discuss.vllm.ai\"><b>User Forum</b></a> | <a href=\"https://slack.vllm.ai\"><b>Developer Slack</b></a> |\n</p>\n\n---\n\n*Latest News* \ud83d\udd25\n\n- [2025/05] We hosted [NYC vLLM Meetup](https://lu.ma/c1rqyf1f)! Please find the meetup slides [here](https://docs.google.com/presentation/d/1_q_aW_ioMJWUImf1s1YM-ZhjXz8cUeL0IJvaquOYBeA/edit?usp=sharing).\n- [2025/05] vLLM is now a hosted project under PyTorch Foundation! Please find the announcement [here](https://pytorch.org/blog/pytorch-foundation-welcomes-vllm/).\n- [2025/04] We hosted [Asia Developer Day](https://www.sginnovate.com/event/limited-availability-morning-evening-slots-remaining-inaugural-vllm-asia-developer-day)! Please find the meetup slides from the vLLM team [here](https://docs.google.com/presentation/d/19cp6Qu8u48ihB91A064XfaXruNYiBOUKrBxAmDOllOo/edit?usp=sharing).\n- [2025/01] We are excited to announce the alpha release of vLLM V1: A major architectural upgrade with 1.7x speedup! Clean code, optimized execution loop, zero-overhead prefix caching, enhanced multimodal support, and more. Please check out our blog post [here](https://blog.vllm.ai/2025/01/27/v1-alpha-release.html).\n\n<details>\n<summary>Previous News</summary>\n\n- [2025/03] We hosted [vLLM x Ollama Inference Night](https://lu.ma/vllm-ollama)! Please find the meetup slides from the vLLM team [here](https://docs.google.com/presentation/d/16T2PDD1YwRnZ4Tu8Q5r6n53c5Lr5c73UV9Vd2_eBo4U/edit?usp=sharing).\n- [2025/03] We hosted [the first vLLM China Meetup](https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg)! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1REHvfQMKGnvz6p3Fd23HhSO4c8j5WPGZV0bKYLwnHyQ/edit?usp=sharing).\n- [2025/03] We hosted [the East Coast vLLM Meetup](https://lu.ma/7mu4k4xx)! Please find the meetup slides [here](https://docs.google.com/presentation/d/1NHiv8EUFF1NLd3fEYODm56nDmL26lEeXCaDgyDlTsRs/edit#slide=id.g31441846c39_0_0).\n- [2025/02] We hosted [the ninth vLLM meetup](https://lu.ma/h7g3kuj9) with Meta! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1jzC_PZVXrVNSFVCW-V4cFXb6pn7zZ2CyP_Flwo05aqg/edit?usp=sharing) and AMD [here](https://drive.google.com/file/d/1Zk5qEJIkTmlQ2eQcXQZlljAx3m9s7nwn/view?usp=sharing). The slides from Meta will not be posted.\n- [2025/01] We hosted [the eighth vLLM meetup](https://lu.ma/zep56hui) with Google Cloud! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1epVkt4Zu8Jz_S5OhEHPc798emsYh2BwYfRuDDVEF7u4/edit?usp=sharing), and Google Cloud team [here](https://drive.google.com/file/d/1h24pHewANyRL11xy5dXUbvRC9F9Kkjix/view?usp=sharing).\n- [2024/12] vLLM joins [pytorch ecosystem](https://pytorch.org/blog/vllm-joins-pytorch)! Easy, Fast, and Cheap LLM Serving for Everyone!\n- [2024/11] We hosted [the seventh vLLM meetup](https://lu.ma/h0qvrajz) with Snowflake! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1e3CxQBV3JsfGp30SwyvS3eM_tW-ghOhJ9PAJGK6KR54/edit?usp=sharing), and Snowflake team [here](https://docs.google.com/presentation/d/1qF3RkDAbOULwz9WK5TOltt2fE9t6uIc_hVNLFAaQX6A/edit?usp=sharing).\n- [2024/10] We have just created a developer slack ([slack.vllm.ai](https://slack.vllm.ai)) focusing on coordinating contributions and discussing features. Please feel free to join us there!\n- [2024/10] Ray Summit 2024 held a special track for vLLM! Please find the opening talk slides from the vLLM team [here](https://docs.google.com/presentation/d/1B_KQxpHBTRa_mDF-tR6i8rWdOU5QoTZNcEg2MKZxEHM/edit?usp=sharing). Learn more from the [talks](https://www.youtube.com/playlist?list=PLzTswPQNepXl6AQwifuwUImLPFRVpksjR) from other vLLM contributors and users!\n- [2024/09] We hosted [the sixth vLLM meetup](https://lu.ma/87q3nvnh) with NVIDIA! Please find the meetup slides [here](https://docs.google.com/presentation/d/1wrLGwytQfaOTd5wCGSPNhoaW3nq0E-9wqyP7ny93xRs/edit?usp=sharing).\n- [2024/07] We hosted [the fifth vLLM meetup](https://lu.ma/lp0gyjqr) with AWS! Please find the meetup slides [here](https://docs.google.com/presentation/d/1RgUD8aCfcHocghoP3zmXzck9vX3RCI9yfUAB2Bbcl4Y/edit?usp=sharing).\n- [2024/07] In partnership with Meta, vLLM officially supports Llama 3.1 with FP8 quantization and pipeline parallelism! Please check out our blog post [here](https://blog.vllm.ai/2024/07/23/llama31.html).\n- [2024/06] We hosted [the fourth vLLM meetup](https://lu.ma/agivllm) with Cloudflare and BentoML! Please find the meetup slides [here](https://docs.google.com/presentation/d/1iJ8o7V2bQEi0BFEljLTwc5G1S10_Rhv3beed5oB0NJ4/edit?usp=sharing).\n- [2024/04] We hosted [the third vLLM meetup](https://robloxandvllmmeetup2024.splashthat.com/) with Roblox! Please find the meetup slides [here](https://docs.google.com/presentation/d/1A--47JAK4BJ39t954HyTkvtfwn0fkqtsL8NGFuslReM/edit?usp=sharing).\n- [2024/01] We hosted [the second vLLM meetup](https://lu.ma/ygxbpzhl) with IBM! Please find the meetup slides [here](https://docs.google.com/presentation/d/12mI2sKABnUw5RBWXDYY-HtHth4iMSNcEoQ10jDQbxgA/edit?usp=sharing).\n- [2023/10] We hosted [the first vLLM meetup](https://lu.ma/first-vllm-meetup) with a16z! Please find the meetup slides [here](https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit?usp=sharing).\n- [2023/08] We would like to express our sincere gratitude to [Andreessen Horowitz](https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/) (a16z) for providing a generous grant to support the open-source development and research of vLLM.\n- [2023/06] We officially released vLLM! FastChat-vLLM integration has powered [LMSYS Vicuna and Chatbot Arena](https://chat.lmsys.org) since mid-April. Check out our [blog post](https://vllm.ai).\n\n</details>\n\n---\n\n## About\n\nvLLM is a fast and easy-to-use library for LLM inference and serving.\n\nOriginally developed in the [Sky Computing Lab](https://sky.cs.berkeley.edu) at UC Berkeley, vLLM has evolved into a community-driven project with contributions from both academia and industry.\n\nvLLM is fast with:\n\n- State-of-the-art serving throughput\n- Efficient management of attention key and value memory with [**PagedAttention**](https://blog.vllm.ai/2023/06/20/vllm.html)\n- Continuous batching of incoming requests\n- Fast model execution with CUDA/HIP graph\n- Quantizations: [GPTQ](https://arxiv.org/abs/2210.17323), [AWQ](https://arxiv.org/abs/2306.00978), [AutoRound](https://arxiv.org/abs/2309.05516), INT4, INT8, and FP8\n- Optimized CUDA kernels, including integration with FlashAttention and FlashInfer\n- Speculative decoding\n- Chunked prefill\n\nvLLM is flexible and easy to use with:\n\n- Seamless integration with popular Hugging Face models\n- High-throughput serving with various decoding algorithms, including *parallel sampling*, *beam search*, and more\n- Tensor, pipeline, data and expert parallelism support for distributed inference\n- Streaming outputs\n- OpenAI-compatible API server\n- Support NVIDIA GPUs, AMD CPUs and GPUs, Intel CPUs and GPUs, PowerPC CPUs, TPU, and AWS Neuron\n- Prefix caching support\n- Multi-LoRA support\n\nvLLM seamlessly supports most popular open-source models on HuggingFace, including:\n\n- Transformer-like LLMs (e.g., Llama)\n- Mixture-of-Expert LLMs (e.g., Mixtral, Deepseek-V2 and V3)\n- Embedding Models (e.g., E5-Mistral)\n- Multi-modal LLMs (e.g., LLaVA)\n\nFind the full list of supported models [here](https://docs.vllm.ai/en/latest/models/supported_models.html).\n\n## Getting Started\n\nInstall vLLM with `pip` or [from source](https://docs.vllm.ai/en/latest/getting_started/installation/gpu/index.html#build-wheel-from-source):\n\n```bash\npip install vllm\n```\n\nVisit our [documentation](https://docs.vllm.ai/en/latest/) to learn more.\n\n- [Installation](https://docs.vllm.ai/en/latest/getting_started/installation.html)\n- [Quickstart](https://docs.vllm.ai/en/latest/getting_started/quickstart.html)\n- [List of Supported Models](https://docs.vllm.ai/en/latest/models/supported_models.html)\n\n## Contributing\n\nWe welcome and value any contributions and collaborations.\nPlease check out [Contributing to vLLM](https://docs.vllm.ai/en/latest/contributing/index.html) for how to get involved.\n\n## Sponsors\n\nvLLM is a community project. Our compute resources for development and testing are supported by the following organizations. Thank you for your support!\n\n<!-- Note: Please sort them in alphabetical order. -->\n<!-- Note: Please keep these consistent with docs/community/sponsors.md -->\nCash Donations:\n\n- a16z\n- Dropbox\n- Sequoia Capital\n- Skywork AI\n- ZhenFund\n\nCompute Resources:\n\n- AMD\n- Anyscale\n- AWS\n- Crusoe Cloud\n- Databricks\n- DeepInfra\n- Google Cloud\n- Intel\n- Lambda Lab\n- Nebius\n- Novita AI\n- NVIDIA\n- Replicate\n- Roblox\n- RunPod\n- Trainy\n- UC Berkeley\n- UC San Diego\n\nSlack Sponsor: Anyscale\n\nWe also have an official fundraising venue through [OpenCollective](https://opencollective.com/vllm). We plan to use the fund to support the development, maintenance, and adoption of vLLM.\n\n## Citation\n\nIf you use vLLM for your research, please cite our [paper](https://arxiv.org/abs/2309.06180):\n\n```bibtex\n@inproceedings{kwon2023efficient,\n  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},\n  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},\n  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},\n  year={2023}\n}\n```\n\n## Contact Us\n\n<!-- --8<-- [start:contact-us] -->\n- For technical questions and feature requests, please use GitHub [Issues](https://github.com/vllm-project/vllm/issues) or [Discussions](https://github.com/vllm-project/vllm/discussions)\n- For discussing with fellow users, please use the [vLLM Forum](https://discuss.vllm.ai)\n- For coordinating contributions and development, please use [Slack](https://slack.vllm.ai)\n- For security disclosures, please use GitHub's [Security Advisories](https://github.com/vllm-project/vllm/security/advisories) feature\n- For collaborations and partnerships, please contact us at [vllm-questions@lists.berkeley.edu](mailto:vllm-questions@lists.berkeley.edu)\n<!-- --8<-- [end:contact-us] -->\n\n## Media Kit\n\n- If you wish to use vLLM's logo, please refer to [our media kit repo](https://github.com/vllm-project/media-kit)\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 649170660,
    "name": "anything-llm",
    "full_name": "Mintplex-Labs/anything-llm",
    "description": "The all-in-one Desktop & Docker AI application with built-in RAG, AI agents, No-code agent builder, MCP compatibility,  and more.",
    "html_url": "https://github.com/Mintplex-Labs/anything-llm",
    "clone_url": "https://github.com/Mintplex-Labs/anything-llm.git",
    "owner_login": "Mintplex-Labs",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/134426827?v=4",
    "stargazers_count": 47477,
    "watchers_count": 47477,
    "forks_count": 4846,
    "open_issues_count": 285,
    "size": 48433,
    "language": "JavaScript",
    "languages": {
      "JavaScript": 4454246,
      "CSS": 78965,
      "Dockerfile": 9329,
      "HTML": 3904,
      "Shell": 2237,
      "HCL": 1211
    },
    "topics": [
      "ai-agents",
      "custom-ai-agents",
      "deepseek",
      "kimi",
      "llama3",
      "llm",
      "lmstudio",
      "local-llm",
      "localai",
      "mcp",
      "mcp-servers",
      "moonshot",
      "multimodal",
      "no-code",
      "ollama",
      "qwen3",
      "rag",
      "vector-database",
      "web-scraping"
    ],
    "license_name": "MIT License",
    "created_at": "2023-06-04T02:29:14+00:00",
    "updated_at": "2025-08-06T01:33:47+00:00",
    "pushed_at": "2025-08-06T00:33:44+00:00",
    "contributors_count": 100,
    "readme_length": 21830,
    "readme_content": "<a name=\"readme-top\"></a>\n\n<p align=\"center\">\n  <a href=\"https://anythingllm.com\"><img src=\"https://github.com/Mintplex-Labs/anything-llm/blob/master/images/wordmark.png?raw=true\" alt=\"AnythingLLM logo\"></a>\n</p>\n\n<div align='center'>\n<a href=\"https://trendshift.io/repositories/2415\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/2415\" alt=\"Mintplex-Labs%2Fanything-llm | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n</div>\n\n<p align=\"center\">\n    <b>AnythingLLM:</b> The all-in-one AI app you were looking for.<br />\n    Chat with your docs, use AI Agents, hyper-configurable, multi-user, & no frustrating setup required.\n</p>\n\n<p align=\"center\">\n  <a href=\"https://discord.gg/6UyHPeGZAC\" target=\"_blank\">\n      <img src=\"https://img.shields.io/badge/chat-mintplex_labs-blue.svg?style=flat&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAMAAABEpIrGAAAAIGNIUk0AAHomAACAhAAA+gAAAIDoAAB1MAAA6mAAADqYAAAXcJy6UTwAAAH1UExURQAAAP////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////r6+ubn5+7u7/3+/v39/enq6urq6/v7+97f39rb26eoqT1BQ0pOT4+Rkuzs7cnKykZKS0NHSHl8fdzd3ejo6UxPUUBDRdzc3RwgIh8jJSAkJm5xcvHx8aanqB4iJFBTVezt7V5hYlJVVuLj43p9fiImKCMnKZKUlaaoqSElJ21wcfT09O3u7uvr6zE0Nr6/wCUpK5qcnf7+/nh7fEdKTHx+f0tPUOTl5aipqiouMGtubz5CRDQ4OsTGxufn515hY7a3uH1/gXBydIOFhlVYWvX29qaoqCQoKs7Pz/Pz87/AwUtOUNfY2dHR0mhrbOvr7E5RUy8zNXR2d/f39+Xl5UZJSx0hIzQ3Odra2/z8/GlsbaGjpERHSezs7L/BwScrLTQ4Odna2zM3Obm7u3x/gKSmp9jZ2T1AQu/v71pdXkVISr2+vygsLiInKTg7PaOlpisvMcXGxzk8PldaXPLy8u7u7rm6u7S1tsDBwvj4+MPExbe4ueXm5s/Q0Kyf7ewAAAAodFJOUwAABClsrNjx/QM2l9/7lhmI6jTB/kA1GgKJN+nea6vy/MLZQYeVKK3rVA5tAAAAAWJLR0QB/wIt3gAAAAd0SU1FB+cKBAAmMZBHjXIAAAISSURBVDjLY2CAAkYmZhZWNnYODnY2VhZmJkYGVMDIycXNw6sBBbw8fFycyEoYGfkFBDVQgKAAPyMjQl5IWEQDDYgIC8FUMDKKsmlgAWyiEBWMjGJY5YEqxMAqGMWFNXAAYXGgAkYJSQ2cQFKCkYFRShq3AmkpRgYJbghbU0tbB0Tr6ukbgGhDI10gySfBwCwDUWBsYmpmDqQtLK2sbTQ0bO3sHYA8GWYGWWj4WTs6Obu4ami4OTm7exhqeHp5+4DCVJZBDmqdr7ufn3+ArkZgkJ+fU3CIRmgYWFiOARYGvo5OQUHhEUAFTkF+kVHRsLBgkIeyYmLjwoOc4hMSk5JTnINS06DC8gwcEEZ6RqZGlpOfc3ZObl5+gZ+TR2ERWFyBQQFMF5eklmqUpQb5+ReU61ZUOvkFVVXXQBSAraitq29o1GiKcfLzc29u0mjxBzq0tQ0kww5xZHtHUGeXhkZhdxBYgZ4d0LI6c4gjwd7siQQraOp1AivQ6CuAKZCDBBRQQQNQgUb/BGf3cqCCiZOcnCe3QQIKHNRTpk6bDgpZjRkzg3pBQTBrdtCcuZCgluAD0vPmL1gIdvSixUuWgqNs2YJ+DUhkEYxuggkGmOQUcckrioPTJCOXEnZ5JS5YslbGnuyVERlDDFvGEUPOWvwqaH6RVkHKeuDMK6SKnHlVhTgx8jeTmqy6Eij7K6nLqiGyPwChsa1MUrnq1wAAACV0RVh0ZGF0ZTpjcmVhdGUAMjAyMy0xMC0wNFQwMDozODo0OSswMDowMB9V0a8AAAAldEVYdGRhdGU6bW9kaWZ5ADIwMjMtMTAtMDRUMDA6Mzg6NDkrMDA6MDBuCGkTAAAAKHRFWHRkYXRlOnRpbWVzdGFtcAAyMDIzLTEwLTA0VDAwOjM4OjQ5KzAwOjAwOR1IzAAAAABJRU5ErkJggg==\" alt=\"Discord\">\n  </a> |\n  <a href=\"https://github.com/Mintplex-Labs/anything-llm/blob/master/LICENSE\" target=\"_blank\">\n      <img src=\"https://img.shields.io/static/v1?label=license&message=MIT&color=white\" alt=\"License\">\n  </a> |\n  <a href=\"https://docs.anythingllm.com\" target=\"_blank\">\n    Docs\n  </a> |\n   <a href=\"https://my.mintplexlabs.com/aio-checkout?product=anythingllm\" target=\"_blank\">\n    Hosted Instance\n  </a>\n</p>\n\n<p align=\"center\">\n  <b>English</b> \u00b7 <a href='./locales/README.zh-CN.md'>\u7b80\u4f53\u4e2d\u6587</a> \u00b7 <a href='./locales/README.ja-JP.md'>\u65e5\u672c\u8a9e</a>\n</p>\n\n<p align=\"center\">\n\ud83d\udc49 AnythingLLM for desktop (Mac, Windows, & Linux)! <a href=\"https://anythingllm.com/download\" target=\"_blank\"> Download Now</a>\n</p>\n\nA full-stack application that enables you to turn any document, resource, or piece of content into context that any LLM can use as a reference during chatting. This application allows you to pick and choose which LLM or Vector Database you want to use as well as supporting multi-user management and permissions.\n\n![Chatting](https://github.com/Mintplex-Labs/anything-llm/assets/16845892/cfc5f47c-bd91-4067-986c-f3f49621a859)\n\n<details>\n<summary><kbd>Watch the demo!</kbd></summary>\n\n[![Watch the video](/images/youtube.png)](https://youtu.be/f95rGD9trL0)\n\n</details>\n\n### Product Overview\n\nAnythingLLM is a full-stack application where you can use commercial off-the-shelf LLMs or popular open source LLMs and vectorDB solutions to build a private ChatGPT with no compromises that you can run locally as well as host remotely and be able to chat intelligently with any documents you provide it.\n\nAnythingLLM divides your documents into objects called `workspaces`. A Workspace functions a lot like a thread, but with the addition of containerization of your documents. Workspaces can share documents, but they do not talk to each other so you can keep your context for each workspace clean.\n\n## Cool features of AnythingLLM\n\n- \ud83c\udd95 [**Full MCP-compatibility**](https://docs.anythingllm.com/mcp-compatibility/overview)\n- \ud83c\udd95 [**No-code AI Agent builder**](https://docs.anythingllm.com/agent-flows/overview)\n- \ud83d\uddbc\ufe0f **Multi-modal support (both closed and open-source LLMs!)**\n- [**Custom AI Agents**](https://docs.anythingllm.com/agent/custom/introduction)\n- \ud83d\udc64 Multi-user instance support and permissioning _Docker version only_\n- \ud83e\uddbe Agents inside your workspace (browse the web, etc)\n- \ud83d\udcac [Custom Embeddable Chat widget for your website](https://github.com/Mintplex-Labs/anythingllm-embed/blob/main/README.md) _Docker version only_\n- \ud83d\udcd6 Multiple document type support (PDF, TXT, DOCX, etc)\n- Simple chat UI with Drag-n-Drop functionality and clear citations.\n- 100% Cloud deployment ready.\n- Works with all popular [closed and open-source LLM providers](#supported-llms-embedder-models-speech-models-and-vector-databases).\n- Built-in cost & time-saving measures for managing very large documents compared to any other chat UI.\n- Full Developer API for custom integrations!\n- Much more...install and find out!\n\n### Supported LLMs, Embedder Models, Speech models, and Vector Databases\n\n**Large Language Models (LLMs):**\n\n- [Any open-source llama.cpp compatible model](/server/storage/models/README.md#text-generation-llm-selection)\n- [OpenAI](https://openai.com)\n- [OpenAI (Generic)](https://openai.com)\n- [Azure OpenAI](https://azure.microsoft.com/en-us/products/ai-services/openai-service)\n- [AWS Bedrock](https://aws.amazon.com/bedrock/)\n- [Anthropic](https://www.anthropic.com/)\n- [NVIDIA NIM (chat models)](https://build.nvidia.com/explore/discover)\n- [Google Gemini Pro](https://ai.google.dev/)\n- [Hugging Face (chat models)](https://huggingface.co/)\n- [Ollama (chat models)](https://ollama.ai/)\n- [LM Studio (all models)](https://lmstudio.ai)\n- [LocalAI (all models)](https://localai.io/)\n- [Together AI (chat models)](https://www.together.ai/)\n- [Fireworks AI  (chat models)](https://fireworks.ai/)\n- [Perplexity (chat models)](https://www.perplexity.ai/)\n- [OpenRouter (chat models)](https://openrouter.ai/)\n- [DeepSeek (chat models)](https://deepseek.com/)\n- [Mistral](https://mistral.ai/)\n- [Groq](https://groq.com/)\n- [Cohere](https://cohere.com/)\n- [KoboldCPP](https://github.com/LostRuins/koboldcpp)\n- [LiteLLM](https://github.com/BerriAI/litellm)\n- [Text Generation Web UI](https://github.com/oobabooga/text-generation-webui)\n- [Apipie](https://apipie.ai/)\n- [xAI](https://x.ai/)\n- [Novita AI (chat models)](https://novita.ai/model-api/product/llm-api?utm_source=github_anything-llm&utm_medium=github_readme&utm_campaign=link)\n- [PPIO](https://ppinfra.com?utm_source=github_anything-llm)\n- [Moonshot AI](https://www.moonshot.ai/)\n\n**Embedder models:**\n\n- [AnythingLLM Native Embedder](/server/storage/models/README.md) (default)\n- [OpenAI](https://openai.com)\n- [Azure OpenAI](https://azure.microsoft.com/en-us/products/ai-services/openai-service)\n- [LocalAI (all)](https://localai.io/)\n- [Ollama (all)](https://ollama.ai/)\n- [LM Studio (all)](https://lmstudio.ai)\n- [Cohere](https://cohere.com/)\n\n**Audio Transcription models:**\n\n- [AnythingLLM Built-in](https://github.com/Mintplex-Labs/anything-llm/tree/master/server/storage/models#audiovideo-transcription) (default)\n- [OpenAI](https://openai.com/)\n\n**TTS (text-to-speech) support:**\n\n- Native Browser Built-in (default)\n- [PiperTTSLocal - runs in browser](https://github.com/rhasspy/piper)\n- [OpenAI TTS](https://platform.openai.com/docs/guides/text-to-speech/voice-options)\n- [ElevenLabs](https://elevenlabs.io/)\n- Any OpenAI Compatible TTS service.\n\n**STT (speech-to-text) support:**\n\n- Native Browser Built-in (default)\n\n**Vector Databases:**\n\n- [LanceDB](https://github.com/lancedb/lancedb) (default)\n- [PGVector](https://github.com/pgvector/pgvector)\n- [Astra DB](https://www.datastax.com/products/datastax-astra)\n- [Pinecone](https://pinecone.io)\n- [Chroma](https://trychroma.com)\n- [Weaviate](https://weaviate.io)\n- [Qdrant](https://qdrant.tech)\n- [Milvus](https://milvus.io)\n- [Zilliz](https://zilliz.com)\n\n### Technical Overview\n\nThis monorepo consists of six main sections:\n\n- `frontend`: A viteJS + React frontend that you can run to easily create and manage all your content the LLM can use.\n- `server`: A NodeJS express server to handle all the interactions and do all the vectorDB management and LLM interactions.\n- `collector`: NodeJS express server that processes and parses documents from the UI.\n- `docker`: Docker instructions and build process + information for building from source.\n- `embed`: Submodule for generation & creation of the [web embed widget](https://github.com/Mintplex-Labs/anythingllm-embed).\n- `browser-extension`: Submodule for the [chrome browser extension](https://github.com/Mintplex-Labs/anythingllm-extension).\n\n## \ud83d\udef3 Self-Hosting\n\nMintplex Labs & the community maintain a number of deployment methods, scripts, and templates that you can use to run AnythingLLM locally. Refer to the table below to read how to deploy on your preferred environment or to automatically deploy.\n| Docker | AWS | GCP | Digital Ocean | Render.com |\n|----------------------------------------|----|-----|---------------|------------|\n| [![Deploy on Docker][docker-btn]][docker-deploy] | [![Deploy on AWS][aws-btn]][aws-deploy] | [![Deploy on GCP][gcp-btn]][gcp-deploy] | [![Deploy on DigitalOcean][do-btn]][do-deploy] | [![Deploy on Render.com][render-btn]][render-deploy] |\n\n| Railway  |  RepoCloud | Elestio |\n| --- | --- | --- |\n| [![Deploy on Railway][railway-btn]][railway-deploy] | [![Deploy on RepoCloud][repocloud-btn]][repocloud-deploy] | [![Deploy on Elestio][elestio-btn]][elestio-deploy] |\n\n[or set up a production AnythingLLM instance without Docker \u2192](./BARE_METAL.md)\n\n## How to setup for development\n\n- `yarn setup` To fill in the required `.env` files you'll need in each of the application sections (from root of repo).\n  - Go fill those out before proceeding. Ensure `server/.env.development` is filled or else things won't work right.\n- `yarn dev:server` To boot the server locally (from root of repo).\n- `yarn dev:frontend` To boot the frontend locally (from root of repo).\n- `yarn dev:collector` To then run the document collector (from root of repo).\n\n[Learn about documents](./server/storage/documents/DOCUMENTS.md)\n\n[Learn about vector caching](./server/storage/vector-cache/VECTOR_CACHE.md)\n\n## External Apps & Integrations\n\n_These are apps that are not maintained by Mintplex Labs, but are compatible with AnythingLLM. A listing here is not an endorsement._\n\n- [Midori AI Subsystem Manager](https://io.midori-ai.xyz/subsystem/anythingllm/) - A streamlined and efficient way to deploy AI systems using Docker container technology.\n- [Coolify](https://coolify.io/docs/services/anythingllm/) - Deploy AnythingLLM with a single click.\n- [GPTLocalhost for Microsoft Word](https://gptlocalhost.com/demo/) - A local Word Add-in for you to use AnythingLLM in Microsoft Word.\n\n## Telemetry & Privacy\n\nAnythingLLM by Mintplex Labs Inc contains a telemetry feature that collects anonymous usage information.\n\n<details>\n<summary><kbd>More about Telemetry & Privacy for AnythingLLM</kbd></summary>\n\n### Why?\n\nWe use this information to help us understand how AnythingLLM is used, to help us prioritize work on new features and bug fixes, and to help us improve AnythingLLM's performance and stability.\n\n### Opting out\n\nSet `DISABLE_TELEMETRY` in your server or docker .env settings to \"true\" to opt out of telemetry. You can also do this in-app by going to the sidebar > `Privacy` and disabling telemetry.\n\n### What do you explicitly track?\n\nWe will only track usage details that help us make product and roadmap decisions, specifically:\n\n- Type of your installation (Docker or Desktop)\n\n- When a document is added or removed. No information _about_ the document. Just that the event occurred. This gives us an idea of use.\n\n- Type of vector database in use. This helps us prioritize changes when updates arrive for that provider.\n\n- Type of LLM provider & model tag in use. This helps us prioritize changes when updates arrive for that provider or model, or combination thereof. eg: reasoning vs regular, multi-modal models, etc.\n\n- When a chat is sent. This is the most regular \"event\" and gives us an idea of the daily-activity of this project across all installations. Again, only the **event** is sent - we have no information on the nature or content of the chat itself.\n\nYou can verify these claims by finding all locations `Telemetry.sendTelemetry` is called. Additionally these events are written to the output log so you can also see the specific data which was sent - if enabled. **No IP or other identifying information is collected**. The Telemetry provider is [PostHog](https://posthog.com/) - an open-source telemetry collection service.\n\nWe take privacy very seriously, and we hope you understand that we want to learn how our tool is used, without using annoying popup surveys, so we can build something worth using. The anonymous data is _never_ shared with third parties, ever.\n\n[View all telemetry events in source code](https://github.com/search?q=repo%3AMintplex-Labs%2Fanything-llm%20.sendTelemetry\\(&type=code)\n\n</details>\n\n## \ud83d\udc4b Contributing\n\n- [Contributing to AnythingLLM](./CONTRIBUTING.md) - How to contribute to AnythingLLM.\n\n## \ud83d\udc96 Sponsors\n\n### Premium Sponsors\n\n<!-- premium-sponsors (reserved for $100/mth sponsors who request to be called out here and/or are non-private sponsors) -->\n<a href=\"https://www.dcsdigital.co.uk\" target=\"_blank\">\n  <img src=\"https://a8cforagenciesportfolio.wordpress.com/wp-content/uploads/2024/08/logo-image-232621379.png\" height=\"100px\" alt=\"User avatar: DCS DIGITAL\" />\n</a>\n<!-- premium-sponsors -->\n\n### All Sponsors\n\n<!-- all-sponsors --><a href=\"https://github.com/jaschadub\"><img src=\"https:&#x2F;&#x2F;github.com&#x2F;jaschadub.png\" width=\"60px\" alt=\"User avatar: Jascha\" /></a><a href=\"https://github.com/KickingAss2024\"><img src=\"https:&#x2F;&#x2F;github.com&#x2F;KickingAss2024.png\" width=\"60px\" alt=\"User avatar: KickAss\" /></a><a href=\"https://github.com/ShadowArcanist\"><img src=\"https:&#x2F;&#x2F;github.com&#x2F;ShadowArcanist.png\" width=\"60px\" alt=\"User avatar: ShadowArcanist\" /></a><a href=\"https://github.com/AtlasVIA\"><img src=\"https:&#x2F;&#x2F;github.com&#x2F;AtlasVIA.png\" width=\"60px\" alt=\"User avatar: Atlas\" /></a><a href=\"https://github.com/cope\"><img src=\"https:&#x2F;&#x2F;github.com&#x2F;cope.png\" width=\"60px\" alt=\"User avatar: Predrag Stojadinovi\u0107\" /></a><a href=\"https://github.com/DiegoSpinola\"><img src=\"https:&#x2F;&#x2F;github.com&#x2F;DiegoSpinola.png\" width=\"60px\" alt=\"User avatar: Diego Spinola\" /></a><a href=\"https://github.com/PortlandKyGuy\"><img src=\"https:&#x2F;&#x2F;github.com&#x2F;PortlandKyGuy.png\" width=\"60px\" alt=\"User avatar: Kyle\" /></a><a href=\"https://github.com/peperunas\"><img src=\"https:&#x2F;&#x2F;github.com&#x2F;peperunas.png\" width=\"60px\" alt=\"User avatar: Giulio De Pasquale\" /></a><a href=\"https://github.com/jasoncdavis0\"><img src=\"https:&#x2F;&#x2F;github.com&#x2F;jasoncdavis0.png\" width=\"60px\" alt=\"User avatar: \" /></a><a href=\"https://github.com/macstadium\"><img src=\"https:&#x2F;&#x2F;github.com&#x2F;macstadium.png\" width=\"60px\" alt=\"User avatar: MacStadium\" /></a><a href=\"https://github.com/armlynobinguar\"><img src=\"https:&#x2F;&#x2F;github.com&#x2F;armlynobinguar.png\" width=\"60px\" alt=\"User avatar: \" /></a><a href=\"https://github.com/MikeHago\"><img src=\"https:&#x2F;&#x2F;github.com&#x2F;MikeHago.png\" width=\"60px\" alt=\"User avatar: \" /></a><a href=\"https://github.com/maaisde\"><img src=\"https:&#x2F;&#x2F;github.com&#x2F;maaisde.png\" width=\"60px\" alt=\"User avatar: \" /></a><a href=\"https://github.com/mhollier117\"><img src=\"https:&#x2F;&#x2F;github.com&#x2F;mhollier117.png\" width=\"60px\" alt=\"User avatar: \" /></a><a href=\"https://github.com/pleabargain\"><img src=\"https:&#x2F;&#x2F;github.com&#x2F;pleabargain.png\" width=\"60px\" alt=\"User avatar: Dennis\" /></a><a href=\"https://github.com/broichan\"><img src=\"https:&#x2F;&#x2F;github.com&#x2F;broichan.png\" width=\"60px\" alt=\"User avatar: Michael Hamilton, Ph.D.\" /></a><a href=\"https://github.com/azim-charaniya\"><img src=\"https:&#x2F;&#x2F;github.com&#x2F;azim-charaniya.png\" width=\"60px\" alt=\"User avatar: \" /></a><a href=\"https://github.com/gabriellemon\"><img src=\"https:&#x2F;&#x2F;github.com&#x2F;gabriellemon.png\" width=\"60px\" alt=\"User avatar: TernaryLabs\" /></a><a href=\"https://github.com/CelaDaniel\"><img src=\"https:&#x2F;&#x2F;github.com&#x2F;CelaDaniel.png\" width=\"60px\" alt=\"User avatar: Daniel Cela\" /></a><a href=\"https://github.com/altrsadmin\"><img src=\"https:&#x2F;&#x2F;github.com&#x2F;altrsadmin.png\" width=\"60px\" alt=\"User avatar: Alesso\" /></a><a href=\"https://github.com/bitjungle\"><img src=\"https:&#x2F;&#x2F;github.com&#x2F;bitjungle.png\" width=\"60px\" alt=\"User avatar: Rune Mathisen\" /></a><a href=\"https://github.com/pcrossleyAC\"><img src=\"https:&#x2F;&#x2F;github.com&#x2F;pcrossleyAC.png\" width=\"60px\" alt=\"User avatar: \" /></a><a href=\"https://github.com/saroj-pattnaik\"><img src=\"https:&#x2F;&#x2F;github.com&#x2F;saroj-pattnaik.png\" width=\"60px\" alt=\"User avatar: \" /></a><a href=\"https://github.com/techmedic5\"><img src=\"https:&#x2F;&#x2F;github.com&#x2F;techmedic5.png\" width=\"60px\" alt=\"User avatar: Alan\" /></a><a href=\"https://github.com/ddocta\"><img src=\"https:&#x2F;&#x2F;github.com&#x2F;ddocta.png\" width=\"60px\" alt=\"User avatar: Damien Peters\" /></a><a href=\"https://github.com/dcsdigital\"><img src=\"https:&#x2F;&#x2F;github.com&#x2F;dcsdigital.png\" width=\"60px\" alt=\"User avatar: DCS Digital\" /></a><a href=\"https://github.com/pm7y\"><img src=\"https:&#x2F;&#x2F;github.com&#x2F;pm7y.png\" width=\"60px\" alt=\"User avatar: Paul Mcilreavy\" /></a><a href=\"https://github.com/tilwolf\"><img src=\"https:&#x2F;&#x2F;github.com&#x2F;tilwolf.png\" width=\"60px\" alt=\"User avatar: Til Wolf\" /></a><a href=\"https://github.com/ozzyoss77\"><img src=\"https:&#x2F;&#x2F;github.com&#x2F;ozzyoss77.png\" width=\"60px\" alt=\"User avatar: Leopoldo Crhistian Riverin Gomez\" /></a><a href=\"https://github.com/AlphaEcho11\"><img src=\"https:&#x2F;&#x2F;github.com&#x2F;AlphaEcho11.png\" width=\"60px\" alt=\"User avatar: AJEsau\" /></a><a href=\"https://github.com/svanomm\"><img src=\"https:&#x2F;&#x2F;github.com&#x2F;svanomm.png\" width=\"60px\" alt=\"User avatar: Steven VanOmmeren\" /></a><a href=\"https://github.com/socketbox\"><img src=\"https:&#x2F;&#x2F;github.com&#x2F;socketbox.png\" width=\"60px\" alt=\"User avatar: Casey Boettcher\" /></a><a href=\"https://github.com/zebbern\"><img src=\"https:&#x2F;&#x2F;github.com&#x2F;zebbern.png\" width=\"60px\" alt=\"User avatar: \" /></a><a href=\"https://github.com/avineetbespin\"><img src=\"https:&#x2F;&#x2F;github.com&#x2F;avineetbespin.png\" width=\"60px\" alt=\"User avatar: Avineet\" /></a><a href=\"https://github.com/invictus-1\"><img src=\"https:&#x2F;&#x2F;github.com&#x2F;invictus-1.png\" width=\"60px\" alt=\"User avatar: Chris\" /></a><!-- all-sponsors -->\n\n## \ud83c\udf1f Contributors\n\n[![anythingllm contributors](https://contrib.rocks/image?repo=mintplex-labs/anything-llm)](https://github.com/mintplex-labs/anything-llm/graphs/contributors)\n\n[![Star History Chart](https://api.star-history.com/svg?repos=mintplex-labs/anything-llm&type=Timeline)](https://star-history.com/#mintplex-labs/anything-llm&Date)\n\n## \ud83d\udd17 More Products\n\n- **[VectorAdmin][vector-admin]:** An all-in-one GUI & tool-suite for managing vector databases.\n- **[OpenAI Assistant Swarm][assistant-swarm]:** Turn your entire library of OpenAI assistants into one single army commanded from a single agent.\n\n<div align=\"right\">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n---\n\nCopyright \u00a9 2025 [Mintplex Labs][profile-link]. <br />\nThis project is [MIT](./LICENSE) licensed.\n\n<!-- LINK GROUP -->\n\n[back-to-top]: https://img.shields.io/badge/-BACK_TO_TOP-222628?style=flat-square\n[profile-link]: https://github.com/mintplex-labs\n[vector-admin]: https://github.com/mintplex-labs/vector-admin\n[assistant-swarm]: https://github.com/Mintplex-Labs/openai-assistant-swarm\n[docker-btn]: ./images/deployBtns/docker.png\n[docker-deploy]: ./docker/HOW_TO_USE_DOCKER.md\n[aws-btn]: ./images/deployBtns/aws.png\n[aws-deploy]: ./cloud-deployments/aws/cloudformation/DEPLOY.md\n[gcp-btn]: https://deploy.cloud.run/button.svg\n[gcp-deploy]: ./cloud-deployments/gcp/deployment/DEPLOY.md\n[do-btn]: https://www.deploytodo.com/do-btn-blue.svg\n[do-deploy]: ./cloud-deployments/digitalocean/terraform/DEPLOY.md\n[render-btn]: https://render.com/images/deploy-to-render-button.svg\n[render-deploy]: https://render.com/deploy?repo=https://github.com/Mintplex-Labs/anything-llm&branch=render\n[render-btn]: https://render.com/images/deploy-to-render-button.svg\n[render-deploy]: https://render.com/deploy?repo=https://github.com/Mintplex-Labs/anything-llm&branch=render\n[railway-btn]: https://railway.app/button.svg\n[railway-deploy]: https://railway.app/template/HNSCS1?referralCode=WFgJkn\n[repocloud-btn]: https://d16t0pc4846x52.cloudfront.net/deploylobe.svg\n[repocloud-deploy]: https://repocloud.io/details/?app_id=276\n[elestio-btn]: https://elest.io/images/logos/deploy-to-elestio-btn.png\n[elestio-deploy]: https://elest.io/open-source/anythingllm\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 787076358,
    "name": "firecrawl",
    "full_name": "mendableai/firecrawl",
    "description": "\ud83d\udd25 Turn entire websites into LLM-ready markdown or structured data. Scrape, crawl and extract with a single API.",
    "html_url": "https://github.com/mendableai/firecrawl",
    "clone_url": "https://github.com/mendableai/firecrawl.git",
    "owner_login": "mendableai",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/135057108?v=4",
    "stargazers_count": 45138,
    "watchers_count": 45138,
    "forks_count": 4196,
    "open_issues_count": 202,
    "size": 62098,
    "language": "TypeScript",
    "languages": {
      "TypeScript": 2085550,
      "Python": 246269,
      "Rust": 175547,
      "Jupyter Notebook": 10546,
      "Shell": 10521,
      "JavaScript": 9449,
      "Dockerfile": 2855,
      "CSS": 1835,
      "Go": 619,
      "HTML": 354,
      "Procfile": 132
    },
    "topics": [
      "ai",
      "ai-scraping",
      "crawler",
      "data",
      "html-to-markdown",
      "llm",
      "markdown",
      "rag",
      "scraper",
      "scraping",
      "web-crawler",
      "webscraping"
    ],
    "license_name": "GNU Affero General Public License v3.0",
    "created_at": "2024-04-15T21:02:29+00:00",
    "updated_at": "2025-08-06T02:05:25+00:00",
    "pushed_at": "2025-08-05T20:41:17+00:00",
    "contributors_count": 96,
    "readme_length": 22689,
    "readme_content": "<h3 align=\"center\">\n  <a name=\"readme-top\"></a>\n  <img\n    src=\"https://raw.githubusercontent.com/mendableai/firecrawl/main/img/firecrawl_logo.png\"\n    height=\"200\"\n  >\n</h3>\n<div align=\"center\">\n    <a href=\"https://github.com/mendableai/firecrawl/blob/main/LICENSE\">\n  <img src=\"https://img.shields.io/github/license/mendableai/firecrawl\" alt=\"License\">\n</a>\n    <a href=\"https://pepy.tech/project/firecrawl-py\">\n  <img src=\"https://static.pepy.tech/badge/firecrawl-py\" alt=\"Downloads\">\n</a>\n<a href=\"https://GitHub.com/mendableai/firecrawl/graphs/contributors\">\n  <img src=\"https://img.shields.io/github/contributors/mendableai/firecrawl.svg\" alt=\"GitHub Contributors\">\n</a>\n<a href=\"https://firecrawl.dev\">\n  <img src=\"https://img.shields.io/badge/Visit-firecrawl.dev-orange\" alt=\"Visit firecrawl.dev\">\n</a>\n</div>\n<div>\n  <p align=\"center\">\n    <a href=\"https://twitter.com/firecrawl_dev\">\n      <img src=\"https://img.shields.io/badge/Follow%20on%20X-000000?style=for-the-badge&logo=x&logoColor=white\" alt=\"Follow on X\" />\n    </a>\n    <a href=\"https://www.linkedin.com/company/104100957\">\n      <img src=\"https://img.shields.io/badge/Follow%20on%20LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white\" alt=\"Follow on LinkedIn\" />\n    </a>\n    <a href=\"https://discord.com/invite/gSmWdAkdwd\">\n      <img src=\"https://img.shields.io/badge/Join%20our%20Discord-5865F2?style=for-the-badge&logo=discord&logoColor=white\" alt=\"Join our Discord\" />\n    </a>\n  </p>\n</div>\n\n# \ud83d\udd25 Firecrawl\n\nEmpower your AI apps with clean data from any website. Featuring advanced scraping, crawling, and data extraction capabilities.\n\n_This repository is in development, and we\u2019re still integrating custom modules into the mono repo. It's not fully ready for self-hosted deployment yet, but you can run it locally._\n\n## What is Firecrawl?\n\n[Firecrawl](https://firecrawl.dev?ref=github) is an API service that takes a URL, crawls it, and converts it into clean markdown or structured data. We crawl all accessible subpages and give you clean data for each. No sitemap required. Check out our [documentation](https://docs.firecrawl.dev).\n\n_Pst. hey, you, join our stargazers :)_\n\n<a href=\"https://github.com/mendableai/firecrawl\">\n  <img src=\"https://img.shields.io/github/stars/mendableai/firecrawl.svg?style=social&label=Star&maxAge=2592000\" alt=\"GitHub stars\">\n</a>\n\n## How to use it?\n\nWe provide an easy to use API with our hosted version. You can find the playground and documentation [here](https://firecrawl.dev/playground). You can also self host the backend if you'd like.\n\nCheck out the following resources to get started:\n- [x] **API**: [Documentation](https://docs.firecrawl.dev/api-reference/introduction)\n- [x] **SDKs**: [Python](https://docs.firecrawl.dev/sdks/python), [Node](https://docs.firecrawl.dev/sdks/node), [Go](https://docs.firecrawl.dev/sdks/go), [Rust](https://docs.firecrawl.dev/sdks/rust)\n- [x] **LLM Frameworks**: [Langchain (python)](https://python.langchain.com/docs/integrations/document_loaders/firecrawl/), [Langchain (js)](https://js.langchain.com/docs/integrations/document_loaders/web_loaders/firecrawl), [Llama Index](https://docs.llamaindex.ai/en/latest/examples/data_connectors/WebPageDemo/#using-firecrawl-reader), [Crew.ai](https://docs.crewai.com/), [Composio](https://composio.dev/tools/firecrawl/all), [PraisonAI](https://docs.praison.ai/firecrawl/), [Superinterface](https://superinterface.ai/docs/assistants/functions/firecrawl), [Vectorize](https://docs.vectorize.io/integrations/source-connectors/firecrawl)\n- [x] **Low-code Frameworks**: [Dify](https://dify.ai/blog/dify-ai-blog-integrated-with-firecrawl), [Langflow](https://docs.langflow.org/), [Flowise AI](https://docs.flowiseai.com/integrations/langchain/document-loaders/firecrawl), [Cargo](https://docs.getcargo.io/integration/firecrawl), [Pipedream](https://pipedream.com/apps/firecrawl/)\n- [x] **Others**: [Zapier](https://zapier.com/apps/firecrawl/integrations), [Pabbly Connect](https://www.pabbly.com/connect/integrations/firecrawl/)\n- [ ] Want an SDK or Integration? Let us know by opening an issue.\n\nTo run locally, refer to guide [here](https://github.com/mendableai/firecrawl/blob/main/CONTRIBUTING.md).\n\n### API Key\n\nTo use the API, you need to sign up on [Firecrawl](https://firecrawl.dev) and get an API key.\n\n### Features\n\n- [**Scrape**](#scraping): scrapes a URL and get its content in LLM-ready format (markdown, structured data via [LLM Extract](#llm-extraction-beta), screenshot, html)\n- [**Crawl**](#crawling): scrapes all the URLs of a web page and return content in LLM-ready format\n- [**Map**](#map): input a website and get all the website urls - extremely fast\n- [**Search**](#search): search the web and get full content from results\n- [**Extract**](#extract): get structured data from single page, multiple pages or entire websites with AI.\n\n### Powerful Capabilities\n- **LLM-ready formats**: markdown, structured data, screenshot, HTML, links, metadata\n- **The hard stuff**: proxies, anti-bot mechanisms, dynamic content (js-rendered), output parsing, orchestration\n- **Customizability**: exclude tags, crawl behind auth walls with custom headers, max crawl depth, etc...\n- **Media parsing**: pdfs, docx, images\n- **Reliability first**: designed to get the data you need - no matter how hard it is\n- **Actions**: click, scroll, input, wait and more before extracting data\n- **Batching (New)**: scrape thousands of URLs at the same time with a new async endpoint.\n\nYou can find all of Firecrawl's capabilities and how to use them in our [documentation](https://docs.firecrawl.dev)\n\n### Crawling\n\nUsed to crawl a URL and all accessible subpages. This submits a crawl job and returns a job ID to check the status of the crawl.\n\n```bash\ncurl -X POST https://api.firecrawl.dev/v1/crawl \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer fc-YOUR_API_KEY' \\\n    -d '{\n      \"url\": \"https://docs.firecrawl.dev\",\n      \"limit\": 10,\n      \"scrapeOptions\": {\n        \"formats\": [\"markdown\", \"html\"]\n      }\n    }'\n```\n\nReturns a crawl job id and the url to check the status of the crawl.\n\n```json\n{\n  \"success\": true,\n  \"id\": \"123-456-789\",\n  \"url\": \"https://api.firecrawl.dev/v1/crawl/123-456-789\"\n}\n```\n\n### Check Crawl Job\n\nUsed to check the status of a crawl job and get its result.\n\n```bash\ncurl -X GET https://api.firecrawl.dev/v1/crawl/123-456-789 \\\n  -H 'Content-Type: application/json' \\\n  -H 'Authorization: Bearer YOUR_API_KEY'\n```\n\n```json\n{\n  \"status\": \"completed\",\n  \"total\": 36,\n  \"creditsUsed\": 36,\n  \"expiresAt\": \"2024-00-00T00:00:00.000Z\",\n  \"data\": [\n    {\n      \"markdown\": \"[Firecrawl Docs home page![light logo](https://mintlify.s3-us-west-1.amazonaws.com/firecrawl/logo/light.svg)!...\",\n      \"html\": \"<!DOCTYPE html><html lang=\\\"en\\\" class=\\\"js-focus-visible lg:[--scroll-mt:9.5rem]\\\" data-js-focus-visible=\\\"\\\">...\",\n      \"metadata\": {\n        \"title\": \"Build a 'Chat with website' using Groq Llama 3 | Firecrawl\",\n        \"language\": \"en\",\n        \"sourceURL\": \"https://docs.firecrawl.dev/learn/rag-llama3\",\n        \"description\": \"Learn how to use Firecrawl, Groq Llama 3, and Langchain to build a 'Chat with your website' bot.\",\n        \"ogLocaleAlternate\": [],\n        \"statusCode\": 200\n      }\n    }\n  ]\n}\n```\n\n### Scraping\n\nUsed to scrape a URL and get its content in the specified formats.\n\n```bash\ncurl -X POST https://api.firecrawl.dev/v1/scrape \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"url\": \"https://docs.firecrawl.dev\",\n      \"formats\" : [\"markdown\", \"html\"]\n    }'\n```\n\nResponse:\n\n```json\n{\n  \"success\": true,\n  \"data\": {\n    \"markdown\": \"Launch Week I is here! [See our Day 2 Release \ud83d\ude80](https://www.firecrawl.dev/blog/launch-week-i-day-2-doubled-rate-limits)[\ud83d\udca5 Get 2 months free...\",\n    \"html\": \"<!DOCTYPE html><html lang=\\\"en\\\" class=\\\"light\\\" style=\\\"color-scheme: light;\\\"><body class=\\\"__variable_36bd41 __variable_d7dc5d font-inter ...\",\n    \"metadata\": {\n      \"title\": \"Home - Firecrawl\",\n      \"description\": \"Firecrawl crawls and converts any website into clean markdown.\",\n      \"language\": \"en\",\n      \"keywords\": \"Firecrawl,Markdown,Data,Mendable,Langchain\",\n      \"robots\": \"follow, index\",\n      \"ogTitle\": \"Firecrawl\",\n      \"ogDescription\": \"Turn any website into LLM-ready data.\",\n      \"ogUrl\": \"https://www.firecrawl.dev/\",\n      \"ogImage\": \"https://www.firecrawl.dev/og.png?123\",\n      \"ogLocaleAlternate\": [],\n      \"ogSiteName\": \"Firecrawl\",\n      \"sourceURL\": \"https://firecrawl.dev\",\n      \"statusCode\": 200\n    }\n  }\n}\n```\n\n### Map\n\nUsed to map a URL and get urls of the website. This returns most links present on the website.\n\n```bash cURL\ncurl -X POST https://api.firecrawl.dev/v1/map \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"url\": \"https://firecrawl.dev\"\n    }'\n```\n\nResponse:\n\n```json\n{\n  \"status\": \"success\",\n  \"links\": [\n    \"https://firecrawl.dev\",\n    \"https://www.firecrawl.dev/pricing\",\n    \"https://www.firecrawl.dev/blog\",\n    \"https://www.firecrawl.dev/playground\",\n    \"https://www.firecrawl.dev/smart-crawl\",\n  ]\n}\n```\n\n#### Map with search\n\nMap with `search` param allows you to search for specific urls inside a website.\n\n```bash cURL\ncurl -X POST https://api.firecrawl.dev/v1/map \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"url\": \"https://firecrawl.dev\",\n      \"search\": \"docs\"\n    }'\n```\n\nResponse will be an ordered list from the most relevant to the least relevant.\n\n```json\n{\n  \"status\": \"success\",\n  \"links\": [\n    \"https://docs.firecrawl.dev\",\n    \"https://docs.firecrawl.dev/sdks/python\",\n    \"https://docs.firecrawl.dev/learn/rag-llama3\",\n  ]\n}\n```\n\n### Search\n\nSearch the web and get full content from results\n\nFirecrawl\u2019s search API allows you to perform web searches and optionally scrape the search results in one operation.\n\n- Choose specific output formats (markdown, HTML, links, screenshots)\n- Search the web with customizable parameters (language, country, etc.)\n- Optionally retrieve content from search results in various formats\n- Control the number of results and set timeouts\n\n```bash\ncurl -X POST https://api.firecrawl.dev/v1/search \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer fc-YOUR_API_KEY\" \\\n  -d '{\n    \"query\": \"what is firecrawl?\",\n    \"limit\": 5\n  }'\n```\n\n#### Response\n\n```json\n{\n  \"success\": true,\n  \"data\": [\n    {\n      \"url\": \"https://firecrawl.dev\",\n      \"title\": \"Firecrawl | Home Page\",\n      \"description\": \"Turn websites into LLM-ready data with Firecrawl\"\n    },\n    {\n      \"url\": \"https://docs.firecrawl.dev\",\n      \"title\": \"Documentation | Firecrawl\",\n      \"description\": \"Learn how to use Firecrawl in your own applications\"\n    }\n  ]\n}\n```\n\n#### With content scraping\n\n```bash\ncurl -X POST https://api.firecrawl.dev/v1/search \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer fc-YOUR_API_KEY\" \\\n  -d '{\n    \"query\": \"what is firecrawl?\",\n    \"limit\": 5,\n    \"scrapeOptions\": {\n      \"formats\": [\"markdown\", \"links\"]\n    }\n  }'\n```\n\n### Extract (Beta)\n\nGet structured data from entire websites with a prompt and/or a schema.\n\nYou can extract structured data from one or multiple URLs, including wildcards:\n\nSingle Page:\nExample: https://firecrawl.dev/some-page\n\nMultiple Pages / Full Domain\nExample: https://firecrawl.dev/*\n\nWhen you use /*, Firecrawl will automatically crawl and parse all URLs it can discover in that domain, then extract the requested data.\n\n```bash\ncurl -X POST https://api.firecrawl.dev/v1/extract \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"urls\": [\n        \"https://firecrawl.dev/*\", \n        \"https://docs.firecrawl.dev/\", \n        \"https://www.ycombinator.com/companies\"\n      ],\n      \"prompt\": \"Extract the company mission, whether it is open source, and whether it is in Y Combinator from the page.\",\n      \"schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"company_mission\": {\n            \"type\": \"string\"\n          },\n          \"is_open_source\": {\n            \"type\": \"boolean\"\n          },\n          \"is_in_yc\": {\n            \"type\": \"boolean\"\n          }\n        },\n        \"required\": [\n          \"company_mission\",\n          \"is_open_source\",\n          \"is_in_yc\"\n        ]\n      }\n    }'\n```\n\n```json\n{\n  \"success\": true,\n  \"id\": \"44aa536d-f1cb-4706-ab87-ed0386685740\",\n  \"urlTrace\": []\n}\n```\n\nIf you are using the sdks, it will auto pull the response for you:\n\n```json\n{\n  \"success\": true,\n  \"data\": {\n    \"company_mission\": \"Firecrawl is the easiest way to extract data from the web. Developers use us to reliably convert URLs into LLM-ready markdown or structured data with a single API call.\",\n    \"supports_sso\": false,\n    \"is_open_source\": true,\n    \"is_in_yc\": true\n  }\n}\n```\n\n### LLM Extraction (Beta)\n\nUsed to extract structured data from scraped pages.\n\n```bash\ncurl -X POST https://api.firecrawl.dev/v1/scrape \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"url\": \"https://www.mendable.ai/\",\n      \"formats\": [\"json\"],\n      \"jsonOptions\": {\n        \"schema\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"company_mission\": {\n                      \"type\": \"string\"\n            },\n            \"supports_sso\": {\n                      \"type\": \"boolean\"\n            },\n            \"is_open_source\": {\n                      \"type\": \"boolean\"\n            },\n            \"is_in_yc\": {\n                      \"type\": \"boolean\"\n            }\n          },\n          \"required\": [\n            \"company_mission\",\n            \"supports_sso\",\n            \"is_open_source\",\n            \"is_in_yc\"\n          ]\n        }\n      }\n    }'\n```\n\n```json\n{\n  \"success\": true,\n  \"data\": {\n    \"content\": \"Raw Content\",\n    \"metadata\": {\n      \"title\": \"Mendable\",\n      \"description\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\n      \"robots\": \"follow, index\",\n      \"ogTitle\": \"Mendable\",\n      \"ogDescription\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\n      \"ogUrl\": \"https://mendable.ai/\",\n      \"ogImage\": \"https://mendable.ai/mendable_new_og1.png\",\n      \"ogLocaleAlternate\": [],\n      \"ogSiteName\": \"Mendable\",\n      \"sourceURL\": \"https://mendable.ai/\"\n    },\n    \"json\": {\n      \"company_mission\": \"Train a secure AI on your technical resources that answers customer and employee questions so your team doesn't have to\",\n      \"supports_sso\": true,\n      \"is_open_source\": false,\n      \"is_in_yc\": true\n    }\n  }\n}\n```\n\n### Extracting without a schema (New)\n\nYou can now extract without a schema by just passing a `prompt` to the endpoint. The llm chooses the structure of the data.\n\n```bash\ncurl -X POST https://api.firecrawl.dev/v1/scrape \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"url\": \"https://docs.firecrawl.dev/\",\n      \"formats\": [\"json\"],\n      \"jsonOptions\": {\n        \"prompt\": \"Extract the company mission from the page.\"\n      }\n    }'\n```\n\n### Interacting with the page with Actions (Cloud-only)\n\nFirecrawl allows you to perform various actions on a web page before scraping its content. This is particularly useful for interacting with dynamic content, navigating through pages, or accessing content that requires user interaction.\n\nHere is an example of how to use actions to navigate to google.com, search for Firecrawl, click on the first result, and take a screenshot.\n\n```bash\ncurl -X POST https://api.firecrawl.dev/v1/scrape \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n        \"url\": \"google.com\",\n        \"formats\": [\"markdown\"],\n        \"actions\": [\n            {\"type\": \"wait\", \"milliseconds\": 2000},\n            {\"type\": \"click\", \"selector\": \"textarea[title=\\\"Search\\\"]\"},\n            {\"type\": \"wait\", \"milliseconds\": 2000},\n            {\"type\": \"write\", \"text\": \"firecrawl\"},\n            {\"type\": \"wait\", \"milliseconds\": 2000},\n            {\"type\": \"press\", \"key\": \"ENTER\"},\n            {\"type\": \"wait\", \"milliseconds\": 3000},\n            {\"type\": \"click\", \"selector\": \"h3\"},\n            {\"type\": \"wait\", \"milliseconds\": 3000},\n            {\"type\": \"screenshot\"}\n        ]\n    }'\n```\n\n### Batch Scraping Multiple URLs (New)\n\nYou can now batch scrape multiple URLs at the same time. It is very similar to how the /crawl endpoint works. It submits a batch scrape job and returns a job ID to check the status of the batch scrape.\n\n```bash\ncurl -X POST https://api.firecrawl.dev/v1/batch/scrape \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"urls\": [\"https://docs.firecrawl.dev\", \"https://docs.firecrawl.dev/sdks/overview\"],\n      \"formats\" : [\"markdown\", \"html\"]\n    }'\n```\n\n\n\n## Using Python SDK\n\n### Installing Python SDK\n\n```bash\npip install firecrawl-py\n```\n\n### Crawl a website\n\n```python\nfrom firecrawl.firecrawl import FirecrawlApp\nfrom firecrawl.firecrawl import ScrapeOptions\n\napp = FirecrawlApp(api_key=\"fc-YOUR_API_KEY\")\n\n# Scrape a website:\nscrape_status = app.scrape_url(\n  'https://firecrawl.dev', \n  formats=[\"markdown\", \"html\"]\n)\nprint(scrape_status)\n\n# Crawl a website:\ncrawl_status = app.crawl_url(\n  'https://firecrawl.dev',\n  limit=100,\n  scrape_options=ScrapeOptions(\n    formats=[\"markdown\", \"html\"],),\n  poll_interval=30\n)\nprint(crawl_status)\n```\n\n### Extracting structured data from a URL\n\nWith LLM extraction, you can easily extract structured data from any URL. We support pydantic schemas to make it easier for you too. Here is how you to use it:\n\n```python\nclass ArticleSchema(BaseModel):\n    title: str\n    points: int \n    by: str\n    commentsURL: str\n\nclass TopArticlesSchema(BaseModel):\n    top: List[ArticleSchema] = Field(..., description=\"Top 5 stories\")\n\njson_config = JsonConfig(schema=TopArticlesSchema.model_json_schema())\n\nllm_extraction_result = app.scrape_url('https://news.ycombinator.com', formats=[\"json\"], json=json_config)\n\nprint(llm_extraction_result.json)\n```\n\n## Using the Node SDK\n\n### Installation\n\nTo install the Firecrawl Node SDK, you can use npm:\n\n```bash\nnpm install @mendable/firecrawl-js\n```\n\n### Usage\n\n1. Get an API key from [firecrawl.dev](https://firecrawl.dev)\n2. Set the API key as an environment variable named `FIRECRAWL_API_KEY` or pass it as a parameter to the `FirecrawlApp` class.\n\n```js\nimport FirecrawlApp, { CrawlParams, CrawlStatusResponse } from '@mendable/firecrawl-js';\n\nconst app = new FirecrawlApp({apiKey: \"fc-YOUR_API_KEY\"});\n\n// Scrape a website\nconst scrapeResponse = await app.scrapeUrl('https://firecrawl.dev', {\n  formats: ['markdown', 'html'],\n});\n\nif (scrapeResponse) {\n  console.log(scrapeResponse)\n}\n\n// Crawl a website\nconst crawlResponse = await app.crawlUrl('https://firecrawl.dev', {\n  limit: 100,\n  scrapeOptions: {\n    formats: ['markdown', 'html'],\n  }\n} satisfies CrawlParams, true, 30) satisfies CrawlStatusResponse;\n\nif (crawlResponse) {\n  console.log(crawlResponse)\n}\n```\n\n\n### Extracting structured data from a URL\n\nWith LLM extraction, you can easily extract structured data from any URL. We support zod schema to make it easier for you too. Here is how to use it:\n\n```js\nimport FirecrawlApp from \"@mendable/firecrawl-js\";\nimport { z } from \"zod\";\n\nconst app = new FirecrawlApp({\n  apiKey: \"fc-YOUR_API_KEY\"\n});\n\n// Define schema to extract contents into\nconst schema = z.object({\n  top: z\n    .array(\n      z.object({\n        title: z.string(),\n        points: z.number(),\n        by: z.string(),\n        commentsURL: z.string(),\n      })\n    )\n    .length(5)\n    .describe(\"Top 5 stories on Hacker News\"),\n});\n\nconst scrapeResult = await app.scrapeUrl(\"https://news.ycombinator.com\", {\n  jsonOptions: { extractionSchema: schema },\n});\n\nconsole.log(scrapeResult.data[\"json\"]);\n```\n\n## Open Source vs Cloud Offering\n\nFirecrawl is open source available under the AGPL-3.0 license. \n\nTo deliver the best possible product, we offer a hosted version of Firecrawl alongside our open-source offering. The cloud solution allows us to continuously innovate and maintain a high-quality, sustainable service for all users.\n\nFirecrawl Cloud is available at [firecrawl.dev](https://firecrawl.dev) and offers a range of features that are not available in the open source version:\n\n![Open Source vs Cloud Offering](https://raw.githubusercontent.com/mendableai/firecrawl/main/img/open-source-cloud.png)\n\n\n## Contributing\n\nWe love contributions! Please read our [contributing guide](CONTRIBUTING.md) before submitting a pull request. If you'd like to self-host, refer to the [self-hosting guide](SELF_HOST.md).\n\n_It is the sole responsibility of the end users to respect websites' policies when scraping, searching and crawling with Firecrawl. Users are advised to adhere to the applicable privacy policies and terms of use of the websites prior to initiating any scraping activities. By default, Firecrawl respects the directives specified in the websites' robots.txt files when crawling. By utilizing Firecrawl, you expressly agree to comply with these conditions._\n\n## Contributors\n\n<a href=\"https://github.com/mendableai/firecrawl/graphs/contributors\">\n  <img alt=\"contributors\" src=\"https://contrib.rocks/image?repo=mendableai/firecrawl\"/>\n</a>\n\n## License Disclaimer\n\nThis project is primarily licensed under the GNU Affero General Public License v3.0 (AGPL-3.0), as specified in the LICENSE file in the root directory of this repository. However, certain components of this project are licensed under the MIT License. Refer to the LICENSE files in these specific directories for details.\n\nPlease note:\n\n- The AGPL-3.0 license applies to all parts of the project unless otherwise specified.\n- The SDKs and some UI components are licensed under the MIT License. Refer to the LICENSE files in these specific directories for details.\n- When using or contributing to this project, ensure you comply with the appropriate license terms for the specific component you are working with.\n\nFor more details on the licensing of specific components, please refer to the LICENSE files in the respective directories or contact the project maintainers.\n\n\n<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n    <a href=\"#readme-top\" style=\"text-decoration: none; color: #007bff; font-weight: bold;\">\n        \u2191 Back to Top \u2191\n    </a>\n</p>\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 560704231,
    "name": "llama_index",
    "full_name": "run-llama/llama_index",
    "description": "LlamaIndex is the leading framework for building LLM-powered agents over your data.",
    "html_url": "https://github.com/run-llama/llama_index",
    "clone_url": "https://github.com/run-llama/llama_index.git",
    "owner_login": "run-llama",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/130722866?v=4",
    "stargazers_count": 43533,
    "watchers_count": 43533,
    "forks_count": 6254,
    "open_issues_count": 290,
    "size": 328183,
    "language": "Python",
    "languages": {
      "Python": 13647551,
      "Jupyter Notebook": 10103417,
      "Makefile": 439601,
      "JavaScript": 85982,
      "Tree-sitter Query": 17376,
      "EdgeQL": 3396,
      "Shell": 3332,
      "Starlark": 255
    },
    "topics": [
      "agents",
      "application",
      "data",
      "fine-tuning",
      "framework",
      "llamaindex",
      "llm",
      "multi-agents",
      "rag",
      "vector-database"
    ],
    "license_name": "MIT License",
    "created_at": "2022-11-02T04:24:54+00:00",
    "updated_at": "2025-08-06T01:59:34+00:00",
    "pushed_at": "2025-08-05T22:12:29+00:00",
    "contributors_count": 100,
    "readme_length": 10821,
    "readme_content": "# \ud83d\uddc2\ufe0f LlamaIndex \ud83e\udd99\n\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/llama-index)](https://pypi.org/project/llama-index/)\n[![Build](https://github.com/run-llama/llama_index/actions/workflows/build_package.yml/badge.svg)](https://github.com/run-llama/llama_index/actions/workflows/build_package.yml)\n[![GitHub contributors](https://img.shields.io/github/contributors/jerryjliu/llama_index)](https://github.com/jerryjliu/llama_index/graphs/contributors)\n[![Discord](https://img.shields.io/discord/1059199217496772688)](https://discord.gg/dGcwcsnxhU)\n[![Twitter](https://img.shields.io/twitter/follow/llama_index)](https://x.com/llama_index)\n[![Reddit](https://img.shields.io/reddit/subreddit-subscribers/LlamaIndex?style=plastic&logo=reddit&label=r%2FLlamaIndex&labelColor=white)](https://www.reddit.com/r/LlamaIndex/)\n[![Ask AI](https://img.shields.io/badge/Phorm-Ask_AI-%23F2777A.svg?&logo=data:image/svg+xml;base64,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)](https://www.phorm.ai/query?projectId=c5863b56-6703-4a5d-87b6-7e6031bf16b6)\n\nLlamaIndex (GPT Index) is a data framework for your LLM application. Building with LlamaIndex typically involves working with LlamaIndex core and a chosen set of integrations (or plugins). There are two ways to start building with LlamaIndex in\nPython:\n\n1. **Starter**: [`llama-index`](https://pypi.org/project/llama-index/). A starter Python package that includes core LlamaIndex as well as a selection of integrations.\n\n2. **Customized**: [`llama-index-core`](https://pypi.org/project/llama-index-core/). Install core LlamaIndex and add your chosen LlamaIndex integration packages on [LlamaHub](https://llamahub.ai/)\n   that are required for your application. There are over 300 LlamaIndex integration\n   packages that work seamlessly with core, allowing you to build with your preferred\n   LLM, embedding, and vector store providers.\n\nThe LlamaIndex Python library is namespaced such that import statements which\ninclude `core` imply that the core package is being used. In contrast, those\nstatements without `core` imply that an integration package is being used.\n\n```python\n# typical pattern\nfrom llama_index.core.xxx import ClassABC  # core submodule xxx\nfrom llama_index.xxx.yyy import (\n    SubclassABC,\n)  # integration yyy for submodule xxx\n\n# concrete example\nfrom llama_index.core.llms import LLM\nfrom llama_index.llms.openai import OpenAI\n```\n\n### Important Links\n\nLlamaIndex.TS [(Typescript/Javascript)](https://github.com/run-llama/LlamaIndexTS)\n\n[Documentation](https://docs.llamaindex.ai/en/stable/)\n\n[X (formerly Twitter)](https://x.com/llama_index)\n\n[LinkedIn](https://www.linkedin.com/company/llamaindex/)\n\n[Reddit](https://www.reddit.com/r/LlamaIndex/)\n\n[Discord](https://discord.gg/dGcwcsnxhU)\n\n### Ecosystem\n\n- LlamaHub [(community library of data loaders)](https://llamahub.ai)\n- LlamaLab [(cutting-edge AGI projects using LlamaIndex)](https://github.com/run-llama/llama-lab)\n\n## \ud83d\ude80 Overview\n\n**NOTE**: This README is not updated as frequently as the documentation. Please check out the documentation above for the latest updates!\n\n### Context\n\n- LLMs are a phenomenal piece of technology for knowledge generation and reasoning. They are pre-trained on large amounts of publicly available data.\n- How do we best augment LLMs with our own private data?\n\nWe need a comprehensive toolkit to help perform this data augmentation for LLMs.\n\n### Proposed Solution\n\nThat's where **LlamaIndex** comes in. LlamaIndex is a \"data framework\" to help you build LLM apps. It provides the following tools:\n\n- Offers **data connectors** to ingest your existing data sources and data formats (APIs, PDFs, docs, SQL, etc.).\n- Provides ways to **structure your data** (indices, graphs) so that this data can be easily used with LLMs.\n- Provides an **advanced retrieval/query interface over your data**: Feed in any LLM input prompt, get back retrieved context and knowledge-augmented output.\n- Allows easy integrations with your outer application framework (e.g. with LangChain, Flask, Docker, ChatGPT, or anything else).\n\nLlamaIndex provides tools for both beginner users and advanced users. Our high-level API allows beginner users to use LlamaIndex to ingest and query their data in\n5 lines of code. Our lower-level APIs allow advanced users to customize and extend any module (data connectors, indices, retrievers, query engines, reranking modules),\nto fit their needs.\n\n## \ud83d\udca1 Contributing\n\nInterested in contributing? Contributions to LlamaIndex core as well as contributing\nintegrations that build on the core are both accepted and highly encouraged! See our [Contribution Guide](CONTRIBUTING.md) for more details.\n\nNew integrations should meaningfully integrate with existing LlamaIndex framework components. At the discretion of LlamaIndex maintainers, some integrations may be declined.\n\n## \ud83d\udcc4 Documentation\n\nFull documentation can be found [here](https://docs.llamaindex.ai/en/latest/)\n\nPlease check it out for the most up-to-date tutorials, how-to guides, references, and other resources!\n\n## \ud83d\udcbb Example Usage\n\n```sh\n# custom selection of integrations to work with core\npip install llama-index-core\npip install llama-index-llms-openai\npip install llama-index-llms-replicate\npip install llama-index-embeddings-huggingface\n```\n\nExamples are in the `docs/examples` folder. Indices are in the `indices` folder (see list of indices below).\n\nTo build a simple vector store index using OpenAI:\n\n```python\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\"YOUR_DATA_DIRECTORY\").load_data()\nindex = VectorStoreIndex.from_documents(documents)\n```\n\nTo build a simple vector store index using non-OpenAI LLMs, e.g. Llama 2 hosted on [Replicate](https://replicate.com/), where you can easily create a free trial API token:\n\n```python\nimport os\n\nos.environ[\"REPLICATE_API_TOKEN\"] = \"YOUR_REPLICATE_API_TOKEN\"\n\nfrom llama_index.core import Settings, VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\nfrom llama_index.llms.replicate import Replicate\nfrom transformers import AutoTokenizer\n\n# set the LLM\nllama2_7b_chat = \"meta/llama-2-7b-chat:8e6975e5ed6174911a6ff3d60540dfd4844201974602551e10e9e87ab143d81e\"\nSettings.llm = Replicate(\n    model=llama2_7b_chat,\n    temperature=0.01,\n    additional_kwargs={\"top_p\": 1, \"max_new_tokens\": 300},\n)\n\n# set tokenizer to match LLM\nSettings.tokenizer = AutoTokenizer.from_pretrained(\n    \"NousResearch/Llama-2-7b-chat-hf\"\n)\n\n# set the embed model\nSettings.embed_model = HuggingFaceEmbedding(\n    model_name=\"BAAI/bge-small-en-v1.5\"\n)\n\ndocuments = SimpleDirectoryReader(\"YOUR_DATA_DIRECTORY\").load_data()\nindex = VectorStoreIndex.from_documents(\n    documents,\n)\n```\n\nTo query:\n\n```python\nquery_engine = index.as_query_engine()\nquery_engine.query(\"YOUR_QUESTION\")\n```\n\nBy default, data is stored in-memory.\nTo persist to disk (under `./storage`):\n\n```python\nindex.storage_context.persist()\n```\n\nTo reload from disk:\n\n```python\nfrom llama_index.core import StorageContext, load_index_from_storage\n\n# rebuild storage context\nstorage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n# load index\nindex = load_index_from_storage(storage_context)\n```\n\n## \ud83d\udd27 Dependencies\n\nWe use poetry as the package manager for all Python packages. As a result, the\ndependencies of each Python package can be found by referencing the `pyproject.toml`\nfile in each of the package's folders.\n\n```bash\ncd <desired-package-folder>\npip install poetry\npoetry install --with dev\n```\n\n## \ud83d\udcd6 Citation\n\nReference to cite if you use LlamaIndex in a paper:\n\n```\n@software{Liu_LlamaIndex_2022,\nauthor = {Liu, Jerry},\ndoi = {10.5281/zenodo.1234},\nmonth = {11},\ntitle = {{LlamaIndex}},\nurl = {https://github.com/jerryjliu/llama_index},\nyear = {2022}\n}\n```\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 725205304,
    "name": "unsloth",
    "full_name": "unslothai/unsloth",
    "description": "Fine-tuning & Reinforcement Learning for LLMs. \ud83e\udda5 Train gpt-oss, Qwen3, Llama 4, DeepSeek-R1, Gemma 3, TTS 2x faster with 70% less VRAM.",
    "html_url": "https://github.com/unslothai/unsloth",
    "clone_url": "https://github.com/unslothai/unsloth.git",
    "owner_login": "unslothai",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/150920049?v=4",
    "stargazers_count": 43259,
    "watchers_count": 43259,
    "forks_count": 3479,
    "open_issues_count": 759,
    "size": 7502,
    "language": "Python",
    "languages": {
      "Python": 1428345,
      "Shell": 1156
    },
    "topics": [
      "agent",
      "ai",
      "deepseek",
      "deepseek-r1",
      "fine-tuning",
      "gemma",
      "gemma3",
      "llama",
      "llama3",
      "llm",
      "llms",
      "lora",
      "mistral",
      "openai",
      "pytorch",
      "qwen",
      "qwen3",
      "text-to-speech",
      "tts",
      "unsloth"
    ],
    "license_name": "Apache License 2.0",
    "created_at": "2023-11-29T16:50:09+00:00",
    "updated_at": "2025-08-06T02:12:47+00:00",
    "pushed_at": "2025-08-02T10:39:39+00:00",
    "contributors_count": 63,
    "readme_length": 25896,
    "readme_content": "<div align=\"center\">\n\n  <a href=\"https://unsloth.ai\"><picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20white%20text.png\">\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png\">\n    <img alt=\"unsloth logo\" src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png\" height=\"110\" style=\"max-width: 100%;\">\n  </picture></a>\n  \n<a href=\"https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/start free finetune button.png\" width=\"154\"></a>\n<a href=\"https://discord.com/invite/unsloth\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png\" width=\"165\"></a>\n<a href=\"https://docs.unsloth.ai\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/Documentation%20Button.png\" width=\"137\"></a>\n\n### Finetune Gemma 3n, Qwen3, Llama 4, Phi-4 & Mistral 2x faster with 80% less VRAM!\n\n![](https://i.ibb.co/sJ7RhGG/image-41.png)\n\n</div>\n\n## \u2728 Finetune for Free\n\nNotebooks are beginner friendly. Read our [guide](https://docs.unsloth.ai/get-started/fine-tuning-guide). Add your dataset, click \"Run All\", and export your finetuned model to GGUF, Ollama, vLLM or Hugging Face.\n\n| Unsloth supports | Free Notebooks | Performance | Memory use |\n|-----------|---------|--------|----------|\n| **Gemma 3n (4B)**      | [\u25b6\ufe0f Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3N_(4B)-Conversational.ipynb)               | 1.5x faster | 50% less |\n| **Qwen3 (14B)**      | [\u25b6\ufe0f Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(14B)-Reasoning-Conversational.ipynb)               | 2x faster | 70% less |\n| **Qwen3 (4B): GRPO**      | [\u25b6\ufe0f Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb)               | 2x faster | 80% less |\n| **Gemma 3 (4B)**      | [\u25b6\ufe0f Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(4B).ipynb)               | 1.6x faster | 60% less |\n| **Llama 3.2 (3B)**      | [\u25b6\ufe0f Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)               | 2x faster | 70% less |\n| **Phi-4 (14B)** | [\u25b6\ufe0f Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb)               | 2x faster | 70% less |\n| **Llama 3.2 Vision (11B)**      | [\u25b6\ufe0f Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)               | 2x faster | 50% less |\n| **Llama 3.1 (8B)**      | [\u25b6\ufe0f Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb)               | 2x faster | 70% less |\n| **Mistral v0.3 (7B)**    | [\u25b6\ufe0f Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-Conversational.ipynb)               | 2.2x faster | 75% less |\n| **Orpheus-TTS (3B)**     | [\u25b6\ufe0f Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Orpheus_(3B)-TTS.ipynb)               | 1.5x faster | 50% less |\n\n- See all our notebooks for: [Kaggle](https://github.com/unslothai/notebooks?tab=readme-ov-file#-kaggle-notebooks), [GRPO](https://docs.unsloth.ai/get-started/unsloth-notebooks#grpo-reasoning-rl-notebooks), **[TTS](https://docs.unsloth.ai/get-started/unsloth-notebooks#text-to-speech-tts-notebooks)** & [Vision](https://docs.unsloth.ai/get-started/unsloth-notebooks#vision-multimodal-notebooks)\n- See [all our models](https://docs.unsloth.ai/get-started/all-our-models) and [all our notebooks](https://github.com/unslothai/notebooks)\n- See detailed documentation for Unsloth [here](https://docs.unsloth.ai/)\n\n## \u26a1 Quickstart\n\n- **Install with pip (recommended)** for Linux devices:\n```\npip install unsloth\n```\nFor Windows install instructions, see [here](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation).\n\n## \ud83e\udda5 Unsloth.ai News\n- \ud83d\udce3 **Gemma 3n** by Google: [Read Blog](https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune). We [uploaded GGUFs, 4-bit models](https://huggingface.co/collections/unsloth/gemma-3n-685d3874830e49e1c93f9339).\n- \ud83d\udce3 **[Text-to-Speech (TTS)](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning)** is now supported, including `sesame/csm-1b` and STT `openai/whisper-large-v3`.\n- \ud83d\udce3 **[Qwen3](https://docs.unsloth.ai/basics/qwen3-how-to-run-and-fine-tune)** is now supported. Qwen3-30B-A3B fits on 17.5GB VRAM.\n- \ud83d\udce3 Introducing **[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)** quants that set new benchmarks on 5-shot MMLU & KL Divergence.\n- \ud83d\udce3 **[Llama 4](https://unsloth.ai/blog/llama4)** by Meta, including Scout & Maverick are now supported.\n- \ud83d\udce3 [**EVERYTHING** is now supported](https://unsloth.ai/blog/gemma3#everything) - all models (BERT, diffusion, Cohere, Mamba), FFT, etc. MultiGPU coming soon. Enable FFT with `full_finetuning = True`, 8-bit with `load_in_8bit = True`.\n- \ud83d\udce3 Introducing Long-context [Reasoning (GRPO)](https://unsloth.ai/blog/grpo) in Unsloth. Train your own reasoning model with just 5GB VRAM. Transform Llama, Phi, Mistral etc. into reasoning LLMs!\n- \ud83d\udce3 [DeepSeek-R1](https://unsloth.ai/blog/deepseek-r1) - run or fine-tune them [with our guide](https://unsloth.ai/blog/deepseek-r1). All model uploads: [here](https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5).\n<details>\n  <summary>Click for more news</summary>\n\n- \ud83d\udce3 Introducing Unsloth [Dynamic 4-bit Quantization](https://unsloth.ai/blog/dynamic-4bit)! We dynamically opt not to quantize certain parameters and this greatly increases accuracy while only using <10% more VRAM than BnB 4-bit. See our collection on [Hugging Face here.](https://huggingface.co/collections/unsloth/unsloth-4-bit-dynamic-quants-67503bb873f89e15276c44e7)\n- \ud83d\udce3 [Phi-4](https://unsloth.ai/blog/phi4) by Microsoft: We also [fixed bugs](https://unsloth.ai/blog/phi4) in Phi-4 and [uploaded GGUFs, 4-bit](https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa).\n- \ud83d\udce3 [Vision models](https://unsloth.ai/blog/vision) now supported! [Llama 3.2 Vision (11B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb), [Qwen 2.5 VL (7B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_VL_(7B)-Vision.ipynb) and [Pixtral (12B) 2409](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Pixtral_(12B)-Vision.ipynb)\n- \ud83d\udce3 [Llama 3.3 (70B)](https://huggingface.co/collections/unsloth/llama-33-all-versions-67535d7d994794b9d7cf5e9f), Meta's latest model is supported.\n- \ud83d\udce3 We worked with Apple to add [Cut Cross Entropy](https://arxiv.org/abs/2411.09009). Unsloth now supports 89K context for Meta's Llama 3.3 (70B) on a 80GB GPU - 13x longer than HF+FA2. For Llama 3.1 (8B), Unsloth enables 342K context, surpassing its native 128K support.\n- \ud83d\udce3 We found and helped fix a [gradient accumulation bug](https://unsloth.ai/blog/gradient)! Please update Unsloth and transformers.\n- \ud83d\udce3 We cut memory usage by a [further 30%](https://unsloth.ai/blog/long-context) and now support [4x longer context windows](https://unsloth.ai/blog/long-context)!\n</details>\n\n## \ud83d\udd17 Links and Resources\n| Type                            | Links                               |\n| ------------------------------- | --------------------------------------- |\n| \ud83d\udcda **Documentation & Wiki**              | [Read Our Docs](https://docs.unsloth.ai) |\n| <img width=\"16\" src=\"https://upload.wikimedia.org/wikipedia/commons/6/6f/Logo_of_Twitter.svg\" />&nbsp; **Twitter (aka X)**              |  [Follow us on X](https://twitter.com/unslothai)|\n| \ud83d\udcbe **Installation**               | [Pip install](https://docs.unsloth.ai/get-started/installing-+-updating)|\n| \ud83d\udd2e **Our Models**            | [Unsloth Releases](https://docs.unsloth.ai/get-started/all-our-models)|\n| \u270d\ufe0f **Blog**                    | [Read our Blogs](https://unsloth.ai/blog)|\n| <img width=\"15\" src=\"https://redditinc.com/hs-fs/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" />&nbsp; **Reddit**                    | [Join our Reddit](https://reddit.com/r/unsloth)|\n\n## \u2b50 Key Features\n- Supports **full-finetuning**, pretraining, 4b-bit, 16-bit and **8-bit** training\n- Supports **all transformer-style models** including [TTS, STT](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning), multimodal, diffusion, [BERT](https://docs.unsloth.ai/get-started/unsloth-notebooks#other-important-notebooks) and more!\n- All kernels written in [OpenAI's Triton](https://openai.com/index/triton/) language. **Manual backprop engine**.\n- **0% loss in accuracy** - no approximation methods - all exact.\n- No change of hardware. Supports NVIDIA GPUs since 2018+. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc) [Check your GPU!](https://developer.nvidia.com/cuda-gpus) GTX 1070, 1080 works, but is slow.\n- Works on **Linux** and **Windows**\n- If you trained a model with \ud83e\udda5Unsloth, you can use this cool sticker! &nbsp; <img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/made with unsloth.png\" width=\"200\" align=\"center\" />\n\n## \ud83d\udcbe Install Unsloth\nYou can also see our documentation for more detailed installation and updating instructions [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n\n### Pip Installation\n**Install with pip (recommended) for Linux devices:**\n```\npip install unsloth\n```\n**To update Unsloth:**\n```\npip install --upgrade --force-reinstall --no-cache-dir unsloth unsloth_zoo\n```\nSee [here](https://github.com/unslothai/unsloth/edit/main/README.md#advanced-pip-installation) for advanced pip install instructions.\n### Windows Installation\n> [!warning]\n> Python 3.13 does not support Unsloth. Use 3.12, 3.11 or 3.10\n\n1. **Install NVIDIA Video Driver:**\n  You should install the latest version of your GPUs driver. Download drivers here: [NVIDIA GPU Drive](https://www.nvidia.com/Download/index.aspx).\n\n3. **Install Visual Studio C++:**\n   You will need Visual Studio, with C++ installed. By default, C++ is not installed with [Visual Studio](https://visualstudio.microsoft.com/vs/community/), so make sure you select all of the C++ options. Also select options for Windows 10/11 SDK. For detailed instructions with options, see [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n\n5. **Install CUDA Toolkit:**\n   Follow the instructions to install [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit-archive).\n\n6. **Install PyTorch:**\n   You will need the correct version of PyTorch that is compatible with your CUDA drivers, so make sure to select them carefully.\n   [Install PyTorch](https://pytorch.org/get-started/locally/).\n\n7. **Install Unsloth:**\n   \n```python\npip install unsloth\n```\n\n#### Notes\nTo run Unsloth directly on Windows:\n- Install Triton from this Windows fork and follow the instructions [here](https://github.com/woct0rdho/triton-windows) (be aware that the Windows fork requires PyTorch >= 2.4 and CUDA 12)\n- In the `SFTConfig`, set `dataset_num_proc=1` to avoid a crashing issue:\n```python\nSFTConfig(\n    dataset_num_proc=1,\n    ...\n)\n```\n\n#### Advanced/Troubleshooting\n\nFor **advanced installation instructions** or if you see weird errors during installations:\n\n1. Install `torch` and `triton`. Go to https://pytorch.org to install it. For example `pip install torch torchvision torchaudio triton`\n2. Confirm if CUDA is installed correctly. Try `nvcc`. If that fails, you need to install `cudatoolkit` or CUDA drivers.\n3. Install `xformers` manually. You can try installing `vllm` and seeing if `vllm` succeeds. Check if `xformers` succeeded with `python -m xformers.info` Go to https://github.com/facebookresearch/xformers. Another option is to install `flash-attn` for Ampere GPUs.\n4. Double check that your versions of Python, CUDA, CUDNN, `torch`, `triton`, and `xformers` are compatible with one another. The [PyTorch Compatibility Matrix](https://github.com/pytorch/pytorch/blob/main/RELEASE.md#release-compatibility-matrix) may be useful. \n5. Finally, install `bitsandbytes` and check it with `python -m bitsandbytes`\n\n### Conda Installation (Optional)\n`\u26a0\ufe0fOnly use Conda if you have it. If not, use Pip`. Select either `pytorch-cuda=11.8,12.1` for CUDA 11.8 or CUDA 12.1. We support `python=3.10,3.11,3.12`.\n```bash\nconda create --name unsloth_env \\\n    python=3.11 \\\n    pytorch-cuda=12.1 \\\n    pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers \\\n    -y\nconda activate unsloth_env\n\npip install unsloth\n```\n\n<details>\n  <summary>If you're looking to install Conda in a Linux environment, <a href=\"https://docs.anaconda.com/miniconda/\">read here</a>, or run the below \ud83d\udd3d</summary>\n  \n  ```bash\n  mkdir -p ~/miniconda3\n  wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh\n  bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3\n  rm -rf ~/miniconda3/miniconda.sh\n  ~/miniconda3/bin/conda init bash\n  ~/miniconda3/bin/conda init zsh\n  ```\n</details>\n\n### Advanced Pip Installation\n`\u26a0\ufe0fDo **NOT** use this if you have Conda.` Pip is a bit more complex since there are dependency issues. The pip command is different for `torch 2.2,2.3,2.4,2.5` and CUDA versions.\n\nFor other torch versions, we support `torch211`, `torch212`, `torch220`, `torch230`, `torch240` and for CUDA versions, we support `cu118` and `cu121` and `cu124`. For Ampere devices (A100, H100, RTX3090) and above, use `cu118-ampere` or `cu121-ampere` or `cu124-ampere`.\n\nFor example, if you have `torch 2.4` and `CUDA 12.1`, use:\n```bash\npip install --upgrade pip\npip install \"unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git\"\n```\n\nAnother example, if you have `torch 2.5` and `CUDA 12.4`, use:\n```bash\npip install --upgrade pip\npip install \"unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git\"\n```\n\nAnd other examples:\n```bash\npip install \"unsloth[cu121-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git\"\npip install \"unsloth[cu118-ampere-torch240] @ git+https://github.com/unslothai/unsloth.git\"\npip install \"unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git\"\npip install \"unsloth[cu118-torch240] @ git+https://github.com/unslothai/unsloth.git\"\n\npip install \"unsloth[cu121-torch230] @ git+https://github.com/unslothai/unsloth.git\"\npip install \"unsloth[cu121-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git\"\n\npip install \"unsloth[cu121-torch250] @ git+https://github.com/unslothai/unsloth.git\"\npip install \"unsloth[cu124-ampere-torch250] @ git+https://github.com/unslothai/unsloth.git\"\n```\n\nOr, run the below in a terminal to get the **optimal** pip installation command:\n```bash\nwget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -\n```\n\nOr, run the below manually in a Python REPL:\n```python\ntry: import torch\nexcept: raise ImportError('Install torch via `pip install torch`')\nfrom packaging.version import Version as V\nv = V(torch.__version__)\ncuda = str(torch.version.cuda)\nis_ampere = torch.cuda.get_device_capability()[0] >= 8\nif cuda != \"12.1\" and cuda != \"11.8\" and cuda != \"12.4\": raise RuntimeError(f\"CUDA = {cuda} not supported!\")\nif   v <= V('2.1.0'): raise RuntimeError(f\"Torch = {v} too old!\")\nelif v <= V('2.1.1'): x = 'cu{}{}-torch211'\nelif v <= V('2.1.2'): x = 'cu{}{}-torch212'\nelif v  < V('2.3.0'): x = 'cu{}{}-torch220'\nelif v  < V('2.4.0'): x = 'cu{}{}-torch230'\nelif v  < V('2.5.0'): x = 'cu{}{}-torch240'\nelif v  < V('2.6.0'): x = 'cu{}{}-torch250'\nelse: raise RuntimeError(f\"Torch = {v} too new!\")\nx = x.format(cuda.replace(\".\", \"\"), \"-ampere\" if is_ampere else \"\")\nprint(f'pip install --upgrade pip && pip install \"unsloth[{x}] @ git+https://github.com/unslothai/unsloth.git\"')\n```\n\n## \ud83d\udcdc Documentation\n- Go to our official [Documentation](https://docs.unsloth.ai) for saving to GGUF, checkpointing, evaluation and more!\n- We support Huggingface's TRL, Trainer, Seq2SeqTrainer or even Pytorch code!\n- We're in \ud83e\udd17Hugging Face's official docs! Check out the [SFT docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth) and [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)!\n- If you want to download models from the ModelScope community, please use an environment variable: `UNSLOTH_USE_MODELSCOPE=1`, and install the modelscope library by: `pip install modelscope -U`.\n\n> unsloth_cli.py also supports `UNSLOTH_USE_MODELSCOPE=1` to download models and datasets. please remember to use the model and dataset id in the ModelScope community.\n\n```python\nfrom unsloth import FastLanguageModel, FastModel\nimport torch\nfrom trl import SFTTrainer, SFTConfig\nfrom datasets import load_dataset\nmax_seq_length = 2048 # Supports RoPE Scaling internally, so choose any!\n# Get LAION dataset\nurl = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\ndataset = load_dataset(\"json\", data_files = {\"train\" : url}, split = \"train\")\n\n# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\nfourbit_models = [\n    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 2x faster\n    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # 4bit for 405b!\n    \"unsloth/Mistral-Small-Instruct-2409\",     # Mistral 22b 2x faster!\n    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n    \"unsloth/Phi-3-medium-4k-instruct\",\n    \"unsloth/gemma-2-9b-bnb-4bit\",\n    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n\n    \"unsloth/Llama-3.2-1B-bnb-4bit\",           # NEW! Llama 3.2 models\n    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n    \"unsloth/Llama-3.2-3B-bnb-4bit\",\n    \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n\n    \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\" # NEW! Llama 3.3 70B!\n] # More models at https://huggingface.co/unsloth\n\nmodel, tokenizer = FastModel.from_pretrained(\n    model_name = \"unsloth/gemma-3-4B-it\",\n    max_seq_length = 2048, # Choose any for long context!\n    load_in_4bit = True,  # 4 bit quantization to reduce memory\n    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n    full_finetuning = False, # [NEW!] We have full finetuning now!\n    # token = \"hf_...\", # use one if using gated models\n)\n\n# Do model patching and add fast LoRA weights\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16,\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    max_seq_length = max_seq_length,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)\n\ntrainer = SFTTrainer(\n    model = model,\n    train_dataset = dataset,\n    tokenizer = tokenizer,\n    args = SFTConfig(\n        max_seq_length = max_seq_length,\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 10,\n        max_steps = 60,\n        logging_steps = 1,\n        output_dir = \"outputs\",\n        optim = \"adamw_8bit\",\n        seed = 3407,\n    ),\n)\ntrainer.train()\n\n# Go to https://github.com/unslothai/unsloth/wiki for advanced tips like\n# (1) Saving to GGUF / merging to 16bit for vLLM\n# (2) Continued training from a saved LoRA adapter\n# (3) Adding an evaluation loop / OOMs\n# (4) Customized chat templates\n```\n\n<a name=\"RL\"></a>\n## \ud83d\udca1 Reinforcement Learning\nRL including DPO, GRPO, PPO, Reward Modelling, Online DPO all work with Unsloth. We're in \ud83e\udd17Hugging Face's official docs! We're on the [GRPO docs](https://huggingface.co/learn/nlp-course/en/chapter12/6) and the [DPO docs](https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth)! List of RL notebooks:\n\n- Advanced Qwen3 GRPO notebook: [Link](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb)\n- ORPO notebook: [Link](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-ORPO.ipynb)\n- DPO Zephyr notebook: [Link](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Zephyr_(7B)-DPO.ipynb)\n- KTO notebook: [Link](https://colab.research.google.com/drive/1MRgGtLWuZX4ypSfGguFgC-IblTvO2ivM?usp=sharing)\n- SimPO notebook: [Link](https://colab.research.google.com/drive/1Hs5oQDovOay4mFA6Y9lQhVJ8TnbFLFh2?usp=sharing)\n\n<details>\n  <summary>Click for DPO code</summary>\n  \n```python\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Optional set GPU device ID\n\nfrom unsloth import FastLanguageModel\nimport torch\nfrom trl import DPOTrainer, DPOConfig\nmax_seq_length = 2048\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/zephyr-sft-bnb-4bit\",\n    max_seq_length = max_seq_length,\n    load_in_4bit = True,\n)\n\n# Do model patching and add fast LoRA weights\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 64,\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 64,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    max_seq_length = max_seq_length,\n)\n\ndpo_trainer = DPOTrainer(\n    model = model,\n    ref_model = None,\n    train_dataset = YOUR_DATASET_HERE,\n    # eval_dataset = YOUR_DATASET_HERE,\n    tokenizer = tokenizer,\n    args = DPOConfig(\n        per_device_train_batch_size = 4,\n        gradient_accumulation_steps = 8,\n        warmup_ratio = 0.1,\n        num_train_epochs = 3,\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        seed = 42,\n        output_dir = \"outputs\",\n        max_length = 1024,\n        max_prompt_length = 512,\n        beta = 0.1,\n    ),\n)\ndpo_trainer.train()\n```\n</details>\n\n## \ud83e\udd47 Performance Benchmarking\n- For our most detailed benchmarks, read our [Llama 3.3 Blog](https://unsloth.ai/blog/llama3-3).\n- Benchmarking of Unsloth was also conducted by [\ud83e\udd17Hugging Face](https://huggingface.co/blog/unsloth-trl).\n\nWe tested using the Alpaca  Dataset, a batch size of 2, gradient accumulation steps of 4, rank = 32, and applied QLoRA on all linear layers (q, k, v, o, gate, up, down):\n  \n| Model          | VRAM  | \ud83e\udda5 Unsloth speed | \ud83e\udda5 VRAM reduction | \ud83e\udda5 Longer context | \ud83d\ude0a Hugging Face + FA2 |\n|----------------|-------|-----------------|----------------|----------------|--------------------|\n| Llama 3.3 (70B)| 80GB  | 2x              | >75%           | 13x longer     | 1x                 |\n| Llama 3.1 (8B) | 80GB  | 2x              | >70%           | 12x longer     | 1x                 |\n\n### Context length benchmarks\n\n#### Llama 3.1 (8B) max. context length\nWe tested Llama 3.1 (8B) Instruct and did 4bit QLoRA on all linear layers (Q, K, V, O, gate, up and down) with rank = 32 with a batch size of 1. We padded all sequences to a certain maximum sequence length to mimic long context finetuning workloads.\n| GPU VRAM | \ud83e\udda5Unsloth context length | Hugging Face + FA2 |\n|----------|-----------------------|-----------------|\n| 8 GB     | 2,972                 | OOM             |\n| 12 GB    | 21,848                | 932             |\n| 16 GB    | 40,724                | 2,551           |\n| 24 GB    | 78,475                | 5,789           |\n| 40 GB    | 153,977               | 12,264          |\n| 48 GB    | 191,728               | 15,502          |\n| 80 GB    | 342,733               | 28,454          |\n\n#### Llama 3.3 (70B) max. context length\nWe tested Llama 3.3 (70B) Instruct on a 80GB A100 and did 4bit QLoRA on all linear layers (Q, K, V, O, gate, up and down) with rank = 32 with a batch size of 1. We padded all sequences to a certain maximum sequence length to mimic long context finetuning workloads.\n\n| GPU VRAM | \ud83e\udda5Unsloth context length | Hugging Face + FA2 |\n|----------|------------------------|------------------|\n| 48 GB    | 12,106                | OOM              |\n| 80 GB    | 89,389                | 6,916            |\n\n<br>\n\n![](https://i.ibb.co/sJ7RhGG/image-41.png)\n<br>\n\n### Citation\n\nYou can cite the Unsloth repo as follows:\n```bibtex\n@software{unsloth,\n  author = {Daniel Han, Michael Han and Unsloth team},\n  title = {Unsloth},\n  url = {http://github.com/unslothai/unsloth},\n  year = {2023}\n}\n```\n\n### Thank You to\n- The [llama.cpp library](https://github.com/ggml-org/llama.cpp) that lets users save models with Unsloth\n- The Hugging Face team and their [TRL library](https://github.com/huggingface/trl)\n- [Erik](https://github.com/erikwijmans) for his help adding [Apple's ML Cross Entropy](https://github.com/apple/ml-cross-entropy) in Unsloth\n- [Etherl](https://github.com/Etherll) for adding support for [TTS, diffusion and BERT models](https://github.com/unslothai/notebooks/pull/34)\n- And of course for every single person who has contributed or has used Unsloth!\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 551376520,
    "name": "Pake",
    "full_name": "tw93/Pake",
    "description": "\ud83e\udd31\ud83c\udffb Turn any webpage into a desktop app with Rust.  \ud83e\udd31\ud83c\udffb \u5229\u7528 Rust \u8f7b\u677e\u6784\u5efa\u8f7b\u91cf\u7ea7\u591a\u7aef\u684c\u9762\u5e94\u7528",
    "html_url": "https://github.com/tw93/Pake",
    "clone_url": "https://github.com/tw93/Pake.git",
    "owner_login": "tw93",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/8736212?v=4",
    "stargazers_count": 40896,
    "watchers_count": 40896,
    "forks_count": 7634,
    "open_issues_count": 61,
    "size": 51153,
    "language": "Rust",
    "languages": {
      "Rust": 17170,
      "Dockerfile": 2111
    },
    "topics": [
      "chatgpt",
      "deepseek",
      "excalidraw",
      "gemini",
      "gemini-ai",
      "high-performance",
      "linux-desktop",
      "llm",
      "mac",
      "mac-desktop",
      "music",
      "no-electron",
      "openai",
      "productivity",
      "programming",
      "rust",
      "tauri",
      "twitter",
      "windows-desktop",
      "youtube"
    ],
    "license_name": "MIT License",
    "created_at": "2022-10-14T09:32:57+00:00",
    "updated_at": "2025-08-06T02:06:15+00:00",
    "pushed_at": "2025-08-05T12:15:30+00:00",
    "contributors_count": 41,
    "readme_length": 24829,
    "readme_content": "<h4 align=\"right\"><strong>English</strong> | <a href=\"https://github.com/tw93/Pake/blob/main/README_CN.md\">\u7b80\u4f53\u4e2d\u6587</a> | <a href=\"https://github.com/tw93/Pake/blob/main/README_JP.md\">\u65e5\u672c\u8a9e</a></h4>\n<p align=\"center\">\n    <img src=https://gw.alipayobjects.com/zos/k/fa/logo-modified.png width=138/>\n</p>\n<h1 align=\"center\">Pake</h1>\n<p align=\"center\"><strong>Turn any webpage into a desktop app with Rust <em>with ease</em>.</strong></p>\n<div align=\"center\">\n    <a href=\"https://twitter.com/HiTw93\" target=\"_blank\">\n    <img alt=\"twitter\" src=\"https://img.shields.io/badge/follow-Tw93-red?style=flat-square&logo=Twitter\"></a>\n    <a href=\"https://t.me/+GclQS9ZnxyI2ODQ1\" target=\"_blank\">\n    <img alt=\"telegram\" src=\"https://img.shields.io/badge/chat-telegram-blueviolet?style=flat-square&logo=Telegram\"></a>\n    <a href=\"https://github.com/tw93/Pake/releases\" target=\"_blank\">\n    <img alt=\"GitHub downloads\" src=\"https://img.shields.io/github/downloads/tw93/Pake/total.svg?style=flat-square\"></a>\n    <a href=\"https://github.com/tw93/Pake/commits\" target=\"_blank\">\n    <img alt=\"GitHub commit\" src=\"https://img.shields.io/github/commit-activity/m/tw93/Pake?style=flat-square\"></a>\n    <a href=\"https://github.com/tw93/Pake/issues?q=is%3Aissue+is%3Aclosed\" target=\"_blank\">\n    <img alt=\"GitHub closed issues\" src=\"https://img.shields.io/github/issues-closed/tw93/Pake.svg?style=flat-square\"></a>\n    <a href=\"https://colab.research.google.com/drive/1bX345znvDZ30848xjRtpgtU8eypWwXrp?usp=sharing\" target=\"_blank\">\n    <img alt=\"Open in Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"></a>\n</div>\n\n<div align=\"left\">Pake supports Mac, Windows, and Linux. Check out README for <a href=\"#popular-packages\">Popular Packages</a>, <a href=\"#command-line-packaging\">Command-Line Packaging</a>, and <a href=\"#development\">Customized Development</a> information. Feel free to share your suggestions in <a href=https://github.com/tw93/Pake/discussions>Discussions</a>.</div>\n\n## Features\n\n- \ud83c\udf90 Nearly 20 times smaller than an Electron package (around 5M!)\n- \ud83d\ude80 With Rust Tauri, Pake is much more lightweight and faster than JS-based frameworks.\n- \ud83d\udce6 Battery-included package \u2014 shortcut pass-through, immersive windows, and minimalist customization.\n- \ud83d\udc7b Pake is just a simple tool \u2014 replace the old bundle approach with Tauri (though PWA is good enough).\n\n## Popular Packages\n\n<table>\n    <tr>\n        <td>WeRead\n            <a href=\"https://github.com/tw93/Pake/releases/latest/download/WeRead.dmg\">Mac</a>\n            <a href=\"https://github.com/tw93/Pake/releases/latest/download/WeRead_x64.msi\">Windows</a>\n            <a href=\"https://github.com/tw93/Pake/releases/latest/download/WeRead_x86_64.deb\">Linux</a>\n        </td>\n        <td>Twitter\n            <a href=\"https://github.com/tw93/Pake/releases/latest/download/Twitter.dmg\">Mac</a>\n            <a href=\"https://github.com/tw93/Pake/releases/latest/download/Twitter_x64.msi\">Windows</a>\n            <a href=\"https://github.com/tw93/Pake/releases/latest/download/Twitter_x86_64.deb\">Linux</a>\n        </td>\n    </tr>\n    <tr>\n        <td><img src=https://raw.githubusercontent.com/tw93/static/main/pake/WeRead.jpg width=600/></td>\n        <td><img src=https://raw.githubusercontent.com/tw93/static/main/pake/Twitter.jpg width=600/></td>\n    </tr>\n     <tr>\n        <td>Grok\n            <a href=\"https://github.com/tw93/Pake/releases/latest/download/Grok.dmg\">Mac</a>\n            <a href=\"https://github.com/tw93/Pake/releases/latest/download/Grok_x64.msi\">Windows</a>\n            <a href=\"https://github.com/tw93/Pake/releases/latest/download/Grok_x86_64.deb\">Linux</a>\n        </td>\n        <td>DeepSeek\n            <a href=\"https://github.com/tw93/Pake/releases/latest/download/DeepSeek.dmg\">Mac</a>\n            <a href=\"https://github.com/tw93/Pake/releases/latest/download/DeepSeek_x64.msi\">Windows</a>\n            <a href=\"https://github.com/tw93/Pake/releases/latest/download/DeepSeek_x86_64.deb\">Linux</a>\n        </td>\n    </tr>\n    <tr>\n        <td><img src=https://raw.githubusercontent.com/tw93/static/main/pake/Grok.png width=600/></td>\n        <td><img src=https://raw.githubusercontent.com/tw93/static/main/pake/DeepSeek.png width=600/></td>\n    </tr>\n    <tr>\n        <td>ChatGPT\n            <a href=\"https://github.com/tw93/Pake/releases/latest/download/ChatGPT.dmg\">Mac</a>\n            <a href=\"https://github.com/tw93/Pake/releases/latest/download/ChatGPT_x64.msi\">Windows</a>\n            <a href=\"https://github.com/tw93/Pake/releases/latest/download/ChatGPT_x86_64.deb\">Linux</a>\n        </td>\n        <td>Gemini\n            <a href=\"https://github.com/tw93/Pake/releases/latest/download/Gemini.dmg\">Mac</a>\n            <a href=\"https://github.com/tw93/Pake/releases/latest/download/Gemini_x64.msi\">Windows</a>\n            <a href=\"https://github.com/tw93/Pake/releases/latest/download/Gemini_x86_64.deb\">Linux</a>\n        </td>\n    </tr>\n    <tr>\n        <td><img src=https://raw.githubusercontent.com/tw93/static/main/pake/ChatGPT.png width=600/></td>\n        <td><img src=https://raw.githubusercontent.com/tw93/static/main/pake/Gemini.png width=600/></td>\n    </tr>\n    <tr>\n      <td>YouTube Music\n            <a href=\"https://github.com/tw93/Pake/releases/latest/download/YouTubeMusic.dmg\">Mac</a>\n            <a href=\"https://github.com/tw93/Pake/releases/latest/download/YouTubeMusic_x64.msi\">Windows</a>\n            <a href=\"https://github.com/tw93/Pake/releases/latest/download/YouTubeMusic_x86_64.deb\">Linux</a>\n      </td>\n      <td>YouTube\n            <a href=\"https://github.com/tw93/Pake/releases/latest/download/YouTube.dmg\">Mac</a>\n            <a href=\"https://github.com/tw93/Pake/releases/latest/download/YouTube_x64.msi\">Windows</a>\n            <a href=\"https://github.com/tw93/Pake/releases/latest/download/YouTube_x86_64.deb\">Linux</a>\n      </td>\n    </tr>\n    <tr>\n        <td><img src=https://raw.githubusercontent.com/tw93/static/main/pake/YouTubeMusic.png width=600 /></td>\n        <td><img src=https://raw.githubusercontent.com/tw93/static/main/pake/YouTube.jpg width=600 /></td>\n    </tr>\n    <tr>\n        <td>LiZhi\n            <a href=\"https://github.com/tw93/Pake/releases/latest/download/LiZhi.dmg\">Mac</a>\n            <a href=\"https://github.com/tw93/Pake/releases/latest/download/LiZhi_x64.msi\">Windows</a>\n            <a href=\"https://github.com/tw93/Pake/releases/latest/download/LiZhi_x86_64.deb\">Linux</a>\n        </td>\n        <td>ProgramMusic\n            <a href=\"https://github.com/tw93/Pake/releases/latest/download/ProgramMusic.dmg\">Mac</a>\n            <a href=\"https://github.com/tw93/Pake/releases/latest/download/ProgramMusic_x64.msi\">Windows</a>\n            <a href=\"https://github.com/tw93/Pake/releases/latest/download/ProgramMusic_x86_64.deb\">Linux</a>\n        </td>\n    </tr>\n    <tr>\n        <td><img src=https://raw.githubusercontent.com/tw93/static/main/pake/LiZhi.jpg width=600/></td>\n        <td><img src=https://raw.githubusercontent.com/tw93/static/main/pake/ProgramMusic.jpg width=600/></td>\n    </tr>\n    <tr>\n         <td>Excalidraw\n            <a href=\"https://github.com/tw93/Pake/releases/latest/download/Excalidraw.dmg\">Mac</a>\n            <a href=\"https://github.com/tw93/Pake/releases/latest/download/Excalidraw_x64.msi\">Windows</a>\n            <a href=\"https://github.com/tw93/Pake/releases/latest/download/Excalidraw_x86_64.deb\">Linux</a>\n        </td>\n        <td>XiaoHongShu\n            <a href=\"https://github.com/tw93/Pake/releases/latest/download/XiaoHongShu.dmg\">Mac</a>\n            <a href=\"https://github.com/tw93/Pake/releases/latest/download/XiaoHongShu_x64.msi\">Windows</a>\n            <a href=\"https://github.com/tw93/Pake/releases/latest/download/XiaoHongShu_x86_64.deb\">Linux</a>\n        </td>\n    </tr>\n    <tr>\n        <td><img src=https://raw.githubusercontent.com/tw93/static/main/pake/Excalidraw.png width=600/></td>\n        <td><img src=https://raw.githubusercontent.com/tw93/static/main/pake/XiaoHongShu.png width=600/></td>\n    </tr>\n</table>\n\n<details>\n<summary>\ud83c\udfc2 You can download more applications from <a href=\"https://github.com/tw93/Pake/releases\">Releases</a>. <b>Click here to expand the shortcuts reference!</b></summary>\n\n<br/>\n\n| Mac                         | Windows/Linux                  | Function                      |\n| --------------------------- | ------------------------------ | ----------------------------- |\n| <kbd>\u2318</kbd> + <kbd>[</kbd> | <kbd>Ctrl</kbd> + <kbd>\u2190</kbd> | Return to the previous page   |\n| <kbd>\u2318</kbd> + <kbd>]</kbd> | <kbd>Ctrl</kbd> + <kbd>\u2192</kbd> | Go to the next page           |\n| <kbd>\u2318</kbd> + <kbd>\u2191</kbd> | <kbd>Ctrl</kbd> + <kbd>\u2191</kbd> | Auto scroll to top of page    |\n| <kbd>\u2318</kbd> + <kbd>\u2193</kbd> | <kbd>Ctrl</kbd> + <kbd>\u2193</kbd> | Auto scroll to bottom of page |\n| <kbd>\u2318</kbd> + <kbd>r</kbd> | <kbd>Ctrl</kbd> + <kbd>r</kbd> | Refresh Page                  |\n| <kbd>\u2318</kbd> + <kbd>w</kbd> | <kbd>Ctrl</kbd> + <kbd>w</kbd> | Hide window, not quite        |\n| <kbd>\u2318</kbd> + <kbd>-</kbd> | <kbd>Ctrl</kbd> + <kbd>-</kbd> | Zoom out the page             |\n| <kbd>\u2318</kbd> + <kbd>+</kbd> | <kbd>Ctrl</kbd> + <kbd>+</kbd> | Zoom in the page              |\n| <kbd>\u2318</kbd> + <kbd>=</kbd> | <kbd>Ctrl</kbd> + <kbd>=</kbd> | Zoom in the Page              |\n| <kbd>\u2318</kbd> + <kbd>0</kbd> | <kbd>Ctrl</kbd> + <kbd>0</kbd> | Reset the page zoom           |\n\nIn addition, double-click the title bar to switch to full-screen mode. For Mac users, you can also use the gesture to go to the previous or next page and drag the title bar to move the window.\n\n</details>\n\n## Before starting\n\n1. **For beginners**: Play with Popular Packages to find out Pake's capabilities, or try to pack your application with [GitHub Actions](<https://github.com/tw93/Pake/wiki/Online-Compilation-(used-by-ordinary-users)>). Don't hesitate to reach for assistance at [Discussion](https://github.com/tw93/Pake/discussions)!\n2. **For developers**: \u201cCommand-Line Packaging\u201d supports macOS fully. For Windows/Linux users, it requires some tinkering. [Configure your environment](https://tauri.app/start/prerequisites/) before getting started.\n3. **For hackers**: For people who are good at both front-end development and Rust, how about customizing your apps' function more with the following [Customized Development](#development)?\n\n## Command-Line Packaging\n\n![Pake](https://raw.githubusercontent.com/tw93/static/main/pake/pake.gif)\n\n**Pake provides a command line tool, making the flow of package customization quicker and easier. See [documentation](./bin/README.md) for more information.**\n\n```bash\n# Install with npm\nnpm install -g pake-cli\n\n# Command usage\npake url [OPTIONS]...\n\n# Feel free to play with Pake! It might take a while to prepare the environment the first time you launch Pake.\npake https://weekly.tw93.fun --name Weekly --hide-title-bar\n\n```\n\nIf you are new to the command line, you can compile packages online with _GitHub Actions_. See the [Tutorial](<https://github.com/tw93/Pake/wiki/Online-Compilation-(used-by-ordinary-users)>) for more information.\n\n## Development\n\nPrepare your environment before starting. Make sure you have Rust `>=1.63` and Node `>=16` (e.g., `16.18.1`) installed on your computer. For installation guidance, see [Tauri documentation](https://tauri.app/start/prerequisites/).\n\nIf you are unfamiliar with these, it is better to try out the above tool to pack with one click.\n\n```sh\n# Install Dependencies\nnpm i\n\n# Local development [Right-click to open debug mode.]\nnpm run dev\n\n# Pack application\nnpm run build\n```\n\n## Advanced Usage\n\n1. You can refer to the [codebase structure](https://github.com/tw93/Pake/wiki/Description-of-Pake's-code-structure) before working on Pake, which will help you much in development.\n2. Modify the `url` and `productName` fields in the `pake.json` file under the src-tauri directory, the \"domain\" field in the `tauri.config.json` file needs to be modified synchronously, as well as the `icon` and `identifier` fields in the `tauri.xxx.conf.json` file. You can select an `icon` from the `icons` directory or download one from [macOSicons](https://macosicons.com/#/) to match your product needs.\n3. For configurations on window properties, you can modify the `pake.json` file to change the value of `width`, `height`, `fullscreen` (or not), `resizable` (or not) of the `windows` property. To adapt to the immersive header on Mac, change `hideTitleBar` to `true`, look for the `Header` element, and add the `padding-top` property.\n4. For advanced usages such as style rewriting, advertisement removal, JS injection, container message communication, and user-defined shortcut keys, see [Advanced Usage of Pake](https://github.com/tw93/Pake/wiki/Advanced-Usage-of-Pake).\n\n## Developers\n\nPake's development can not be without these Hackers. They contributed a lot of capabilities for Pake. Also, welcome to follow them! \u2764\ufe0f\n\n<!-- readme: contributors -start -->\n<table>\n<tr>\n    <td align=\"center\">\n        <a href=\"https://github.com/tw93\">\n            <img src=\"https://avatars.githubusercontent.com/u/8736212?v=4\" width=\"90;\" alt=\"tw93\"/>\n            <br />\n            <sub><b>Tw93</b></sub>\n        </a>\n    </td>\n    <td align=\"center\">\n        <a href=\"https://github.com/Tlntin\">\n            <img src=\"https://avatars.githubusercontent.com/u/28218658?v=4\" width=\"90;\" alt=\"Tlntin\"/>\n            <br />\n            <sub><b>Tlntin</b></sub>\n        </a>\n    </td>\n    <td align=\"center\">\n        <a href=\"https://github.com/jeasonnow\">\n            <img src=\"https://avatars.githubusercontent.com/u/16950207?v=4\" width=\"90;\" alt=\"jeasonnow\"/>\n            <br />\n            <sub><b>Santree</b></sub>\n        </a>\n    </td>\n    <td align=\"center\">\n        <a href=\"https://github.com/pan93412\">\n            <img src=\"https://avatars.githubusercontent.com/u/28441561?v=4\" width=\"90;\" alt=\"pan93412\"/>\n            <br />\n            <sub><b>Pan93412</b></sub>\n        </a>\n    </td>\n    <td align=\"center\">\n        <a href=\"https://github.com/stone-w4tch3r\">\n            <img src=\"https://avatars.githubusercontent.com/u/100294019?v=4\" width=\"90;\" alt=\"stone-w4tch3r\"/>\n            <br />\n            <sub><b>\u0414\u0430\u043d\u0438\u043b \u0411\u0438\u0437\u0438\u043c\u043e\u0432</b></sub>\n        </a>\n    </td>\n    <td align=\"center\">\n        <a href=\"https://github.com/wanghanzhen\">\n            <img src=\"https://avatars.githubusercontent.com/u/25301012?v=4\" width=\"90;\" alt=\"wanghanzhen\"/>\n            <br />\n            <sub><b>Volare</b></sub>\n        </a>\n    </td>\n    <td align=\"center\">\n        <a href=\"https://github.com/liby\">\n            <img src=\"https://avatars.githubusercontent.com/u/38807139?v=4\" width=\"90;\" alt=\"liby\"/>\n            <br />\n            <sub><b>Bryan Lee</b></sub>\n        </a>\n    </td></tr>\n<tr>\n    <td align=\"center\">\n        <a href=\"https://github.com/essesoul\">\n            <img src=\"https://avatars.githubusercontent.com/u/58624474?v=4\" width=\"90;\" alt=\"essesoul\"/>\n            <br />\n            <sub><b>Essesoul</b></sub>\n        </a>\n    </td>\n    <td align=\"center\">\n        <a href=\"https://github.com/YangguangZhou\">\n            <img src=\"https://avatars.githubusercontent.com/u/61733195?v=4\" width=\"90;\" alt=\"YangguangZhou\"/>\n            <br />\n            <sub><b>Jerry Zhou</b></sub>\n        </a>\n    </td>\n    <td align=\"center\">\n        <a href=\"https://github.com/AielloChan\">\n            <img src=\"https://avatars.githubusercontent.com/u/7900765?v=4\" width=\"90;\" alt=\"AielloChan\"/>\n            <br />\n            <sub><b>Aiello</b></sub>\n        </a>\n    </td>\n    <td align=\"center\">\n        <a href=\"https://github.com/m1911star\">\n            <img src=\"https://avatars.githubusercontent.com/u/4948120?v=4\" width=\"90;\" alt=\"m1911star\"/>\n            <br />\n            <sub><b>Horus</b></sub>\n        </a>\n    </td>\n    <td align=\"center\">\n        <a href=\"https://github.com/Pake-Actions\">\n            <img src=\"https://avatars.githubusercontent.com/u/126550811?v=4\" width=\"90;\" alt=\"Pake-Actions\"/>\n            <br />\n            <sub><b>Pake Actions</b></sub>\n        </a>\n    </td>\n    <td align=\"center\">\n        <a href=\"https://github.com/GoodbyeNJN\">\n            <img src=\"https://avatars.githubusercontent.com/u/6856639?v=4\" width=\"90;\" alt=\"GoodbyeNJN\"/>\n            <br />\n            <sub><b>GoodbyeNJN</b></sub>\n        </a>\n    </td>\n    <td align=\"center\">\n        <a href=\"https://github.com/kittizz\">\n            <img src=\"https://avatars.githubusercontent.com/u/62899732?v=4\" width=\"90;\" alt=\"kittizz\"/>\n            <br />\n            <sub><b>Kittizz</b></sub>\n        </a>\n    </td></tr>\n<tr>\n    <td align=\"center\">\n        <a href=\"https://github.com/mattbajorek\">\n            <img src=\"https://avatars.githubusercontent.com/u/17235301?v=4\" width=\"90;\" alt=\"mattbajorek\"/>\n            <br />\n            <sub><b>Matt Bajorek</b></sub>\n        </a>\n    </td>\n    <td align=\"center\">\n        <a href=\"https://github.com/QingZ11\">\n            <img src=\"https://avatars.githubusercontent.com/u/38887077?v=4\" width=\"90;\" alt=\"QingZ11\"/>\n            <br />\n            <sub><b>Steam</b></sub>\n        </a>\n    </td>\n    <td align=\"center\">\n        <a href=\"https://github.com/Tianj0o\">\n            <img src=\"https://avatars.githubusercontent.com/u/68584284?v=4\" width=\"90;\" alt=\"Tianj0o\"/>\n            <br />\n            <sub><b>Qitianjia</b></sub>\n        </a>\n    </td>\n    <td align=\"center\">\n        <a href=\"https://github.com/xinyii\">\n            <img src=\"https://avatars.githubusercontent.com/u/17895104?v=4\" width=\"90;\" alt=\"xinyii\"/>\n            <br />\n            <sub><b>Yi Xin</b></sub>\n        </a>\n    </td>\n    <td align=\"center\">\n        <a href=\"https://github.com/g1eny0ung\">\n            <img src=\"https://avatars.githubusercontent.com/u/15034155?v=4\" width=\"90;\" alt=\"g1eny0ung\"/>\n            <br />\n            <sub><b>Yue Yang</b></sub>\n        </a>\n    </td>\n    <td align=\"center\">\n        <a href=\"https://github.com/lkieryan\">\n            <img src=\"https://avatars.githubusercontent.com/u/187804088?v=4\" width=\"90;\" alt=\"lkieryan\"/>\n            <br />\n            <sub><b>Kieran</b></sub>\n        </a>\n    </td>\n    <td align=\"center\">\n        <a href=\"https://github.com/exposir\">\n            <img src=\"https://avatars.githubusercontent.com/u/33340988?v=4\" width=\"90;\" alt=\"exposir\"/>\n            <br />\n            <sub><b>\u5b5f\u4e16\u535a</b></sub>\n        </a>\n    </td></tr>\n<tr>\n    <td align=\"center\">\n        <a href=\"https://github.com/2nthony\">\n            <img src=\"https://avatars.githubusercontent.com/u/19513289?v=4\" width=\"90;\" alt=\"2nthony\"/>\n            <br />\n            <sub><b>2nthony</b></sub>\n        </a>\n    </td>\n    <td align=\"center\">\n        <a href=\"https://github.com/ACGNnsj\">\n            <img src=\"https://avatars.githubusercontent.com/u/22112141?v=4\" width=\"90;\" alt=\"ACGNnsj\"/>\n            <br />\n            <sub><b>Null</b></sub>\n        </a>\n    </td>\n    <td align=\"center\">\n        <a href=\"https://github.com/imabutahersiddik\">\n            <img src=\"https://avatars.githubusercontent.com/u/138387257?v=4\" width=\"90;\" alt=\"imabutahersiddik\"/>\n            <br />\n            <sub><b>Abu Taher Siddik</b></sub>\n        </a>\n    </td>\n    <td align=\"center\">\n        <a href=\"https://github.com/kidylee\">\n            <img src=\"https://avatars.githubusercontent.com/u/841310?v=4\" width=\"90;\" alt=\"kidylee\"/>\n            <br />\n            <sub><b>An Li</b></sub>\n        </a>\n    </td>\n    <td align=\"center\">\n        <a href=\"https://github.com/nekomeowww\">\n            <img src=\"https://avatars.githubusercontent.com/u/11081491?v=4\" width=\"90;\" alt=\"nekomeowww\"/>\n            <br />\n            <sub><b>Ayaka Neko</b></sub>\n        </a>\n    </td>\n    <td align=\"center\">\n        <a href=\"https://github.com/turkyden\">\n            <img src=\"https://avatars.githubusercontent.com/u/24560160?v=4\" width=\"90;\" alt=\"turkyden\"/>\n            <br />\n            <sub><b>Dengju Deng</b></sub>\n        </a>\n    </td>\n    <td align=\"center\">\n        <a href=\"https://github.com/Fechin\">\n            <img src=\"https://avatars.githubusercontent.com/u/2541482?v=4\" width=\"90;\" alt=\"Fechin\"/>\n            <br />\n            <sub><b>Fechin</b></sub>\n        </a>\n    </td></tr>\n<tr>\n    <td align=\"center\">\n        <a href=\"https://github.com/ImgBotApp\">\n            <img src=\"https://avatars.githubusercontent.com/u/31427850?v=4\" width=\"90;\" alt=\"ImgBotApp\"/>\n            <br />\n            <sub><b>Imgbot</b></sub>\n        </a>\n    </td>\n    <td align=\"center\">\n        <a href=\"https://github.com/droid-Q\">\n            <img src=\"https://avatars.githubusercontent.com/u/708277?v=4\" width=\"90;\" alt=\"droid-Q\"/>\n            <br />\n            <sub><b>Jiaqi Gu</b></sub>\n        </a>\n    </td>\n    <td align=\"center\">\n        <a href=\"https://github.com/Milo123459\">\n            <img src=\"https://avatars.githubusercontent.com/u/50248166?v=4\" width=\"90;\" alt=\"Milo123459\"/>\n            <br />\n            <sub><b>Milo</b></sub>\n        </a>\n    </td>\n    <td align=\"center\">\n        <a href=\"https://github.com/princemaple\">\n            <img src=\"https://avatars.githubusercontent.com/u/1329716?v=4\" width=\"90;\" alt=\"princemaple\"/>\n            <br />\n            <sub><b>Po Chen</b></sub>\n        </a>\n    </td>\n    <td align=\"center\">\n        <a href=\"https://github.com/beautifulrem\">\n            <img src=\"https://avatars.githubusercontent.com/u/98527099?v=4\" width=\"90;\" alt=\"beautifulrem\"/>\n            <br />\n            <sub><b>Xie Ruiqi</b></sub>\n        </a>\n    </td>\n    <td align=\"center\">\n        <a href=\"https://github.com/bocanhcam\">\n            <img src=\"https://avatars.githubusercontent.com/u/35592955?v=4\" width=\"90;\" alt=\"bocanhcam\"/>\n            <br />\n            <sub><b>Null</b></sub>\n        </a>\n    </td>\n    <td align=\"center\">\n        <a href=\"https://github.com/geekvest\">\n            <img src=\"https://avatars.githubusercontent.com/u/126322776?v=4\" width=\"90;\" alt=\"geekvest\"/>\n            <br />\n            <sub><b>Null</b></sub>\n        </a>\n    </td></tr>\n<tr>\n    <td align=\"center\">\n        <a href=\"https://github.com/houhoz\">\n            <img src=\"https://avatars.githubusercontent.com/u/19684376?v=4\" width=\"90;\" alt=\"houhoz\"/>\n            <br />\n            <sub><b>Hyzhao</b></sub>\n        </a>\n    </td>\n    <td align=\"center\">\n        <a href=\"https://github.com/lakca\">\n            <img src=\"https://avatars.githubusercontent.com/u/16255922?v=4\" width=\"90;\" alt=\"lakca\"/>\n            <br />\n            <sub><b>Null</b></sub>\n        </a>\n    </td>\n    <td align=\"center\">\n        <a href=\"https://github.com/liudonghua123\">\n            <img src=\"https://avatars.githubusercontent.com/u/2276718?v=4\" width=\"90;\" alt=\"liudonghua123\"/>\n            <br />\n            <sub><b>Liudonghua</b></sub>\n        </a>\n    </td>\n    <td align=\"center\">\n        <a href=\"https://github.com/liusishan\">\n            <img src=\"https://avatars.githubusercontent.com/u/33129823?v=4\" width=\"90;\" alt=\"liusishan\"/>\n            <br />\n            <sub><b>Liusishan</b></sub>\n        </a>\n    </td>\n    <td align=\"center\">\n        <a href=\"https://github.com/piaoyidage\">\n            <img src=\"https://avatars.githubusercontent.com/u/5135405?v=4\" width=\"90;\" alt=\"piaoyidage\"/>\n            <br />\n            <sub><b>Ranger</b></sub>\n        </a>\n    </td>\n    <td align=\"center\">\n        <a href=\"https://github.com/hetz\">\n            <img src=\"https://avatars.githubusercontent.com/u/820141?v=4\" width=\"90;\" alt=\"hetz\"/>\n            <br />\n            <sub><b>\u8d3a\u5929\u5353</b></sub>\n        </a>\n    </td></tr>\n</table>\n<!-- readme: contributors -end -->\n\n## Frequently Asked Questions\n\n1. Right-clicking on an image element in the page to open the menu and select download image or other events does not work (common in MacOS systems). This issue is due to the MacOS built-in webview not supporting this feature.\n\n## Support\n\n1. I have two cats, TangYuan and Coke. If you think Pake delights your life, you can feed them <a href=\"https://miaoyan.app/cats.html?name=Pake\" target=\"_blank\">some canned food \ud83e\udd69</a>.\n2. If you like Pake, you can star it on GitHub. Also, welcome to [recommend Pake](https://twitter.com/intent/tweet?url=https://github.com/tw93/Pake&text=%23Pake%20-%20A%20simple%20Rust%20packaged%20web%20pages%20to%20generate%20Mac%20App%20tool,%20compared%20to%20traditional%20Electron%20package,%20the%20size%20of%20nearly%2040%20times%20smaller,%20generally%20about%202M,%20the%20underlying%20use%20of%20Tauri,%20performance%20experience%20than%20the%20JS%20framework%20is%20much%20lighter~) to your friends.\n3. You can follow my [Twitter](https://twitter.com/HiTw93) to get the latest news of Pake or join our [Telegram](https://t.me/+GclQS9ZnxyI2ODQ1) chat group.\n4. I hope that you enjoy playing with it. Let us know if you find a website that would be great for a Mac App!\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 522158088,
    "name": "chatgpt-on-wechat",
    "full_name": "zhayujie/chatgpt-on-wechat",
    "description": "\u57fa\u4e8e\u5927\u6a21\u578b\u642d\u5efa\u7684\u804a\u5929\u673a\u5668\u4eba\uff0c\u540c\u65f6\u652f\u6301 \u5fae\u4fe1\u516c\u4f17\u53f7\u3001\u4f01\u4e1a\u5fae\u4fe1\u5e94\u7528\u3001\u98de\u4e66\u3001\u9489\u9489 \u7b49\u63a5\u5165\uff0c\u53ef\u9009\u62e9ChatGPT/Claude/DeepSeek/\u6587\u5fc3\u4e00\u8a00/\u8baf\u98de\u661f\u706b/\u901a\u4e49\u5343\u95ee/ Gemini/GLM-4/Kimi/LinkAI\uff0c\u80fd\u5904\u7406\u6587\u672c\u3001\u8bed\u97f3\u548c\u56fe\u7247\uff0c\u8bbf\u95ee\u64cd\u4f5c\u7cfb\u7edf\u548c\u4e92\u8054\u7f51\uff0c\u652f\u6301\u57fa\u4e8e\u81ea\u6709\u77e5\u8bc6\u5e93\u8fdb\u884c\u5b9a\u5236\u4f01\u4e1a\u667a\u80fd\u5ba2\u670d\u3002",
    "html_url": "https://github.com/zhayujie/chatgpt-on-wechat",
    "clone_url": "https://github.com/zhayujie/chatgpt-on-wechat.git",
    "owner_login": "zhayujie",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/26161723?v=4",
    "stargazers_count": 38379,
    "watchers_count": 38379,
    "forks_count": 9362,
    "open_issues_count": 344,
    "size": 4462,
    "language": "Python",
    "languages": {
      "Python": 788442,
      "HTML": 51019,
      "Shell": 9492,
      "Dockerfile": 77
    },
    "topics": [
      "ai",
      "ai-agent",
      "chatgpt",
      "claude-4",
      "deepseek",
      "dingtalk",
      "feishu-bot",
      "gemini",
      "gpt-4",
      "kimi",
      "linkai",
      "llm",
      "mcp",
      "multi-agent",
      "openai",
      "python3",
      "qwen",
      "rag",
      "wechat",
      "wechat-bot"
    ],
    "license_name": "MIT License",
    "created_at": "2022-08-07T08:33:41+00:00",
    "updated_at": "2025-08-06T02:03:22+00:00",
    "pushed_at": "2025-06-29T14:41:10+00:00",
    "contributors_count": 78,
    "readme_length": 24557,
    "readme_content": "<p align=\"center\"><img src= \"https://github.com/user-attachments/assets/31fb4eab-3be4-477d-aa76-82cf62bfd12c\" alt=\"Chatgpt-on-Wechat\" width=\"600\" /></p>\n\n<p align=\"center\">\n   <a href=\"https://github.com/zhayujie/chatgpt-on-wechat/releases/latest\"><img src=\"https://img.shields.io/github/v/release/zhayujie/chatgpt-on-wechat\" alt=\"Latest release\"></a>\n  <a href=\"https://github.com/zhayujie/chatgpt-on-wechat/blob/master/LICENSE\"><img src=\"https://img.shields.io/github/license/zhayujie/chatgpt-on-wechat\" alt=\"License: MIT\"></a>\n  <a href=\"https://github.com/zhayujie/chatgpt-on-wechat\"><img src=\"https://img.shields.io/github/stars/zhayujie/chatgpt-on-wechat?style=flat-square\" alt=\"Stars\"></a> <br/>\n</p>\n\n**chatgpt-on-wechat**\uff08\u7b80\u79f0CoW\uff09\u9879\u76ee\u662f\u57fa\u4e8e\u5927\u6a21\u578b\u7684\u667a\u80fd\u5bf9\u8bdd\u673a\u5668\u4eba\uff0c\u652f\u6301\u81ea\u7531\u5207\u6362\u591a\u79cd\u6a21\u578b\uff0c\u53ef\u63a5\u5165\u7f51\u9875\u3001\u5fae\u4fe1\u516c\u4f17\u53f7\u3001\u4f01\u4e1a\u5fae\u4fe1\u5e94\u7528\u3001\u98de\u4e66\u3001\u9489\u9489\u4e2d\u4f7f\u7528\uff0c\u80fd\u5904\u7406\u6587\u672c\u3001\u8bed\u97f3\u3001\u56fe\u7247\u3001\u6587\u4ef6\u7b49\u591a\u6a21\u6001\u6d88\u606f\uff0c\u652f\u6301\u901a\u8fc7\u63d2\u4ef6\u8bbf\u95ee\u64cd\u4f5c\u7cfb\u7edf\u548c\u4e92\u8054\u7f51\u7b49\u5916\u90e8\u8d44\u6e90\uff0c\u4ee5\u53ca\u57fa\u4e8e\u81ea\u6709\u77e5\u8bc6\u5e93\u5b9a\u5236\u4f01\u4e1aAI\u5e94\u7528\u3002\n\n# \u7b80\u4ecb\n\n> \u8be5\u9879\u76ee\u65e2\u662f\u4e00\u4e2a\u53ef\u4ee5\u5f00\u7bb1\u5373\u7528\u7684\u5bf9\u8bdd\u673a\u5668\u4eba\uff0c\u4e5f\u662f\u4e00\u4e2a\u652f\u6301\u9ad8\u5ea6\u6269\u5c55\u7684AI\u5e94\u7528\u6846\u67b6\uff0c\u53ef\u4ee5\u901a\u8fc7\u4e3a\u9879\u76ee\u6dfb\u52a0\u5927\u6a21\u578b\u63a5\u53e3\u3001\u63a5\u5165\u6e20\u9053\u3001\u81ea\u5b9a\u4e49\u63d2\u4ef6\u6765\u7075\u6d3b\u5b9e\u73b0\u5404\u79cd\u5b9a\u5236\u9700\u6c42\u3002\u652f\u6301\u7684\u529f\u80fd\u5982\u4e0b\uff1a\n\n-  \u2705   **\u591a\u7aef\u90e8\u7f72\uff1a** \u6709\u591a\u79cd\u90e8\u7f72\u65b9\u5f0f\u53ef\u9009\u62e9\u4e14\u529f\u80fd\u5b8c\u5907\uff0c\u76ee\u524d\u5df2\u652f\u6301\u7f51\u9875\u3001\u5fae\u4fe1\u516c\u4f17\u53f7\u3001\u4f01\u4e1a\u5fae\u4fe1\u5e94\u7528\u3001\u98de\u4e66\u3001\u9489\u9489\u7b49\u90e8\u7f72\u65b9\u5f0f\n-  \u2705   **\u57fa\u7840\u5bf9\u8bdd\uff1a** \u79c1\u804a\u53ca\u7fa4\u804a\u7684AI\u667a\u80fd\u56de\u590d\uff0c\u652f\u6301\u591a\u8f6e\u4f1a\u8bdd\u4e0a\u4e0b\u6587\u8bb0\u5fc6\uff0c\u57fa\u7840\u6a21\u578b\u652f\u6301OpenAI, Claude, Gemini, DeepSeek, \u901a\u4e49\u5343\u95ee, Kimi, \u6587\u5fc3\u4e00\u8a00, \u8baf\u98de\u661f\u706b, ChatGLM, MiniMax, GiteeAI, ModelScope, LinkAI\n-  \u2705   **\u8bed\u97f3\u80fd\u529b\uff1a** \u53ef\u8bc6\u522b\u8bed\u97f3\u6d88\u606f\uff0c\u901a\u8fc7\u6587\u5b57\u6216\u8bed\u97f3\u56de\u590d\uff0c\u652f\u6301 openai(whisper/tts), azure, baidu, google \u7b49\u591a\u79cd\u8bed\u97f3\u6a21\u578b\n-  \u2705   **\u56fe\u50cf\u80fd\u529b\uff1a** \u652f\u6301\u56fe\u7247\u751f\u6210\u3001\u56fe\u7247\u8bc6\u522b\u3001\u56fe\u751f\u56fe\uff0c\u53ef\u9009\u62e9 Dall-E-3, stable diffusion, replicate, midjourney, CogView-3, vision\u6a21\u578b\n-  \u2705   **\u4e30\u5bcc\u63d2\u4ef6\uff1a** \u652f\u6301\u81ea\u5b9a\u4e49\u63d2\u4ef6\u6269\u5c55\uff0c\u5df2\u5b9e\u73b0\u591a\u89d2\u8272\u5207\u6362\u3001\u654f\u611f\u8bcd\u8fc7\u6ee4\u3001\u804a\u5929\u8bb0\u5f55\u603b\u7ed3\u3001\u6587\u6863\u603b\u7ed3\u548c\u5bf9\u8bdd\u3001\u8054\u7f51\u641c\u7d22\u3001\u667a\u80fd\u4f53\u7b49\u5185\u7f6e\u63d2\u4ef6\n-  \u2705   **Agent\u80fd\u529b\uff1a** \u652f\u6301\u8bbf\u95ee\u6d4f\u89c8\u5668\u3001\u7ec8\u7aef\u3001\u6587\u4ef6\u7cfb\u7edf\u3001\u641c\u7d22\u5f15\u64ce\u7b49\u5404\u7c7b\u5de5\u5177\uff0c\u5e76\u53ef\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u5b8c\u6210\u590d\u6742\u4efb\u52a1\uff0c\u57fa\u4e8e [AgentMesh](https://github.com/MinimalFuture/AgentMesh) \u6846\u67b6\u5b9e\u73b0\n-  \u2705   **\u77e5\u8bc6\u5e93\uff1a** \u901a\u8fc7\u4e0a\u4f20\u77e5\u8bc6\u5e93\u81ea\u5b9a\u4e49\u4e13\u5c5e\u673a\u5668\u4eba\uff0c\u53ef\u4f5c\u4e3a\u6570\u5b57\u5206\u8eab\u3001\u667a\u80fd\u5ba2\u670d\u3001\u4f01\u4e1a\u667a\u80fd\u4f53\u4f7f\u7528\uff0c\u57fa\u4e8e [LinkAI](https://link-ai.tech) \u5b9e\u73b0\n\n## \u58f0\u660e\n\n1. \u672c\u9879\u76ee\u9075\u5faa [MIT\u5f00\u6e90\u534f\u8bae](/LICENSE)\uff0c\u4ec5\u7528\u4e8e\u6280\u672f\u7814\u7a76\u548c\u5b66\u4e60\uff0c\u4f7f\u7528\u672c\u9879\u76ee\u65f6\u9700\u9075\u5b88\u6240\u5728\u5730\u6cd5\u5f8b\u6cd5\u89c4\u3001\u76f8\u5173\u653f\u7b56\u4ee5\u53ca\u4f01\u4e1a\u7ae0\u7a0b\uff0c\u7981\u6b62\u7528\u4e8e\u4efb\u4f55\u8fdd\u6cd5\u6216\u4fb5\u72af\u4ed6\u4eba\u6743\u76ca\u7684\u884c\u4e3a\u3002\u4efb\u4f55\u4e2a\u4eba\u3001\u56e2\u961f\u548c\u4f01\u4e1a\uff0c\u65e0\u8bba\u4ee5\u4f55\u79cd\u65b9\u5f0f\u4f7f\u7528\u8be5\u9879\u76ee\u3001\u5bf9\u4f55\u5bf9\u8c61\u63d0\u4f9b\u670d\u52a1\uff0c\u6240\u4ea7\u751f\u7684\u4e00\u5207\u540e\u679c\uff0c\u672c\u9879\u76ee\u5747\u4e0d\u627f\u62c5\u4efb\u4f55\u8d23\u4efb\n2. \u5883\u5185\u4f7f\u7528\u8be5\u9879\u76ee\u65f6\uff0c\u5efa\u8bae\u4f7f\u7528\u56fd\u5185\u5382\u5546\u7684\u5927\u6a21\u578b\u670d\u52a1\uff0c\u5e76\u8fdb\u884c\u5fc5\u8981\u7684\u5185\u5bb9\u5b89\u5168\u5ba1\u6838\u53ca\u8fc7\u6ee4\n3. \u672c\u9879\u76ee\u5f53\u524d\u4e3b\u8981\u63a5\u5165\u534f\u540c\u529e\u516c\u5e73\u53f0\uff0c\u63a8\u8350\u4f7f\u7528\u7f51\u9875\u3001\u516c\u4f17\u53f7\u3001\u4f01\u5fae\u81ea\u5efa\u5e94\u7528\u3001\u9489\u9489\u3001\u98de\u4e66\u7b49\u63a5\u5165\u901a\u9053\uff0c\u5176\u4ed6\u901a\u9053\u4e3a\u5386\u53f2\u4ea7\u7269\u6682\u4e0d\u7ef4\u62a4\n\n## \u6f14\u793a\n\nDEMO\u89c6\u9891\uff1ahttps://cdn.link-ai.tech/doc/cow_demo.mp4\n\n## \u793e\u533a\n\n\u6dfb\u52a0\u5c0f\u52a9\u624b\u5fae\u4fe1\u52a0\u5165\u5f00\u6e90\u9879\u76ee\u4ea4\u6d41\u7fa4\uff1a\n\n<img width=\"140\" src=\"https://img-1317903499.cos.ap-guangzhou.myqcloud.com/docs/open-community.png\">\n\n<br/>\n\n# \u4f01\u4e1a\u670d\u52a1\n\n<a href=\"https://link-ai.tech\" target=\"_blank\"><img width=\"720\" src=\"https://cdn.link-ai.tech/image/link-ai-intro.jpg\"></a>\n\n> [LinkAI](https://link-ai.tech/) \u662f\u9762\u5411\u4f01\u4e1a\u548c\u5f00\u53d1\u8005\u7684\u4e00\u7ad9\u5f0fAI\u667a\u80fd\u4f53\u5e73\u53f0\uff0c\u805a\u5408\u591a\u6a21\u6001\u5927\u6a21\u578b\u3001\u77e5\u8bc6\u5e93\u3001Agent \u63d2\u4ef6\u3001\u5de5\u4f5c\u6d41\u7b49\u80fd\u529b\uff0c\u652f\u6301\u4e00\u952e\u63a5\u5165\u4e3b\u6d41\u5e73\u53f0\u5e76\u8fdb\u884c\u7ba1\u7406\uff0c\u652f\u6301SaaS\u3001\u79c1\u6709\u5316\u90e8\u7f72\u7b49\u591a\u79cd\u6a21\u5f0f\u3002\n>\n> LinkAI \u76ee\u524d\u5df2\u5728\u667a\u80fd\u5ba2\u670d\u3001\u79c1\u57df\u8fd0\u8425\u3001\u4f01\u4e1a\u6548\u7387\u52a9\u624b\u7b49\u573a\u666f\u79ef\u7d2f\u4e86\u4e30\u5bcc\u7684AI\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u6d88\u8d39\u3001\u5065\u5eb7\u3001\u6587\u6559\u3001\u79d1\u6280\u5236\u9020\u7b49\u5404\u884c\u4e1a\u6c89\u6dc0\u4e86\u5927\u6a21\u578b\u843d\u5730\u5e94\u7528\u7684\u6700\u4f73\u5b9e\u8df5\uff0c\u81f4\u529b\u4e8e\u5e2e\u52a9\u66f4\u591a\u4f01\u4e1a\u548c\u5f00\u53d1\u8005\u62e5\u62b1 AI \u751f\u4ea7\u529b\u3002\n\n**\u4ea7\u54c1\u54a8\u8be2\u548c\u4f01\u4e1a\u670d\u52a1** \u53ef\u8054\u7cfb\u4ea7\u54c1\u5ba2\u670d\uff1a\n\n<img width=\"150\" src=\"https://cdn.link-ai.tech/portal/linkai-customer-service.png\">\n\n<br/>\n\n# \ud83c\udff7 \u66f4\u65b0\u65e5\u5fd7\n\n>**2025.05.23\uff1a** [1.7.6\u7248\u672c](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.6) \u4f18\u5316web\u7f51\u9875channel\u3001\u65b0\u589e [AgentMesh\u591a\u667a\u80fd\u4f53\u63d2\u4ef6](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/plugins/agent/README.md)\u3001\u767e\u5ea6\u8bed\u97f3\u5408\u6210\u4f18\u5316\u3001\u4f01\u5fae\u5e94\u7528`access_token`\u83b7\u53d6\u4f18\u5316\u3001\u652f\u6301`claude-4-sonnet`\u548c`claude-4-opus`\u6a21\u578b\n\n>**2025.04.11\uff1a** [1.7.5\u7248\u672c](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.5) \u65b0\u589e\u652f\u6301 [wechatferry](https://github.com/zhayujie/chatgpt-on-wechat/pull/2562) \u534f\u8bae\u3001\u65b0\u589e deepseek \u6a21\u578b\u3001\u65b0\u589e\u652f\u6301\u817e\u8baf\u4e91\u8bed\u97f3\u80fd\u529b\u3001\u65b0\u589e\u652f\u6301 ModelScope \u548c Gitee-AI API\u63a5\u53e3\n\n>**2024.12.13\uff1a** [1.7.4\u7248\u672c](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.4) \u65b0\u589e Gemini 2.0 \u6a21\u578b\u3001\u65b0\u589eweb channel\u3001\u89e3\u51b3\u5185\u5b58\u6cc4\u6f0f\u95ee\u9898\u3001\u89e3\u51b3 `#reloadp` \u547d\u4ee4\u91cd\u8f7d\u4e0d\u751f\u6548\u95ee\u9898\n\n>**2024.10.31\uff1a** [1.7.3\u7248\u672c](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.3) \u7a0b\u5e8f\u7a33\u5b9a\u6027\u63d0\u5347\u3001\u6570\u636e\u5e93\u529f\u80fd\u3001Claude\u6a21\u578b\u4f18\u5316\u3001linkai\u63d2\u4ef6\u4f18\u5316\u3001\u79bb\u7ebf\u901a\u77e5\n\n\u66f4\u591a\u66f4\u65b0\u5386\u53f2\u8bf7\u67e5\u770b: [\u66f4\u65b0\u65e5\u5fd7](/docs/version/release-notes.md)\n\n<br/>\n\n# \ud83d\ude80 \u5feb\u901f\u5f00\u59cb\n\n\u9879\u76ee\u63d0\u4f9b\u4e86\u4e00\u952e\u5b89\u88c5\u3001\u542f\u52a8\u3001\u7ba1\u7406\u7a0b\u5e8f\u7684\u811a\u672c\uff0c\u53ef\u4ee5\u9009\u62e9\u4f7f\u7528\u811a\u672c\u5feb\u901f\u8fd0\u884c\uff0c\u4e5f\u53ef\u4ee5\u6839\u636e\u8be6\u7ec6\u6307\u5f15\u4e00\u6b65\u6b65\u5b89\u88c5\u8fd0\u884c\u3002\n\n- \u8be6\u7ec6\u6587\u6863\uff1a[\u5feb\u901f\u5f00\u59cb](https://docs.link-ai.tech/cow/quick-start)\n\n- \u4e00\u952e\u5b89\u88c5\u811a\u672c\u8bf4\u660e\uff1a[\u4e00\u952e\u5b89\u88c5\u811a\u672c](https://github.com/zhayujie/chatgpt-on-wechat/wiki/%E4%B8%80%E9%94%AE%E5%AE%89%E8%A3%85%E5%90%AF%E5%8A%A8%E8%84%9A%E6%9C%AC)\n\n```bash\nbash <(curl -sS https://cdn.link-ai.tech/code/cow/install.sh)\n```\n\n- \u9879\u76ee\u7ba1\u7406\u811a\u672c\u8bf4\u660e\uff1a[\u9879\u76ee\u7ba1\u7406\u811a\u672c](https://github.com/zhayujie/chatgpt-on-wechat/wiki/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86%E8%84%9A%E6%9C%AC)\n\n## \u4e00\u3001\u51c6\u5907\n\n### 1. \u6a21\u578b\u8d26\u53f7\n\n\u9879\u76ee\u9ed8\u8ba4\u4f7f\u7528ChatGPT\u6a21\u578b\uff0c\u9700\u524d\u5f80 [OpenAI\u5e73\u53f0](https://platform.openai.com/api-keys) \u521b\u5efaAPI Key\u5e76\u586b\u5165\u9879\u76ee\u914d\u7f6e\u6587\u4ef6\u4e2d\u3002\u540c\u65f6\u652f\u6301\u5176\u4ed6\u56fd\u5185\u5916\u4ea7\u5546\u4ee5\u53ca\u7b2c\u4e09\u65b9\u81ea\u5b9a\u4e49\u6a21\u578b\u63a5\u53e3\uff0c\u8be6\u60c5\u53c2\u8003\uff1a[\u6a21\u578b\u8bf4\u660e](#\u6a21\u578b\u8bf4\u660e)\u3002\n\n\u540c\u65f6\u652f\u6301\u4f7f\u7528 **LinkAI\u5e73\u53f0** \u63a5\u53e3\uff0c\u53ef\u805a\u5408\u4f7f\u7528 OpenAI\u3001Claude\u3001DeepSeek\u3001Kimi\u3001Qwen \u7b49\u591a\u79cd\u5e38\u7528\u6a21\u578b\uff0c\u5e76\u652f\u6301\u77e5\u8bc6\u5e93\u3001\u5de5\u4f5c\u6d41\u3001\u8054\u7f51\u641c\u7d22\u3001MJ\u7ed8\u56fe\u3001\u6587\u6863\u603b\u7ed3\u7b49\u80fd\u529b\u3002\u4fee\u6539\u914d\u7f6e\u5373\u53ef\u4e00\u952e\u542f\u7528\uff0c\u53c2\u8003 [\u63a5\u5165\u6587\u6863](https://link-ai.tech/platform/link-app/wechat)\u3002\n\n### 2.\u73af\u5883\u5b89\u88c5\n\n\u652f\u6301 Linux\u3001MacOS\u3001Windows \u7cfb\u7edf\uff0c\u540c\u65f6\u9700\u5b89\u88c5 `Python`\uff0cPython\u7248\u672c\u9700\u8981\u57283.7\u4ee5\u4e0a\uff0c\u63a8\u8350\u4f7f\u75283.9\u7248\u672c\u3002\n\n> \u6ce8\u610f\uff1a\u9009\u62e9Docker\u90e8\u7f72\u5219\u65e0\u9700\u5b89\u88c5python\u73af\u5883\u548c\u4e0b\u8f7d\u6e90\u7801\uff0c\u53ef\u76f4\u63a5\u5feb\u8fdb\u5230\u4e0b\u4e00\u8282\u3002\n\n**(1) \u514b\u9686\u9879\u76ee\u4ee3\u7801\uff1a**\n\n```bash\ngit clone https://github.com/zhayujie/chatgpt-on-wechat\ncd chatgpt-on-wechat/\n```\n\n\u82e5\u9047\u5230\u7f51\u7edc\u95ee\u9898\u53ef\u4f7f\u7528\u56fd\u5185\u4ed3\u5e93\u5730\u5740\uff1ahttps://gitee.com/zhayujie/chatgpt-on-wechat\n\n**(2) \u5b89\u88c5\u6838\u5fc3\u4f9d\u8d56 (\u5fc5\u9009)\uff1a**\n\n```bash\npip3 install -r requirements.txt\n```\n\n**(3) \u62d3\u5c55\u4f9d\u8d56 (\u53ef\u9009\uff0c\u5efa\u8bae\u5b89\u88c5)\uff1a**\n\n```bash\npip3 install -r requirements-optional.txt\n```\n\u5982\u679c\u67d0\u9879\u4f9d\u8d56\u5b89\u88c5\u5931\u8d25\u53ef\u6ce8\u91ca\u6389\u5bf9\u5e94\u7684\u884c\u540e\u91cd\u8bd5\u3002\n\n## \u4e8c\u3001\u914d\u7f6e\n\n\u914d\u7f6e\u6587\u4ef6\u7684\u6a21\u677f\u5728\u6839\u76ee\u5f55\u7684`config-template.json`\u4e2d\uff0c\u9700\u590d\u5236\u8be5\u6a21\u677f\u521b\u5efa\u6700\u7ec8\u751f\u6548\u7684 `config.json` \u6587\u4ef6\uff1a\n\n```bash\n  cp config-template.json config.json\n```\n\n\u7136\u540e\u5728`config.json`\u4e2d\u586b\u5165\u914d\u7f6e\uff0c\u4ee5\u4e0b\u662f\u5bf9\u9ed8\u8ba4\u914d\u7f6e\u7684\u8bf4\u660e\uff0c\u53ef\u6839\u636e\u9700\u8981\u8fdb\u884c\u81ea\u5b9a\u4e49\u4fee\u6539\uff08\u6ce8\u610f\u5b9e\u9645\u4f7f\u7528\u65f6\u8bf7\u53bb\u6389\u6ce8\u91ca\uff0c\u4fdd\u8bc1JSON\u683c\u5f0f\u7684\u89c4\u8303\uff09\uff1a\n\n```bash\n# config.json \u6587\u4ef6\u5185\u5bb9\u793a\u4f8b\n{\n  \"channel_type\": \"web\",                                      # \u63a5\u5165\u6e20\u9053\u7c7b\u578b\uff0c\u9ed8\u8ba4\u4e3aweb\uff0c\u652f\u6301\u4fee\u6539\u4e3a:terminal, wechatmp, wechatmp_service, wechatcom_app, dingtalk, feishu\n  \"model\": \"gpt-4o-mini\",                                     # \u6a21\u578b\u540d\u79f0, \u652f\u6301 gpt-4o-mini, gpt-4.1, gpt-4o, deepseek-reasoner, wenxin, xunfei, glm-4, claude-3-7-sonnet-latest, moonshot\u7b49\n  \"open_ai_api_key\": \"YOUR API KEY\",                          # \u5982\u679c\u4f7f\u7528openAI\u6a21\u578b\u5219\u586b\u5165\u4e0a\u9762\u521b\u5efa\u7684 OpenAI API KEY\n  \"open_ai_api_base\": \"https://api.openai.com/v1\",            # OpenAI\u63a5\u53e3\u4ee3\u7406\u5730\u5740\uff0c\u4fee\u6539\u6b64\u9879\u53ef\u63a5\u5165\u7b2c\u4e09\u65b9\u6a21\u578b\u63a5\u53e3\n  \"proxy\": \"\",                                                # \u4ee3\u7406\u5ba2\u6237\u7aef\u7684ip\u548c\u7aef\u53e3\uff0c\u56fd\u5185\u73af\u5883\u5f00\u542f\u4ee3\u7406\u7684\u9700\u8981\u586b\u5199\u8be5\u9879\uff0c\u5982 \"127.0.0.1:7890\"\n  \"single_chat_prefix\": [\"bot\", \"@bot\"],                      # \u79c1\u804a\u65f6\u6587\u672c\u9700\u8981\u5305\u542b\u8be5\u524d\u7f00\u624d\u80fd\u89e6\u53d1\u673a\u5668\u4eba\u56de\u590d\n  \"single_chat_reply_prefix\": \"[bot] \",                       # \u79c1\u804a\u65f6\u81ea\u52a8\u56de\u590d\u7684\u524d\u7f00\uff0c\u7528\u4e8e\u533a\u5206\u771f\u4eba\n  \"group_chat_prefix\": [\"@bot\"],                              # \u7fa4\u804a\u65f6\u5305\u542b\u8be5\u524d\u7f00\u5219\u4f1a\u89e6\u53d1\u673a\u5668\u4eba\u56de\u590d\n  \"group_name_white_list\": [\"ChatGPT\u6d4b\u8bd5\u7fa4\", \"ChatGPT\u6d4b\u8bd5\u7fa42\"], # \u5f00\u542f\u81ea\u52a8\u56de\u590d\u7684\u7fa4\u540d\u79f0\u5217\u8868\n  \"group_chat_in_one_session\": [\"ChatGPT\u6d4b\u8bd5\u7fa4\"],              # \u652f\u6301\u4f1a\u8bdd\u4e0a\u4e0b\u6587\u5171\u4eab\u7684\u7fa4\u540d\u79f0  \n  \"image_create_prefix\": [\"\u753b\", \"\u770b\", \"\u627e\"],                   # \u5f00\u542f\u56fe\u7247\u56de\u590d\u7684\u524d\u7f00\n  \"conversation_max_tokens\": 1000,                            # \u652f\u6301\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u7684\u6700\u591a\u5b57\u7b26\u6570\n  \"speech_recognition\": false,                                # \u662f\u5426\u5f00\u542f\u8bed\u97f3\u8bc6\u522b\n  \"group_speech_recognition\": false,                          # \u662f\u5426\u5f00\u542f\u7fa4\u7ec4\u8bed\u97f3\u8bc6\u522b\n  \"voice_reply_voice\": false,                                 # \u662f\u5426\u4f7f\u7528\u8bed\u97f3\u56de\u590d\u8bed\u97f3\n  \"character_desc\": \"\u4f60\u662f\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684AI\u667a\u80fd\u52a9\u624b\uff0c\u65e8\u5728\u56de\u7b54\u5e76\u89e3\u51b3\u4eba\u4eec\u7684\u4efb\u4f55\u95ee\u9898\uff0c\u5e76\u4e14\u53ef\u4ee5\u4f7f\u7528\u591a\u79cd\u8bed\u8a00\u4e0e\u4eba\u4ea4\u6d41\u3002\",  # \u7cfb\u7edf\u63d0\u793a\u8bcd\n  # \u8ba2\u9605\u6b22\u8fce\u8bed\uff0c\u516c\u4f17\u53f7\u548c\u4f01\u4e1a\u5fae\u4fe1channel\u4e2d\u4f7f\u7528\uff0c\u5f53\u88ab\u8ba2\u9605\u65f6\u4f1a\u81ea\u52a8\u56de\u590d\u4ee5\u4e0b\u5185\u5bb9\n  \"subscribe_msg\": \"\u611f\u8c22\u60a8\u7684\u5173\u6ce8\uff01\\n\u8fd9\u91cc\u662fAI\u667a\u80fd\u52a9\u624b\uff0c\u53ef\u4ee5\u81ea\u7531\u5bf9\u8bdd\u3002\\n\u652f\u6301\u8bed\u97f3\u5bf9\u8bdd\u3002\\n\u652f\u6301\u56fe\u7247\u8f93\u5165\u3002\\n\u652f\u6301\u56fe\u7247\u8f93\u51fa\uff0c\u753b\u5b57\u5f00\u5934\u7684\u6d88\u606f\u5c06\u6309\u8981\u6c42\u521b\u4f5c\u56fe\u7247\u3002\\n\u652f\u6301tool\u3001\u89d2\u8272\u626e\u6f14\u548c\u6587\u5b57\u5192\u9669\u7b49\u4e30\u5bcc\u7684\u63d2\u4ef6\u3002\\n\u8f93\u5165{trigger_prefix}#help \u67e5\u770b\u8be6\u7ec6\u6307\u4ee4\u3002\",\n  \"use_linkai\": false,                                        # \u662f\u5426\u4f7f\u7528LinkAI\u63a5\u53e3\uff0c\u9ed8\u8ba4\u5173\u95ed\uff0c\u8bbe\u7f6e\u4e3atrue\u540e\u53ef\u5bf9\u63a5LinkAI\u5e73\u53f0\u7684\u667a\u80fd\u4f53\n  \"linkai_api_key\": \"\",                                       # LinkAI Api Key\n  \"linkai_app_code\": \"\"                                       # LinkAI \u5e94\u7528\u6216\u5de5\u4f5c\u6d41\u7684code\n}\n```\n\n**\u8be6\u7ec6\u914d\u7f6e\u8bf4\u660e:** \n\n<details>\n<summary>1. \u5355\u804a\u914d\u7f6e</summary>\n\n+ \u4e2a\u4eba\u804a\u5929\u4e2d\uff0c\u9700\u8981\u4ee5 \"bot\"\u6216\"@bot\" \u4e3a\u5f00\u5934\u7684\u5185\u5bb9\u89e6\u53d1\u673a\u5668\u4eba\uff0c\u5bf9\u5e94\u914d\u7f6e\u9879 `single_chat_prefix` (\u5982\u679c\u4e0d\u9700\u8981\u4ee5\u524d\u7f00\u89e6\u53d1\u53ef\u4ee5\u586b\u5199  `\"single_chat_prefix\": [\"\"]`)\n+ \u673a\u5668\u4eba\u56de\u590d\u7684\u5185\u5bb9\u4f1a\u4ee5 \"[bot] \" \u4f5c\u4e3a\u524d\u7f00\uff0c \u4ee5\u533a\u5206\u771f\u4eba\uff0c\u5bf9\u5e94\u7684\u914d\u7f6e\u9879\u4e3a `single_chat_reply_prefix` (\u5982\u679c\u4e0d\u9700\u8981\u524d\u7f00\u53ef\u4ee5\u586b\u5199 `\"single_chat_reply_prefix\": \"\"`)\n</details>\n\n\n<details>\n<summary>2. \u7fa4\u804a\u914d\u7f6e</summary>\n\n+ \u7fa4\u7ec4\u804a\u5929\u4e2d\uff0c\u7fa4\u540d\u79f0\u9700\u914d\u7f6e\u5728 `group_name_white_list ` \u4e2d\u624d\u80fd\u5f00\u542f\u7fa4\u804a\u81ea\u52a8\u56de\u590d\u3002\u5982\u679c\u60f3\u5bf9\u6240\u6709\u7fa4\u804a\u751f\u6548\uff0c\u53ef\u4ee5\u76f4\u63a5\u586b\u5199 `\"group_name_white_list\": [\"ALL_GROUP\"]`\n+ \u9ed8\u8ba4\u53ea\u8981\u88ab\u4eba @ \u5c31\u4f1a\u89e6\u53d1\u673a\u5668\u4eba\u81ea\u52a8\u56de\u590d\uff1b\u53e6\u5916\u7fa4\u804a\u5929\u4e2d\u53ea\u8981\u68c0\u6d4b\u5230\u4ee5 \"@bot\" \u5f00\u5934\u7684\u5185\u5bb9\uff0c\u540c\u6837\u4f1a\u81ea\u52a8\u56de\u590d\uff08\u65b9\u4fbf\u81ea\u5df1\u89e6\u53d1\uff09\uff0c\u8fd9\u5bf9\u5e94\u914d\u7f6e\u9879 `group_chat_prefix`\n+ \u53ef\u9009\u914d\u7f6e: `group_name_keyword_white_list`\u914d\u7f6e\u9879\u652f\u6301\u6a21\u7cca\u5339\u914d\u7fa4\u540d\u79f0\uff0c`group_chat_keyword`\u914d\u7f6e\u9879\u5219\u652f\u6301\u6a21\u7cca\u5339\u914d\u7fa4\u6d88\u606f\u5185\u5bb9\uff0c\u7528\u6cd5\u4e0e\u4e0a\u8ff0\u4e24\u4e2a\u914d\u7f6e\u9879\u76f8\u540c\u3002\uff08Contributed by [evolay](https://github.com/evolay))\n+ `group_chat_in_one_session`\uff1a\u4f7f\u7fa4\u804a\u5171\u4eab\u4e00\u4e2a\u4f1a\u8bdd\u4e0a\u4e0b\u6587\uff0c\u914d\u7f6e `[\"ALL_GROUP\"]` \u5219\u4f5c\u7528\u4e8e\u6240\u6709\u7fa4\u804a\n</details>\n\n<details>\n<summary>3. \u8bed\u97f3\u914d\u7f6e</summary>\n\n+ \u6dfb\u52a0 `\"speech_recognition\": true` \u5c06\u5f00\u542f\u8bed\u97f3\u8bc6\u522b\uff0c\u9ed8\u8ba4\u4f7f\u7528openai\u7684whisper\u6a21\u578b\u8bc6\u522b\u4e3a\u6587\u5b57\uff0c\u540c\u65f6\u4ee5\u6587\u5b57\u56de\u590d\uff0c\u8be5\u53c2\u6570\u4ec5\u652f\u6301\u79c1\u804a (\u6ce8\u610f\u7531\u4e8e\u8bed\u97f3\u6d88\u606f\u65e0\u6cd5\u5339\u914d\u524d\u7f00\uff0c\u4e00\u65e6\u5f00\u542f\u5c06\u5bf9\u6240\u6709\u8bed\u97f3\u81ea\u52a8\u56de\u590d\uff0c\u652f\u6301\u8bed\u97f3\u89e6\u53d1\u753b\u56fe)\uff1b\n+ \u6dfb\u52a0 `\"group_speech_recognition\": true` \u5c06\u5f00\u542f\u7fa4\u7ec4\u8bed\u97f3\u8bc6\u522b\uff0c\u9ed8\u8ba4\u4f7f\u7528openai\u7684whisper\u6a21\u578b\u8bc6\u522b\u4e3a\u6587\u5b57\uff0c\u540c\u65f6\u4ee5\u6587\u5b57\u56de\u590d\uff0c\u53c2\u6570\u4ec5\u652f\u6301\u7fa4\u804a (\u4f1a\u5339\u914dgroup_chat_prefix\u548cgroup_chat_keyword, \u652f\u6301\u8bed\u97f3\u89e6\u53d1\u753b\u56fe)\uff1b\n+ \u6dfb\u52a0 `\"voice_reply_voice\": true` \u5c06\u5f00\u542f\u8bed\u97f3\u56de\u590d\u8bed\u97f3\uff08\u540c\u65f6\u4f5c\u7528\u4e8e\u79c1\u804a\u548c\u7fa4\u804a\uff09\n</details>\n\n<details>\n<summary>4. \u5176\u4ed6\u914d\u7f6e</summary>\n\n+ `model`: \u6a21\u578b\u540d\u79f0\uff0c\u76ee\u524d\u652f\u6301 `gpt-4o-mini`, `gpt-4.1`, `gpt-4o`, `gpt-3.5-turbo`, `wenxin` , `claude` , `gemini`, `glm-4`,  `xunfei`, `moonshot`\u7b49\uff0c\u5168\u90e8\u6a21\u578b\u540d\u79f0\u53c2\u8003[common/const.py](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/common/const.py)\u6587\u4ef6\n+ `temperature`,`frequency_penalty`,`presence_penalty`: Chat API\u63a5\u53e3\u53c2\u6570\uff0c\u8be6\u60c5\u53c2\u8003[OpenAI\u5b98\u65b9\u6587\u6863\u3002](https://platform.openai.com/docs/api-reference/chat)\n+ `proxy`\uff1a\u7531\u4e8e\u76ee\u524d `openai` \u63a5\u53e3\u56fd\u5185\u65e0\u6cd5\u8bbf\u95ee\uff0c\u9700\u914d\u7f6e\u4ee3\u7406\u5ba2\u6237\u7aef\u7684\u5730\u5740\uff0c\u8be6\u60c5\u53c2\u8003  [#351](https://github.com/zhayujie/chatgpt-on-wechat/issues/351)\n+ \u5bf9\u4e8e\u56fe\u50cf\u751f\u6210\uff0c\u5728\u6ee1\u8db3\u4e2a\u4eba\u6216\u7fa4\u7ec4\u89e6\u53d1\u6761\u4ef6\u5916\uff0c\u8fd8\u9700\u8981\u989d\u5916\u7684\u5173\u952e\u8bcd\u524d\u7f00\u6765\u89e6\u53d1\uff0c\u5bf9\u5e94\u914d\u7f6e `image_create_prefix `\n+ \u5173\u4e8eOpenAI\u5bf9\u8bdd\u53ca\u56fe\u7247\u63a5\u53e3\u7684\u53c2\u6570\u914d\u7f6e\uff08\u5185\u5bb9\u81ea\u7531\u5ea6\u3001\u56de\u590d\u5b57\u6570\u9650\u5236\u3001\u56fe\u7247\u5927\u5c0f\u7b49\uff09\uff0c\u53ef\u4ee5\u53c2\u8003 [\u5bf9\u8bdd\u63a5\u53e3](https://beta.openai.com/docs/api-reference/completions) \u548c [\u56fe\u50cf\u63a5\u53e3](https://beta.openai.com/docs/api-reference/completions)  \u6587\u6863\uff0c\u5728[`config.py`](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/config.py)\u4e2d\u68c0\u67e5\u54ea\u4e9b\u53c2\u6570\u5728\u672c\u9879\u76ee\u4e2d\u662f\u53ef\u914d\u7f6e\u7684\u3002\n+ `conversation_max_tokens`\uff1a\u8868\u793a\u80fd\u591f\u8bb0\u5fc6\u7684\u4e0a\u4e0b\u6587\u6700\u5927\u5b57\u6570\uff08\u4e00\u95ee\u4e00\u7b54\u4e3a\u4e00\u7ec4\u5bf9\u8bdd\uff0c\u5982\u679c\u7d2f\u79ef\u7684\u5bf9\u8bdd\u5b57\u6570\u8d85\u51fa\u9650\u5236\uff0c\u5c31\u4f1a\u4f18\u5148\u79fb\u9664\u6700\u65e9\u7684\u4e00\u7ec4\u5bf9\u8bdd\uff09\n+ `rate_limit_chatgpt`\uff0c`rate_limit_dalle`\uff1a\u6bcf\u5206\u949f\u6700\u9ad8\u95ee\u7b54\u901f\u7387\u3001\u753b\u56fe\u901f\u7387\uff0c\u8d85\u901f\u540e\u6392\u961f\u6309\u5e8f\u5904\u7406\u3002\n+ `clear_memory_commands`: \u5bf9\u8bdd\u5185\u6307\u4ee4\uff0c\u4e3b\u52a8\u6e05\u7a7a\u524d\u6587\u8bb0\u5fc6\uff0c\u5b57\u7b26\u4e32\u6570\u7ec4\u53ef\u81ea\u5b9a\u4e49\u6307\u4ee4\u522b\u540d\u3002\n+ `hot_reload`: \u7a0b\u5e8f\u9000\u51fa\u540e\uff0c\u6682\u5b58\u7b49\u4e8e\u72b6\u6001\uff0c\u9ed8\u8ba4\u5173\u95ed\u3002\n+ `character_desc` \u914d\u7f6e\u4e2d\u4fdd\u5b58\u7740\u4f60\u5bf9\u673a\u5668\u4eba\u8bf4\u7684\u4e00\u6bb5\u8bdd\uff0c\u4ed6\u4f1a\u8bb0\u4f4f\u8fd9\u6bb5\u8bdd\u5e76\u4f5c\u4e3a\u4ed6\u7684\u8bbe\u5b9a\uff0c\u4f60\u53ef\u4ee5\u4e3a\u4ed6\u5b9a\u5236\u4efb\u4f55\u4eba\u683c      (\u5173\u4e8e\u4f1a\u8bdd\u4e0a\u4e0b\u6587\u7684\u66f4\u591a\u5185\u5bb9\u53c2\u8003\u8be5 [issue](https://github.com/zhayujie/chatgpt-on-wechat/issues/43))\n+ `subscribe_msg`\uff1a\u8ba2\u9605\u6d88\u606f\uff0c\u516c\u4f17\u53f7\u548c\u4f01\u4e1a\u5fae\u4fe1channel\u4e2d\u8bf7\u586b\u5199\uff0c\u5f53\u88ab\u8ba2\u9605\u65f6\u4f1a\u81ea\u52a8\u56de\u590d\uff0c \u53ef\u4f7f\u7528\u7279\u6b8a\u5360\u4f4d\u7b26\u3002\u76ee\u524d\u652f\u6301\u7684\u5360\u4f4d\u7b26\u6709{trigger_prefix}\uff0c\u5728\u7a0b\u5e8f\u4e2d\u5b83\u4f1a\u81ea\u52a8\u66ff\u6362\u6210bot\u7684\u89e6\u53d1\u8bcd\u3002\n</details>\n\n<details>\n<summary>5. LinkAI\u914d\u7f6e</summary>\n\n+ `use_linkai`: \u662f\u5426\u4f7f\u7528LinkAI\u63a5\u53e3\uff0c\u9ed8\u8ba4\u5173\u95ed\uff0c\u8bbe\u7f6e\u4e3atrue\u540e\u53ef\u5bf9\u63a5LinkAI\u5e73\u53f0\u7684Agent\uff0c\u4f7f\u7528\u77e5\u8bc6\u5e93\u3001\u5de5\u4f5c\u6d41\u3001\u8054\u7f51\u641c\u7d22\u3001`Midjourney` \u7ed8\u753b\u7b49\u80fd\u529b, \u53c2\u8003 [\u6587\u6863](https://link-ai.tech/platform/link-app/wechat)\n+ `linkai_api_key`: LinkAI Api Key\uff0c\u53ef\u5728 [\u63a7\u5236\u53f0](https://link-ai.tech/console/interface) \u521b\u5efa\n+ `linkai_app_code`: LinkAI \u5e94\u7528\u6216\u5de5\u4f5c\u6d41\u7684code\uff0c\u9009\u586b\n</details>\n\n\u6ce8\uff1a\u5b8c\u6574\u914d\u7f6e\u9879\u8bf4\u660e\u53ef\u5728 [`config.py`](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/config.py) \u6587\u4ef6\u4e2d\u67e5\u770b\u3002\n\n## \u4e09\u3001\u8fd0\u884c\n\n### 1.\u672c\u5730\u8fd0\u884c\n\n\u5982\u679c\u662f\u4e2a\u4eba\u8ba1\u7b97\u673a **\u672c\u5730\u8fd0\u884c**\uff0c\u76f4\u63a5\u5728\u9879\u76ee\u6839\u76ee\u5f55\u4e0b\u6267\u884c\uff1a\n\n```bash\npython3 app.py         # windows\u73af\u5883\u4e0b\u8be5\u547d\u4ee4\u901a\u5e38\u4e3a python app.py\n```\n\n\u8fd0\u884c\u540e\u9ed8\u8ba4\u4f1a\u542f\u52a8\u4e00\u4e2aweb\u670d\u52a1\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbf\u95ee `http://localhost:9899/chat` \u5728\u7f51\u9875\u7aef\u5bf9\u8bdd\u3002\u5982\u679c\u9700\u8981\u63a5\u5165\u5176\u4ed6\u5e94\u7528\u901a\u9053\u53ea\u9700\u4fee\u6539 `config.json` \u914d\u7f6e\u6587\u4ef6\u4e2d\u7684 `channel_type` \u53c2\u6570\uff0c\u8be6\u60c5\u53c2\u8003\uff1a[\u901a\u9053\u8bf4\u660e](#\u901a\u9053\u8bf4\u660e)\u3002\n\n\u5411\u673a\u5668\u4eba\u53d1\u9001 `#help` \u6d88\u606f\u53ef\u4ee5\u67e5\u770b\u53ef\u7528\u6307\u4ee4\u53ca\u63d2\u4ef6\u7684\u8bf4\u660e\u3002\n\n### 2.\u670d\u52a1\u5668\u90e8\u7f72\n\n\u5728\u670d\u52a1\u5668\u4e2d\u53ef\u4f7f\u7528 `nohup` \u547d\u4ee4\u5728\u540e\u53f0\u8fd0\u884c\u7a0b\u5e8f\uff1a\n\n```bash\nnohup python3 app.py & tail -f nohup.out\n```\n\n\u6267\u884c\u540e\u7a0b\u5e8f\u8fd0\u884c\u4e8e\u670d\u52a1\u5668\u540e\u53f0\uff0c\u53ef\u901a\u8fc7 `ctrl+c` \u5173\u95ed\u65e5\u5fd7\uff0c\u4e0d\u4f1a\u5f71\u54cd\u540e\u53f0\u7a0b\u5e8f\u7684\u8fd0\u884c\u3002\u4f7f\u7528 `ps -ef | grep app.py | grep -v grep` \u547d\u4ee4\u53ef\u67e5\u770b\u8fd0\u884c\u4e8e\u540e\u53f0\u7684\u8fdb\u7a0b\uff0c\u5982\u679c\u60f3\u8981\u91cd\u65b0\u542f\u52a8\u7a0b\u5e8f\u53ef\u4ee5\u5148 `kill` \u6389\u5bf9\u5e94\u7684\u8fdb\u7a0b\u3002 \u65e5\u5fd7\u5173\u95ed\u540e\u5982\u679c\u60f3\u8981\u518d\u6b21\u6253\u5f00\u53ea\u9700\u8f93\u5165 `tail -f nohup.out`\u3002 \n\n\u6b64\u5916\uff0c\u9879\u76ee\u7684 `scripts` \u76ee\u5f55\u4e0b\u6709\u4e00\u952e\u8fd0\u884c\u3001\u5173\u95ed\u7a0b\u5e8f\u7684\u811a\u672c\u4f9b\u4f7f\u7528\u3002 \u8fd0\u884c\u540e\u9ed8\u8ba4channel\u4e3aweb\uff0c\u901a\u8fc7\u53ef\u4ee5\u901a\u8fc7\u4fee\u6539\u914d\u7f6e\u6587\u4ef6\u8fdb\u884c\u5207\u6362\u3002\n\n\n### 3.Docker\u90e8\u7f72\n\n\u4f7f\u7528docker\u90e8\u7f72\u65e0\u9700\u4e0b\u8f7d\u6e90\u7801\u548c\u5b89\u88c5\u4f9d\u8d56\uff0c\u53ea\u9700\u8981\u83b7\u53d6 `docker-compose.yml` \u914d\u7f6e\u6587\u4ef6\u5e76\u542f\u52a8\u5bb9\u5668\u5373\u53ef\u3002\n\n> \u524d\u63d0\u662f\u9700\u8981\u5b89\u88c5\u597d `docker` \u53ca `docker-compose`\uff0c\u5b89\u88c5\u6210\u529f\u540e\u6267\u884c `docker -v` \u548c `docker-compose version` (\u6216 `docker compose version`) \u53ef\u67e5\u770b\u5230\u7248\u672c\u53f7\u3002\u5b89\u88c5\u5730\u5740\u4e3a [docker\u5b98\u7f51](https://docs.docker.com/engine/install/) \u3002\n\n**(1) \u4e0b\u8f7d docker-compose.yml \u6587\u4ef6**\n\n```bash\nwget https://cdn.link-ai.tech/code/cow/docker-compose.yml\n```\n\n\u4e0b\u8f7d\u5b8c\u6210\u540e\u6253\u5f00 `docker-compose.yml` \u586b\u5199\u6240\u9700\u914d\u7f6e\uff0c\u4f8b\u5982 `CHANNEL_TYPE`\u3001`OPEN_AI_API_KEY` \u548c\u7b49\u914d\u7f6e\u3002\n\n**(2) \u542f\u52a8\u5bb9\u5668**\n\n\u5728 `docker-compose.yml` \u6240\u5728\u76ee\u5f55\u4e0b\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u542f\u52a8\u5bb9\u5668\uff1a\n\n```bash\nsudo docker compose up -d         # \u82e5docker-compose\u4e3a 1.X \u7248\u672c\uff0c\u5219\u6267\u884c `sudo  docker-compose up -d`\n```\n\n\u8fd0\u884c\u547d\u4ee4\u540e\uff0c\u4f1a\u81ea\u52a8\u53d6 [docker hub](https://hub.docker.com/r/zhayujie/chatgpt-on-wechat) \u62c9\u53d6\u6700\u65b0release\u7248\u672c\u7684\u955c\u50cf\u3002\u5f53\u6267\u884c `sudo docker ps` \u80fd\u67e5\u770b\u5230 NAMES \u4e3a chatgpt-on-wechat \u7684\u5bb9\u5668\u5373\u8868\u793a\u8fd0\u884c\u6210\u529f\u3002\u6700\u540e\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u53ef\u67e5\u770b\u5bb9\u5668\u7684\u8fd0\u884c\u65e5\u5fd7\uff1a\n\n```bash\nsudo docker logs -f chatgpt-on-wechat\n```\n\n**(3) \u63d2\u4ef6\u4f7f\u7528**\n\n\u5982\u679c\u9700\u8981\u5728docker\u5bb9\u5668\u4e2d\u4fee\u6539\u63d2\u4ef6\u914d\u7f6e\uff0c\u53ef\u901a\u8fc7\u6302\u8f7d\u7684\u65b9\u5f0f\u5b8c\u6210\uff0c\u5c06 [\u63d2\u4ef6\u914d\u7f6e\u6587\u4ef6](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/plugins/config.json.template)\n\u91cd\u547d\u540d\u4e3a `config.json`\uff0c\u653e\u7f6e\u4e8e `docker-compose.yml` \u76f8\u540c\u76ee\u5f55\u4e0b\uff0c\u5e76\u5728 `docker-compose.yml` \u4e2d\u7684 `chatgpt-on-wechat` \u90e8\u5206\u4e0b\u6dfb\u52a0 `volumes` \u6620\u5c04:\n\n```\nvolumes:\n  - ./config.json:/app/plugins/config.json\n```\n**\u6ce8**\uff1a\u4f7f\u7528docker\u65b9\u5f0f\u90e8\u7f72\u7684\u8be6\u7ec6\u6559\u7a0b\u53ef\u4ee5\u53c2\u8003\uff1a[docker\u90e8\u7f72CoW\u9879\u76ee](https://www.wangpc.cc/ai/docker-deploy-cow/)\n\n\n## \u6a21\u578b\u8bf4\u660e\n\n\u4ee5\u4e0b\u5bf9\u6240\u6709\u53ef\u652f\u6301\u7684\u6a21\u578b\u7684\u914d\u7f6e\u548c\u4f7f\u7528\u65b9\u6cd5\u8fdb\u884c\u8bf4\u660e\uff0c\u6a21\u578b\u63a5\u53e3\u5b9e\u73b0\u5728\u9879\u76ee\u7684 `bot/` \u76ee\u5f55\u4e0b\u3002\n>\u90e8\u5206\u6a21\u578b\u5382\u5546\u63a5\u5165\u6709\u5b98\u65b9sdk\u548cOpenAI\u517c\u5bb9\u4e24\u79cd\u65b9\u5f0f\uff0c\u5efa\u8bae\u4f7f\u7528OpenAI\u517c\u5bb9\u7684\u65b9\u5f0f\u3002\n\n<details>\n<summary>OpenAI</summary>\n\n1. API Key\u521b\u5efa\uff1a\u5728 [OpenAI\u5e73\u53f0](https://platform.openai.com/api-keys) \u521b\u5efaAPI Key\n\n2. \u586b\u5199\u914d\u7f6e\n\n```json\n{\n    \"model\": \"gpt-4.1-mini\",\n    \"open_ai_api_key\": \"YOUR_API_KEY\",\n    \"open_ai_api_base\": \"https://api.openai.com/v1\",\n    \"bot_type\": \"chatGPT\"\n}\n```\n\n - `model`: \u4e0eOpenAI\u63a5\u53e3\u7684 [model\u53c2\u6570](https://platform.openai.com/docs/models) \u4e00\u81f4\uff0c\u652f\u6301\u5305\u62ec o\u7cfb\u5217\u3001gpt-4\u7cfb\u5217\u3001gpt-3.5\u7cfb\u5217\u7b49\u6a21\u578b\n - `open_ai_api_base`: \u5982\u679c\u9700\u8981\u63a5\u5165\u7b2c\u4e09\u65b9\u4ee3\u7406\u63a5\u53e3\uff0c\u53ef\u901a\u8fc7\u4fee\u6539\u8be5\u53c2\u6570\u8fdb\u884c\u63a5\u5165\n - `bot_type`: \u4f7f\u7528OpenAI\u76f8\u5173\u6a21\u578b\u65f6\u65e0\u9700\u586b\u5199\u3002\u5f53\u4f7f\u7528\u7b2c\u4e09\u65b9\u4ee3\u7406\u63a5\u53e3\u63a5\u5165Claude\u7b49\u975eOpenAI\u5b98\u65b9\u6a21\u578b\u65f6\uff0c\u8be5\u53c2\u6570\u8bbe\u4e3a `chatGPT`\n</details>\n\n<details>\n<summary>LinkAI</summary>\n\n1. API Key\u521b\u5efa\uff1a\u5728 [LinkAI\u5e73\u53f0](https://link-ai.tech/console/interface) \u521b\u5efaAPI Key \n\n2. \u586b\u5199\u914d\u7f6e\n\n```json\n{\n \"use_linkai\": true,\n \"linkai_api_key\": \"YOUR API KEY\",\n \"linkai_app_code\": \"YOUR APP CODE\"\n}\n```\n\n+ `use_linkai`: \u662f\u5426\u4f7f\u7528LinkAI\u63a5\u53e3\uff0c\u9ed8\u8ba4\u5173\u95ed\uff0c\u8bbe\u7f6e\u4e3atrue\u540e\u53ef\u5bf9\u63a5LinkAI\u5e73\u53f0\u7684\u667a\u80fd\u4f53\uff0c\u4f7f\u7528\u77e5\u8bc6\u5e93\u3001\u5de5\u4f5c\u6d41\u3001\u6570\u636e\u5e93\u3001\u8054\u7f51\u641c\u7d22\u3001MCP\u5de5\u5177\u7b49\u4e30\u5bcc\u7684Agent\u80fd\u529b, \u53c2\u8003 [\u6587\u6863](https://link-ai.tech/platform/link-app/wechat)\n+ `linkai_api_key`: LinkAI\u5e73\u53f0\u7684API Key\uff0c\u53ef\u5728 [\u63a7\u5236\u53f0](https://link-ai.tech/console/interface) \u4e2d\u521b\u5efa\n+ `linkai_app_code`: LinkAI\u667a\u80fd\u4f53 (\u5e94\u7528\u6216\u5de5\u4f5c\u6d41) \u7684code\uff0c\u9009\u586b\u3002\u667a\u80fd\u4f53\u521b\u5efa\u53ef\u53c2\u8003 [\u8bf4\u660e\u6587\u6863](https://docs.link-ai.tech/platform/quick-start)\n+ `model`: model\u5b57\u6bb5\u586b\u5199\u7a7a\u5219\u76f4\u63a5\u4f7f\u7528\u667a\u80fd\u4f53\u7684\u6a21\u578b\uff0c\u53ef\u5728\u5e73\u53f0\u4e2d\u7075\u6d3b\u5207\u6362\uff0c[\u6a21\u578b\u5217\u8868](https://link-ai.tech/console/models)\u4e2d\u7684\u5168\u90e8\u6a21\u578b\u5747\u53ef\u4f7f\u7528\n</details>\n\n<details>\n<summary>DeepSeek</summary>\n\n1. API Key\u521b\u5efa\uff1a\u5728 [DeepSeek\u5e73\u53f0](https://platform.deepseek.com/api_keys) \u521b\u5efaAPI Key \n\n2. \u586b\u5199\u914d\u7f6e\n\n```json\n{\n  \"bot_type\": \"chatGPT\",\n  \"model\": \"deepseek-chat\",\n  \"open_ai_api_key\": \"sk-xxxxxxxxxxx\",\n  \"open_ai_api_base\": \"https://api.deepseek.com/v1\"\n}\n```\n\n - `bot_type`: OpenAI\u517c\u5bb9\u65b9\u5f0f\n - `model`: \u53ef\u586b `deepseek-chat\u3001deepseek-reasoner`\uff0c\u5206\u522b\u5bf9\u5e94\u7684\u662f V3 \u548c R1 \u6a21\u578b\n - `open_ai_api_key`: DeepSeek\u5e73\u53f0\u7684 API Key\n - `open_ai_api_base`: DeepSeek\u5e73\u53f0 BASE URL\n</details>\n\n<details>\n<summary>Azure</summary>\n\n1. API Key\u521b\u5efa\uff1a\u5728 [DeepSeek\u5e73\u53f0](https://platform.deepseek.com/api_keys) \u521b\u5efaAPI Key \n\n2. \u586b\u5199\u914d\u7f6e\n\n```json\n{\n  \"model\": \"\",\n  \"use_azure_chatgpt\": true,\n  \"open_ai_api_key\": \"e7ffc5dd84f14521a53f14a40231ea78\",\n  \"open_ai_api_base\": \"https://linkai-240917.openai.azure.com/\",\n  \"azure_deployment_id\": \"gpt-4.1\",\n  \"azure_api_version\": \"2025-01-01-preview\"\n}\n```\n\n - `model`: \u7559\u7a7a\u5373\u53ef\n - `use_azure_chatgpt`: \u8bbe\u4e3a true \n - `open_ai_api_key`: Azure\u5e73\u53f0\u7684\u5bc6\u94a5\n - `open_ai_api_base`: Azure\u5e73\u53f0\u7684 BASE URL\n - `azure_deployment_id`: Azure\u5e73\u53f0\u90e8\u7f72\u7684\u6a21\u578b\u540d\u79f0\n - `azure_api_version`: api\u7248\u672c\u4ee5\u53ca\u4ee5\u4e0a\u53c2\u6570\u53ef\u4ee5\u5728\u90e8\u7f72\u7684 [\u6a21\u578b\u914d\u7f6e](https://oai.azure.com/resource/deployments) \u754c\u9762\u67e5\u770b\n</details>\n\n<details>\n<summary>Claude</summary>\n\n1. API Key\u521b\u5efa\uff1a\u5728 [Claude\u63a7\u5236\u53f0](https://console.anthropic.com/settings/keys) \u521b\u5efaAPI Key\n\n2. \u586b\u5199\u914d\u7f6e\n\n```json\n{\n    \"model\": \"claude-sonnet-4-0\",\n    \"claude_api_key\": \"YOUR_API_KEY\"\n}\n```\n - `model`: \u53c2\u8003 [\u5b98\u65b9\u6a21\u578bID](https://docs.anthropic.com/en/docs/about-claude/models/overview#model-aliases) \uff0c\u4f8b\u5982`claude-opus-4-0`\u3001`claude-3-7-sonnet-latest`\u7b49\n</details>\n\n<details>\n<summary>\u901a\u4e49\u5343\u95ee</summary>\n\n\u65b9\u5f0f\u4e00\uff1a\u5b98\u65b9SDK\u63a5\u5165\uff0c\u914d\u7f6e\u5982\u4e0b\uff1a\n\n```json\n{\n    \"model\": \"qwen-turbo\",\n    \"dashscope_api_key\": \"sk-qVxxxxG\"\n}\n```\n - `model`: \u53ef\u586b\u5199`qwen-turbo\u3001qwen-plus\u3001qwen-max`\n - `dashscope_api_key`: \u901a\u4e49\u5343\u95ee\u7684 API-KEY\uff0c\u53c2\u8003 [\u5b98\u65b9\u6587\u6863](https://bailian.console.aliyun.com/?tab=api#/api) \uff0c\u5728 [\u63a7\u5236\u53f0](https://bailian.console.aliyun.com/?tab=model#/api-key) \u521b\u5efa\n \n\u65b9\u5f0f\u4e8c\uff1aOpenAI\u517c\u5bb9\u65b9\u5f0f\u63a5\u5165\uff0c\u914d\u7f6e\u5982\u4e0b\uff1a\n```json\n{\n  \"bot_type\": \"chatGPT\",\n  \"model\": \"qwen-turbo\",\n  \"open_ai_api_base\": \"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n  \"open_ai_api_key\": \"sk-qVxxxxG\"\n}\n```\n- `bot_type`: OpenAI\u517c\u5bb9\u65b9\u5f0f\n- `model`: \u652f\u6301\u5b98\u65b9\u6240\u6709\u6a21\u578b\uff0c\u53c2\u8003[\u6a21\u578b\u5217\u8868](https://help.aliyun.com/zh/model-studio/models?spm=a2c4g.11186623.0.0.78d84823Kth5on#9f8890ce29g5u)\n- `open_ai_api_base`: \u901a\u4e49\u5343\u95eeAPI\u7684 BASE URL\n- `open_ai_api_key`: \u901a\u4e49\u5343\u95ee\u7684 API-KEY\uff0c\u53c2\u8003 [\u5b98\u65b9\u6587\u6863](https://bailian.console.aliyun.com/?tab=api#/api) \uff0c\u5728 [\u63a7\u5236\u53f0](https://bailian.console.aliyun.com/?tab=model#/api-key) \u521b\u5efa\n</details>\n\n<details>\n<summary>Gemini</summary>\n\nAPI Key\u521b\u5efa\uff1a\u5728 [\u63a7\u5236\u53f0](https://aistudio.google.com/app/apikey?hl=zh-cn) \u521b\u5efaAPI Key \uff0c\u914d\u7f6e\u5982\u4e0b\n```json\n{\n    \"model\": \"gemini-2.5-pro\",\n    \"gemini_api_key\": \"\"\n}\n```\n - `model`: \u53c2\u8003[\u5b98\u65b9\u6587\u6863-\u6a21\u578b\u5217\u8868](https://ai.google.dev/gemini-api/docs/models?hl=zh-cn)\n</details>\n\n<details>\n<summary>Moonshot</summary>\n\n\u65b9\u5f0f\u4e00\uff1a\u5b98\u65b9\u63a5\u5165\uff0c\u914d\u7f6e\u5982\u4e0b\uff1a\n\n```json\n{\n    \"model\": \"moonshot-v1-8k\",\n    \"moonshot_api_key\": \"moonshot-v1-8k\"\n}\n```\n - `model`: \u53ef\u586b\u5199`moonshot-v1-8k\u3001 moonshot-v1-32k\u3001 moonshot-v1-128k`\n - `moonshot_api_key`: Moonshot\u7684API-KEY\uff0c\u5728 [\u63a7\u5236\u53f0](https://platform.moonshot.cn/console/api-keys) \u521b\u5efa\n \n\u65b9\u5f0f\u4e8c\uff1aOpenAI\u517c\u5bb9\u65b9\u5f0f\u63a5\u5165\uff0c\u914d\u7f6e\u5982\u4e0b\uff1a\n```json\n{\n  \"bot_type\": \"chatGPT\",\n  \"model\": \"moonshot-v1-8k\",\n  \"open_ai_api_base\": \"https://api.moonshot.cn/v1\",\n  \"open_ai_api_key\": \"\"\n}\n```\n- `bot_type`: OpenAI\u517c\u5bb9\u65b9\u5f0f\n- `model`: \u53ef\u586b\u5199`moonshot-v1-8k\u3001 moonshot-v1-32k\u3001 moonshot-v1-128k`\n- `open_ai_api_base`: Moonshot\u7684 BASE URL\n- `open_ai_api_key`: Moonshot\u7684 API-KEY\uff0c\u5728 [\u63a7\u5236\u53f0](https://platform.moonshot.cn/console/api-keys) \u521b\u5efa\n</details>\n\n<details>\n<summary>\u767e\u5ea6\u6587\u5fc3</summary>\n\u65b9\u5f0f\u4e00\uff1a\u5b98\u65b9SDK\u63a5\u5165\uff0c\u914d\u7f6e\u5982\u4e0b\uff1a\n\n```json\n{\n    \"model\": \"wenxin\", \n    \"baidu_wenxin_api_key\": \"IajztZ0bDxgnP9bEykU7lBer\",\n    \"baidu_wenxin_secret_key\": \"EDPZn6L24uAS9d8RWFfotK47dPvkjD6G\"\n}\n```\n - `model`: \u53ef\u586b `wenxin`\u548c`wenxin-4`\uff0c\u5bf9\u5e94\u6a21\u578b\u4e3a \u6587\u5fc3-3.5 \u548c \u6587\u5fc3-4.0\n - `baidu_wenxin_api_key`\uff1a\u53c2\u8003 [\u5343\u5e06\u5e73\u53f0-access_token\u9274\u6743](https://cloud.baidu.com/doc/WENXINWORKSHOP/s/dlv4pct3s) \u6587\u6863\u83b7\u53d6 API Key\n - `baidu_wenxin_secret_key`\uff1a\u53c2\u8003 [\u5343\u5e06\u5e73\u53f0-access_token\u9274\u6743](https://cloud.baidu.com/doc/WENXINWORKSHOP/s/dlv4pct3s) \u6587\u6863\u83b7\u53d6 Secret Key\n\n\u65b9\u5f0f\u4e8c\uff1aOpenAI\u517c\u5bb9\u65b9\u5f0f\u63a5\u5165\uff0c\u914d\u7f6e\u5982\u4e0b\uff1a\n```json\n{\n  \"bot_type\": \"chatGPT\",\n  \"model\": \"qwen-turbo\",\n  \"open_ai_api_base\": \"https://qianfan.baidubce.com/v2\",\n  \"open_ai_api_key\": \"bce-v3/ALTxxxxxxd2b\"\n}\n```\n- `bot_type`: OpenAI\u517c\u5bb9\u65b9\u5f0f\n- `model`: \u652f\u6301\u5b98\u65b9\u6240\u6709\u6a21\u578b\uff0c\u53c2\u8003[\u6a21\u578b\u5217\u8868](https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Wm9cvy6rl)\n- `open_ai_api_base`: \u767e\u5ea6\u6587\u5fc3API\u7684 BASE URL\n- `open_ai_api_key`: \u767e\u5ea6\u6587\u5fc3\u7684 API-KEY\uff0c\u53c2\u8003 [\u5b98\u65b9\u6587\u6863](https://cloud.baidu.com/doc/qianfan-api/s/ym9chdsy5) \uff0c\u5728 [\u63a7\u5236\u53f0](https://console.bce.baidu.com/iam/#/iam/apikey/list) \u521b\u5efaAPI Key\n\n</details>\n\n<details>\n<summary>\u8baf\u98de\u661f\u706b</summary>\n\n\u65b9\u5f0f\u4e00\uff1a\u5b98\u65b9\u63a5\u5165\uff0c\u914d\u7f6e\u5982\u4e0b\uff1a\n\u53c2\u8003 [\u5b98\u65b9\u6587\u6863-\u5feb\u901f\u6307\u5f15](https://www.xfyun.cn/doc/platform/quickguide.html#%E7%AC%AC%E4%BA%8C%E6%AD%A5-%E5%88%9B%E5%BB%BA%E6%82%A8%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%BA%94%E7%94%A8-%E5%BC%80%E5%A7%8B%E4%BD%BF%E7%94%A8%E6%9C%8D%E5%8A%A1) \u83b7\u53d6 `APPID\u3001 APISecret\u3001 APIKey` \u4e09\u4e2a\u53c2\u6570\n\n```json\n{\n  \"model\": \"xunfei\",\n  \"xunfei_app_id\": \"\",\n  \"xunfei_api_key\": \"\",\n  \"xunfei_api_secret\": \"\",\n  \"xunfei_domain\": \"4.0Ultra\",\n  \"xunfei_spark_url\": \"wss://spark-api.xf-yun.com/v4.0/chat\"\n}\n```\n - `model`: \u586b `xunfei`\n - `xunfei_domain`: \u53ef\u586b\u5199 `4.0Ultra\u3001 generalv3.5\u3001 max-32k\u3001 generalv3\u3001 pro-128k\u3001 lite`\n - `xunfei_spark_url`: \u586b\u5199\u53c2\u8003 [\u5b98\u65b9\u6587\u6863-\u8bf7\u6c42\u5730\u5740](https://www.xfyun.cn/doc/spark/Web.html#_1-1-%E8%AF%B7%E6%B1%82%E5%9C%B0%E5%9D%80) \u7684\u8bf4\u660e\n \n\u65b9\u5f0f\u4e8c\uff1aOpenAI\u517c\u5bb9\u65b9\u5f0f\u63a5\u5165\uff0c\u914d\u7f6e\u5982\u4e0b\uff1a\n```json\n{\n  \"bot_type\": \"chatGPT\",\n  \"model\": \"4.0Ultra\",\n  \"open_ai_api_base\": \"https://spark-api-open.xf-yun.com/v1\",\n  \"open_ai_api_key\": \"\"\n}\n```\n- `bot_type`: OpenAI\u517c\u5bb9\u65b9\u5f0f\n- `model`: \u53ef\u586b\u5199 `4.0Ultra\u3001 generalv3.5\u3001 max-32k\u3001 generalv3\u3001 pro-128k\u3001 lite`\n- `open_ai_api_base`: \u8baf\u98de\u661f\u706b\u5e73\u53f0\u7684 BASE URL\n- `open_ai_api_key`: \u8baf\u98de\u661f\u706b\u5e73\u53f0\u7684[APIPassword](https://console.xfyun.cn/services/bm3) \uff0c\u56e0\u6a21\u578b\u800c\u5df2\n</details>\n\n<details>\n<summary>\u667a\u8c31AI</summary>\n\n\u65b9\u5f0f\u4e00\uff1a\u5b98\u65b9\u63a5\u5165\uff0c\u914d\u7f6e\u5982\u4e0b\uff1a\n\n```json\n{\n  \"model\": \"glm-4-plus\",\n  \"zhipu_ai_api_key\": \"\"\n}\n```\n - `model`: \u53ef\u586b `glm-4-plus\u3001glm-4-air-250414\u3001glm-4-airx\u3001glm-4-long \u3001glm-4-flashx \u3001glm-4-flash-250414`, \u53c2\u8003 [glm-4\u7cfb\u5217\u6a21\u578b\u7f16\u7801](https://bigmodel.cn/dev/api/normal-model/glm-4)\n - `zhipu_ai_api_key`: \u667a\u8c31AI\u5e73\u53f0\u7684 API KEY\uff0c\u5728 [\u63a7\u5236\u53f0](https://www.bigmodel.cn/usercenter/proj-mgmt/apikeys) \u521b\u5efa\n \n\u65b9\u5f0f\u4e8c\uff1aOpenAI\u517c\u5bb9\u65b9\u5f0f\u63a5\u5165\uff0c\u914d\u7f6e\u5982\u4e0b\uff1a\n```json\n{\n  \"bot_type\": \"chatGPT\",\n  \"model\": \"glm-4-plus\",\n  \"open_ai_api_base\": \"https://open.bigmodel.cn/api/paas/v4\",\n  \"open_ai_api_key\": \"\"\n}\n```\n- `bot_type`: OpenAI\u517c\u5bb9\u65b9\u5f0f\n- `model`: \u53ef\u586b `glm-4-plus\u3001glm-4-air-250414\u3001glm-4-airx\u3001glm-4-long \u3001glm-4-flashx \u3001glm-4-flash-250414`, \u53c2\u8003 [glm-4\u7cfb\u5217\u6a21\u578b\u7f16\u7801](https://bigmodel.cn/dev/api/normal-model/glm-4) \n- `open_ai_api_base`: \u667a\u8c31AI\u5e73\u53f0\u7684 BASE URL\n- `open_ai_api_key`: \u667a\u8c31AI\u5e73\u53f0\u7684 API KEY\uff0c\u5728 [\u63a7\u5236\u53f0](https://www.bigmodel.cn/usercenter/proj-mgmt/apikeys) \u521b\u5efa\n</details>\n\n<details>\n<summary>MiniMax</summary>\n\n\u65b9\u5f0f\u4e00\uff1a\u5b98\u65b9\u63a5\u5165\uff0c\u914d\u7f6e\u5982\u4e0b\uff1a\n\n```json\n{\n    \"model\": \"abab6.5-chat\",\n    \"Minimax_api_key\": \"\",\n    \"Minimax_group_id\": \"\"\n}\n```\n - `model`: \u53ef\u586b\u5199`abab6.5-chat`\n - `Minimax_api_key`\uff1aMiniMax\u5e73\u53f0\u7684API-KEY\uff0c\u5728 [\u63a7\u5236\u53f0](https://platform.minimaxi.com/user-center/basic-information/interface-key) \u521b\u5efa\n - `Minimax_group_id`: \u5728 [\u8d26\u6237\u4fe1\u606f](https://platform.minimaxi.com/user-center/basic-information) \u53f3\u4e0a\u89d2\u83b7\u53d6\n \n\u65b9\u5f0f\u4e8c\uff1aOpenAI\u517c\u5bb9\u65b9\u5f0f\u63a5\u5165\uff0c\u914d\u7f6e\u5982\u4e0b\uff1a\n```json\n{\n  \"bot_type\": \"chatGPT\",\n  \"model\": \"MiniMax-M1\",\n  \"open_ai_api_base\": \"https://api.minimaxi.com/v1\",\n  \"open_ai_api_key\": \"\"\n}\n```\n- `bot_type`: OpenAI\u517c\u5bb9\u65b9\u5f0f\n- `model`: \u53ef\u586b`MiniMax-M1\u3001MiniMax-Text-01`\uff0c\u53c2\u8003[API\u6587\u6863](https://platform.minimaxi.com/document/%E5%AF%B9%E8%AF%9D?key=66701d281d57f38758d581d0#QklxsNSbaf6kM4j6wjO5eEek)\n- `open_ai_api_base`: MiniMax\u5e73\u53f0API\u7684 BASE URL\n- `open_ai_api_key`: MiniMax\u5e73\u53f0\u7684API-KEY\uff0c\u5728 [\u63a7\u5236\u53f0](https://platform.minimaxi.com/user-center/basic-information/interface-key) \u521b\u5efa\n</details>\n\n<details>\n<summary>ModelScope</summary>\n\n```json\n{\n  \"bot_type\": \"modelscope\",\n  \"model\": \"Qwen/QwQ-32B\",\n  \"modelscope_api_key\": \"your_api_key\",\n  \"modelscope_base_url\": \"https://api-inference.modelscope.cn/v1/chat/completions\",\n  \"text_to_image\": \"MusePublic/489_ckpt_FLUX_1\"\n}\n```\n\n- `bot_type`: modelscope\u63a5\u53e3\u683c\u5f0f\n- `model`: \u53c2\u8003[\u6a21\u578b\u5217\u8868](https://www.modelscope.cn/models?filter=inference_type&page=1)\n- `modelscope_api_key`: \u53c2\u8003 [\u5b98\u65b9\u6587\u6863-\u8bbf\u95ee\u4ee4\u724c](https://modelscope.cn/docs/accounts/token) \uff0c\u5728 [\u63a7\u5236\u53f0](https://modelscope.cn/my/myaccesstoken) \n- `modelscope_base_url`: modelscope\u5e73\u53f0\u7684 BASE URL\n- `text_to_image`: \u56fe\u50cf\u751f\u6210\u6a21\u578b\uff0c\u53c2\u8003[\u6a21\u578b\u5217\u8868](https://www.modelscope.cn/models?filter=inference_type&page=1)\n</details>\n\n\n## \u901a\u9053\u8bf4\u660e\n\n\u4ee5\u4e0b\u5bf9\u53ef\u63a5\u5165\u901a\u9053\u7684\u914d\u7f6e\u65b9\u5f0f\u8fdb\u884c\u8bf4\u660e\uff0c\u5e94\u7528\u901a\u9053\u4ee3\u7801\u5728\u9879\u76ee\u7684 `channel/` \u76ee\u5f55\u4e0b\u3002\n\n<details>\n<summary>Web</summary>\n\n\u9879\u76ee\u542f\u52a8\u540e\u9ed8\u8ba4\u8fd0\u884cweb\u901a\u9053\uff0c\u914d\u7f6e\u5982\u4e0b\uff1a\n\n```json\n{\n    \"channel_type\": \"web\",\n    \"web_port\": 9899\n}\n```\n- `web_port`: \u9ed8\u8ba4\u4e3a 9899\uff0c\u53ef\u6309\u9700\u66f4\u6539\uff0c\u9700\u8981\u670d\u52a1\u5668\u9632\u706b\u5899\u548c\u5b89\u5168\u7ec4\u653e\u884c\u8be5\u7aef\u53e3\n- \u5982\u672c\u5730\u8fd0\u884c\uff0c\u542f\u52a8\u540e\u8bf7\u8bbf\u95ee `http://localhost:port/chat` \uff1b\u5982\u670d\u52a1\u5668\u8fd0\u884c\uff0c\u8bf7\u8bbf\u95ee `http://ip:port/chat` \n> \u6ce8\uff1a\u8bf7\u5c06\u4e0a\u8ff0 url \u4e2d\u7684 ip \u6216\u8005 port \u66ff\u6362\u4e3a\u5b9e\u9645\u7684\u503c\n</details>\n\n<details>\n<summary>Terminal</summary>\n\n\u4fee\u6539 `config.json` \u4e2d\u7684 `channel_type` \u5b57\u6bb5\uff1a\n\n```json\n{\n    \"channel_type\": \"terminal\"\n}\n```\n\n\u8fd0\u884c\u540e\u53ef\u5728\u7ec8\u7aef\u4e0e\u673a\u5668\u4eba\u8fdb\u884c\u5bf9\u8bdd\u3002\n\n</details>\n\n<details>\n<summary>\u5fae\u4fe1\u516c\u4f17\u53f7</summary>\n\n\u672c\u9879\u76ee\u652f\u6301\u8ba2\u9605\u53f7\u548c\u670d\u52a1\u53f7\u4e24\u79cd\u516c\u4f17\u53f7\uff0c\u901a\u8fc7\u670d\u52a1\u53f7(`wechatmp_service`)\u4f53\u9a8c\u66f4\u4f73\u3002\u5c06\u4e0b\u5217\u914d\u7f6e\u52a0\u5165 `config.json`\uff1a\n\n```json\n{\n    \"channel_type\": \"wechatmp\",\n    \"wechatmp_token\": \"TOKEN\",\n    \"wechatmp_port\": 80,\n    \"wechatmp_app_id\": \"APPID\",\n    \"wechatmp_app_secret\": \"APPSECRET\",\n    \"wechatmp_aes_key\": \"\"\n}\n```\n- `channel_type`: \u4e2a\u4eba\u8ba2\u9605\u53f7\u4e3a`wechatmp`\uff0c\u4f01\u4e1a\u670d\u52a1\u53f7\u4e3a`wechatmp_service`\n\n\u8be6\u7ec6\u6b65\u9aa4\u548c\u53c2\u6570\u8bf4\u660e\u53c2\u8003 [\u5fae\u4fe1\u516c\u4f17\u53f7\u63a5\u5165](https://docs.link-ai.tech/cow/multi-platform/wechat-mp)\n\n</details>\n\n<details>\n<summary>\u4f01\u4e1a\u5fae\u4fe1\u5e94\u7528</summary>\n\n\u4f01\u4e1a\u5fae\u4fe1\u81ea\u5efa\u5e94\u7528\u63a5\u5165\u9700\u5728\u540e\u53f0\u521b\u5efa\u5e94\u7528\u5e76\u542f\u7528\u6d88\u606f\u56de\u8c03\uff0c\u914d\u7f6e\u793a\u4f8b\uff1a\n\n```json\n{\n    \"channel_type\": \"wechatcom_app\",\n    \"wechatcom_corp_id\": \"CORPID\",\n    \"wechatcomapp_token\": \"TOKEN\",\n    \"wechatcomapp_port\": 9898,\n    \"wechatcomapp_secret\": \"SECRET\",\n    \"wechatcomapp_agent_id\": \"AGENTID\",\n    \"wechatcomapp_aes_key\": \"AESKEY\"\n}\n```\n\u8be6\u7ec6\u6b65\u9aa4\u548c\u53c2\u6570\u8bf4\u660e\u53c2\u8003 [\u4f01\u5fae\u81ea\u5efa\u5e94\u7528\u63a5\u5165](https://docs.link-ai.tech/cow/multi-platform/wechat-com)\n\n</details>\n\n<details>\n<summary>\u9489\u9489</summary>\n\n\u9489\u9489\u9700\u8981\u5728\u5f00\u653e\u5e73\u53f0\u521b\u5efa\u667a\u80fd\u673a\u5668\u4eba\u5e94\u7528\uff0c\u5c06\u4ee5\u4e0b\u914d\u7f6e\u586b\u5165 `config.json`\uff1a\n\n```json\n{\n    \"channel_type\": \"dingtalk\",\n    \"dingtalk_client_id\": \"CLIENT_ID\",\n    \"dingtalk_client_secret\": \"CLIENT_SECRET\"\n}\n```\n\u8be6\u7ec6\u6b65\u9aa4\u548c\u53c2\u6570\u8bf4\u660e\u53c2\u8003 [\u9489\u9489\u63a5\u5165](https://docs.link-ai.tech/cow/multi-platform/dingtalk)\n</details>\n\n<details>\n<summary>\u98de\u4e66</summary>\n\n\u901a\u8fc7\u81ea\u5efa\u5e94\u7528\u63a5\u5165AI\u76f8\u5173\u80fd\u529b\u5230\u98de\u4e66\u5e94\u7528\u4e2d\uff0c\u9ed8\u8ba4\u5df2\u662f\u98de\u4e66\u7684\u4f01\u4e1a\u7528\u6237\uff0c\u4e14\u5177\u6709\u4f01\u4e1a\u7ba1\u7406\u6743\u9650\uff0c\u5c06\u4ee5\u4e0b\u914d\u7f6e\u586b\u5165 `config.json`\uff1a\uff1a\n\n```json\n{\n    \"channel_type\": \"feishu\",\n    \"feishu_app_id\": \"APP_ID\",\n    \"feishu_app_secret\": \"APP_SECRET\",\n    \"feishu_token\": \"VERIFICATION_TOKEN\",\n    \"feishu_port\": 80\n}\n```\n\u8be6\u7ec6\u6b65\u9aa4\u548c\u53c2\u6570\u8bf4\u660e\u53c2\u8003 [\u98de\u4e66\u63a5\u5165](https://docs.link-ai.tech/cow/multi-platform/feishu)\n</details>\n\n<br/>\n\n# \ud83d\udd17 \u76f8\u5173\u9879\u76ee\n\n- [bot-on-anything](https://github.com/zhayujie/bot-on-anything)\uff1a\u8f7b\u91cf\u548c\u9ad8\u53ef\u6269\u5c55\u7684\u5927\u6a21\u578b\u5e94\u7528\u6846\u67b6\uff0c\u652f\u6301\u63a5\u5165Slack, Telegram, Discord, Gmail\u7b49\u6d77\u5916\u5e73\u53f0\uff0c\u53ef\u4f5c\u4e3a\u672c\u9879\u76ee\u7684\u8865\u5145\u4f7f\u7528\u3002\n- [AgentMesh](https://github.com/MinimalFuture/AgentMesh)\uff1a\u5f00\u6e90\u7684\u591a\u667a\u80fd\u4f53(Multi-Agent)\u6846\u67b6\uff0c\u53ef\u4ee5\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u56e2\u961f\u7684\u534f\u540c\u6765\u89e3\u51b3\u590d\u6742\u95ee\u9898\u3002\u672c\u9879\u76ee\u57fa\u4e8e\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86[Agent\u63d2\u4ef6](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/plugins/agent/README.md)\uff0c\u53ef\u8bbf\u95ee\u7ec8\u7aef\u3001\u6d4f\u89c8\u5668\u3001\u6587\u4ef6\u7cfb\u7edf\u3001\u641c\u7d22\u5f15\u64ce \u7b49\u5404\u7c7b\u5de5\u5177\uff0c\u5e76\u5b9e\u73b0\u4e86\u591a\u667a\u80fd\u4f53\u534f\u540c\u3002\n\n\n\n# \ud83d\udd0e \u5e38\u89c1\u95ee\u9898\n\nFAQs\uff1a <https://github.com/zhayujie/chatgpt-on-wechat/wiki/FAQs>\n\n\u6216\u76f4\u63a5\u5728\u7ebf\u54a8\u8be2 [\u9879\u76ee\u5c0f\u52a9\u624b](https://link-ai.tech/app/Kv2fXJcH)  (\u77e5\u8bc6\u5e93\u6301\u7eed\u5b8c\u5584\u4e2d\uff0c\u56de\u590d\u4f9b\u53c2\u8003)\n\n# \ud83d\udee0\ufe0f \u5f00\u53d1\n\n\u6b22\u8fce\u63a5\u5165\u66f4\u591a\u5e94\u7528\u901a\u9053\uff0c\u53c2\u8003 [Terminal\u4ee3\u7801](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/channel/terminal/terminal_channel.py) \u65b0\u589e\u81ea\u5b9a\u4e49\u901a\u9053\uff0c\u5b9e\u73b0\u63a5\u6536\u548c\u53d1\u9001\u6d88\u606f\u903b\u8f91\u5373\u53ef\u5b8c\u6210\u63a5\u5165\u3002 \u540c\u65f6\u6b22\u8fce\u8d21\u732e\u65b0\u7684\u63d2\u4ef6\uff0c\u53c2\u8003 [\u63d2\u4ef6\u5f00\u53d1\u6587\u6863](https://github.com/zhayujie/chatgpt-on-wechat/tree/master/plugins)\u3002\n\n# \u2709 \u8054\u7cfb\n\n\u6b22\u8fce\u63d0\u4ea4PR\u3001Issues\u8fdb\u884c\u53cd\u9988\uff0c\u4ee5\u53ca\u901a\u8fc7 \ud83c\udf1fStar \u652f\u6301\u5e76\u5173\u6ce8\u9879\u76ee\u66f4\u65b0\u3002\u9879\u76ee\u8fd0\u884c\u9047\u5230\u95ee\u9898\u53ef\u4ee5\u67e5\u770b [\u5e38\u89c1\u95ee\u9898\u5217\u8868](https://github.com/zhayujie/chatgpt-on-wechat/wiki/FAQs) \uff0c\u4ee5\u53ca\u524d\u5f80 [Issues](https://github.com/zhayujie/chatgpt-on-wechat/issues) \u4e2d\u641c\u7d22\u3002\u4e2a\u4eba\u5f00\u53d1\u8005\u53ef\u52a0\u5165\u5f00\u6e90\u4ea4\u6d41\u7fa4\u53c2\u4e0e\u66f4\u591a\u8ba8\u8bba\uff0c\u4f01\u4e1a\u7528\u6237\u53ef\u8054\u7cfb[\u4ea7\u54c1\u5ba2\u670d](https://cdn.link-ai.tech/portal/linkai-customer-service.png)\u54a8\u8be2\u3002\n\n# \ud83c\udf1f \u8d21\u732e\u8005\n\n![cow contributors](https://contrib.rocks/image?repo=zhayujie/chatgpt-on-wechat&max=1000)\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 640079149,
    "name": "quivr",
    "full_name": "QuivrHQ/quivr",
    "description": "Opiniated RAG for integrating GenAI in your apps \ud83e\udde0   Focus on your product rather than the RAG. Easy integration in existing products with customisation!  Any LLM: GPT4, Groq, Llama. Any Vectorstore: PGVector, Faiss. Any Files. Anyway you want. ",
    "html_url": "https://github.com/QuivrHQ/quivr",
    "clone_url": "https://github.com/QuivrHQ/quivr.git",
    "owner_login": "QuivrHQ",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/159330290?v=4",
    "stargazers_count": 38236,
    "watchers_count": 38236,
    "forks_count": 3663,
    "open_issues_count": 21,
    "size": 130685,
    "language": "Python",
    "languages": {
      "Python": 245300,
      "Shell": 1667
    },
    "topics": [
      "ai",
      "api",
      "chatbot",
      "chatgpt",
      "database",
      "docker",
      "framework",
      "frontend",
      "groq",
      "html",
      "javascript",
      "llm",
      "openai",
      "postgresql",
      "privacy",
      "rag",
      "react",
      "security",
      "typescript",
      "vector"
    ],
    "license_name": "Other",
    "created_at": "2023-05-12T23:53:30+00:00",
    "updated_at": "2025-08-06T00:04:12+00:00",
    "pushed_at": "2025-07-09T12:55:23+00:00",
    "contributors_count": 100,
    "readme_length": 6798,
    "readme_content": "# Quivr - Your Second Brain, Empowered by Generative AI\n\n<div align=\"center\">\n    <img src=\"./logo.png\" alt=\"Quivr-logo\" width=\"31%\"  style=\"border-radius: 50%; padding-bottom: 20px\"/>\n</div>\n\n[![Discord Follow](https://dcbadge.vercel.app/api/server/HUpRgp2HG8?style=flat)](https://discord.gg/HUpRgp2HG8)\n[![GitHub Repo stars](https://img.shields.io/github/stars/quivrhq/quivr?style=social)](https://github.com/quivrhq/quivr)\n[![Twitter Follow](https://img.shields.io/twitter/follow/StanGirard?style=social)](https://twitter.com/_StanGirard)\n\nQuivr, helps you build your second brain, utilizes the power of GenerativeAI to be your personal assistant !\n\n## Key Features \ud83c\udfaf\n\n- **Opiniated RAG**: We created a RAG that is opinionated, fast and efficient so you can focus on your product\n- **LLMs**: Quivr works with any LLM, you can use it with OpenAI, Anthropic, Mistral, Gemma, etc.\n- **Any File**: Quivr works with any file, you can use it with PDF, TXT, Markdown, etc and even add your own parsers.\n- **Customize your RAG**: Quivr allows you to customize your RAG, add internet search, add tools, etc.\n- **Integrations with Megaparse**: Quivr works with [Megaparse](https://github.com/quivrhq/megaparse), so you can ingest your files with Megaparse and use the RAG with Quivr.\n\n>We take care of the RAG so you can focus on your product. Simply install quivr-core and add it to your project. You can now ingest your files and ask questions.*\n\n**We will be improving the RAG and adding more features, stay tuned!**\n\n\nThis is the core of Quivr, the brain of Quivr.com.\n\n<!-- ## Demo Highlight \ud83c\udfa5\n\nhttps://github.com/quivrhq/quivr/assets/19614572/a6463b73-76c7-4bc0-978d-70562dca71f5 -->\n\n## Getting Started \ud83d\ude80\n\nYou can find everything on the [documentation](https://core.quivr.com/).\n\n### Prerequisites \ud83d\udccb\n\nEnsure you have the following installed:\n\n- Python 3.10 or newer\n\n### 30 seconds Installation \ud83d\udcbd\n\n\n- **Step 1**: Install the package\n\n  \n\n  ```bash\n  pip install quivr-core # Check that the installation worked\n  ```\n\n\n- **Step 2**: Create a RAG with 5 lines of code\n\n  ```python\n  import tempfile\n\n  from quivr_core import Brain\n\n  if __name__ == \"__main__\":\n      with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".txt\") as temp_file:\n          temp_file.write(\"Gold is a liquid of blue-like colour.\")\n          temp_file.flush()\n\n          brain = Brain.from_files(\n              name=\"test_brain\",\n              file_paths=[temp_file.name],\n          )\n\n          answer = brain.ask(\n              \"what is gold? asnwer in french\"\n          )\n          print(\"answer:\", answer)\n  ```\n## Configuration\n\n### Workflows\n\n#### Basic RAG\n\n![](docs/docs/workflows/examples/basic_rag.excalidraw.png)\n\n\nCreating a basic RAG workflow like the one above is simple, here are the steps:\n\n\n1. Add your API Keys to your environment variables\n```python\nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"myopenai_apikey\"\n\n```\nQuivr supports APIs from Anthropic, OpenAI, and Mistral. It also supports local models using Ollama.\n\n1. Create the YAML file ``basic_rag_workflow.yaml`` and copy the following content in it\n```yaml\nworkflow_config:\n  name: \"standard RAG\"\n  nodes:\n    - name: \"START\"\n      edges: [\"filter_history\"]\n\n    - name: \"filter_history\"\n      edges: [\"rewrite\"]\n\n    - name: \"rewrite\"\n      edges: [\"retrieve\"]\n\n    - name: \"retrieve\"\n      edges: [\"generate_rag\"]\n\n    - name: \"generate_rag\" # the name of the last node, from which we want to stream the answer to the user\n      edges: [\"END\"]\n\n# Maximum number of previous conversation iterations\n# to include in the context of the answer\nmax_history: 10\n\n# Reranker configuration\nreranker_config:\n  # The reranker supplier to use\n  supplier: \"cohere\"\n\n  # The model to use for the reranker for the given supplier\n  model: \"rerank-multilingual-v3.0\"\n\n  # Number of chunks returned by the reranker\n  top_n: 5\n\n# Configuration for the LLM\nllm_config:\n\n  # maximum number of tokens passed to the LLM to generate the answer\n  max_input_tokens: 4000\n\n  # temperature for the LLM\n  temperature: 0.7\n```\n\n3. Create a Brain with the default configuration\n```python\nfrom quivr_core import Brain\n\nbrain = Brain.from_files(name = \"my smart brain\",\n                        file_paths = [\"./my_first_doc.pdf\", \"./my_second_doc.txt\"],\n                        )\n\n```\n\n4. Launch a Chat\n```python\nbrain.print_info()\n\nfrom rich.console import Console\nfrom rich.panel import Panel\nfrom rich.prompt import Prompt\nfrom quivr_core.config import RetrievalConfig\n\nconfig_file_name = \"./basic_rag_workflow.yaml\"\n\nretrieval_config = RetrievalConfig.from_yaml(config_file_name)\n\nconsole = Console()\nconsole.print(Panel.fit(\"Ask your brain !\", style=\"bold magenta\"))\n\nwhile True:\n    # Get user input\n    question = Prompt.ask(\"[bold cyan]Question[/bold cyan]\")\n\n    # Check if user wants to exit\n    if question.lower() == \"exit\":\n        console.print(Panel(\"Goodbye!\", style=\"bold yellow\"))\n        break\n\n    answer = brain.ask(question, retrieval_config=retrieval_config)\n    # Print the answer with typing effect\n    console.print(f\"[bold green]Quivr Assistant[/bold green]: {answer.answer}\")\n\n    console.print(\"-\" * console.width)\n\nbrain.print_info()\n```\n\n5. You are now all set up to talk with your brain and test different retrieval strategies by simply changing the configuration file!\n\n## Go further\n\nYou can go further with Quivr by adding internet search, adding tools, etc. Check the [documentation](https://core.quivr.com/) for more information.\n\n\n## Contributors \u2728\n\nThanks go to these wonderful people:\n<a href=\"https://github.com/quivrhq/quivr/graphs/contributors\">\n<img src=\"https://contrib.rocks/image?repo=quivrhq/quivr\" />\n</a>\n\n## Contribute \ud83e\udd1d\n\nDid you get a pull request? Open it, and we'll review it as soon as possible. Check out our project board [here](https://github.com/users/StanGirard/projects/5) to see what we're currently focused on, and feel free to bring your fresh ideas to the table!\n\n- [Open Issues](https://github.com/quivrhq/quivr/issues)\n- [Open Pull Requests](https://github.com/quivrhq/quivr/pulls)\n- [Good First Issues](https://github.com/quivrhq/quivr/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n\n## Partners \u2764\ufe0f\n\nThis project would not be possible without the support of our partners. Thank you for your support!\n\n\n<a href=\"https://ycombinator.com/\">\n    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b2/Y_Combinator_logo.svg/1200px-Y_Combinator_logo.svg.png\" alt=\"YCombinator\" style=\"padding: 10px\" width=\"70px\">\n</a>\n<a href=\"https://www.theodo.fr/\">\n  <img src=\"https://avatars.githubusercontent.com/u/332041?s=200&v=4\" alt=\"Theodo\" style=\"padding: 10px\" width=\"70px\">\n</a>\n\n## License \ud83d\udcc4\n\nThis project is licensed under the Apache 2.0 License - see the [LICENSE](LICENSE) file for details\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 656099147,
    "name": "mem0",
    "full_name": "mem0ai/mem0",
    "description": "Universal memory layer for AI Agents; Announcing OpenMemory MCP - local and secure memory management.",
    "html_url": "https://github.com/mem0ai/mem0",
    "clone_url": "https://github.com/mem0ai/mem0.git",
    "owner_login": "mem0ai",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/137054526?v=4",
    "stargazers_count": 37833,
    "watchers_count": 37833,
    "forks_count": 3923,
    "open_issues_count": 548,
    "size": 40016,
    "language": "Python",
    "languages": {
      "Python": 1920175,
      "TypeScript": 710852,
      "MDX": 262459,
      "Jupyter Notebook": 159346,
      "JavaScript": 29771,
      "Makefile": 5291,
      "Dockerfile": 4464,
      "CSS": 4335,
      "SCSS": 3746,
      "Shell": 3512,
      "Mako": 1324
    },
    "topics": [
      "agent",
      "ai",
      "aiagent",
      "application",
      "chatbots",
      "chatgpt",
      "embeddings",
      "llm",
      "long-term-memory",
      "memory",
      "memory-management",
      "python",
      "rag",
      "state-management",
      "vector-database"
    ],
    "license_name": "Apache License 2.0",
    "created_at": "2023-06-20T08:58:36+00:00",
    "updated_at": "2025-08-06T02:05:55+00:00",
    "pushed_at": "2025-08-05T22:42:07+00:00",
    "contributors_count": 100,
    "readme_length": 6484,
    "readme_content": "<p align=\"center\">\n  <a href=\"https://github.com/mem0ai/mem0\">\n    <img src=\"docs/images/banner-sm.png\" width=\"800px\" alt=\"Mem0 - The Memory Layer for Personalized AI\">\n  </a>\n</p>\n<p align=\"center\" style=\"display: flex; justify-content: center; gap: 20px; align-items: center;\">\n  <a href=\"https://trendshift.io/repositories/11194\" target=\"blank\">\n    <img src=\"https://trendshift.io/api/badge/repositories/11194\" alt=\"mem0ai%2Fmem0 | Trendshift\" width=\"250\" height=\"55\"/>\n  </a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://mem0.ai\">Learn more</a>\n  \u00b7\n  <a href=\"https://mem0.dev/DiG\">Join Discord</a>\n  \u00b7\n  <a href=\"https://mem0.dev/demo\">Demo</a>\n  \u00b7\n  <a href=\"https://mem0.dev/openmemory\">OpenMemory</a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://mem0.dev/DiG\">\n    <img src=\"https://dcbadge.vercel.app/api/server/6PzXDgEjG5?style=flat\" alt=\"Mem0 Discord\">\n  </a>\n  <a href=\"https://pepy.tech/project/mem0ai\">\n    <img src=\"https://img.shields.io/pypi/dm/mem0ai\" alt=\"Mem0 PyPI - Downloads\">\n  </a>\n  <a href=\"https://github.com/mem0ai/mem0\">\n    <img src=\"https://img.shields.io/github/commit-activity/m/mem0ai/mem0?style=flat-square\" alt=\"GitHub commit activity\">\n  </a>\n  <a href=\"https://pypi.org/project/mem0ai\" target=\"blank\">\n    <img src=\"https://img.shields.io/pypi/v/mem0ai?color=%2334D058&label=pypi%20package\" alt=\"Package version\">\n  </a>\n  <a href=\"https://www.npmjs.com/package/mem0ai\" target=\"blank\">\n    <img src=\"https://img.shields.io/npm/v/mem0ai\" alt=\"Npm package\">\n  </a>\n  <a href=\"https://www.ycombinator.com/companies/mem0\">\n    <img src=\"https://img.shields.io/badge/Y%20Combinator-S24-orange?style=flat-square\" alt=\"Y Combinator S24\">\n  </a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://mem0.ai/research\"><strong>\ud83d\udcc4 Building Production-Ready AI Agents with Scalable Long-Term Memory \u2192</strong></a>\n</p>\n<p align=\"center\">\n  <strong>\u26a1 +26% Accuracy vs. OpenAI Memory \u2022 \ud83d\ude80 91% Faster \u2022 \ud83d\udcb0 90% Fewer Tokens</strong>\n</p>\n\n##  \ud83d\udd25 Research Highlights\n- **+26% Accuracy** over OpenAI Memory on the LOCOMO benchmark\n- **91% Faster Responses** than full-context, ensuring low-latency at scale\n- **90% Lower Token Usage** than full-context, cutting costs without compromise\n- [Read the full paper](https://mem0.ai/research)\n\n# Introduction\n\n[Mem0](https://mem0.ai) (\"mem-zero\") enhances AI assistants and agents with an intelligent memory layer, enabling personalized AI interactions. It remembers user preferences, adapts to individual needs, and continuously learns over time\u2014ideal for customer support chatbots, AI assistants, and autonomous systems.\n\n### Key Features & Use Cases\n\n**Core Capabilities:**\n- **Multi-Level Memory**: Seamlessly retains User, Session, and Agent state with adaptive personalization\n- **Developer-Friendly**: Intuitive API, cross-platform SDKs, and a fully managed service option\n\n**Applications:**\n- **AI Assistants**: Consistent, context-rich conversations\n- **Customer Support**: Recall past tickets and user history for tailored help\n- **Healthcare**: Track patient preferences and history for personalized care\n- **Productivity & Gaming**: Adaptive workflows and environments based on user behavior\n\n## \ud83d\ude80 Quickstart Guide <a name=\"quickstart\"></a>\n\nChoose between our hosted platform or self-hosted package:\n\n### Hosted Platform\n\nGet up and running in minutes with automatic updates, analytics, and enterprise security.\n\n1. Sign up on [Mem0 Platform](https://app.mem0.ai)\n2. Embed the memory layer via SDK or API keys\n\n### Self-Hosted (Open Source)\n\nInstall the sdk via pip:\n\n```bash\npip install mem0ai\n```\n\nInstall sdk via npm:\n```bash\nnpm install mem0ai\n```\n\n### Basic Usage\n\nMem0 requires an LLM to function, with `gpt-4o-mini` from OpenAI as the default. However, it supports a variety of LLMs; for details, refer to our [Supported LLMs documentation](https://docs.mem0.ai/components/llms/overview).\n\nFirst step is to instantiate the memory:\n\n```python\nfrom openai import OpenAI\nfrom mem0 import Memory\n\nopenai_client = OpenAI()\nmemory = Memory()\n\ndef chat_with_memories(message: str, user_id: str = \"default_user\") -> str:\n    # Retrieve relevant memories\n    relevant_memories = memory.search(query=message, user_id=user_id, limit=3)\n    memories_str = \"\\n\".join(f\"- {entry['memory']}\" for entry in relevant_memories[\"results\"])\n\n    # Generate Assistant response\n    system_prompt = f\"You are a helpful AI. Answer the question based on query and memories.\\nUser Memories:\\n{memories_str}\"\n    messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": message}]\n    response = openai_client.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n    assistant_response = response.choices[0].message.content\n\n    # Create new memories from the conversation\n    messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n    memory.add(messages, user_id=user_id)\n\n    return assistant_response\n\ndef main():\n    print(\"Chat with AI (type 'exit' to quit)\")\n    while True:\n        user_input = input(\"You: \").strip()\n        if user_input.lower() == 'exit':\n            print(\"Goodbye!\")\n            break\n        print(f\"AI: {chat_with_memories(user_input)}\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\nFor detailed integration steps, see the [Quickstart](https://docs.mem0.ai/quickstart) and [API Reference](https://docs.mem0.ai/api-reference).\n\n## \ud83d\udd17 Integrations & Demos\n\n- **ChatGPT with Memory**: Personalized chat powered by Mem0 ([Live Demo](https://mem0.dev/demo))\n- **Browser Extension**: Store memories across ChatGPT, Perplexity, and Claude ([Chrome Extension](https://chromewebstore.google.com/detail/onihkkbipkfeijkadecaafbgagkhglop?utm_source=item-share-cb))\n- **Langgraph Support**: Build a customer bot with Langgraph + Mem0 ([Guide](https://docs.mem0.ai/integrations/langgraph))\n- **CrewAI Integration**: Tailor CrewAI outputs with Mem0 ([Example](https://docs.mem0.ai/integrations/crewai))\n\n## \ud83d\udcda Documentation & Support\n\n- Full docs: https://docs.mem0.ai\n- Community: [Discord](https://mem0.dev/DiG) \u00b7 [Twitter](https://x.com/mem0ai)\n- Contact: founders@mem0.ai\n\n## Citation\n\nWe now have a paper you can cite:\n\n```bibtex\n@article{mem0,\n  title={Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory},\n  author={Chhikara, Prateek and Khant, Dev and Aryan, Saket and Singh, Taranjeet and Yadav, Deshraj},\n  journal={arXiv preprint arXiv:2504.19413},\n  year={2025}\n}\n```\n\n## \u2696\ufe0f License\n\nApache 2.0 \u2014 see the [LICENSE](LICENSE) file for details.",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 806709826,
    "name": "ChatTTS",
    "full_name": "2noise/ChatTTS",
    "description": "A generative speech model for daily dialogue.",
    "html_url": "https://github.com/2noise/ChatTTS",
    "clone_url": "https://github.com/2noise/ChatTTS.git",
    "owner_login": "2noise",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/164844019?v=4",
    "stargazers_count": 37392,
    "watchers_count": 37392,
    "forks_count": 4042,
    "open_issues_count": 68,
    "size": 10047,
    "language": "Python",
    "languages": {
      "Python": 332391,
      "Go": 1408,
      "Shell": 261
    },
    "topics": [
      "agent",
      "chat",
      "chatgpt",
      "chattts",
      "chinese",
      "chinese-language",
      "english",
      "english-language",
      "gpt",
      "llm",
      "llm-agent",
      "natural-language-inference",
      "python",
      "text-to-speech",
      "torch",
      "torchaudio",
      "tts"
    ],
    "license_name": "GNU Affero General Public License v3.0",
    "created_at": "2024-05-27T18:26:49+00:00",
    "updated_at": "2025-08-06T01:35:42+00:00",
    "pushed_at": "2025-07-06T15:11:14+00:00",
    "contributors_count": 53,
    "readme_length": 11289,
    "readme_content": "<div align=\"center\">\n\n<a href=\"https://trendshift.io/repositories/10489\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/10489\" alt=\"2noise%2FChatTTS | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n\n# ChatTTS\nA generative speech model for daily dialogue.\n\n[![Licence](https://img.shields.io/github/license/2noise/ChatTTS?style=for-the-badge)](https://github.com/2noise/ChatTTS/blob/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/ChatTTS.svg?style=for-the-badge&color=green)](https://pypi.org/project/ChatTTS)\n\n[![Huggingface](https://img.shields.io/badge/\ud83e\udd17%20-Models-yellow.svg?style=for-the-badge)](https://huggingface.co/2Noise/ChatTTS)\n[![Open In Colab](https://img.shields.io/badge/Colab-F9AB00?style=for-the-badge&logo=googlecolab&color=525252)](https://colab.research.google.com/github/2noise/ChatTTS/blob/main/examples/ipynb/colab.ipynb)\n[![Discord](https://img.shields.io/badge/Discord-7289DA?style=for-the-badge&logo=discord&logoColor=white)](https://discord.gg/Ud5Jxgx5yD)\n\n**English** | [**\u7b80\u4f53\u4e2d\u6587**](docs/cn/README.md) | [**\u65e5\u672c\u8a9e**](docs/jp/README.md) | [**\u0420\u0443\u0441\u0441\u043a\u0438\u0439**](docs/ru/README.md) | [**Espa\u00f1ol**](docs/es/README.md) | [**Fran\u00e7ais**](docs/fr/README.md) | [**\ud55c\uad6d\uc5b4**](docs/kr/README.md)\n\n</div>\n\n## Introduction\n> [!Note]\n> This repo contains the algorithm infrastructure and some simple examples.\n\n> [!Tip]\n> For the extended end-user products, please refer to the index repo [Awesome-ChatTTS](https://github.com/libukai/Awesome-ChatTTS/tree/en) maintained by the community.\n\nChatTTS is a text-to-speech model designed specifically for dialogue scenarios such as LLM assistant.\n\n### Supported Languages\n- [x] English\n- [x] Chinese\n- [ ] Coming Soon...\n\n### Highlights\n> You can refer to **[this video on Bilibili](https://www.bilibili.com/video/BV1zn4y1o7iV)** for the detailed description.\n\n1. **Conversational TTS**: ChatTTS is optimized for dialogue-based tasks, enabling natural and expressive speech synthesis. It supports multiple speakers, facilitating interactive conversations.\n2. **Fine-grained Control**: The model could predict and control fine-grained prosodic features, including laughter, pauses, and interjections. \n3. **Better Prosody**: ChatTTS surpasses most of open-source TTS models in terms of prosody. We provide pretrained models to support further research and development.\n\n### Dataset & Model\n> [!Important]\n> The released model is for academic purposes only.\n\n- The main model is trained with Chinese and English audio data of 100,000+ hours.\n- The open-source version on **[HuggingFace](https://huggingface.co/2Noise/ChatTTS)** is a 40,000 hours pre-trained model without SFT.\n\n### Roadmap\n- [x] Open-source the 40k-hours-base model and spk_stats file.\n- [x] Streaming audio generation.\n- [x] Open-source DVAE encoder and zero shot inferring code.\n- [ ] Multi-emotion controlling.\n- [ ] ChatTTS.cpp (new repo in `2noise` org is welcomed)\n\n### Licenses\n\n#### The Code\n\nThe code is published under `AGPLv3+` license.\n\n#### The model\n\nThe model is published under `CC BY-NC 4.0` license. It is intended for educational and research use, and should not be used for any commercial or illegal purposes. The authors do not guarantee the accuracy, completeness, or reliability of the information. The information and data used in this repo, are for academic and research purposes only. The data obtained from publicly available sources, and the authors do not claim any ownership or copyright over the data.\n\n### Disclaimer\n\nChatTTS is a powerful text-to-speech system. However, it is very important to utilize this technology responsibly and ethically. To limit the use of ChatTTS, we added a small amount of high-frequency noise during the training of the 40,000-hour model, and compressed the audio quality as much as possible using MP3 format, to prevent malicious actors from potentially using it for criminal purposes. At the same time, we have internally trained a detection model and plan to open-source it in the future.\n\n### Contact\n> GitHub issues/PRs are always welcomed.\n\n#### Formal Inquiries\nFor formal inquiries about the model and roadmap, please contact us at **open-source@2noise.com**.\n\n#### Online Chat\n##### 1. QQ Group (Chinese Social APP)\n- **Group 1**, 808364215\n- **Group 2**, 230696694\n- **Group 3**, 933639842\n- **Group 4**, 608667975\n\n##### 2. Discord Server\nJoin by clicking [here](https://discord.gg/Ud5Jxgx5yD).\n\n## Get Started\n### Clone Repo\n```bash\ngit clone https://github.com/2noise/ChatTTS\ncd ChatTTS\n```\n\n### Install requirements\n#### 1. Install Directly\n```bash\npip install --upgrade -r requirements.txt\n```\n\n#### 2. Install from conda\n```bash\nconda create -n chattts python=3.11\nconda activate chattts\npip install -r requirements.txt\n```\n\n#### Optional: Install vLLM (Linux only)\n```bash\npip install safetensors vllm==0.2.7 torchaudio\n```\n\n#### Unrecommended Optional: Install TransformerEngine if using NVIDIA GPU (Linux only)\n> [!Warning]\n> DO NOT INSTALL! \n> The adaptation of TransformerEngine is currently under development and CANNOT run properly now. \n> Only install it on developing purpose. See more details on at #672 #676\n\n> [!Note]\n> The installation process is very slow.\n\n```bash\npip install git+https://github.com/NVIDIA/TransformerEngine.git@stable\n```\n\n#### Unrecommended Optional: Install FlashAttention-2 (mainly NVIDIA GPU)\n> [!Warning]\n> DO NOT INSTALL! \n> Currently the FlashAttention-2 will slow down the generating speed according to [this issue](https://github.com/huggingface/transformers/issues/26990). \n> Only install it on developing purpose.\n\n> [!Note]\n> See supported devices at the [Hugging Face Doc](https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2).\n\n\n```bash\npip install flash-attn --no-build-isolation\n```\n\n### Quick Start\n> Make sure you are under the project root directory when you execute these commands below.\n\n#### 1. Launch WebUI\n```bash\npython examples/web/webui.py\n```\n\n#### 2. Infer by Command Line\n> It will save audio to `./output_audio_n.mp3`\n\n```bash\npython examples/cmd/run.py \"Your text 1.\" \"Your text 2.\"\n```\n\n## Installation\n\n1. Install the stable version from PyPI\n```bash\npip install ChatTTS\n```\n\n2. Install the latest version from GitHub\n```bash\npip install git+https://github.com/2noise/ChatTTS\n```\n\n3. Install from local directory in dev mode\n```bash\npip install -e .\n```\n\n### Basic Usage\n\n```python\nimport ChatTTS\nimport torch\nimport torchaudio\n\nchat = ChatTTS.Chat()\nchat.load(compile=False) # Set to True for better performance\n\ntexts = [\"PUT YOUR 1st TEXT HERE\", \"PUT YOUR 2nd TEXT HERE\"]\n\nwavs = chat.infer(texts)\n\nfor i in range(len(wavs)):\n    \"\"\"\n    In some versions of torchaudio, the first line works but in other versions, so does the second line.\n    \"\"\"\n    try:\n        torchaudio.save(f\"basic_output{i}.wav\", torch.from_numpy(wavs[i]).unsqueeze(0), 24000)\n    except:\n        torchaudio.save(f\"basic_output{i}.wav\", torch.from_numpy(wavs[i]), 24000)\n```\n\n### Advanced Usage\n\n```python\n###################################\n# Sample a speaker from Gaussian.\n\nrand_spk = chat.sample_random_speaker()\nprint(rand_spk) # save it for later timbre recovery\n\nparams_infer_code = ChatTTS.Chat.InferCodeParams(\n    spk_emb = rand_spk, # add sampled speaker \n    temperature = .3,   # using custom temperature\n    top_P = 0.7,        # top P decode\n    top_K = 20,         # top K decode\n)\n\n###################################\n# For sentence level manual control.\n\n# use oral_(0-9), laugh_(0-2), break_(0-7) \n# to generate special token in text to synthesize.\nparams_refine_text = ChatTTS.Chat.RefineTextParams(\n    prompt='[oral_2][laugh_0][break_6]',\n)\n\nwavs = chat.infer(\n    texts,\n    params_refine_text=params_refine_text,\n    params_infer_code=params_infer_code,\n)\n\n###################################\n# For word level manual control.\n\ntext = 'What is [uv_break]your favorite english food?[laugh][lbreak]'\nwavs = chat.infer(text, skip_refine_text=True, params_refine_text=params_refine_text,  params_infer_code=params_infer_code)\n\"\"\"\nIn some versions of torchaudio, the first line works but in other versions, so does the second line.\n\"\"\"\ntry:\n    torchaudio.save(\"word_level_output.wav\", torch.from_numpy(wavs[0]).unsqueeze(0), 24000)\nexcept:\n    torchaudio.save(\"word_level_output.wav\", torch.from_numpy(wavs[0]), 24000)\n```\n\n<details open>\n  <summary><h4>Example: self introduction</h4></summary>\n\n```python\ninputs_en = \"\"\"\nchat T T S is a text to speech model designed for dialogue applications. \n[uv_break]it supports mixed language input [uv_break]and offers multi speaker \ncapabilities with precise control over prosodic elements like \n[uv_break]laughter[uv_break][laugh], [uv_break]pauses, [uv_break]and intonation. \n[uv_break]it delivers natural and expressive speech,[uv_break]so please\n[uv_break] use the project responsibly at your own risk.[uv_break]\n\"\"\".replace('\\n', '') # English is still experimental.\n\nparams_refine_text = ChatTTS.Chat.RefineTextParams(\n    prompt='[oral_2][laugh_0][break_4]',\n)\n\naudio_array_en = chat.infer(inputs_en, params_refine_text=params_refine_text)\ntorchaudio.save(\"self_introduction_output.wav\", torch.from_numpy(audio_array_en[0]), 24000)\n```\n\n<table>\n<tr>\n<td align=\"center\">\n\n**male speaker**\n\n</td>\n<td align=\"center\">\n\n**female speaker**\n\n</td>\n</tr>\n<tr>\n<td align=\"center\">\n\n[male speaker](https://github.com/2noise/ChatTTS/assets/130631963/e0f51251-db7f-4d39-a0e9-3e095bb65de1)\n\n</td>\n<td align=\"center\">\n\n[female speaker](https://github.com/2noise/ChatTTS/assets/130631963/f5dcdd01-1091-47c5-8241-c4f6aaaa8bbd)\n\n</td>\n</tr>\n</table>\n\n\n</details>\n\n## FAQ\n\n#### 1. How much VRAM do I need? How about infer speed?\nFor a 30-second audio clip, at least 4GB of GPU memory is required. For the 4090 GPU, it can generate audio corresponding to approximately 7 semantic tokens per second. The Real-Time Factor (RTF) is around 0.3.\n\n#### 2. Model stability is not good enough, with issues such as multi speakers or poor audio quality.\n\nThis is a problem that typically occurs with autoregressive models (for bark and valle). It's generally difficult to avoid. One can try multiple samples to find a suitable result.\n\n#### 3. Besides laughter, can we control anything else? Can we control other emotions?\n\nIn the current released model, the only token-level control units are `[laugh]`, `[uv_break]`, and `[lbreak]`. In future versions, we may open-source models with additional emotional control capabilities.\n\n## Acknowledgements\n- [bark](https://github.com/suno-ai/bark), [XTTSv2](https://github.com/coqui-ai/TTS) and [valle](https://arxiv.org/abs/2301.02111) demonstrate a remarkable TTS result by an autoregressive-style system.\n- [fish-speech](https://github.com/fishaudio/fish-speech) reveals capability of GVQ as audio tokenizer for LLM modeling.\n- [vocos](https://github.com/gemelo-ai/vocos) which is used as a pretrained vocoder.\n\n## Special Appreciation\n- [wlu-audio lab](https://audio.westlake.edu.cn/) for early algorithm experiments.\n\n## Thanks to all contributors for their efforts\n[![contributors](https://contrib.rocks/image?repo=2noise/ChatTTS)](https://github.com/2noise/ChatTTS/graphs/contributors)\n\n<div align=\"center\">\n\n  ![counter](https://counter.seku.su/cmoe?name=chattts&theme=mbs)\n\n</div>\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 396569538,
    "name": "khoj",
    "full_name": "khoj-ai/khoj",
    "description": "Your AI second brain. Self-hostable. Get answers from the web or your docs. Build custom agents, schedule automations, do deep research. Turn any online or local LLM into your personal, autonomous AI (gpt, claude, gemini, llama, qwen, mistral). Get started - free.",
    "html_url": "https://github.com/khoj-ai/khoj",
    "clone_url": "https://github.com/khoj-ai/khoj.git",
    "owner_login": "khoj-ai",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/134046886?v=4",
    "stargazers_count": 30667,
    "watchers_count": 30667,
    "forks_count": 1763,
    "open_issues_count": 82,
    "size": 116565,
    "language": "Python",
    "languages": {
      "Python": 1508063,
      "TypeScript": 1099639,
      "CSS": 104750,
      "HTML": 77724,
      "Emacs Lisp": 76457,
      "JavaScript": 70989,
      "Shell": 14565,
      "Dockerfile": 10306,
      "Java": 2888
    },
    "topics": [
      "agent",
      "ai",
      "assistant",
      "chat",
      "chatgpt",
      "emacs",
      "image-generation",
      "llama3",
      "llamacpp",
      "llm",
      "obsidian",
      "obsidian-md",
      "offline-llm",
      "productivity",
      "rag",
      "research",
      "self-hosted",
      "semantic-search",
      "stt",
      "whatsapp-ai"
    ],
    "license_name": "GNU Affero General Public License v3.0",
    "created_at": "2021-08-16T01:48:44+00:00",
    "updated_at": "2025-08-06T02:12:48+00:00",
    "pushed_at": "2025-08-02T06:50:25+00:00",
    "contributors_count": 57,
    "readme_length": 4755,
    "readme_content": "<p align=\"center\"><img src=\"https://assets.khoj.dev/khoj-logo-sideways-1200x540.png\" width=\"230\" alt=\"Khoj Logo\"></p>\n\n<div align=\"center\">\n\n[![test](https://github.com/khoj-ai/khoj/actions/workflows/test.yml/badge.svg)](https://github.com/khoj-ai/khoj/actions/workflows/test.yml)\n[![docker](https://github.com/khoj-ai/khoj/actions/workflows/dockerize.yml/badge.svg)](https://github.com/khoj-ai/khoj/pkgs/container/khoj)\n[![pypi](https://github.com/khoj-ai/khoj/actions/workflows/pypi.yml/badge.svg)](https://pypi.org/project/khoj/)\n[![discord](https://img.shields.io/discord/1112065956647284756?style=plastic&label=discord)](https://discord.gg/BDgyabRM6e)\n\n</div>\n\n<div align=\"center\">\n<b>Your AI second brain</b>\n</div>\n\n<br />\n\n<div align=\"center\">\n\n[\ud83d\udcd1 Docs](https://docs.khoj.dev)\n<span>&nbsp;&nbsp;\u2022&nbsp;&nbsp;</span>\n[\ud83c\udf10 Web](https://khoj.dev)\n<span>&nbsp;&nbsp;\u2022&nbsp;&nbsp;</span>\n[\ud83d\udd25 App](https://app.khoj.dev)\n<span>&nbsp;&nbsp;\u2022&nbsp;&nbsp;</span>\n[\ud83d\udcac Discord](https://discord.gg/BDgyabRM6e)\n<span>&nbsp;&nbsp;\u2022&nbsp;&nbsp;</span>\n[\u270d\ud83c\udffd Blog](https://blog.khoj.dev)\n\n<a href=\"https://trendshift.io/repositories/10318\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/10318\" alt=\"khoj-ai%2Fkhoj | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n\n</div>\n\n***\n\n### \ud83c\udf81 New\n* Start any message with `/research` to try out the experimental research mode with Khoj.\n* Anyone can now [create custom agents](https://blog.khoj.dev/posts/create-agents-on-khoj/) with tunable personality, tools and knowledge bases.\n* [Read](https://blog.khoj.dev/posts/evaluate-khoj-quality/) about Khoj's excellent performance on modern retrieval and reasoning benchmarks.\n\n***\n\n## Overview\n\n[Khoj](https://khoj.dev) is a personal AI app to extend your capabilities. It smoothly scales up from an on-device personal AI to a cloud-scale enterprise AI.\n\n- Chat with any local or online LLM (e.g llama3, qwen, gemma, mistral, gpt, claude, gemini, deepseek).\n- Get answers from the internet and your docs (including image, pdf, markdown, org-mode, word, notion files).\n- Access it from your Browser, Obsidian, Emacs, Desktop, Phone or Whatsapp.\n- Create agents with custom knowledge, persona, chat model and tools to take on any role.\n- Automate away repetitive research. Get personal newsletters and smart notifications delivered to your inbox.\n- Find relevant docs quickly and easily using our advanced semantic search.\n- Generate images, talk out loud, play your messages.\n- Khoj is open-source, self-hostable. Always.\n- Run it privately on [your computer](https://docs.khoj.dev/get-started/setup) or try it on our [cloud app](https://app.khoj.dev).\n\n***\n\n## See it in action\n\n![demo_chat](https://github.com/khoj-ai/khoj/blob/master/documentation/assets/img/quadratic_equation_khoj_web.gif?raw=true)\n\nGo to https://app.khoj.dev to see Khoj live.\n\n## Full feature list\nYou can see the full feature list [here](https://docs.khoj.dev/category/features).\n\n## Self-Host\n\nTo get started with self-hosting Khoj, [read the docs](https://docs.khoj.dev/get-started/setup).\n\n## Enterprise\n\nKhoj is available as a cloud service, on-premises, or as a hybrid solution. To learn more about Khoj Enterprise, [visit our website](https://khoj.dev/teams).\n\n## Frequently Asked Questions (FAQ)\n\nQ: Can I use Khoj without self-hosting?\n\nYes! You can use Khoj right away at [https://app.khoj.dev](https://app.khoj.dev) \u2014 no setup required.\n\nQ: What kinds of documents can Khoj read?\n\nKhoj supports a wide variety: PDFs, Markdown, Notion, Word docs, org-mode files, and more.\n\nQ: How can I make my own agent?\n\nCheck out [this blog post](https://blog.khoj.dev/posts/create-agents-on-khoj/) for a step-by-step guide to custom agents.\nFor more questions, head over to our [Discord](https://discord.gg/BDgyabRM6e)!\n\n\n## Contributors\nCheers to our awesome contributors! \ud83c\udf89\n\n<a href=\"https://github.com/khoj-ai/khoj/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=khoj-ai/khoj\" />\n</a>\n\nMade with [contrib.rocks](https://contrib.rocks).\n\n### Interested in Contributing?\nKhoj is open source. It is sustained by the community and we\u2019d love for you to join it! Whether you\u2019re a coder, designer, writer, or enthusiast, there\u2019s a place for you.\n\nWhy Contribute?\n- Make an Impact: Help build, test and improve a tool used by thousands to boost productivity.\n- Learn & Grow: Work on cutting-edge AI, LLMs, and semantic search technologies.\n\nYou can help us build new features, improve the project documentation, report issues and fix bugs. If you're a developer, please see our [Contributing Guidelines](https://docs.khoj.dev/contributing/development) and check out [good first issues](https://github.com/khoj-ai/khoj/contribute) to work on.\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 778431525,
    "name": "graphrag",
    "full_name": "microsoft/graphrag",
    "description": "A modular graph-based Retrieval-Augmented Generation (RAG) system",
    "html_url": "https://github.com/microsoft/graphrag",
    "clone_url": "https://github.com/microsoft/graphrag.git",
    "owner_login": "microsoft",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/6154722?v=4",
    "stargazers_count": 27053,
    "watchers_count": 27053,
    "forks_count": 2808,
    "open_issues_count": 203,
    "size": 215342,
    "language": "Python",
    "languages": {
      "Python": 1292905,
      "Jupyter Notebook": 56942,
      "Dockerfile": 647,
      "Shell": 512
    },
    "topics": [
      "gpt",
      "gpt-4",
      "gpt4",
      "graphrag",
      "llm",
      "llms",
      "rag"
    ],
    "license_name": "MIT License",
    "created_at": "2024-03-27T17:57:52+00:00",
    "updated_at": "2025-08-06T02:10:35+00:00",
    "pushed_at": "2025-08-04T20:30:58+00:00",
    "contributors_count": 48,
    "readme_length": 4557,
    "readme_content": "# GraphRAG\n\n\ud83d\udc49 [Microsoft Research Blog Post](https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/)<br/>\n\ud83d\udc49 [Read the docs](https://microsoft.github.io/graphrag)<br/>\n\ud83d\udc49 [GraphRAG Arxiv](https://arxiv.org/pdf/2404.16130)\n\n<div align=\"left\">\n  <a href=\"https://pypi.org/project/graphrag/\">\n    <img alt=\"PyPI - Version\" src=\"https://img.shields.io/pypi/v/graphrag\">\n  </a>\n  <a href=\"https://pypi.org/project/graphrag/\">\n    <img alt=\"PyPI - Downloads\" src=\"https://img.shields.io/pypi/dm/graphrag\">\n  </a>\n  <a href=\"https://github.com/microsoft/graphrag/issues\">\n    <img alt=\"GitHub Issues\" src=\"https://img.shields.io/github/issues/microsoft/graphrag\">\n  </a>\n  <a href=\"https://github.com/microsoft/graphrag/discussions\">\n    <img alt=\"GitHub Discussions\" src=\"https://img.shields.io/github/discussions/microsoft/graphrag\">\n  </a>\n</div>\n\n## Overview\n\nThe GraphRAG project is a data pipeline and transformation suite that is designed to extract meaningful, structured data from unstructured text using the power of LLMs.\n\nTo learn more about GraphRAG and how it can be used to enhance your LLM's ability to reason about your private data, please visit the <a href=\"https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/\" target=\"_blank\">Microsoft Research Blog Post.</a>\n\n## Quickstart\n\nTo get started with the GraphRAG system we recommend trying the [command line quickstart](https://microsoft.github.io/graphrag/get_started/).\n\n## Repository Guidance\n\nThis repository presents a methodology for using knowledge graph memory structures to enhance LLM outputs. Please note that the provided code serves as a demonstration and is not an officially supported Microsoft offering.\n\n\u26a0\ufe0f *Warning: GraphRAG indexing can be an expensive operation, please read all of the documentation to understand the process and costs involved, and start small.*\n\n## Diving Deeper\n\n- To learn about our contribution guidelines, see [CONTRIBUTING.md](./CONTRIBUTING.md)\n- To start developing _GraphRAG_, see [DEVELOPING.md](./DEVELOPING.md)\n- Join the conversation and provide feedback in the [GitHub Discussions tab!](https://github.com/microsoft/graphrag/discussions)\n\n## Prompt Tuning\n\nUsing _GraphRAG_ with your data out of the box may not yield the best possible results.\nWe strongly recommend to fine-tune your prompts following the [Prompt Tuning Guide](https://microsoft.github.io/graphrag/prompt_tuning/overview/) in our documentation.\n\n## Versioning\n\nPlease see the [breaking changes](./breaking-changes.md) document for notes on our approach to versioning the project.\n\n*Always run `graphrag init --root [path] --force` between minor version bumps to ensure you have the latest config format. Run the provided migration notebook between major version bumps if you want to avoid re-indexing prior datasets. Note that this will overwrite your configuration and prompts, so backup if necessary.*\n\n## Responsible AI FAQ\n\nSee [RAI_TRANSPARENCY.md](./RAI_TRANSPARENCY.md)\n\n- [What is GraphRAG?](./RAI_TRANSPARENCY.md#what-is-graphrag)\n- [What can GraphRAG do?](./RAI_TRANSPARENCY.md#what-can-graphrag-do)\n- [What are GraphRAG\u2019s intended use(s)?](./RAI_TRANSPARENCY.md#what-are-graphrags-intended-uses)\n- [How was GraphRAG evaluated? What metrics are used to measure performance?](./RAI_TRANSPARENCY.md#how-was-graphrag-evaluated-what-metrics-are-used-to-measure-performance)\n- [What are the limitations of GraphRAG? How can users minimize the impact of GraphRAG\u2019s limitations when using the system?](./RAI_TRANSPARENCY.md#what-are-the-limitations-of-graphrag-how-can-users-minimize-the-impact-of-graphrags-limitations-when-using-the-system)\n- [What operational factors and settings allow for effective and responsible use of GraphRAG?](./RAI_TRANSPARENCY.md#what-operational-factors-and-settings-allow-for-effective-and-responsible-use-of-graphrag)\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n\n## Privacy\n\n[Microsoft Privacy Statement](https://privacy.microsoft.com/en-us/privacystatement)\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 671269505,
    "name": "litellm",
    "full_name": "BerriAI/litellm",
    "description": "Python SDK, Proxy Server (LLM Gateway) to call 100+ LLM APIs in OpenAI format - [Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, Replicate, Groq]",
    "html_url": "https://github.com/BerriAI/litellm",
    "clone_url": "https://github.com/BerriAI/litellm.git",
    "owner_login": "BerriAI",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/121462774?v=4",
    "stargazers_count": 26892,
    "watchers_count": 26892,
    "forks_count": 3727,
    "open_issues_count": 1087,
    "size": 493238,
    "language": "Python",
    "languages": {
      "Python": 18582294,
      "TypeScript": 2354443,
      "HTML": 95853,
      "JavaScript": 26152,
      "Shell": 5119,
      "Makefile": 4005,
      "Dockerfile": 3380,
      "Ruby": 3229,
      "Smarty": 2462,
      "CSS": 1324,
      "Bicep": 909
    },
    "topics": [
      "ai-gateway",
      "anthropic",
      "azure-openai",
      "bedrock",
      "gateway",
      "langchain",
      "litellm",
      "llm",
      "llm-gateway",
      "llmops",
      "mcp-gateway",
      "openai",
      "openai-proxy",
      "vertex-ai"
    ],
    "license_name": "Other",
    "created_at": "2023-07-27T00:09:52+00:00",
    "updated_at": "2025-08-06T02:08:58+00:00",
    "pushed_at": "2025-08-06T01:46:57+00:00",
    "contributors_count": 100,
    "readme_length": 36644,
    "readme_content": "<h1 align=\"center\">\n        \ud83d\ude85 LiteLLM\n    </h1>\n    <p align=\"center\">\n        <p align=\"center\">\n        <a href=\"https://render.com/deploy?repo=https://github.com/BerriAI/litellm\" target=\"_blank\" rel=\"nofollow\"><img src=\"https://render.com/images/deploy-to-render-button.svg\" alt=\"Deploy to Render\"></a>\n        <a href=\"https://railway.app/template/HLP0Ub?referralCode=jch2ME\">\n          <img src=\"https://railway.app/button.svg\" alt=\"Deploy on Railway\">\n        </a>\n        </p>\n        <p align=\"center\">Call all LLM APIs using the OpenAI format [Bedrock, Huggingface, VertexAI, TogetherAI, Azure, OpenAI, Groq etc.]\n        <br>\n    </p>\n<h4 align=\"center\"><a href=\"https://docs.litellm.ai/docs/simple_proxy\" target=\"_blank\">LiteLLM Proxy Server (LLM Gateway)</a> | <a href=\"https://docs.litellm.ai/docs/hosted\" target=\"_blank\"> Hosted Proxy (Preview)</a> | <a href=\"https://docs.litellm.ai/docs/enterprise\"target=\"_blank\">Enterprise Tier</a></h4>\n<h4 align=\"center\">\n    <a href=\"https://pypi.org/project/litellm/\" target=\"_blank\">\n        <img src=\"https://img.shields.io/pypi/v/litellm.svg\" alt=\"PyPI Version\">\n    </a>\n    <a href=\"https://www.ycombinator.com/companies/berriai\">\n        <img src=\"https://img.shields.io/badge/Y%20Combinator-W23-orange?style=flat-square\" alt=\"Y Combinator W23\">\n    </a>\n    <a href=\"https://wa.link/huol9n\">\n        <img src=\"https://img.shields.io/static/v1?label=Chat%20on&message=WhatsApp&color=success&logo=WhatsApp&style=flat-square\" alt=\"Whatsapp\">\n    </a>\n    <a href=\"https://discord.gg/wuPM9dRgDw\">\n        <img src=\"https://img.shields.io/static/v1?label=Chat%20on&message=Discord&color=blue&logo=Discord&style=flat-square\" alt=\"Discord\">\n    </a>\n    <a href=\"https://join.slack.com/share/enQtOTE0ODczMzk2Nzk4NC01YjUxNjY2YjBlYTFmNDRiZTM3NDFiYTM3MzVkODFiMDVjOGRjMmNmZTZkZTMzOWQzZGQyZWIwYjQ0MWExYmE3\">\n        <img src=\"https://img.shields.io/static/v1?label=Chat%20on&message=Slack&color=black&logo=Slack&style=flat-square\" alt=\"Slack\">\n    </a>\n</h4>\n\nLiteLLM manages:\n\n- Translate inputs to provider's `completion`, `embedding`, and `image_generation` endpoints\n- [Consistent output](https://docs.litellm.ai/docs/completion/output), text responses will always be available at `['choices'][0]['message']['content']`\n- Retry/fallback logic across multiple deployments (e.g. Azure/OpenAI) - [Router](https://docs.litellm.ai/docs/routing)\n- Set Budgets & Rate limits per project, api key, model [LiteLLM Proxy Server (LLM Gateway)](https://docs.litellm.ai/docs/simple_proxy)\n\n[**Jump to LiteLLM Proxy (LLM Gateway) Docs**](https://github.com/BerriAI/litellm?tab=readme-ov-file#openai-proxy---docs) <br>\n[**Jump to Supported LLM Providers**](https://github.com/BerriAI/litellm?tab=readme-ov-file#supported-providers-docs)\n\n\ud83d\udea8 **Stable Release:** Use docker images with the `-stable` tag. These have undergone 12 hour load tests, before being published. [More information about the release cycle here](https://docs.litellm.ai/docs/proxy/release_cycle)\n\nSupport for more providers. Missing a provider or LLM Platform, raise a [feature request](https://github.com/BerriAI/litellm/issues/new?assignees=&labels=enhancement&projects=&template=feature_request.yml&title=%5BFeature%5D%3A+).\n\n# Usage ([**Docs**](https://docs.litellm.ai/docs/))\n\n> [!IMPORTANT]\n> LiteLLM v1.0.0 now requires `openai>=1.0.0`. Migration guide [here](https://docs.litellm.ai/docs/migration)  \n> LiteLLM v1.40.14+ now requires `pydantic>=2.0.0`. No changes required.\n\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/liteLLM_Getting_Started.ipynb\">\n  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n\n```shell\npip install litellm\n```\n\n```python\nfrom litellm import completion\nimport os\n\n## set ENV variables\nos.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"\nos.environ[\"ANTHROPIC_API_KEY\"] = \"your-anthropic-key\"\n\nmessages = [{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}]\n\n# openai call\nresponse = completion(model=\"openai/gpt-4o\", messages=messages)\n\n# anthropic call\nresponse = completion(model=\"anthropic/claude-sonnet-4-20250514\", messages=messages)\nprint(response)\n```\n\n### Response (OpenAI Format)\n\n```json\n{\n    \"id\": \"chatcmpl-1214900a-6cdd-4148-b663-b5e2f642b4de\",\n    \"created\": 1751494488,\n    \"model\": \"claude-sonnet-4-20250514\",\n    \"object\": \"chat.completion\",\n    \"system_fingerprint\": null,\n    \"choices\": [\n        {\n            \"finish_reason\": \"stop\",\n            \"index\": 0,\n            \"message\": {\n                \"content\": \"Hello! I'm doing well, thank you for asking. I'm here and ready to help with whatever you'd like to discuss or work on. How are you doing today?\",\n                \"role\": \"assistant\",\n                \"tool_calls\": null,\n                \"function_call\": null\n            }\n        }\n    ],\n    \"usage\": {\n        \"completion_tokens\": 39,\n        \"prompt_tokens\": 13,\n        \"total_tokens\": 52,\n        \"completion_tokens_details\": null,\n        \"prompt_tokens_details\": {\n            \"audio_tokens\": null,\n            \"cached_tokens\": 0\n        },\n        \"cache_creation_input_tokens\": 0,\n        \"cache_read_input_tokens\": 0\n    }\n}\n```\n\nCall any model supported by a provider, with `model=<provider_name>/<model_name>`. There might be provider-specific details here, so refer to [provider docs for more information](https://docs.litellm.ai/docs/providers)\n\n## Async ([Docs](https://docs.litellm.ai/docs/completion/stream#async-completion))\n\n```python\nfrom litellm import acompletion\nimport asyncio\n\nasync def test_get_response():\n    user_message = \"Hello, how are you?\"\n    messages = [{\"content\": user_message, \"role\": \"user\"}]\n    response = await acompletion(model=\"openai/gpt-4o\", messages=messages)\n    return response\n\nresponse = asyncio.run(test_get_response())\nprint(response)\n```\n\n## Streaming ([Docs](https://docs.litellm.ai/docs/completion/stream))\n\nliteLLM supports streaming the model response back, pass `stream=True` to get a streaming iterator in response.  \nStreaming is supported for all models (Bedrock, Huggingface, TogetherAI, Azure, OpenAI, etc.)\n\n```python\nfrom litellm import completion\nresponse = completion(model=\"openai/gpt-4o\", messages=messages, stream=True)\nfor part in response:\n    print(part.choices[0].delta.content or \"\")\n\n# claude sonnet 4\nresponse = completion('anthropic/claude-sonnet-4-20250514', messages, stream=True)\nfor part in response:\n    print(part)\n```\n\n### Response chunk (OpenAI Format)\n\n```json\n{\n    \"id\": \"chatcmpl-fe575c37-5004-4926-ae5e-bfbc31f356ca\",\n    \"created\": 1751494808,\n    \"model\": \"claude-sonnet-4-20250514\",\n    \"object\": \"chat.completion.chunk\",\n    \"system_fingerprint\": null,\n    \"choices\": [\n        {\n            \"finish_reason\": null,\n            \"index\": 0,\n            \"delta\": {\n                \"provider_specific_fields\": null,\n                \"content\": \"Hello\",\n                \"role\": \"assistant\",\n                \"function_call\": null,\n                \"tool_calls\": null,\n                \"audio\": null\n            },\n            \"logprobs\": null\n        }\n    ],\n    \"provider_specific_fields\": null,\n    \"stream_options\": null,\n    \"citations\": null\n}\n```\n\n## Logging Observability ([Docs](https://docs.litellm.ai/docs/observability/callbacks))\n\nLiteLLM exposes pre defined callbacks to send data to Lunary, MLflow, Langfuse, DynamoDB, s3 Buckets, Helicone, Promptlayer, Traceloop, Athina, Slack\n\n```python\nfrom litellm import completion\n\n## set env variables for logging tools (when using MLflow, no API key set up is required)\nos.environ[\"LUNARY_PUBLIC_KEY\"] = \"your-lunary-public-key\"\nos.environ[\"HELICONE_API_KEY\"] = \"your-helicone-auth-key\"\nos.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"\"\nos.environ[\"LANGFUSE_SECRET_KEY\"] = \"\"\nos.environ[\"ATHINA_API_KEY\"] = \"your-athina-api-key\"\n\nos.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"\n\n# set callbacks\nlitellm.success_callback = [\"lunary\", \"mlflow\", \"langfuse\", \"athina\", \"helicone\"] # log input/output to lunary, langfuse, supabase, athina, helicone etc\n\n#openai call\nresponse = completion(model=\"openai/gpt-4o\", messages=[{\"role\": \"user\", \"content\": \"Hi \ud83d\udc4b - i'm openai\"}])\n```\n\n# LiteLLM Proxy Server (LLM Gateway) - ([Docs](https://docs.litellm.ai/docs/simple_proxy))\n\nTrack spend + Load Balance across multiple projects\n\n[Hosted Proxy (Preview)](https://docs.litellm.ai/docs/hosted)\n\nThe proxy provides:\n\n1. [Hooks for auth](https://docs.litellm.ai/docs/proxy/virtual_keys#custom-auth)\n2. [Hooks for logging](https://docs.litellm.ai/docs/proxy/logging#step-1---create-your-custom-litellm-callback-class)\n3. [Cost tracking](https://docs.litellm.ai/docs/proxy/virtual_keys#tracking-spend)\n4. [Rate Limiting](https://docs.litellm.ai/docs/proxy/users#set-rate-limits)\n\n## \ud83d\udcd6 Proxy Endpoints - [Swagger Docs](https://litellm-api.up.railway.app/)\n\n\n## Quick Start Proxy - CLI\n\n```shell\npip install 'litellm[proxy]'\n```\n\n### Step 1: Start litellm proxy\n\n```shell\n$ litellm --model huggingface/bigcode/starcoder\n\n#INFO: Proxy running on http://0.0.0.0:4000\n```\n\n### Step 2: Make ChatCompletions Request to Proxy\n\n\n> [!IMPORTANT]\n> \ud83d\udca1 [Use LiteLLM Proxy with Langchain (Python, JS), OpenAI SDK (Python, JS) Anthropic SDK, Mistral SDK, LlamaIndex, Instructor, Curl](https://docs.litellm.ai/docs/proxy/user_keys)  \n\n```python\nimport openai # openai v1.0.0+\nclient = openai.OpenAI(api_key=\"anything\",base_url=\"http://0.0.0.0:4000\") # set proxy to base_url\n# request sent to model set on litellm proxy, `litellm --model`\nresponse = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"this is a test request, write a short poem\"\n    }\n])\n\nprint(response)\n```\n\n## Proxy Key Management ([Docs](https://docs.litellm.ai/docs/proxy/virtual_keys))\n\nConnect the proxy with a Postgres DB to create proxy keys\n\n```bash\n# Get the code\ngit clone https://github.com/BerriAI/litellm\n\n# Go to folder\ncd litellm\n\n# Add the master key - you can change this after setup\necho 'LITELLM_MASTER_KEY=\"sk-1234\"' > .env\n\n# Add the litellm salt key - you cannot change this after adding a model\n# It is used to encrypt / decrypt your LLM API Key credentials\n# We recommend - https://1password.com/password-generator/ \n# password generator to get a random hash for litellm salt key\necho 'LITELLM_SALT_KEY=\"sk-1234\"' >> .env\n\nsource .env\n\n# Start\ndocker-compose up\n```\n\n\nUI on `/ui` on your proxy server\n![ui_3](https://github.com/BerriAI/litellm/assets/29436595/47c97d5e-b9be-4839-b28c-43d7f4f10033)\n\nSet budgets and rate limits across multiple projects\n`POST /key/generate`\n\n### Request\n\n```shell\ncurl 'http://0.0.0.0:4000/key/generate' \\\n--header 'Authorization: Bearer sk-1234' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\"models\": [\"gpt-3.5-turbo\", \"gpt-4\", \"claude-2\"], \"duration\": \"20m\",\"metadata\": {\"user\": \"ishaan@berri.ai\", \"team\": \"core-infra\"}}'\n```\n\n### Expected Response\n\n```shell\n{\n    \"key\": \"sk-kdEXbIqZRwEeEiHwdg7sFA\", # Bearer token\n    \"expires\": \"2023-11-19T01:38:25.838000+00:00\" # datetime object\n}\n```\n\n## Supported Providers ([Docs](https://docs.litellm.ai/docs/providers))\n\n| Provider                                                                            | [Completion](https://docs.litellm.ai/docs/#basic-usage) | [Streaming](https://docs.litellm.ai/docs/completion/stream#streaming-responses) | [Async Completion](https://docs.litellm.ai/docs/completion/stream#async-completion) | [Async Streaming](https://docs.litellm.ai/docs/completion/stream#async-streaming) | [Async Embedding](https://docs.litellm.ai/docs/embedding/supported_embedding) | [Async Image Generation](https://docs.litellm.ai/docs/image_generation) |\n|-------------------------------------------------------------------------------------|---------------------------------------------------------|---------------------------------------------------------------------------------|-------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------|\n| [openai](https://docs.litellm.ai/docs/providers/openai)                             | \u2705                                                       | \u2705                                                                               | \u2705                                                                                   | \u2705                                                                                 | \u2705                                                                             | \u2705                                                                       |\n| [Meta - Llama API](https://docs.litellm.ai/docs/providers/meta_llama)                               | \u2705                                                       | \u2705                                                                               | \u2705                                                                                   | \u2705                                                                                 |                                                                              |                                                                        |\n| [azure](https://docs.litellm.ai/docs/providers/azure)                               | \u2705                                                       | \u2705                                                                               | \u2705                                                                                   | \u2705                                                                                 | \u2705                                                                             | \u2705                                                                       |\n| [AI/ML API](https://docs.litellm.ai/docs/providers/aiml)                               | \u2705                                                       | \u2705                                                                               | \u2705                                                                                   | \u2705                                                                                 | \u2705                                                                             | \u2705                                                                       |\n| [aws - sagemaker](https://docs.litellm.ai/docs/providers/aws_sagemaker)             | \u2705                                                       | \u2705                                                                               | \u2705                                                                                   | \u2705                                                                                 | \u2705                                                                             |                                                                         |\n| [aws - bedrock](https://docs.litellm.ai/docs/providers/bedrock)                     | \u2705                                                       | \u2705                                                                               | \u2705                                                                                   | \u2705                                                                                 | \u2705                                                                             |                                                                         |\n| [google - vertex_ai](https://docs.litellm.ai/docs/providers/vertex)                 | \u2705                                                       | \u2705                                                                               | \u2705                                                                                   | \u2705                                                                                 | \u2705                                                                             | \u2705                                                                       |\n| [google - palm](https://docs.litellm.ai/docs/providers/palm)                        | \u2705                                                       | \u2705                                                                               | \u2705                                                                                   | \u2705                                                                                 |                                                                               |                                                                         |\n| [google AI Studio - gemini](https://docs.litellm.ai/docs/providers/gemini)          | \u2705                                                       | \u2705                                                                               | \u2705                                                                                   | \u2705                                                                                 |                                                                               |                                                                         |\n| [mistral ai api](https://docs.litellm.ai/docs/providers/mistral)                    | \u2705                                                       | \u2705                                                                               | \u2705                                                                                   | \u2705                                                                                 | \u2705                                                                             |                                                                         |\n| [cloudflare AI Workers](https://docs.litellm.ai/docs/providers/cloudflare_workers)  | \u2705                                                       | \u2705                                                                               | \u2705                                                                                   | \u2705                                                                                 |                                                                               |                                                                         |\n| [cohere](https://docs.litellm.ai/docs/providers/cohere)                             | \u2705                                                       | \u2705                                                                               | \u2705                                                                                   | \u2705                                                                                 | \u2705                                                                             |                                                                         |\n| [anthropic](https://docs.litellm.ai/docs/providers/anthropic)                       | \u2705                                                       | \u2705                                                                               | \u2705                                                                                   | \u2705                                                                                 |                                                                               |                                                                         |\n| [empower](https://docs.litellm.ai/docs/providers/empower)                    | \u2705                                                      | \u2705                                                                              | \u2705                                                                                  | \u2705                                                                                |\n| [huggingface](https://docs.litellm.ai/docs/providers/huggingface)                   | \u2705                                                       | \u2705                                                                               | \u2705                                                                                   | \u2705                                                                                 | \u2705                                                                             |                                                                         |\n| [replicate](https://docs.litellm.ai/docs/providers/replicate)                       | \u2705                                                       | \u2705                                                                               | \u2705                                                                                   | \u2705                                                                                 |                                                                               |                                                                         |\n| [together_ai](https://docs.litellm.ai/docs/providers/togetherai)                    | \u2705                                                       | \u2705                                                                               | \u2705                                                                                   | \u2705                                                                                 |                                                                               |                                                                         |\n| [openrouter](https://docs.litellm.ai/docs/providers/openrouter)                     | \u2705                                                       | \u2705                                                                               | \u2705                                                                                   | \u2705                                                                                 |                                                                               |                                                                         |\n| [ai21](https://docs.litellm.ai/docs/providers/ai21)                                 | \u2705                                                       | \u2705                                                                               | \u2705                                                                                   | \u2705                                                                                 |                                                                               |                                                                         |\n| [baseten](https://docs.litellm.ai/docs/providers/baseten)                           | \u2705                                                       | \u2705                                                                               | \u2705                                                                                   | \u2705                                                                                 |                                                                               |                                                                         |\n| [vllm](https://docs.litellm.ai/docs/providers/vllm)                                 | \u2705                                                       | \u2705                                                                               | \u2705                                                                                   | \u2705                                                                                 |                                                                               |                                                                         |\n| [nlp_cloud](https://docs.litellm.ai/docs/providers/nlp_cloud)                       | \u2705                                                       | \u2705                                                                               | \u2705                                                                                   | \u2705                                                                                 |                                                                               |                                                                         |\n| [aleph alpha](https://docs.litellm.ai/docs/providers/aleph_alpha)                   | \u2705                                                       | \u2705                                                                               | \u2705                                                                                   | \u2705                                                                                 |                                                                               |                                                                         |\n| [petals](https://docs.litellm.ai/docs/providers/petals)                             | \u2705                                                       | \u2705                                                                               | \u2705                                                                                   | \u2705                                                                                 |                                                                               |                                                                         |\n| [ollama](https://docs.litellm.ai/docs/providers/ollama)                             | \u2705                                                       | \u2705                                                                               | \u2705                                                                                   | \u2705                                                                                 | \u2705                                                                             |                                                                         |\n| [deepinfra](https://docs.litellm.ai/docs/providers/deepinfra)                       | \u2705                                                       | \u2705                                                                               | \u2705                                                                                   | \u2705                                                                                 |                                                                               |                                                                         |\n| [perplexity-ai](https://docs.litellm.ai/docs/providers/perplexity)                  | \u2705                                                       | \u2705                                                                               | \u2705                                                                                   | \u2705                                                                                 |                                                                               |                                                                         |\n| [Groq AI](https://docs.litellm.ai/docs/providers/groq)                              | \u2705                                                       | \u2705                                                                               | \u2705                                                                                   | \u2705                                                                                 |                                                                               |                                                                         |\n| [Deepseek](https://docs.litellm.ai/docs/providers/deepseek)                         | \u2705                                                       | \u2705                                                                               | \u2705                                                                                   | \u2705                                                                                 |                                                                               |                                                                         |\n| [anyscale](https://docs.litellm.ai/docs/providers/anyscale)                         | \u2705                                                       | \u2705                                                                               | \u2705                                                                                   | \u2705                                                                                 |                                                                               |                                                                         |\n| [IBM - watsonx.ai](https://docs.litellm.ai/docs/providers/watsonx)                  | \u2705                                                       | \u2705                                                                               | \u2705                                                                                   | \u2705                                                                                 | \u2705                                                                             |                                                                         |\n| [voyage ai](https://docs.litellm.ai/docs/providers/voyage)                          |                                                         |                                                                                 |                                                                                     |                                                                                   | \u2705                                                                             |                                                                         |\n| [xinference [Xorbits Inference]](https://docs.litellm.ai/docs/providers/xinference) |                                                         |                                                                                 |                                                                                     |                                                                                   | \u2705                                                                             |                                                                         |\n| [FriendliAI](https://docs.litellm.ai/docs/providers/friendliai)                              | \u2705                                                       | \u2705                                                                               | \u2705                                                                                   | \u2705                                                                                 |                                                                               |                                                                         |\n| [Galadriel](https://docs.litellm.ai/docs/providers/galadriel)                              | \u2705                                                       | \u2705                                                                               | \u2705                                                                                   | \u2705                                                                                 |                                                                               |                                                                         |\n| [Novita AI](https://novita.ai/models/llm?utm_source=github_litellm&utm_medium=github_readme&utm_campaign=github_link)                     | \u2705                                                       | \u2705                                                                               | \u2705                                                                                   | \u2705                                                                                 |                                                                               |                                                                         |\n| [Featherless AI](https://docs.litellm.ai/docs/providers/featherless_ai)                              | \u2705                                                       | \u2705                                                                               | \u2705                                                                                   | \u2705                                                                                 |                                                                               |                                                                         |\n| [Nebius AI Studio](https://docs.litellm.ai/docs/providers/nebius)                             | \u2705                                                       | \u2705                                                                               | \u2705                                                                                   | \u2705                                                                                 | \u2705                                                                             |                                                                         |\n\n[**Read the Docs**](https://docs.litellm.ai/docs/)\n\n## Contributing\n\nInterested in contributing? Contributions to LiteLLM Python SDK, Proxy Server, and LLM integrations are both accepted and highly encouraged! \n\n**Quick start:** `git clone` \u2192 `make install-dev` \u2192 `make format` \u2192 `make lint` \u2192 `make test-unit`\n\nSee our comprehensive [Contributing Guide (CONTRIBUTING.md)](CONTRIBUTING.md) for detailed instructions.\n\n# Enterprise\nFor companies that need better security, user management and professional support\n\n[Talk to founders](https://calendly.com/d/4mp-gd3-k5k/litellm-1-1-onboarding-chat)\n\nThis covers: \n- \u2705 **Features under the [LiteLLM Commercial License](https://docs.litellm.ai/docs/proxy/enterprise):**\n- \u2705 **Feature Prioritization**\n- \u2705 **Custom Integrations**\n- \u2705 **Professional Support - Dedicated discord + slack**\n- \u2705 **Custom SLAs**\n- \u2705 **Secure access with Single Sign-On**\n\n# Contributing\n\nWe welcome contributions to LiteLLM! Whether you're fixing bugs, adding features, or improving documentation, we appreciate your help.\n\n## Quick Start for Contributors\n\n```bash\ngit clone https://github.com/BerriAI/litellm.git\ncd litellm\nmake install-dev    # Install development dependencies\nmake format         # Format your code\nmake lint           # Run all linting checks\nmake test-unit      # Run unit tests\n```\n\nFor detailed contributing guidelines, see [CONTRIBUTING.md](CONTRIBUTING.md).\n\n## Code Quality / Linting\n\nLiteLLM follows the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html).\n\nOur automated checks include:\n- **Black** for code formatting\n- **Ruff** for linting and code quality\n- **MyPy** for type checking\n- **Circular import detection**\n- **Import safety checks**\n\nRun all checks locally:\n```bash\nmake lint           # Run all linting (matches CI)\nmake format-check   # Check formatting only\n```\n\nAll these checks must pass before your PR can be merged.\n\n\n# Support / talk with founders\n\n- [Schedule Demo \ud83d\udc4b](https://calendly.com/d/4mp-gd3-k5k/berriai-1-1-onboarding-litellm-hosted-version)\n- [Community Discord \ud83d\udcad](https://discord.gg/wuPM9dRgDw)\n- [Community Slack \ud83d\udcad](https://join.slack.com/share/enQtOTE0ODczMzk2Nzk4NC01YjUxNjY2YjBlYTFmNDRiZTM3NDFiYTM3MzVkODFiMDVjOGRjMmNmZTZkZTMzOWQzZGQyZWIwYjQ0MWExYmE3)\n- Our numbers \ud83d\udcde +1 (770) 8783-106 / \u202d+1 (412) 618-6238\u202c\n- Our emails \u2709\ufe0f ishaan@berri.ai / krrish@berri.ai\n\n# Why did we build this\n\n- **Need for simplicity**: Our code started to get extremely complicated managing & translating calls between Azure, OpenAI and Cohere.\n\n# Contributors\n\n<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->\n<!-- prettier-ignore-start -->\n<!-- markdownlint-disable -->\n\n<!-- markdownlint-restore -->\n<!-- prettier-ignore-end -->\n\n<!-- ALL-CONTRIBUTORS-LIST:END -->\n\n<a href=\"https://github.com/BerriAI/litellm/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=BerriAI/litellm\" />\n</a>\n\n\n## Run in Developer mode\n### Services\n1. Setup .env file in root\n2. Run dependant services `docker-compose up db prometheus`\n\n### Backend\n1. (In root) create virtual environment `python -m venv .venv`\n2. Activate virtual environment `source .venv/bin/activate`\n3. Install dependencies `pip install -e \".[all]\"`\n4. Start proxy backend `uvicorn litellm.proxy.proxy_server:app --host localhost --port 4000 --reload`\n\n### Frontend\n1. Navigate to `ui/litellm-dashboard`\n2. Install dependencies `npm install`\n3. Run `npm run dev` to start the dashboard\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 221654678,
    "name": "haystack",
    "full_name": "deepset-ai/haystack",
    "description": "AI orchestration framework to build customizable, production-ready LLM applications. Connect components (models, vector DBs, file converters) to pipelines or agents that can interact with your data. With advanced retrieval methods, it's best suited for building RAG, question answering, semantic search or conversational agent chatbots.",
    "html_url": "https://github.com/deepset-ai/haystack",
    "clone_url": "https://github.com/deepset-ai/haystack.git",
    "owner_login": "deepset-ai",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/51827949?v=4",
    "stargazers_count": 21768,
    "watchers_count": 21768,
    "forks_count": 2291,
    "open_issues_count": 127,
    "size": 51278,
    "language": "Python",
    "languages": {
      "Python": 4319306,
      "HTML": 91718,
      "Jinja": 5351,
      "Gherkin": 4343,
      "HCL": 591
    },
    "topics": [
      "agent",
      "agents",
      "ai",
      "gemini",
      "generative-ai",
      "gpt-4",
      "information-retrieval",
      "large-language-models",
      "llm",
      "machine-learning",
      "nlp",
      "orchestration",
      "python",
      "pytorch",
      "question-answering",
      "rag",
      "retrieval-augmented-generation",
      "semantic-search",
      "summarization",
      "transformers"
    ],
    "license_name": "Apache License 2.0",
    "created_at": "2019-11-14T09:05:28+00:00",
    "updated_at": "2025-08-06T02:02:02+00:00",
    "pushed_at": "2025-08-05T16:02:36+00:00",
    "contributors_count": 100,
    "readme_length": 13018,
    "readme_content": "<div align=\"center\">\n  <a href=\"https://haystack.deepset.ai/\"><img src=\"https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/banner.png\" alt=\"Green logo of a stylized white 'H' with the text 'Haystack, by deepset.'\u00a0Abstract green and yellow diagrams in the background.\"></a>\n\n|         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n| ------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| CI/CD   | [![Tests](https://github.com/deepset-ai/haystack/actions/workflows/tests.yml/badge.svg)](https://github.com/deepset-ai/haystack/actions/workflows/tests.yml) [![types - Mypy](https://img.shields.io/badge/types-Mypy-blue.svg)](https://github.com/python/mypy) [![Coverage Status](https://coveralls.io/repos/github/deepset-ai/haystack/badge.svg?branch=main)](https://coveralls.io/github/deepset-ai/haystack?branch=main) [![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff) |\n| Docs    | [![Website](https://img.shields.io/website?label=documentation&up_message=online&url=https%3A%2F%2Fdocs.haystack.deepset.ai)](https://docs.haystack.deepset.ai)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n| Package | [![PyPI](https://img.shields.io/pypi/v/haystack-ai)](https://pypi.org/project/haystack-ai/) ![PyPI - Downloads](https://img.shields.io/pypi/dm/haystack-ai?color=blue&logo=pypi&logoColor=gold) ![PyPI - Python Version](https://img.shields.io/pypi/pyversions/haystack-ai?logo=python&logoColor=gold) [![Conda Version](https://img.shields.io/conda/vn/conda-forge/haystack-ai.svg)](https://anaconda.org/conda-forge/haystack-ai) [![GitHub](https://img.shields.io/github/license/deepset-ai/haystack?color=blue)](LICENSE) [![License Compliance](https://github.com/deepset-ai/haystack/actions/workflows/license_compliance.yml/badge.svg)](https://github.com/deepset-ai/haystack/actions/workflows/license_compliance.yml) |\n| Meta    | [![Discord](https://img.shields.io/discord/993534733298450452?logo=discord)](https://discord.com/invite/xYvH6drSmA) [![Twitter Follow](https://img.shields.io/twitter/follow/haystack_ai)](https://twitter.com/haystack_ai)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n</div>\n\n[Haystack](https://haystack.deepset.ai/) is an end-to-end LLM framework that allows you to build applications powered by\nLLMs, Transformer models, vector search and more. Whether you want to perform retrieval-augmented generation (RAG),\ndocument search, question answering or answer generation, Haystack can orchestrate state-of-the-art embedding models\nand LLMs into pipelines to build end-to-end NLP applications and solve your use case.\n\n## Table of Contents\n\n- [Installation](#installation)\n- [Documentation](#documentation)\n- [Features](#features)\n- [Use Cases](#features)\n- [Hayhooks (REST API Deployment)](#-tip-1)\n- [Haystack Enterprise](#haystack-enterprise-best-practices-and-expert-support)\n- [deepset Studio](#-deepset-studio-your-development-environment-for-haystack)\n- [Telemetry](#telemetry)\n- [\ud83d\udd96 Community](#-community)\n- [Contributing to Haystack](#contributing-to-haystack)\n- [Who Uses Haystack](#who-uses-haystack)\n\n\n## Installation\n\nThe simplest way to get Haystack is via pip:\n\n```sh\npip install haystack-ai\n```\n\nInstall from the `main` branch to try the newest features:\n```sh\npip install git+https://github.com/deepset-ai/haystack.git@main\n```\n\nHaystack supports multiple installation methods including Docker images. For a comprehensive guide please refer\nto the [documentation](https://docs.haystack.deepset.ai/docs/installation).\n\n## Documentation\n\nIf you're new to the project, check out [\"What is Haystack?\"](https://haystack.deepset.ai/overview/intro) then go\nthrough the [\"Get Started Guide\"](https://haystack.deepset.ai/overview/quick-start) and build your first LLM application\nin a matter of minutes. Keep learning with the [tutorials](https://haystack.deepset.ai/tutorials). For more advanced\nuse cases, or just to get some inspiration, you can browse our Haystack recipes in the\n[Cookbook](https://haystack.deepset.ai/cookbook).\n\nAt any given point, hit the [documentation](https://docs.haystack.deepset.ai/docs/intro) to learn more about Haystack, what can it do for you and the technology behind.\n\n## Features\n\n- **Technology agnostic:** Allow users the flexibility to decide what vendor or technology they want and make it easy to switch out any component for another. Haystack allows you to use and compare models available from OpenAI, Cohere and Hugging Face, as well as your own local models or models hosted on Azure, Bedrock and SageMaker.\n- **Explicit:** Make it transparent how different moving parts can \u201ctalk\u201d to each other so it's easier to fit your tech stack and use case.\n- **Flexible:** Haystack provides all tooling in one place: database access, file conversion, cleaning, splitting, training, eval, inference, and more. And whenever custom behavior is desirable, it's easy to create custom components.\n- **Extensible:** Provide a uniform and easy way for the community and third parties to build their own components and foster an open ecosystem around Haystack.\n\nSome examples of what you can do with Haystack:\n\n-   Build **retrieval augmented generation (RAG)** by making use of one of the available vector databases and customizing your LLM interaction, the sky is the limit \ud83d\ude80\n-   Perform Question Answering **in natural language** to find granular answers in your documents.\n-   Perform **semantic search** and retrieve documents according to meaning.\n-   Build applications that can make complex decisions making to answer complex queries: such as systems that can resolve complex customer queries, do knowledge search on many disconnected resources and so on.\n-   Scale to millions of docs using retrievers and production-scale components.\n-   Use **off-the-shelf models** or **fine-tune** them to your data.\n-   Use **user feedback** to evaluate, benchmark, and continuously improve your models.\n\n> [!TIP]\n>\n> Would you like to deploy and serve Haystack pipelines as REST APIs yourself? [Hayhooks](https://github.com/deepset-ai/hayhooks) provides a simple way to wrap your pipelines with custom logic and expose them via HTTP endpoints, including OpenAI-compatible chat completion endpoints and compatibility with fully-featured chat interfaces like [open-webui](https://openwebui.com/).\n\n## Haystack Enterprise: Best Practices and Expert Support\n\nGet expert support from the Haystack team, build faster with enterprise-grade templates, and scale securely with deployment guides for cloud and on-prem environments - all with **Haystack Enterprise**. Read more about it our [announcement post](https://haystack.deepset.ai/blog/announcing-haystack-enterprise).\n\n\ud83d\udc49 [Get Haystack Enterprise](https://www.deepset.ai/products-and-services/haystack-enterprise?utm_source=github.com&utm_medium=referral&utm_campaign=haystack_enterprise) \n\n## deepset Studio: Your Development Environment for Haystack\n\nUse **deepset Studio** to visually create, deploy, and test your Haystack pipelines. Learn more about it in our [announcement post](https://haystack.deepset.ai/blog/announcing-studio).\n\n![studio](https://github.com/user-attachments/assets/e4f09746-20b5-433e-8261-eca224ac23b3)\n\n\ud83d\udc49 [Sign up](https://landing.deepset.ai/deepset-studio-signup)!\n\n> [!TIP]\n><img src=\"https://github.com/deepset-ai/haystack/raw/main/docs/img/deepset-platform-logo-alternative.jpeg\"  width=20%>\n>\n> Are you looking for a managed solution that benefits from Haystack? [deepset AI Platform](https://www.deepset.ai/products-and-services/deepset-ai-platform?utm_campaign=developer-relations&utm_source=haystack&utm_medium=readme) is our fully managed, end-to-end platform to integrate LLMs with your data, which uses Haystack for the LLM pipelines architecture.\n\n## Telemetry\n\nHaystack collects **anonymous** usage statistics of pipeline components. We receive an event every time these components are initialized. This way, we know which components are most relevant to our community.\n\nRead more about telemetry in Haystack or how you can opt out in [Haystack docs](https://docs.haystack.deepset.ai/docs/telemetry).\n\n## \ud83d\udd96 Community\n\nIf you have a feature request or a bug report, feel free to open an [issue in Github](https://github.com/deepset-ai/haystack/issues). We regularly check these and you can expect a quick response. If you'd like to discuss a topic, or get more general advice on how to make Haystack work for your project, you can start a thread in [Github Discussions](https://github.com/deepset-ai/haystack/discussions) or our [Discord channel](https://discord.com/invite/VBpFzsgRVF). We also check [\ud835\udd4f (Twitter)](https://twitter.com/haystack_ai) and [Stack Overflow](https://stackoverflow.com/questions/tagged/haystack).\n\n## Contributing to Haystack\n\nWe are very open to the community's contributions - be it a quick fix of a typo, or a completely new feature! You don't need to be a Haystack expert to provide meaningful improvements. To learn how to get started, check out our [Contributor Guidelines](https://github.com/deepset-ai/haystack/blob/main/CONTRIBUTING.md) first.\n\nThere are several ways you can contribute to Haystack:\n- Contribute to the main Haystack project\n- Contribute an integration on [haystack-core-integrations](https://github.com/deepset-ai/haystack-core-integrations)\n\n> [!TIP]\n>\ud83d\udc49 **[Check out the full list of issues that are open to contributions](https://github.com/orgs/deepset-ai/projects/14)**\n\n## Who Uses Haystack\n\nHere's a list of projects and companies using Haystack. Want to add yours? Open a PR, add it to the list and let the\nworld know that you use Haystack!\n\n-   [Airbus](https://www.airbus.com/en)\n-   [Alcatel-Lucent](https://www.al-enterprise.com/)\n-   [Apple](https://www.apple.com/)\n-   [BetterUp](https://www.betterup.com/)\n-   [Databricks](https://www.databricks.com/)\n-   [Deepset](https://deepset.ai/)\n-   [Etalab](https://www.deepset.ai/blog/improving-on-site-search-for-government-agencies-etalab)\n-   [Infineon](https://www.infineon.com/)\n-   [Intel](https://github.com/intel/open-domain-question-and-answer#readme)\n-   [Intelijus](https://www.intelijus.ai/)\n-   [Intel Labs](https://github.com/IntelLabs/fastRAG#readme)\n-   [LEGO](https://github.com/larsbaunwall/bricky#readme)\n-   [Meta](https://www.meta.com/about)\n-   [Netflix](https://netflix.com)\n-   [NOS Portugal](https://www.nos.pt/en/welcome)\n-   [Nvidia](https://developer.nvidia.com/blog/reducing-development-time-for-intelligent-virtual-assistants-in-contact-centers/)\n-   [PostHog](https://github.com/PostHog/max-ai#readme)\n-   [Rakuten](https://www.rakuten.com/)\n-   [Sooth.ai](https://www.deepset.ai/blog/advanced-neural-search-with-sooth-ai)\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 198350484,
    "name": "unilm",
    "full_name": "microsoft/unilm",
    "description": "Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities",
    "html_url": "https://github.com/microsoft/unilm",
    "clone_url": "https://github.com/microsoft/unilm.git",
    "owner_login": "microsoft",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/6154722?v=4",
    "stargazers_count": 21594,
    "watchers_count": 21594,
    "forks_count": 2646,
    "open_issues_count": 670,
    "size": 77212,
    "language": "Python",
    "languages": {
      "Python": 29353110,
      "Jupyter Notebook": 4386194,
      "Shell": 794546,
      "Cuda": 203684,
      "C++": 107673,
      "Cython": 62333,
      "Lua": 21192,
      "HTML": 6691,
      "Batchfile": 3220,
      "Makefile": 3064,
      "Perl": 2767,
      "Mako": 2289,
      "C": 1869,
      "Dockerfile": 330
    },
    "topics": [
      "beit",
      "beit-3",
      "bitnet",
      "deepnet",
      "document-ai",
      "foundation-models",
      "kosmos",
      "kosmos-1",
      "layoutlm",
      "layoutxlm",
      "llm",
      "minilm",
      "mllm",
      "multimodal",
      "nlp",
      "pre-trained-model",
      "textdiffuser",
      "trocr",
      "unilm",
      "xlm-e"
    ],
    "license_name": "MIT License",
    "created_at": "2019-07-23T04:15:28+00:00",
    "updated_at": "2025-08-05T18:27:11+00:00",
    "pushed_at": "2025-07-03T09:28:33+00:00",
    "contributors_count": 60,
    "readme_length": 33516,
    "readme_content": "<!--# Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities-->\n## [aka.ms/GeneralAI](https://aka.ms/GeneralAI)\n# Hiring\nWe are hiring at all levels (including FTE researchers and interns)! If you are interested in working with us on Foundation Models (aka large-scale pre-trained models) and General AI, NLP, MT, Speech, Document AI and Multimodal AI, please send your resume to <a href=\"mailto:fuwei@microsoft.com\" class=\"x-hidden-focus\">fuwei@microsoft.com</a>.\n\n# Foundation Architecture\n### TorchScale - A Library of Foundation Architectures ([repo](https://github.com/microsoft/torchscale))\n\nFundamental research to develop new architectures for foundation models and AI, focusing on modeling generality and capability, as well as training stability and efficiency.\n\n> Stability - [**DeepNet**](https://github.com/microsoft/unilm/tree/master/deepnet): scaling Transformers to 1,000 Layers and beyond\n\n> Generality - [**Foundation Transformers (Magneto)**](https://arxiv.org/abs/2210.06423): towards true general-purpose modeling across tasks and modalities (including language, vision, speech, and multimodal)\n\n> Capability - A [**Length-Extrapolatable**](https://arxiv.org/abs/2212.10554) Transformer\n\n> Efficiency & Transferability - [**X-MoE**](https://github.com/microsoft/unilm/tree/master/xmoe): scalable & finetunable sparse Mixture-of-Experts (MoE)\n\n### The Revolution of Model Architecture\n\n> [**BitNet**](https://arxiv.org/abs/2310.11453): 1-bit Transformers for Large Language Models\n\n> [**RetNet**](https://arxiv.org/abs/2307.08621): Retentive Network: A Successor to Transformer for Large Language Models\n\n> [**LongNet**](https://arxiv.org/abs/2307.02486): Scaling Transformers to 1,000,000,000 Tokens\n\n# Foundation Models\n\n### The Evolution of (M)LLM (Multimodal LLM)\n\n> [**Kosmos-2.5**](https://github.com/microsoft/unilm/tree/master/kosmos-2.5): **A Multimodal Literate Model**\n\n> [**Kosmos-2**](https://github.com/microsoft/unilm/tree/master/kosmos-2): **Grounding Multimodal Large Language Models to the World**\n\n> [**Kosmos-1**](https://arxiv.org/abs/2302.14045): **A Multimodal Large Language Model (MLLM)**\n\n> [**MetaLM**](https://github.com/microsoft/unilm/tree/master/metalm): **Language Models are General-Purpose Interfaces**\n\n**The Big Convergence** - Large-scale self-supervised pre-training across ```tasks``` (predictive and generative), ```languages``` (100+ languages), and ```modalities``` (language, image, audio, layout/format + language, vision + language, audio + language, etc.)\n\n<!--## Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities-->\n\n### Language & Multilingual\n> [**UniLM**](https://github.com/microsoft/unilm/tree/master/unilm): unified pre-training for language understanding and generation\n\n> [**InfoXLM/XLM-E**](https://github.com/microsoft/unilm/tree/master/infoxlm): multilingual/cross-lingual pre-trained models for 100+ languages\n\n> [**DeltaLM/mT6**](https://github.com/microsoft/unilm/tree/master/deltalm): encoder-decoder pre-training for language generation and translation for 100+ languages\n\n> [**MiniLM**](https://github.com/microsoft/unilm/tree/master/minilm): small and fast pre-trained models for language understanding and generation\n\n> [**AdaLM**](https://github.com/microsoft/unilm/tree/master/adalm): domain, language, and task adaptation of pre-trained models\n\n> [**EdgeLM**](https://github.com/microsoft/unilm/tree/master/edgelm)(```NEW```): small pre-trained models on edge/client devices\n\n> [**SimLM**](https://github.com/microsoft/unilm/tree/master/simlm) (```NEW```): large-scale pre-training for similarity matching\n\n> [**E5**](https://github.com/microsoft/unilm/tree/master/e5) (```NEW```): text embeddings\n\n> [**MiniLLM**](https://arxiv.org/abs/2306.08543) (```NEW```): Knowledge Distillation of Large Language Models\n\n### Vision\n> [**BEiT**](https://github.com/microsoft/unilm/tree/master/beit)/[**BEiT-2**](https://github.com/microsoft/unilm/tree/master/beit2): generative self-supervised pre-training for vision / BERT Pre-Training of Image Transformers\n\n> [**DiT**](https://github.com/microsoft/unilm/tree/master/dit): self-supervised pre-training for Document Image Transformers\n\n> [**TextDiffuser**](https://github.com/microsoft/unilm/tree/master/textdiffuser)/[**TextDiffuser-2**](https://github.com/microsoft/unilm/tree/master/textdiffuser-2) (```NEW```): Diffusion Models as Text Painters\n\n### Speech\n> [**WavLM**](https://github.com/microsoft/unilm/tree/master/wavlm): speech pre-training for full stack tasks\n\n> [**VALL-E**](https://github.com/microsoft/unilm/tree/master/valle): a neural codec language model for TTS\n\n### Multimodal (X + Language)\n> [**LayoutLM**](https://github.com/microsoft/unilm/tree/master/layoutlm)/[**LayoutLMv2**](https://github.com/microsoft/unilm/tree/master/layoutlmv2)/[**LayoutLMv3**](https://github.com/microsoft/unilm/tree/master/layoutlmv3): multimodal (text + layout/format + image) **Document Foundation Model** for [Document AI](https://www.microsoft.com/en-us/research/project/document-ai/) (e.g. scanned documents, PDF, etc.)\n\n> [**LayoutXLM**](https://github.com/microsoft/unilm/tree/master/layoutxlm): multimodal (text + layout/format + image) **Document Foundation Model** for multilingual Document AI\n\n> [**MarkupLM**](https://github.com/microsoft/unilm/tree/master/markuplm): markup language model pre-training for visually-rich document understanding\n\n> [**XDoc**](https://github.com/microsoft/unilm/tree/master/xdoc): unified pre-training for cross-format document understanding\n\n> [**UniSpeech**](https://arxiv.org/abs/2101.07597): unified pre-training for self-supervised learning and supervised learning for ASR\n\n> [**UniSpeech-SAT**](https://arxiv.org/pdf/2110.05752.pdf): universal speech representation learning with speaker-aware pre-training\n\n> [**SpeechT5**](https://arxiv.org/abs/2110.07205): encoder-decoder pre-training for spoken language processing\n\n> [**SpeechLM**](https://arxiv.org/abs/2209.15329): Enhanced Speech Pre-Training with Unpaired Textual Data\n\n> [**VLMo**](https://github.com/microsoft/unilm/tree/master/vlmo): Unified vision-language pre-training \n\n> [**VL-BEiT**](https://github.com/microsoft/unilm/tree/master/vl-beit) (```NEW```): Generative Vision-Language Pre-training - evolution of **BEiT** to multimodal\n\n> [**BEiT-3**](https://github.com/microsoft/unilm/tree/master/beit3) (```NEW```): a general-purpose multimodal foundation model, and a major milestone of **The Big Convergence** of Large-scale Pre-training Across Tasks, Languages, and Modalities.\n### Toolkits\n> [**s2s-ft**](https://github.com/microsoft/unilm/tree/master/s2s-ft): sequence-to-sequence fine-tuning toolkit\n\n> [**Aggressive Decoding**](https://arxiv.org/pdf/2205.10350.pdf) (```NEW```): lossless and efficient sequence-to-sequence decoding algorithm\n\n### Applications\n> [**TrOCR**](https://github.com/microsoft/unilm/tree/master/trocr): transformer-based OCR w/ pre-trained models\n \n> [**LayoutReader**](https://github.com/microsoft/unilm/tree/master/layoutreader): pre-training of text and layout for reading order detection\n\n> [**XLM-T**](https://github.com/microsoft/unilm/tree/master/xlmt): multilingual NMT w/ pretrained cross-lingual encoders\n\n## Links\n### LLMOps ([repo](https://github.com/microsoft/lmops))\nGeneral technology for enabling AI capabilities w/ LLMs and MLLMs.\n\n### RedStone ([repo](https://github.com/microsoft/redstone))\nCurating General, Code, Math, and QA Data for Large Language Models.\n\n## News\n- December, 2024: [**RedStone**](https://github.com/microsoft/redstone) was released!\n- December, 2023: [**LongNet**](https://github.com/microsoft/unilm/tree/master/longnet) and [**LongViT**](https://github.com/microsoft/unilm/tree/master/longvit) released\n- [Model Release] Dec, 2023: [**TextDiffuser-2**](https://github.com/microsoft/unilm/tree/master/textdiffuser-2) models, code and [demo](https://huggingface.co/spaces/JingyeChen22/TextDiffuser-2). \n- Sep, 2023: [**Kosmos-2.5**](https://arxiv.org/abs/2309.11419) - a multimodal literate model for machine reading of text-intensive images.\n- [Model Release] May, 2023: [**TextDiffuser**](https://github.com/microsoft/unilm/tree/master/textdiffuser) models and code.\n- [Model Release] March, 2023: [**BEiT-3**](https://github.com/microsoft/unilm/tree/master/beit3) pretrained models and code.\n- March, 2023: [**Kosmos-1**](https://arxiv.org/abs/2302.14045) - a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot).\n- January, 2023: [**VALL-E**](https://arxiv.org/abs/2301.02111) a language modeling approach for text to speech synthesis (TTS), which achieves state-of-the-art zero-shot TTS performance. See https://aka.ms/valle for demos of our work.\n- [Model Release] January, 2023: [**E5**](https://github.com/microsoft/unilm/tree/master/e5) - Text Embeddings by Weakly-Supervised Contrastive Pre-training.\n- November, 2022: [**TorchScale 0.1.1**](https://github.com/microsoft/torchscale) was released!\n- November, 2022: [**TrOCR**](https://arxiv.org/abs/2109.10282) was accepted by AAAI 2023.\n- [Model Release] November, 2022: [**XDoc**](https://github.com/microsoft/unilm/tree/master/xdoc) **BASE** models for cross-format document understanding.\n- [Model Release] September, 2022: [**TrOCR**](https://github.com/microsoft/unilm/tree/master/trocr) **BASE** and **LARGE** models for Scene Text Recognition (STR).\n- [Model Release] September, 2022: [**BEiT v2**](https://github.com/microsoft/unilm/tree/master/beit2) code and pretrained models.\n- August, 2022: [**BEiT-3**](https://arxiv.org/abs/2208.10442) - a general-purpose multimodal foundation model, which achieves state-of-the-art transfer performance on both vision and vision-language tasks\n- July, 2022: [**SimLM**](https://github.com/microsoft/unilm/tree/master/simlm) - Large-scale self-supervised pre-training for similarity matching\n- June, 2022: [**DiT**](https://arxiv.org/abs/2203.02378) and [**LayoutLMv3**](https://arxiv.org/abs/2204.08387) were accepted by ACM Multimedia 2022.\n- June, 2022: [**MetaLM**](https://github.com/microsoft/unilm/tree/master/metalm) - Language models are general-purpose interfaces to foundation models (language/multilingual, vision, speech, and multimodal)\n- June, 2022: [**VL-BEiT**](https://github.com/microsoft/unilm/tree/master/vl-beit) - bidirectional multimodal Transformer learned from scratch with one unified pretraining task, one shared backbone, and one-stage training, supporting both vision and vision-language tasks.\n- [Model Release] June, 2022: [**LayoutLMv3 Chinese**](https://github.com/microsoft/unilm/tree/master/layoutlmv3#form-understanding-on-xfund) - Chinese version of LayoutLMv3\n- [Code Release] May, 2022: [**Aggressive Decoding**](https://github.com/microsoft/unilm/tree/master/decoding) - Lossless Speedup for Seq2seq Generation\n- April, 2022: **Transformers at Scale** = [DeepNet](https://arxiv.org/abs/2203.00555) + [X-MoE](https://arxiv.org/abs/2204.09179)\n- [Model Release] April, 2022: [**LayoutLMv3**](https://github.com/microsoft/unilm/tree/master/layoutlmv3) - Pre-training for Document AI with Unified Text and Image Masking\n- [Model Release] March, 2022: [**EdgeFormer**](https://github.com/microsoft/unilm/tree/master/edgelm) - Parameter-efficient Transformer for On-device Seq2seq Generation\n- [Model Release] March, 2022: [**DiT**](https://github.com/microsoft/unilm/tree/master/dit) - Self-supervised Document Image Transformer. Demos: [Document Layout Analysis](https://huggingface.co/spaces/nielsr/dit-document-layout-analysis), [Document Image Classification](https://huggingface.co/spaces/microsoft/document-image-transformer)\n- January, 2022: [**BEiT**](https://openreview.net/forum?id=p-BhZSz59o4) was accepted by **ICLR 2022 as Oral presentation** (54 out of 3391).\n- [Model Release] December 16th, 2021: [**TrOCR**](https://github.com/microsoft/unilm/tree/master/trocr) **small** models for handwritten and printed texts, with 3x inference speedup.\n- November 24th, 2021: [**VLMo**](https://github.com/microsoft/unilm/tree/master/vlmo) as the new SOTA on the [VQA Challenge](https://eval.ai/web/challenges/challenge-page/830/leaderboard/2278)\n- November, 2021: [Multilingual translation at scale: 10000 language pairs and beyond](https://www.microsoft.com/en-us/translator/blog/2021/11/22/multilingual-translation-at-scale-10000-language-pairs-and-beyond/)\n- [Model Release] November, 2021: [**MarkupLM**](https://github.com/microsoft/unilm/tree/master/markuplm) - Pre-training for text and markup language (e.g. HTML/XML)\n- [Model Release] November, 2021: [**VLMo**](https://github.com/microsoft/unilm/tree/master/vlmo) - Unified vision-language pre-training w/ [**BEiT**](https://github.com/microsoft/unilm/tree/master/beit)\n- October, 2021: [**WavLM**](https://github.com/microsoft/unilm/tree/master/wavlm) Large achieves state-of-the-art performance on the [SUPERB](https://superbbenchmark.org/leaderboard) benchmark\n- [Model Release] October, 2021: [**WavLM**](https://github.com/microsoft/unilm/tree/master/wavlm) - Large-scale self-supervised pre-trained models for speech. \n- [Model Release] October 2021: [**TrOCR**](https://huggingface.co/transformers/master/model_doc/trocr.html) is on [HuggingFace](https://github.com/huggingface/transformers)\n- September 28th, 2021: T-ULRv5 (aka <a href=\"https://arxiv.org/abs/2106.16138\" target=\"_blank\">XLM-E</a>/<a href=\"https://arxiv.org/abs/2007.07834\" target=\"_blank\">InfoXLM</a>) as the SOTA on the <a href=\"https://sites.research.google/xtreme\" target=\"_blank\">XTREME</a> leaderboard. // <a href=\"https://www.microsoft.com/en-us/research/blog/microsoft-turing-universal-language-representation-model-t-ulrv5-tops-xtreme-leaderboard-and-trains-100x-faster/\" target=\"_blank\">Blog</a>\n- [Model Release] September, 2021: [**LayoutLM-cased**](https://huggingface.co/microsoft/layoutlm-base-cased) are on [HuggingFace](https://github.com/huggingface/transformers)\n- [Model Release] September, 2021: [**TrOCR**](https://github.com/microsoft/unilm/tree/master/trocr) - Transformer-based OCR w/ pre-trained [**BEiT**](https://github.com/microsoft/unilm/tree/master/beit) and RoBERTa models.\n- August 2021: [**LayoutLMv2**](https://huggingface.co/transformers/master/model_doc/layoutlmv2.html) and [**LayoutXLM**](https://huggingface.co/transformers/master/model_doc/layoutxlm.html) are on [HuggingFace](https://github.com/huggingface/transformers)\n- [Model Release] August, 2021: [**LayoutReader**](https://github.com/microsoft/unilm/tree/master/layoutreader) - Built with LayoutLM to improve general reading order detection.\n- [Model Release] August, 2021: [**DeltaLM**](https://github.com/microsoft/unilm/tree/master/deltalm) - Encoder-decoder pre-training for language generation and translation.\n- August 2021: [**BEiT**](https://huggingface.co/transformers/master/model_doc/beit.html) is on [HuggingFace](https://github.com/huggingface/transformers)\n- [Model Release] July, 2021: [**BEiT**](https://github.com/microsoft/unilm/tree/master/beit) - Towards BERT moment for CV\n- [Model Release] June, 2021: [**LayoutLMv2**](https://github.com/microsoft/unilm/tree/master/layoutlmv2), [**LayoutXLM**](https://github.com/microsoft/unilm/tree/master/layoutxlm), [**MiniLMv2**](https://github.com/microsoft/unilm/tree/master/minilm), and [**AdaLM**](https://github.com/microsoft/unilm/tree/master/adalm).\n- May, 2021: [LayoutLMv2](https://github.com/microsoft/unilm/tree/master/layoutlmv2), InfoXLMv2, MiniLMv2, UniLMv3, and AdaLM were accepted by ACL 2021.\n- April, 2021: [LayoutXLM](https://github.com/microsoft/unilm/tree/master/layoutxlm) is coming by extending the LayoutLM into multilingual support! A multilingual form understanding benchmark [XFUND](https://github.com/doc-analysis/XFUND) is also introduced, which includes forms with human labeled key-value pairs in 7 languages (Chinese, Japanese, Spanish, French, Italian, German, Portuguese).\n- March, 2021: [InfoXLM](https://github.com/microsoft/unilm/tree/master/infoxlm) was accepted by NAACL 2021.\n- December 29th, 2020: [LayoutLMv2](https://arxiv.org/abs/2012.14740) is coming with the new SOTA on a wide variety of document AI tasks, including [DocVQA](https://rrc.cvc.uab.es/?ch=17&com=evaluation&task=1) and [SROIE](https://rrc.cvc.uab.es/?ch=13&com=evaluation&task=3) leaderboard.\n- October 8th, 2020: T-ULRv2 (aka [InfoXLM](https://arxiv.org/abs/2007.07834)) as the SOTA on the [XTREME](https://sites.research.google/xtreme) leaderboard. // [Blog](https://www.microsoft.com/en-us/research/blog/microsoft-turing-universal-language-representation-model-t-ulrv2-tops-xtreme-leaderboard/)\n- September, 2020: [MiniLM](https://github.com/microsoft/unilm/tree/master/minilm) was accepted by NeurIPS 2020.\n- July 16, 2020: [**InfoXLM** (Multilingual UniLM)](https://github.com/microsoft/unilm/tree/master/infoxlm) [arXiv](https://arxiv.org/pdf/2007.07834.pdf)\n- June, 2020: [UniLMv2](https://github.com/microsoft/unilm/tree/master/unilm) was accepted by ICML 2020; [LayoutLM](https://github.com/microsoft/unilm/tree/master/layoutlm) was accepted by KDD 2020.\n- April 5, 2020: [**Multilingual MiniLM**](https://github.com/microsoft/unilm/tree/master/minilm) released!\n- September, 2019: [UniLMv1](https://github.com/microsoft/unilm/tree/master/unilm-v1) was accepted by NeurIPS 2019.\n\n<!--\n## Release\n\n**\\*\\*\\*\\*\\* ```New October, 2022```: [XDoc](https://github.com/microsoft/unilm/tree/master/xdoc) release \\*\\*\\*\\*\\***\n\n- [x] [**XDoc**](https://github.com/microsoft/unilm/tree/master/xdoc) (October 7, 2022): XDoc, a unified pre-trained model which deals with different document formats in a single model. For parameter efficiency, we share backbone parameters for different formats such as the word embedding layer and the Transformer layers. Meanwhile, we introduce adaptive layers with lightweight parameters to enhance the distinction across different formats. Experimental results have demonstrated that with only 36.7% parameters, XDoc achieves comparable or even better performance on a variety of downstream tasks compared with the individual pre-trained models, which is cost effective for real-world deployment. \"[XDoc: Unified Pre-training for Cross-Format Document Understanding](https://arxiv.org/abs/2210.02849) ```EMNLP 2022```\"\n\n**\\*\\*\\*\\*\\* ```New May, 2022```: [Aggressive Decoding](https://github.com/microsoft/unilm/tree/master/decoding) release \\*\\*\\*\\*\\***\n\n- [x] [**Aggressive Decoding**](https://github.com/microsoft/unilm/tree/master/decoding) (May 20, 2022): Aggressive Decoding, a novel decoding paradigm for lossless speedup for seq2seq generation. Unlike the previous efforts (e.g., non-autoregressive decoding) speeding up seq2seq generation at the cost of quality loss, Aggressive Decoding aims to yield the identical (or better) generation compared with autoregressive decoding but in a significant speedup: For the seq2seq tasks characterized by highly similar inputs and outputs (e.g., Grammatical Error Correction and Text Simplification), the Input-guided Aggressive Decoding can introduce a 7x-9x speedup for the popular 6-layer Transformer on GPU with the identical results as greedy decoding; For other general seq2seq tasks (e.g., Machine Translation and Abstractive Summarization), the Generalized Aggressive Decoding can have a 3x-5x speedup with the identical or even better quality. \"[Lossless Acceleration for Seq2seq Generation with Aggressive Decoding](https://arxiv.org/pdf/2205.10350.pdf)\"\n\n**\\*\\*\\*\\*\\* ```New April, 2022```: [LayoutLMv3](https://github.com/microsoft/unilm/tree/master/layoutlmv3) release \\*\\*\\*\\*\\***\n\n- [x] [**LayoutLM 3.0**](https://github.com/microsoft/unilm/tree/master/layoutlmv3) (April 19, 2022): LayoutLMv3, a multimodal pre-trained Transformer for Document AI with unified text and image masking. Additionally, it is also pre-trained with a word-patch alignment objective to learn cross-modal alignment by predicting whether the corresponding image patch of a text word is masked. The simple unified architecture and training objectives make LayoutLMv3 a general-purpose pre-trained model for both text-centric and image-centric Document AI tasks. Experimental results show that LayoutLMv3 achieves state-of-the-art performance not only in text-centric tasks, including form understanding, receipt understanding, and document visual question answering, but also in image-centric tasks such as document image classification and document layout analysis. \"[LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking](https://arxiv.org/abs/2204.08387) ```ACM MM 2022```\"\n\n**\\*\\*\\*\\*\\* ```March, 2022```: [EdgeFormer](https://github.com/microsoft/unilm/tree/master/edgelm) release \\*\\*\\*\\*\\***\n\n- [x] [**EdgeFormer**](https://github.com/microsoft/unilm/tree/master/edgelm) (March 18, 2022): EdgeFormer, the first publicly available pretrained parameter-efficient Transformer for on-device seq2seq generation. EdgeFormer has only 11 million parameters, taking up less than 15MB disk size after int8 quantization and compression, which can process a sentence of the length of 20-30 tokens with acceptable latency on two middle-to-high end CPU cores and less than 50MB memory footprint. The pretrained EdgeFormer can be fine-tuned to English seq2seq tasks and achieve promising results -- significantly better than the strong paramter-efficient Transformer baseline (pretrained Universal Transformer) and full-parameterized Transformer-base model without pretraining, which we believe can largely facilitate on-device seq2seq generation in practice. \"[EdgeFormer: A Parameter-Efficient Transformer for On-Device Seq2seq Generation](https://arxiv.org/abs/2202.07959)\"\n\n\n**\\*\\*\\*\\*\\* ```March, 2022```: [DiT](https://github.com/microsoft/unilm/tree/master/dit) release \\*\\*\\*\\*\\***\n\n- [x] [**DiT**](https://github.com/microsoft/unilm/tree/master/dit) (March 4, 2022): DiT, a self-supervised pre-trained Document Image Transformer model using large-scale unlabeled text images for Document AI tasks, which is essential since no supervised counterparts ever exist due to the lack of human labeled document images. We leverage DiT as the backbone network in a variety of vision-based Document AI tasks, including document image classification, document layout analysis, table detection as well as text detection for OCR. Experiment results have illustrated that the self-supervised pre-trained DiT model achieves new state-of-the-art results on these downstream tasks, e.g. document image classification (91.11 \u2192 92.69), document layout analysis (91.0 \u2192 94.9), table detection (94.23 \u2192 96.55) and text detection for OCR (93.07 \u2192 94.29). \"[DiT: Self-supervised Pre-training for Document Image Transformer](https://arxiv.org/abs/2203.02378) ```ACM MM 2022```\"\n\n\n**\\*\\*\\*\\*\\* ```October, 2021```: [WavLM](https://github.com/microsoft/unilm/tree/master/wavlm) release \\*\\*\\*\\*\\***\n\n- [x] [**WavLM**](https://github.com/microsoft/unilm/tree/master/wavlm) (October 27, 2021):  WavLM, a new pre-trained speech model, to solve full-stack downstream speech tasks. \nWavLM integrates the gated relative position embedding structure and the utterance mixing method, to model both spoken content and speaker identity preservation. WavLM is trained on  94k hours of public audio data, which is larger than other released checkpoints for English Speech modeling. WavLM Large achieves state-of-the-art performance on the SUPERB benchmark, and brings significant improvements for various speech processing tasks on their representative benchmarks. \"[WavLM: Large-Scale Self-Supervised  Pre-training   for Full Stack Speech Processing](https://arxiv.org/pdf/2110.13900.pdf)\"\n\n**\\*\\*\\*\\*\\* ```October, 2021```: [MarkupLM](https://github.com/microsoft/unilm/tree/master/markuplm) release \\*\\*\\*\\*\\***\n\n- [x] [**MarkupLM**](https://github.com/microsoft/unilm/tree/master/markuplm) (October 19, 2021): MarkupLM, a simple yet effective pre-training approach for text and markup language. With the Transformer architecture, MarkupLM integrates different input embeddings including text embeddings, position embeddings, and XPath embeddings. Furthermore, we also propose new pre-training objectives that are specially designed for understanding the markup language. We evaluate the pre-trained MarkupLM model on the WebSRC and SWDE datasets. Experiments show that MarkupLM significantly outperforms several SOTA baselines in these tasks. \"[MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Understanding](https://arxiv.org/abs/2110.08518) ```ACL 2022```\"\n\n**\\*\\*\\*\\*\\* ```September, 2021```: [TrOCR](https://github.com/microsoft/unilm/tree/master/trocr) release \\*\\*\\*\\*\\***\n\n- [x] [**TrOCR**](https://github.com/microsoft/unilm/tree/master/trocr) (September 22, 2021): Transformer-based OCR with pre-trained models, which leverages the Transformer architecture for both image understanding and bpe-level text generation. The TrOCR model is simple but effective (convolution free), and can be pre-trained with large-scale synthetic data and fine-tuned with human-labeled datasets. \"[TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://arxiv.org/abs/2109.10282) ```AAAI 2023```\"\n\n**\\*\\*\\*\\*\\* ```August, 2021```: [LayoutReader](https://github.com/microsoft/unilm/tree/master/layoutreader) release \\*\\*\\*\\*\\***\n\n- [x] [**LayoutReader**](https://github.com/microsoft/unilm/tree/master/layoutreader) (August 26, 2021): pre-training of text and layout for reading order detection. The pre-trained LayoutReader significantly improves both open-source and commercial OCR engines in ordering text lines. Meanwhile, we also created a reading order benchmark dataset [ReadingBank](https://github.com/doc-analysis/ReadingBank) to further empower the research in this area. \"[LayoutReader: Pre-training of Text and Layout for Reading Order Detection](https://arxiv.org/abs/2108.11591) ```EMNLP 2021```\"\n\n**\\*\\*\\*\\*\\* ```August, 2021```: [DeltaLM](https://github.com/microsoft/unilm/tree/master/deltalm) release \\*\\*\\*\\*\\***\n\n- [x] [**DeltaLM**](https://github.com/microsoft/unilm/tree/master/deltalm) (August, 2021): encoder-decoder pre-training for language generation and translation. DeltaLM **ranks first** on the [WMT21 multilingual translation task](http://www.statmt.org/wmt21/large-scale-multilingual-translation-task.html). The task requires a model to translate between 102 languages. \"[DeltaLM: Encoder-Decoder Pre-training for Language Generation and Translation by Augmenting Pretrained Multilingual Encoders.](https://arxiv.org/abs/2106.13736)\"\n\n**\\*\\*\\*\\*\\* ```July, 2021```: [BEiT](https://github.com/microsoft/unilm/tree/master/beit) release \\*\\*\\*\\*\\***\n\n- [x] [**BEiT**](https://github.com/microsoft/unilm/tree/master/beit) (June 15, 2021): BERT Pre-Training of Image Transformers. BEiT-large achieves **[state-of-the-art results on ADE20K](https://paperswithcode.com/sota/semantic-segmentation-on-ade20k) (a big jump to 57.0 mIoU) for semantic segmentation**. BEiT-large achieves **state-of-the-art ImageNet top-1 accuracy (88.6%) under the setting without extra data other than ImageNet-22k**. \"[BEiT: BERT Pre-Training of Image Transformers](https://arxiv.org/abs/2106.08254)\"\n\n\n\n**\\*\\*\\*\\*\\* ```June, 2021```: [LayoutXLM](https://github.com/microsoft/unilm/tree/master/layoutxlm) | [AdaLM](https://github.com/microsoft/unilm/tree/master/adalm) | [MiniLMv2](https://github.com/microsoft/unilm/tree/master/minilm) release \\*\\*\\*\\*\\***\n\n- [x] [**LayoutXLM**](https://github.com/microsoft/unilm/tree/master/layoutxlm) (April 17, 2021): multimodal pre-training for multilingual visually-rich document understanding. The pre-trained LayoutXLM model has significantly outperformed the existing SOTA cross-lingual pre-trained models on the FUNSD and multilingual [XFUND](https://github.com/doc-analysis/XFUND) dataset including 7 languages (Chinese, Japanese, Spanish, French, Italian, German, Portuguese). \"[LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding](https://arxiv.org/abs/2104.08836)\"\n- [x] [**AdaLM**](https://github.com/microsoft/unilm/tree/master/adalm) (June 2021): a simple yet effective approach for domain adaptation of pre-trained models. Biomedical specific pre-trained models are released. \"[Adapt-and-Distill: Developing Small, Fast and Effective Pretrained Language Models for Domains](#) ```ACL 2021```\"\n- [x] [**MiniLMv2**](https://github.com/microsoft/unilm/tree/master/minilm) (December, 2020): a simple yet effective task-agnostic knowledge distillation method, namely multi-head self-attention relation distillation, for compressing large pre-trained Transformers into small and fast pre-trained models. MiniLMv2 significantly outperforms MiniLMv1. Both English and multilingual MiniLM models are released. \"[MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers](https://arxiv.org/abs/2012.15828) ```ACL 2021```\"\n\n**\\*\\*\\*\\*\\* ```May, 2021```: [LayoutLMv2](https://github.com/microsoft/unilm/tree/master/layoutlmv2) | [LayoutXLM](https://github.com/microsoft/unilm/tree/master/layoutxlm) release \\*\\*\\*\\*\\***\n\n- [x] [**LayoutLM 2.0**](https://github.com/microsoft/unilm/tree/master/layoutlmv2) (December 29, 2020): multimodal pre-training for visually-rich document understanding by leveraging text, layout and image information in a single framework. It is coming with new SOTA on a wide range of document understanding tasks, including FUNSD (0.7895 -> 0.8420), CORD (0.9493 -> 0.9601), SROIE (0.9524 -> 0.9781), Kleister-NDA (0.834 -> 0.852), RVL-CDIP (0.9443 -> 0.9564), and DocVQA (0.7295 -> 0.8672). \"[LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding](https://arxiv.org/abs/2012.14740) ```ACL 2021```\"\n\n**\\*\\*\\*\\*\\* ```February, 2020```: [UniLM v2](https://github.com/microsoft/unilm/tree/master/unilm) | [MiniLM v1](https://github.com/microsoft/unilm/tree/master/minilm) | [LayoutLM v1](https://github.com/microsoft/unilm/tree/master/layoutlm) | [s2s-ft v1](https://github.com/microsoft/unilm/tree/master/s2s-ft) release \\*\\*\\*\\*\\***\n\n- [x] [**LayoutLM 1.0**](https://github.com/microsoft/unilm/tree/master/layoutlm) (February 18, 2020): pre-trained models for document (image) understanding (e.g. receipts, forms, etc.) . It achieves new SOTA results in several downstream tasks, including form understanding (the FUNSD dataset from 70.72 to 79.27), receipt understanding (the [ICDAR 2019 SROIE leaderboard](https://rrc.cvc.uab.es/?ch=13&com=evaluation&task=3) from 94.02 to 95.24) and document image classification (the RVL-CDIP dataset from 93.07 to 94.42). \"[LayoutLM: Pre-training of Text and Layout for Document Image Understanding](https://arxiv.org/abs/1912.13318) ```KDD 2020```\"\n- [x] [**s2s-ft 1.0**](https://github.com/microsoft/unilm/tree/master/s2s-ft) (February 26, 2020): A PyTorch package used to fine-tune pre-trained Transformers for sequence-to-sequence language generation. \"[s2s-ft: Fine-Tuning Pre-Trained Transformers for Sequence-to-Sequence Learning](#)\"\n- [x] [**MiniLM 1.0**](https://github.com/microsoft/unilm/tree/master/minilm) (February 26, 2020): deep self-attention distillation is all you need (for task-agnostic knowledge distillation of pre-trained Transformers). MiniLM (12-layer, 384-hidden) achieves 2.7x speedup and comparable results over BERT-base (12-layer, 768-hidden) on NLU tasks as well as strong results on NLG tasks. The even smaller MiniLM (6-layer, 384-hidden) obtains 5.3x speedup and produces very competitive results. \"[MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers](https://arxiv.org/abs/2002.10957) ```NeurIPS 2020```\"\n- [x] [**UniLM 2.0**](https://github.com/microsoft/unilm/tree/master/unilm) (February 28, 2020): **unified pre-training** of bi-directional LM (via autoencoding) and sequence-to-sequence LM (via partially autoregressive) w/ **Pseudo-Masked Language Model** for language understanding and generation. UniLM v2 achieves new SOTA in a wide range of natural language understanding and generation tasks. \"[UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training](https://arxiv.org/abs/2002.12804) ```ICML 2020```\"\n\n\n\n**\\*\\*\\*\\*\\* October 1st, 2019: UniLM v1 release \\*\\*\\*\\*\\***\n\n- [x] [**UniLM v1**](https://github.com/microsoft/unilm/tree/master/unilm-v1) (September 30, 2019): the code and pre-trained models for the ```NeurIPS 2019``` paper entitled \"[Unified Language Model Pre-training for Natural Language Understanding and Generation](https://arxiv.org/abs/1905.03197)\". UniLM (v1) achieves the **new SOTA results** in **NLG** (especially **sequence-to-sequence generation**) tasks, including abstractive summarization (the Gigaword and CNN/DM datasets), question generation (the SQuAD QG dataset), etc. \n\n-->\n\n## License\nThis project is licensed under the license found in the LICENSE file in the root directory of this source tree.\nPortions of the source code are based on the [transformers](https://github.com/huggingface/transformers) project.\n\n[Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct)\n\n### Contact Information\n\nFor help or issues using the pre-trained models, please submit a GitHub issue.\n\nFor other communications, please contact [Furu Wei](https://thegenerality.com) (`fuwei@microsoft.com`).\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 631258918,
    "name": "pandas-ai",
    "full_name": "sinaptik-ai/pandas-ai",
    "description": "Chat with your database or your datalake (SQL, CSV, parquet). PandasAI makes data analysis conversational using LLMs and RAG.",
    "html_url": "https://github.com/sinaptik-ai/pandas-ai",
    "clone_url": "https://github.com/sinaptik-ai/pandas-ai.git",
    "owner_login": "sinaptik-ai",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/154438448?v=4",
    "stargazers_count": 21263,
    "watchers_count": 21263,
    "forks_count": 2068,
    "open_issues_count": 15,
    "size": 57441,
    "language": "Python",
    "languages": {
      "Python": 684080,
      "Makefile": 3843,
      "Dockerfile": 299
    },
    "topics": [
      "ai",
      "csv",
      "data",
      "data-analysis",
      "data-science",
      "data-visualization",
      "database",
      "datalake",
      "gpt-4",
      "llm",
      "pandas",
      "sql",
      "text-to-sql"
    ],
    "license_name": "Other",
    "created_at": "2023-04-22T12:58:01+00:00",
    "updated_at": "2025-08-05T23:41:04+00:00",
    "pushed_at": "2025-07-31T15:22:00+00:00",
    "contributors_count": 100,
    "readme_length": 5892,
    "readme_content": "# ![PandasAI](assets/logo.png)\n\n[![Release](https://img.shields.io/pypi/v/pandasai?label=Release&style=flat-square)](https://pypi.org/project/pandasai/)\n[![CI](https://github.com/sinaptik-ai/pandas-ai/actions/workflows/ci-core.yml/badge.svg)](https://github.com/sinaptik-ai/pandas-ai/actions/workflows/ci-core.yml/badge.svg)\n[![CD](https://github.com/sinaptik-ai/pandas-ai/actions/workflows/cd.yml/badge.svg)](https://github.com/sinaptik-ai/pandas-ai/actions/workflows/cd.yml/badge.svg)\n[![Coverage](https://codecov.io/gh/sinaptik-ai/pandas-ai/branch/main/graph/badge.svg)](https://codecov.io/gh/sinaptik-ai/pandas-ai)\n[![Discord](https://dcbadge.vercel.app/api/server/kF7FqH2FwS?style=flat&compact=true)](https://discord.gg/KYKj9F2FRH)\n[![Downloads](https://static.pepy.tech/badge/pandasai)](https://pepy.tech/project/pandasai) [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1ZnO-njhL7TBOYPZaqvMvGtsjckZKrv2E?usp=sharing)\n\nPandasAI is a Python platform that makes it easy to ask questions to your data in natural language. It helps non-technical users to interact with their data in a more natural way, and it helps technical users to save time, and effort when working with data.\n\n# \ud83d\udd27 Getting started\n\nYou can find the full documentation for PandasAI [here](https://pandas-ai.readthedocs.io/en/latest/).\n\nYou can either decide to use PandasAI in your Jupyter notebooks, Streamlit apps, or use the client and server architecture from the repo.\n\n## \ud83d\udcda Using the library\n\n### Python Requirements\n\nPython version `3.8+ <3.12`\n\n### \ud83d\udce6 Installation\n\nYou can install the PandasAI library using pip or poetry.\n\nWith pip:\n\n```bash\npip install \"pandasai>=3.0.0b2\"\n```\n\nWith poetry:\n\n```bash\npoetry add \"pandasai>=3.0.0b2\"\n```\n\n### \ud83d\udcbb Usage\n\n#### Ask questions\n\n```python\nimport pandasai as pai\nfrom pandasai_openai.openai import OpenAI\n\nllm = OpenAI(\"OPEN_AI_API_KEY\")\n\npai.config.set({\n    \"llm\": llm\n})\n\n# Sample DataFrame\ndf = pai.DataFrame({\n    \"country\": [\"United States\", \"United Kingdom\", \"France\", \"Germany\", \"Italy\", \"Spain\", \"Canada\", \"Australia\", \"Japan\", \"China\"],\n    \"revenue\": [5000, 3200, 2900, 4100, 2300, 2100, 2500, 2600, 4500, 7000]\n})\n\ndf.chat('Which are the top 5 countries by sales?')\n```\n\n```\nChina, United States, Japan, Germany, Australia\n```\n\n---\n\nOr you can ask more complex questions:\n\n```python\ndf.chat(\n    \"What is the total sales for the top 3 countries by sales?\"\n)\n```\n\n```\nThe total sales for the top 3 countries by sales is 16500.\n```\n\n#### Visualize charts\n\nYou can also ask PandasAI to generate charts for you:\n\n```python\ndf.chat(\n    \"Plot the histogram of countries showing for each one the gd. Use different colors for each bar\",\n)\n```\n\n![Chart](assets/histogram-chart.png?raw=true)\n\n#### Multiple DataFrames\n\nYou can also pass in multiple dataframes to PandasAI and ask questions relating them.\n\n```python\nimport pandasai as pai\nfrom pandasai_openai.openai import OpenAI\n\nemployees_data = {\n    'EmployeeID': [1, 2, 3, 4, 5],\n    'Name': ['John', 'Emma', 'Liam', 'Olivia', 'William'],\n    'Department': ['HR', 'Sales', 'IT', 'Marketing', 'Finance']\n}\n\nsalaries_data = {\n    'EmployeeID': [1, 2, 3, 4, 5],\n    'Salary': [5000, 6000, 4500, 7000, 5500]\n}\n\nllm = OpenAI(\"OPEN_AI_API_KEY\")\n\npai.config.set({\n    \"llm\": llm\n})\n\nemployees_df = pai.DataFrame(employees_data)\nsalaries_df = pai.DataFrame(salaries_data)\n\n\npai.chat(\"Who gets paid the most?\", employees_df, salaries_df)\n```\n\n```\nOlivia gets paid the most.\n```\n\n#### Docker Sandbox\n\nYou can run PandasAI in a Docker sandbox, providing a secure, isolated environment to execute code safely and mitigate the risk of malicious attacks.\n\n##### Python Requirements\n\n```bash\npip install \"pandasai-docker\"\n```\n\n##### Usage\n\n```python\nimport pandasai as pai\nfrom pandasai_docker import DockerSandbox\nfrom pandasai_openai.openai import OpenAI\n\n# Initialize the sandbox\nsandbox = DockerSandbox()\nsandbox.start()\n\nemployees_data = {\n    'EmployeeID': [1, 2, 3, 4, 5],\n    'Name': ['John', 'Emma', 'Liam', 'Olivia', 'William'],\n    'Department': ['HR', 'Sales', 'IT', 'Marketing', 'Finance']\n}\n\nsalaries_data = {\n    'EmployeeID': [1, 2, 3, 4, 5],\n    'Salary': [5000, 6000, 4500, 7000, 5500]\n}\n\nllm = OpenAI(\"OPEN_AI_API_KEY\")\n\npai.config.set({\n    \"llm\": llm\n})\n\nemployees_df = pai.DataFrame(employees_data)\nsalaries_df = pai.DataFrame(salaries_data)\n\npai.chat(\"Who gets paid the most?\", employees_df, salaries_df, sandbox=sandbox)\n\n# Don't forget to stop the sandbox when done\nsandbox.stop()\n```\n\n```\nOlivia gets paid the most.\n```\n\nYou can find more examples in the [examples](examples) directory.\n\n## \ud83d\udcdc License\n\nPandasAI is available under the MIT expat license, except for the `pandasai/ee` directory of this repository, which has its [license here](https://github.com/sinaptik-ai/pandas-ai/blob/main/ee/LICENSE).\n\nIf you are interested in managed PandasAI Cloud or self-hosted Enterprise Offering, [contact us](https://getpanda.ai/pricing).\n\n## Resources\n\n> **Beta Notice**  \n> Release v3 is currently in beta. The following documentation and examples reflect the features and functionality in progress and may change before the final release.\n\n- [Docs](https://pandas-ai.readthedocs.io/en/latest/) for comprehensive documentation\n- [Examples](examples) for example notebooks\n- [Discord](https://discord.gg/KYKj9F2FRH) for discussion with the community and PandasAI team\n\n## \ud83e\udd1d Contributing\n\nContributions are welcome! Please check the outstanding issues and feel free to open a pull request.\nFor more information, please check out the [contributing guidelines](CONTRIBUTING.md).\n\n### Thank you!\n\n[![Contributors](https://contrib.rocks/image?repo=sinaptik-ai/pandas-ai)](https://github.com/sinaptik-ai/pandas-ai/graphs/contributors)\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 145553672,
    "name": "funNLP",
    "full_name": "fighting41love/funNLP",
    "description": "\u4e2d\u82f1\u6587\u654f\u611f\u8bcd\u3001\u8bed\u8a00\u68c0\u6d4b\u3001\u4e2d\u5916\u624b\u673a/\u7535\u8bdd\u5f52\u5c5e\u5730/\u8fd0\u8425\u5546\u67e5\u8be2\u3001\u540d\u5b57\u63a8\u65ad\u6027\u522b\u3001\u624b\u673a\u53f7\u62bd\u53d6\u3001\u8eab\u4efd\u8bc1\u62bd\u53d6\u3001\u90ae\u7bb1\u62bd\u53d6\u3001\u4e2d\u65e5\u6587\u4eba\u540d\u5e93\u3001\u4e2d\u6587\u7f29\u5199\u5e93\u3001\u62c6\u5b57\u8bcd\u5178\u3001\u8bcd\u6c47\u60c5\u611f\u503c\u3001\u505c\u7528\u8bcd\u3001\u53cd\u52a8\u8bcd\u8868\u3001\u66b4\u6050\u8bcd\u8868\u3001\u7e41\u7b80\u4f53\u8f6c\u6362\u3001\u82f1\u6587\u6a21\u62df\u4e2d\u6587\u53d1\u97f3\u3001\u6c6a\u5cf0\u6b4c\u8bcd\u751f\u6210\u5668\u3001\u804c\u4e1a\u540d\u79f0\u8bcd\u5e93\u3001\u540c\u4e49\u8bcd\u5e93\u3001\u53cd\u4e49\u8bcd\u5e93\u3001\u5426\u5b9a\u8bcd\u5e93\u3001\u6c7d\u8f66\u54c1\u724c\u8bcd\u5e93\u3001\u6c7d\u8f66\u96f6\u4ef6\u8bcd\u5e93\u3001\u8fde\u7eed\u82f1\u6587\u5207\u5272\u3001\u5404\u79cd\u4e2d\u6587\u8bcd\u5411\u91cf\u3001\u516c\u53f8\u540d\u5b57\u5927\u5168\u3001\u53e4\u8bd7\u8bcd\u5e93\u3001IT\u8bcd\u5e93\u3001\u8d22\u7ecf\u8bcd\u5e93\u3001\u6210\u8bed\u8bcd\u5e93\u3001\u5730\u540d\u8bcd\u5e93\u3001\u5386\u53f2\u540d\u4eba\u8bcd\u5e93\u3001\u8bd7\u8bcd\u8bcd\u5e93\u3001\u533b\u5b66\u8bcd\u5e93\u3001\u996e\u98df\u8bcd\u5e93\u3001\u6cd5\u5f8b\u8bcd\u5e93\u3001\u6c7d\u8f66\u8bcd\u5e93\u3001\u52a8\u7269\u8bcd\u5e93\u3001\u4e2d\u6587\u804a\u5929\u8bed\u6599\u3001\u4e2d\u6587\u8c23\u8a00\u6570\u636e\u3001\u767e\u5ea6\u4e2d\u6587\u95ee\u7b54\u6570\u636e\u96c6\u3001\u53e5\u5b50\u76f8\u4f3c\u5ea6\u5339\u914d\u7b97\u6cd5\u96c6\u5408\u3001bert\u8d44\u6e90\u3001\u6587\u672c\u751f\u6210&\u6458\u8981\u76f8\u5173\u5de5\u5177\u3001cocoNLP\u4fe1\u606f\u62bd\u53d6\u5de5\u5177\u3001\u56fd\u5185\u7535\u8bdd\u53f7\u7801\u6b63\u5219\u5339\u914d\u3001\u6e05\u534e\u5927\u5b66XLORE:\u4e2d\u82f1\u6587\u8de8\u8bed\u8a00\u767e\u79d1\u77e5\u8bc6\u56fe\u8c31\u3001\u6e05\u534e\u5927\u5b66\u4eba\u5de5\u667a\u80fd\u6280\u672f\u7cfb\u5217\u62a5\u544a\u3001\u81ea\u7136\u8bed\u8a00\u751f\u6210\u3001NLU\u592a\u96be\u4e86\u7cfb\u5217\u3001\u81ea\u52a8\u5bf9\u8054\u6570\u636e\u53ca\u673a\u5668\u4eba\u3001\u7528\u6237\u540d\u9ed1\u540d\u5355\u5217\u8868\u3001\u7f6a\u540d\u6cd5\u52a1\u540d\u8bcd\u53ca\u5206\u7c7b\u6a21\u578b\u3001\u5fae\u4fe1\u516c\u4f17\u53f7\u8bed\u6599\u3001cs224n\u6df1\u5ea6\u5b66\u4e60\u81ea\u7136\u8bed\u8a00\u5904\u7406\u8bfe\u7a0b\u3001\u4e2d\u6587\u624b\u5199\u6c49\u5b57\u8bc6\u522b\u3001\u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406 \u8bed\u6599/\u6570\u636e\u96c6\u3001\u53d8\u91cf\u547d\u540d\u795e\u5668\u3001\u5206\u8bcd\u8bed\u6599\u5e93+\u4ee3\u7801\u3001\u4efb\u52a1\u578b\u5bf9\u8bdd\u82f1\u6587\u6570\u636e\u96c6\u3001ASR \u8bed\u97f3\u6570\u636e\u96c6 + \u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4e2d\u6587\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u3001\u7b11\u58f0\u68c0\u6d4b\u5668\u3001Microsoft\u591a\u8bed\u8a00\u6570\u5b57/\u5355\u4f4d/\u5982\u65e5\u671f\u65f6\u95f4\u8bc6\u522b\u5305\u3001\u4e2d\u534e\u65b0\u534e\u5b57\u5178\u6570\u636e\u5e93\u53caapi(\u5305\u62ec\u5e38\u7528\u6b47\u540e\u8bed\u3001\u6210\u8bed\u3001\u8bcd\u8bed\u548c\u6c49\u5b57)\u3001\u6587\u6863\u56fe\u8c31\u81ea\u52a8\u751f\u6210\u3001SpaCy \u4e2d\u6587\u6a21\u578b\u3001Common Voice\u8bed\u97f3\u8bc6\u522b\u6570\u636e\u96c6\u65b0\u7248\u3001\u795e\u7ecf\u7f51\u7edc\u5173\u7cfb\u62bd\u53d6\u3001\u57fa\u4e8ebert\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3001\u5173\u952e\u8bcd(Keyphrase)\u62bd\u53d6\u5305pke\u3001\u57fa\u4e8e\u533b\u7597\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\u7684\u95ee\u7b54\u7cfb\u7edf\u3001\u57fa\u4e8e\u4f9d\u5b58\u53e5\u6cd5\u4e0e\u8bed\u4e49\u89d2\u8272\u6807\u6ce8\u7684\u4e8b\u4ef6\u4e09\u5143\u7ec4\u62bd\u53d6\u3001\u4f9d\u5b58\u53e5\u6cd5\u5206\u67904\u4e07\u53e5\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u3001cnocr\uff1a\u7528\u6765\u505a\u4e2d\u6587OCR\u7684Python3\u5305\u3001\u4e2d\u6587\u4eba\u7269\u5173\u7cfb\u77e5\u8bc6\u56fe\u8c31\u9879\u76ee\u3001\u4e2d\u6587nlp\u7ade\u8d5b\u9879\u76ee\u53ca\u4ee3\u7801\u6c47\u603b\u3001\u4e2d\u6587\u5b57\u7b26\u6570\u636e\u3001speech-aligner: \u4ece\u201c\u4eba\u58f0\u8bed\u97f3\u201d\u53ca\u5176\u201c\u8bed\u8a00\u6587\u672c\u201d\u4ea7\u751f\u97f3\u7d20\u7ea7\u522b\u65f6\u95f4\u5bf9\u9f50\u6807\u6ce8\u7684\u5de5\u5177\u3001AmpliGraph: \u77e5\u8bc6\u56fe\u8c31\u8868\u793a\u5b66\u4e60(Python)\u5e93\uff1a\u77e5\u8bc6\u56fe\u8c31\u6982\u5ff5\u94fe\u63a5\u9884\u6d4b\u3001Scattertext \u6587\u672c\u53ef\u89c6\u5316(python)\u3001\u8bed\u8a00/\u77e5\u8bc6\u8868\u793a\u5de5\u5177\uff1aBERT & ERNIE\u3001\u4e2d\u6587\u5bf9\u6bd4\u82f1\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406NLP\u7684\u533a\u522b\u7efc\u8ff0\u3001Synonyms\u4e2d\u6587\u8fd1\u4e49\u8bcd\u5de5\u5177\u5305\u3001HarvestText\u9886\u57df\u81ea\u9002\u5e94\u6587\u672c\u6316\u6398\u5de5\u5177\uff08\u65b0\u8bcd\u53d1\u73b0-\u60c5\u611f\u5206\u6790-\u5b9e\u4f53\u94fe\u63a5\u7b49\uff09\u3001word2word\uff1a(Python)\u65b9\u4fbf\u6613\u7528\u7684\u591a\u8bed\u8a00\u8bcd-\u8bcd\u5bf9\u96c6\uff1a62\u79cd\u8bed\u8a00/3,564\u4e2a\u591a\u8bed\u8a00\u5bf9\u3001\u8bed\u97f3\u8bc6\u522b\u8bed\u6599\u751f\u6210\u5de5\u5177\uff1a\u4ece\u5177\u6709\u97f3\u9891/\u5b57\u5e55\u7684\u5728\u7ebf\u89c6\u9891\u521b\u5efa\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b(ASR)\u8bed\u6599\u5e93\u3001\u6784\u5efa\u533b\u7597\u5b9e\u4f53\u8bc6\u522b\u7684\u6a21\u578b\uff08\u5305\u542b\u8bcd\u5178\u548c\u8bed\u6599\u6807\u6ce8\uff09\u3001\u5355\u6587\u6863\u975e\u76d1\u7763\u7684\u5173\u952e\u8bcd\u62bd\u53d6\u3001Kashgari\u4e2d\u4f7f\u7528gpt-2\u8bed\u8a00\u6a21\u578b\u3001\u5f00\u6e90\u7684\u91d1\u878d\u6295\u8d44\u6570\u636e\u63d0\u53d6\u5de5\u5177\u3001\u6587\u672c\u81ea\u52a8\u6458\u8981\u5e93TextTeaser: \u4ec5\u652f\u6301\u82f1\u6587\u3001\u4eba\u6c11\u65e5\u62a5\u8bed\u6599\u5904\u7406\u5de5\u5177\u96c6\u3001\u4e00\u4e9b\u5173\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u57fa\u672c\u6a21\u578b\u3001\u57fa\u4e8e14W\u6b4c\u66f2\u77e5\u8bc6\u5e93\u7684\u95ee\u7b54\u5c1d\u8bd5--\u529f\u80fd\u5305\u62ec\u6b4c\u8bcd\u63a5\u9f99and\u5df2\u77e5\u6b4c\u8bcd\u627e\u6b4c\u66f2\u4ee5\u53ca\u6b4c\u66f2\u6b4c\u624b\u6b4c\u8bcd\u4e09\u89d2\u5173\u7cfb\u7684\u95ee\u7b54\u3001\u57fa\u4e8eSiamese bilstm\u6a21\u578b\u7684\u76f8\u4f3c\u53e5\u5b50\u5224\u5b9a\u6a21\u578b\u5e76\u63d0\u4f9b\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u6d4b\u8bd5\u6570\u636e\u96c6\u3001\u7528Transformer\u7f16\u89e3\u7801\u6a21\u578b\u5b9e\u73b0\u7684\u6839\u636eHacker News\u6587\u7ae0\u6807\u9898\u81ea\u52a8\u751f\u6210\u8bc4\u8bba\u3001\u7528BERT\u8fdb\u884c\u5e8f\u5217\u6807\u8bb0\u548c\u6587\u672c\u5206\u7c7b\u7684\u6a21\u677f\u4ee3\u7801\u3001LitBank\uff1aNLP\u6570\u636e\u96c6\u2014\u2014\u652f\u6301\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u8ba1\u7b97\u4eba\u6587\u5b66\u79d1\u4efb\u52a1\u7684100\u90e8\u5e26\u6807\u8bb0\u82f1\u6587\u5c0f\u8bf4\u8bed\u6599\u3001\u767e\u5ea6\u5f00\u6e90\u7684\u57fa\u51c6\u4fe1\u606f\u62bd\u53d6\u7cfb\u7edf\u3001\u865a\u5047\u65b0\u95fb\u6570\u636e\u96c6\u3001Facebook: LAMA\u8bed\u8a00\u6a21\u578b\u5206\u6790\uff0c\u63d0\u4f9bTransformer-XL/BERT/ELMo/GPT\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u7edf\u4e00\u8bbf\u95ee\u63a5\u53e3\u3001CommonsenseQA\uff1a\u9762\u5411\u5e38\u8bc6\u7684\u82f1\u6587QA\u6311\u6218\u3001\u4e2d\u6587\u77e5\u8bc6\u56fe\u8c31\u8d44\u6599\u3001\u6570\u636e\u53ca\u5de5\u5177\u3001\u5404\u5927\u516c\u53f8\u5185\u90e8\u91cc\u5927\u725b\u5206\u4eab\u7684\u6280\u672f\u6587\u6863 PDF \u6216\u8005 PPT\u3001\u81ea\u7136\u8bed\u8a00\u751f\u6210SQL\u8bed\u53e5\uff08\u82f1\u6587\uff09\u3001\u4e2d\u6587NLP\u6570\u636e\u589e\u5f3a\uff08EDA\uff09\u5de5\u5177\u3001\u82f1\u6587NLP\u6570\u636e\u589e\u5f3a\u5de5\u5177 \u3001\u57fa\u4e8e\u533b\u836f\u77e5\u8bc6\u56fe\u8c31\u7684\u667a\u80fd\u95ee\u7b54\u7cfb\u7edf\u3001\u4eac\u4e1c\u5546\u54c1\u77e5\u8bc6\u56fe\u8c31\u3001\u57fa\u4e8emongodb\u5b58\u50a8\u7684\u519b\u4e8b\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u9879\u76ee\u3001\u57fa\u4e8e\u8fdc\u76d1\u7763\u7684\u4e2d\u6587\u5173\u7cfb\u62bd\u53d6\u3001\u8bed\u97f3\u60c5\u611f\u5206\u6790\u3001\u4e2d\u6587ULMFiT-\u60c5\u611f\u5206\u6790-\u6587\u672c\u5206\u7c7b-\u8bed\u6599\u53ca\u6a21\u578b\u3001\u4e00\u4e2a\u62cd\u7167\u505a\u9898\u7a0b\u5e8f\u3001\u4e16\u754c\u5404\u56fd\u5927\u89c4\u6a21\u4eba\u540d\u5e93\u3001\u4e00\u4e2a\u5229\u7528\u6709\u8da3\u4e2d\u6587\u8bed\u6599\u5e93 qingyun \u8bad\u7ec3\u51fa\u6765\u7684\u4e2d\u6587\u804a\u5929\u673a\u5668\u4eba\u3001\u4e2d\u6587\u804a\u5929\u673a\u5668\u4ebaseqGAN\u3001\u7701\u5e02\u533a\u9547\u884c\u653f\u533a\u5212\u6570\u636e\u5e26\u62fc\u97f3\u6807\u6ce8\u3001\u6559\u80b2\u884c\u4e1a\u65b0\u95fb\u8bed\u6599\u5e93\u5305\u542b\u81ea\u52a8\u6587\u6458\u529f\u80fd\u3001\u5f00\u653e\u4e86\u5bf9\u8bdd\u673a\u5668\u4eba-\u77e5\u8bc6\u56fe\u8c31-\u8bed\u4e49\u7406\u89e3-\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177\u53ca\u6570\u636e\u3001\u4e2d\u6587\u77e5\u8bc6\u56fe\u8c31\uff1a\u57fa\u4e8e\u767e\u5ea6\u767e\u79d1\u4e2d\u6587\u9875\u9762-\u62bd\u53d6\u4e09\u5143\u7ec4\u4fe1\u606f-\u6784\u5efa\u4e2d\u6587\u77e5\u8bc6\u56fe\u8c31\u3001masr: \u4e2d\u6587\u8bed\u97f3\u8bc6\u522b-\u63d0\u4f9b\u9884\u8bad\u7ec3\u6a21\u578b-\u9ad8\u8bc6\u522b\u7387\u3001Python\u97f3\u9891\u6570\u636e\u589e\u5e7f\u5e93\u3001\u4e2d\u6587\u5168\u8bcd\u8986\u76d6BERT\u53ca\u4e24\u4efd\u9605\u8bfb\u7406\u89e3\u6570\u636e\u3001ConvLab\uff1a\u5f00\u6e90\u591a\u57df\u7aef\u5230\u7aef\u5bf9\u8bdd\u7cfb\u7edf\u5e73\u53f0\u3001\u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6570\u636e\u96c6\u3001\u57fa\u4e8e\u6700\u65b0\u7248\u672crasa\u642d\u5efa\u7684\u5bf9\u8bdd\u7cfb\u7edf\u3001\u57fa\u4e8eTensorFlow\u548cBERT\u7684\u7ba1\u9053\u5f0f\u5b9e\u4f53\u53ca\u5173\u7cfb\u62bd\u53d6\u3001\u4e00\u4e2a\u5c0f\u578b\u7684\u8bc1\u5238\u77e5\u8bc6\u56fe\u8c31/\u77e5\u8bc6\u5e93\u3001\u590d\u76d8\u6240\u6709NLP\u6bd4\u8d5b\u7684TOP\u65b9\u6848\u3001OpenCLaP\uff1a\u591a\u9886\u57df\u5f00\u6e90\u4e2d\u6587\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4ed3\u5e93\u3001UER\uff1a\u57fa\u4e8e\u4e0d\u540c\u8bed\u6599+\u7f16\u7801\u5668+\u76ee\u6807\u4efb\u52a1\u7684\u4e2d\u6587\u9884\u8bad\u7ec3\u6a21\u578b\u4ed3\u5e93\u3001\u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5411\u91cf\u5408\u96c6\u3001\u57fa\u4e8e\u91d1\u878d-\u53f8\u6cd5\u9886\u57df(\u517c\u6709\u95f2\u804a\u6027\u8d28)\u7684\u804a\u5929\u673a\u5668\u4eba\u3001g2pC\uff1a\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u6c49\u8bed\u8bfb\u97f3\u81ea\u52a8\u6807\u8bb0\u6a21\u5757\u3001Zincbase \u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u5de5\u5177\u5305\u3001\u8bd7\u6b4c\u8d28\u91cf\u8bc4\u4ef7/\u7ec6\u7c92\u5ea6\u60c5\u611f\u8bd7\u6b4c\u8bed\u6599\u5e93\u3001\u5feb\u901f\u8f6c\u5316\u300c\u4e2d\u6587\u6570\u5b57\u300d\u548c\u300c\u963f\u62c9\u4f2f\u6570\u5b57\u300d\u3001\u767e\u5ea6\u77e5\u9053\u95ee\u7b54\u8bed\u6599\u5e93\u3001\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u95ee\u7b54\u7cfb\u7edf\u3001jieba_fast \u52a0\u901f\u7248\u7684jieba\u3001\u6b63\u5219\u8868\u8fbe\u5f0f\u6559\u7a0b\u3001\u4e2d\u6587\u9605\u8bfb\u7406\u89e3\u6570\u636e\u96c6\u3001\u57fa\u4e8eBERT\u7b49\u6700\u65b0\u8bed\u8a00\u6a21\u578b\u7684\u62bd\u53d6\u5f0f\u6458\u8981\u63d0\u53d6\u3001Python\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u8fdb\u884c\u6587\u672c\u6458\u8981\u7684\u7efc\u5408\u6307\u5357\u3001\u77e5\u8bc6\u56fe\u8c31\u6df1\u5ea6\u5b66\u4e60\u76f8\u5173\u8d44\u6599\u6574\u7406\u3001\u7ef4\u57fa\u5927\u89c4\u6a21\u5e73\u884c\u6587\u672c\u8bed\u6599\u3001StanfordNLP 0.2.0\uff1a\u7eafPython\u7248\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5305\u3001NeuralNLP-NeuralClassifier\uff1a\u817e\u8baf\u5f00\u6e90\u6df1\u5ea6\u5b66\u4e60\u6587\u672c\u5206\u7c7b\u5de5\u5177\u3001\u7aef\u5230\u7aef\u7684\u5c01\u95ed\u57df\u5bf9\u8bdd\u7cfb\u7edf\u3001\u4e2d\u6587\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff1aNeuroNER vs. BertNER\u3001\u65b0\u95fb\u4e8b\u4ef6\u7ebf\u7d22\u62bd\u53d6\u30012019\u5e74\u767e\u5ea6\u7684\u4e09\u5143\u7ec4\u62bd\u53d6\u6bd4\u8d5b\uff1a\u201c\u79d1\u5b66\u7a7a\u95f4\u961f\u201d\u6e90\u7801\u3001\u57fa\u4e8e\u4f9d\u5b58\u53e5\u6cd5\u7684\u5f00\u653e\u57df\u6587\u672c\u77e5\u8bc6\u4e09\u5143\u7ec4\u62bd\u53d6\u548c\u77e5\u8bc6\u5e93\u6784\u5efa\u3001\u4e2d\u6587\u7684GPT2\u8bad\u7ec3\u4ee3\u7801\u3001ML-NLP - \u673a\u5668\u5b66\u4e60(Machine Learning)NLP\u9762\u8bd5\u4e2d\u5e38\u8003\u5230\u7684\u77e5\u8bc6\u70b9\u548c\u4ee3\u7801\u5b9e\u73b0\u3001nlp4han:\u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177\u96c6(\u65ad\u53e5/\u5206\u8bcd/\u8bcd\u6027\u6807\u6ce8/\u7ec4\u5757/\u53e5\u6cd5\u5206\u6790/\u8bed\u4e49\u5206\u6790/NER/N\u5143\u8bed\u6cd5/HMM/\u4ee3\u8bcd\u6d88\u89e3/\u60c5\u611f\u5206\u6790/\u62fc\u5199\u68c0\u67e5\u3001XLM\uff1aFacebook\u7684\u8de8\u8bed\u8a00\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u3001\u7528\u57fa\u4e8eBERT\u7684\u5fae\u8c03\u548c\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u6765\u8fdb\u884c\u77e5\u8bc6\u56fe\u8c31\u767e\u5ea6\u767e\u79d1\u4eba\u7269\u8bcd\u6761\u5c5e\u6027\u62bd\u53d6\u3001\u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406\u76f8\u5173\u7684\u5f00\u653e\u4efb\u52a1-\u6570\u636e\u96c6-\u5f53\u524d\u6700\u4f73\u7ed3\u679c\u3001CoupletAI - \u57fa\u4e8eCNN+Bi-LSTM+Attention \u7684\u81ea\u52a8\u5bf9\u5bf9\u8054\u7cfb\u7edf\u3001\u62bd\u8c61\u77e5\u8bc6\u56fe\u8c31\u3001MiningZhiDaoQACorpus - 580\u4e07\u767e\u5ea6\u77e5\u9053\u95ee\u7b54\u6570\u636e\u6316\u6398\u9879\u76ee\u3001brat rapid annotation tool: \u5e8f\u5217\u6807\u6ce8\u5de5\u5177\u3001\u5927\u89c4\u6a21\u4e2d\u6587\u77e5\u8bc6\u56fe\u8c31\u6570\u636e\uff1a1.4\u4ebf\u5b9e\u4f53\u3001\u6570\u636e\u589e\u5f3a\u5728\u673a\u5668\u7ffb\u8bd1\u53ca\u5176\u4ed6nlp\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u53ca\u6548\u679c\u3001allennlp\u9605\u8bfb\u7406\u89e3:\u652f\u6301\u591a\u79cd\u6570\u636e\u548c\u6a21\u578b\u3001PDF\u8868\u683c\u6570\u636e\u63d0\u53d6\u5de5\u5177 \u3001 Graphbrain\uff1aAI\u5f00\u6e90\u8f6f\u4ef6\u5e93\u548c\u79d1\u7814\u5de5\u5177\uff0c\u76ee\u7684\u662f\u4fc3\u8fdb\u81ea\u52a8\u610f\u4e49\u63d0\u53d6\u548c\u6587\u672c\u7406\u89e3\u4ee5\u53ca\u77e5\u8bc6\u7684\u63a2\u7d22\u548c\u63a8\u65ad\u3001\u7b80\u5386\u81ea\u52a8\u7b5b\u9009\u7cfb\u7edf\u3001\u57fa\u4e8e\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7684\u7b80\u5386\u81ea\u52a8\u6458\u8981\u3001\u4e2d\u6587\u8bed\u8a00\u7406\u89e3\u6d4b\u8bc4\u57fa\u51c6\uff0c\u5305\u62ec\u4ee3\u8868\u6027\u7684\u6570\u636e\u96c6&\u57fa\u51c6\u6a21\u578b&\u8bed\u6599\u5e93&\u6392\u884c\u699c\u3001\u6811\u6d1e OCR \u6587\u5b57\u8bc6\u522b \u3001\u4ece\u5305\u542b\u8868\u683c\u7684\u626b\u63cf\u56fe\u7247\u4e2d\u8bc6\u522b\u8868\u683c\u548c\u6587\u5b57\u3001\u8bed\u58f0\u8fc1\u79fb\u3001Python\u53e3\u8bed\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177\u96c6(\u82f1\u6587)\u3001 similarity\uff1a\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u5de5\u5177\u5305\uff0cjava\u7f16\u5199\u3001\u6d77\u91cf\u4e2d\u6587\u9884\u8bad\u7ec3ALBERT\u6a21\u578b \u3001Transformers 2.0 \u3001\u57fa\u4e8e\u5927\u89c4\u6a21\u97f3\u9891\u6570\u636e\u96c6Audioset\u7684\u97f3\u9891\u589e\u5f3a \u3001Poplar\uff1a\u7f51\u9875\u7248\u81ea\u7136\u8bed\u8a00\u6807\u6ce8\u5de5\u5177\u3001\u56fe\u7247\u6587\u5b57\u53bb\u9664\uff0c\u53ef\u7528\u4e8e\u6f2b\u753b\u7ffb\u8bd1 \u3001186\u79cd\u8bed\u8a00\u7684\u6570\u5b57\u53eb\u6cd5\u5e93\u3001Amazon\u53d1\u5e03\u57fa\u4e8e\u77e5\u8bc6\u7684\u4eba-\u4eba\u5f00\u653e\u9886\u57df\u5bf9\u8bdd\u6570\u636e\u96c6 \u3001\u4e2d\u6587\u6587\u672c\u7ea0\u9519\u6a21\u5757\u4ee3\u7801\u3001\u7e41\u7b80\u4f53\u8f6c\u6362 \u3001 Python\u5b9e\u73b0\u7684\u591a\u79cd\u6587\u672c\u53ef\u8bfb\u6027\u8bc4\u4ef7\u6307\u6807\u3001\u7c7b\u4f3c\u4e8e\u4eba\u540d/\u5730\u540d/\u7ec4\u7ec7\u673a\u6784\u540d\u7684\u547d\u540d\u4f53\u8bc6\u522b\u6570\u636e\u96c6 \u3001\u4e1c\u5357\u5927\u5b66\u300a\u77e5\u8bc6\u56fe\u8c31\u300b\u7814\u7a76\u751f\u8bfe\u7a0b(\u8d44\u6599)\u3001. \u82f1\u6587\u62fc\u5199\u68c0\u67e5\u5e93 \u3001 wwsearch\u662f\u4f01\u4e1a\u5fae\u4fe1\u540e\u53f0\u81ea\u7814\u7684\u5168\u6587\u68c0\u7d22\u5f15\u64ce\u3001CHAMELEON\uff1a\u6df1\u5ea6\u5b66\u4e60\u65b0\u95fb\u63a8\u8350\u7cfb\u7edf\u5143\u67b6\u6784 \u3001 8\u7bc7\u8bba\u6587\u68b3\u7406BERT\u76f8\u5173\u6a21\u578b\u8fdb\u5c55\u4e0e\u53cd\u601d\u3001DocSearch\uff1a\u514d\u8d39\u6587\u6863\u641c\u7d22\u5f15\u64ce\u3001 LIDA\uff1a\u8f7b\u91cf\u4ea4\u4e92\u5f0f\u5bf9\u8bdd\u6807\u6ce8\u5de5\u5177 \u3001aili - the fastest in-memory index in the East \u4e1c\u534a\u7403\u6700\u5feb\u5e76\u53d1\u7d22\u5f15 \u3001\u77e5\u8bc6\u56fe\u8c31\u8f66\u97f3\u5de5\u4f5c\u9879\u76ee\u3001\u81ea\u7136\u8bed\u8a00\u751f\u6210\u8d44\u6e90\u5927\u5168 \u3001\u4e2d\u65e5\u97e9\u5206\u8bcd\u5e93mecab\u7684Python\u63a5\u53e3\u5e93\u3001\u4e2d\u6587\u6587\u672c\u6458\u8981/\u5173\u952e\u8bcd\u63d0\u53d6\u3001\u6c49\u5b57\u5b57\u7b26\u7279\u5f81\u63d0\u53d6\u5668 (featurizer)\uff0c\u63d0\u53d6\u6c49\u5b57\u7684\u7279\u5f81\uff08\u53d1\u97f3\u7279\u5f81\u3001\u5b57\u5f62\u7279\u5f81\uff09\u7528\u505a\u6df1\u5ea6\u5b66\u4e60\u7684\u7279\u5f81\u3001\u4e2d\u6587\u751f\u6210\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bc4 \u3001\u4e2d\u6587\u7f29\u5199\u6570\u636e\u96c6\u3001\u4e2d\u6587\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bc4 - \u4ee3\u8868\u6027\u7684\u6570\u636e\u96c6-\u57fa\u51c6(\u9884\u8bad\u7ec3)\u6a21\u578b-\u8bed\u6599\u5e93-baseline-\u5de5\u5177\u5305-\u6392\u884c\u699c\u3001PySS3\uff1a\u9762\u5411\u53ef\u89e3\u91caAI\u7684SS3\u6587\u672c\u5206\u7c7b\u5668\u673a\u5668\u53ef\u89c6\u5316\u5de5\u5177 \u3001\u4e2d\u6587NLP\u6570\u636e\u96c6\u5217\u8868\u3001COPE - \u683c\u5f8b\u8bd7\u7f16\u8f91\u7a0b\u5e8f\u3001doccano\uff1a\u57fa\u4e8e\u7f51\u9875\u7684\u5f00\u6e90\u534f\u540c\u591a\u8bed\u8a00\u6587\u672c\u6807\u6ce8\u5de5\u5177 \u3001PreNLP\uff1a\u81ea\u7136\u8bed\u8a00\u9884\u5904\u7406\u5e93\u3001\u7b80\u5355\u7684\u7b80\u5386\u89e3\u6790\u5668\uff0c\u7528\u6765\u4ece\u7b80\u5386\u4e2d\u63d0\u53d6\u5173\u952e\u4fe1\u606f\u3001\u7528\u4e8e\u4e2d\u6587\u95f2\u804a\u7684GPT2\u6a21\u578b\uff1aGPT2-chitchat\u3001\u57fa\u4e8e\u68c0\u7d22\u804a\u5929\u673a\u5668\u4eba\u591a\u8f6e\u54cd\u5e94\u9009\u62e9\u76f8\u5173\u8d44\u6e90\u5217\u8868(Leaderboards\u3001Datasets\u3001Papers)\u3001(Colab)\u62bd\u8c61\u6587\u672c\u6458\u8981\u5b9e\u73b0\u96c6\u9526(\u6559\u7a0b \u3001\u8bcd\u8bed\u62fc\u97f3\u6570\u636e\u3001\u9ad8\u6548\u6a21\u7cca\u641c\u7d22\u5de5\u5177\u3001NLP\u6570\u636e\u589e\u5e7f\u8d44\u6e90\u96c6\u3001\u5fae\u8f6f\u5bf9\u8bdd\u673a\u5668\u4eba\u6846\u67b6 \u3001 GitHub Typo Corpus\uff1a\u5927\u89c4\u6a21GitHub\u591a\u8bed\u8a00\u62fc\u5199\u9519\u8bef/\u8bed\u6cd5\u9519\u8bef\u6570\u636e\u96c6\u3001TextCluster\uff1a\u77ed\u6587\u672c\u805a\u7c7b\u9884\u5904\u7406\u6a21\u5757 Short text cluster\u3001\u9762\u5411\u8bed\u97f3\u8bc6\u522b\u7684\u4e2d\u6587\u6587\u672c\u89c4\u8303\u5316\u3001BLINK\uff1a\u6700\u5148\u8fdb\u7684\u5b9e\u4f53\u94fe\u63a5\u5e93\u3001BertPunc\uff1a\u57fa\u4e8eBERT\u7684\u6700\u5148\u8fdb\u6807\u70b9\u4fee\u590d\u6a21\u578b\u3001Tokenizer\uff1a\u5feb\u901f\u3001\u53ef\u5b9a\u5236\u7684\u6587\u672c\u8bcd\u6761\u5316\u5e93\u3001\u4e2d\u6587\u8bed\u8a00\u7406\u89e3\u6d4b\u8bc4\u57fa\u51c6\uff0c\u5305\u62ec\u4ee3\u8868\u6027\u7684\u6570\u636e\u96c6\u3001\u57fa\u51c6(\u9884\u8bad\u7ec3)\u6a21\u578b\u3001\u8bed\u6599\u5e93\u3001\u6392\u884c\u699c\u3001spaCy \u533b\u5b66\u6587\u672c\u6316\u6398\u4e0e\u4fe1\u606f\u63d0\u53d6 \u3001 NLP\u4efb\u52a1\u793a\u4f8b\u9879\u76ee\u4ee3\u7801\u96c6\u3001 python\u62fc\u5199\u68c0\u67e5\u5e93\u3001chatbot-list - \u884c\u4e1a\u5185\u5173\u4e8e\u667a\u80fd\u5ba2\u670d\u3001\u804a\u5929\u673a\u5668\u4eba\u7684\u5e94\u7528\u548c\u67b6\u6784\u3001\u7b97\u6cd5\u5206\u4eab\u548c\u4ecb\u7ecd\u3001\u8bed\u97f3\u8d28\u91cf\u8bc4\u4ef7\u6307\u6807(MOSNet, BSSEval, STOI, PESQ, SRMR)\u3001 \u7528138GB\u8bed\u6599\u8bad\u7ec3\u7684\u6cd5\u6587RoBERTa\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b \u3001BERT-NER-Pytorch\uff1a\u4e09\u79cd\u4e0d\u540c\u6a21\u5f0f\u7684BERT\u4e2d\u6587NER\u5b9e\u9a8c\u3001\u65e0\u9053\u8bcd\u5178 - \u6709\u9053\u8bcd\u5178\u7684\u547d\u4ee4\u884c\u7248\u672c\uff0c\u652f\u6301\u82f1\u6c49\u4e92\u67e5\u548c\u5728\u7ebf\u67e5\u8be2\u30012019\u5e74NLP\u4eae\u70b9\u56de\u987e\u3001 Chinese medical dialogue data \u4e2d\u6587\u533b\u7597\u5bf9\u8bdd\u6570\u636e\u96c6 \u3001\u6700\u597d\u7684\u6c49\u5b57\u6570\u5b57(\u4e2d\u6587\u6570\u5b57)-\u963f\u62c9\u4f2f\u6570\u5b57\u8f6c\u6362\u5de5\u5177\u3001 \u57fa\u4e8e\u767e\u79d1\u77e5\u8bc6\u5e93\u7684\u4e2d\u6587\u8bcd\u8bed\u591a\u8bcd\u4e49/\u4e49\u9879\u83b7\u53d6\u4e0e\u7279\u5b9a\u53e5\u5b50\u8bcd\u8bed\u8bed\u4e49\u6d88\u6b67\u3001awesome-nlp-sentiment-analysis - \u60c5\u611f\u5206\u6790\u3001\u60c5\u7eea\u539f\u56e0\u8bc6\u522b\u3001\u8bc4\u4ef7\u5bf9\u8c61\u548c\u8bc4\u4ef7\u8bcd\u62bd\u53d6\u3001LineFlow\uff1a\u9762\u5411\u6240\u6709\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u7684NLP\u6570\u636e\u9ad8\u6548\u52a0\u8f7d\u5668\u3001\u4e2d\u6587\u533b\u5b66NLP\u516c\u5f00\u8d44\u6e90\u6574\u7406 \u3001MedQuAD\uff1a(\u82f1\u6587)\u533b\u5b66\u95ee\u7b54\u6570\u636e\u96c6\u3001\u5c06\u81ea\u7136\u8bed\u8a00\u6570\u5b57\u4e32\u89e3\u6790\u8f6c\u6362\u4e3a\u6574\u6570\u548c\u6d6e\u70b9\u6570\u3001Transfer Learning in Natural Language Processing (NLP) \u3001\u9762\u5411\u8bed\u97f3\u8bc6\u522b\u7684\u4e2d\u6587/\u82f1\u6587\u53d1\u97f3\u8f9e\u5178\u3001Tokenizers\uff1a\u6ce8\u91cd\u6027\u80fd\u4e0e\u591a\u529f\u80fd\u6027\u7684\u6700\u5148\u8fdb\u5206\u8bcd\u5668\u3001CLUENER \u7ec6\u7c92\u5ea6\u547d\u540d\u5b9e\u4f53\u8bc6\u522b Fine Grained Named Entity Recognition\u3001 \u57fa\u4e8eBERT\u7684\u4e2d\u6587\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3001\u4e2d\u6587\u8c23\u8a00\u6570\u636e\u5e93\u3001NLP\u6570\u636e\u96c6/\u57fa\u51c6\u4efb\u52a1\u5927\u5217\u8868\u3001nlp\u76f8\u5173\u7684\u4e00\u4e9b\u8bba\u6587\u53ca\u4ee3\u7801, \u5305\u62ec\u4e3b\u9898\u6a21\u578b\u3001\u8bcd\u5411\u91cf(Word Embedding)\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b(NER)\u3001\u6587\u672c\u5206\u7c7b(Text Classificatin)\u3001\u6587\u672c\u751f\u6210(Text Generation)\u3001\u6587\u672c\u76f8\u4f3c\u6027(Text Similarity)\u8ba1\u7b97\u7b49\uff0c\u6d89\u53ca\u5230\u5404\u79cd\u4e0enlp\u76f8\u5173\u7684\u7b97\u6cd5\uff0c\u57fa\u4e8ekeras\u548ctensorflow \u3001Python\u6587\u672c\u6316\u6398/NLP\u5b9e\u6218\u793a\u4f8b\u3001 Blackstone\uff1a\u9762\u5411\u975e\u7ed3\u6784\u5316\u6cd5\u5f8b\u6587\u672c\u7684spaCy pipeline\u548cNLP\u6a21\u578b\u901a\u8fc7\u540c\u4e49\u8bcd\u66ff\u6362\u5b9e\u73b0\u6587\u672c\u201c\u53d8\u8138\u201d \u3001\u4e2d\u6587 \u9884\u8bad\u7ec3 ELECTREA \u6a21\u578b: \u57fa\u4e8e\u5bf9\u6297\u5b66\u4e60 pretrain Chinese Model \u3001albert-chinese-ner - \u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578bALBERT\u505a\u4e2d\u6587NER \u3001\u57fa\u4e8eGPT2\u7684\u7279\u5b9a\u4e3b\u9898\u6587\u672c\u751f\u6210/\u6587\u672c\u589e\u5e7f\u3001\u5f00\u6e90\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5408\u96c6\u3001\u591a\u8bed\u8a00\u53e5\u5411\u91cf\u5305\u3001\u7f16\u7801\u3001\u6807\u8bb0\u548c\u5b9e\u73b0\uff1a\u4e00\u79cd\u53ef\u63a7\u9ad8\u6548\u7684\u6587\u672c\u751f\u6210\u65b9\u6cd5\u3001 \u82f1\u6587\u810f\u8bdd\u5927\u5217\u8868 \u3001attnvis\uff1aGPT2\u3001BERT\u7b49transformer\u8bed\u8a00\u6a21\u578b\u6ce8\u610f\u529b\u4ea4\u4e92\u53ef\u89c6\u5316\u3001CoVoST\uff1aFacebook\u53d1\u5e03\u7684\u591a\u8bed\u79cd\u8bed\u97f3-\u6587\u672c\u7ffb\u8bd1\u8bed\u6599\u5e93\uff0c\u5305\u62ec11\u79cd\u8bed\u8a00(\u6cd5\u8bed\u3001\u5fb7\u8bed\u3001\u8377\u5170\u8bed\u3001\u4fc4\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u610f\u5927\u5229\u8bed\u3001\u571f\u8033\u5176\u8bed\u3001\u6ce2\u65af\u8bed\u3001\u745e\u5178\u8bed\u3001\u8499\u53e4\u8bed\u548c\u4e2d\u6587)\u7684\u8bed\u97f3\u3001\u6587\u5b57\u8f6c\u5f55\u53ca\u82f1\u6587\u8bd1\u6587\u3001Jiagu\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177 - \u4ee5BiLSTM\u7b49\u6a21\u578b\u4e3a\u57fa\u7840\uff0c\u63d0\u4f9b\u77e5\u8bc6\u56fe\u8c31\u5173\u7cfb\u62bd\u53d6 \u4e2d\u6587\u5206\u8bcd \u8bcd\u6027\u6807\u6ce8 \u547d\u540d\u5b9e\u4f53\u8bc6\u522b \u60c5\u611f\u5206\u6790 \u65b0\u8bcd\u53d1\u73b0 \u5173\u952e\u8bcd \u6587\u672c\u6458\u8981 \u6587\u672c\u805a\u7c7b\u7b49\u529f\u80fd\u3001\u7528unet\u5b9e\u73b0\u5bf9\u6587\u6863\u8868\u683c\u7684\u81ea\u52a8\u68c0\u6d4b\uff0c\u8868\u683c\u91cd\u5efa\u3001NLP\u4e8b\u4ef6\u63d0\u53d6\u6587\u732e\u8d44\u6e90\u5217\u8868 \u3001 \u91d1\u878d\u9886\u57df\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7814\u7a76\u8d44\u6e90\u5927\u5217\u8868\u3001CLUEDatasetSearch - \u4e2d\u82f1\u6587NLP\u6570\u636e\u96c6\uff1a\u641c\u7d22\u6240\u6709\u4e2d\u6587NLP\u6570\u636e\u96c6\uff0c\u9644\u5e38\u7528\u82f1\u6587NLP\u6570\u636e\u96c6 \u3001medical_NER - \u4e2d\u6587\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u547d\u540d\u5b9e\u4f53\u8bc6\u522b \u3001(\u54c8\u4f5b)\u8bb2\u56e0\u679c\u63a8\u7406\u7684\u514d\u8d39\u4e66\u3001\u77e5\u8bc6\u56fe\u8c31\u76f8\u5173\u5b66\u4e60\u8d44\u6599/\u6570\u636e\u96c6/\u5de5\u5177\u8d44\u6e90\u5927\u5217\u8868\u3001Forte\uff1a\u7075\u6d3b\u5f3a\u5927\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406pipeline\u5de5\u5177\u96c6 \u3001Python\u5b57\u7b26\u4e32\u76f8\u4f3c\u6027\u7b97\u6cd5\u5e93\u3001PyLaia\uff1a\u9762\u5411\u624b\u5199\u6587\u6863\u5206\u6790\u7684\u6df1\u5ea6\u5b66\u4e60\u5de5\u5177\u5305\u3001TextFooler\uff1a\u9488\u5bf9\u6587\u672c\u5206\u7c7b/\u63a8\u7406\u7684\u5bf9\u6297\u6587\u672c\u751f\u6210\u6a21\u5757\u3001Haystack\uff1a\u7075\u6d3b\u3001\u5f3a\u5927\u7684\u53ef\u6269\u5c55\u95ee\u7b54(QA)\u6846\u67b6\u3001\u4e2d\u6587\u5173\u952e\u77ed\u8bed\u62bd\u53d6\u5de5\u5177",
    "html_url": "https://github.com/fighting41love/funNLP",
    "clone_url": "https://github.com/fighting41love/funNLP.git",
    "owner_login": "fighting41love",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/11475294?v=4",
    "stargazers_count": 75258,
    "watchers_count": 75258,
    "forks_count": 14947,
    "open_issues_count": 40,
    "size": 174188,
    "language": "Python",
    "languages": {
      "Python": 365
    },
    "topics": [],
    "license_name": null,
    "created_at": "2018-08-21T11:20:39+00:00",
    "updated_at": "2025-08-06T02:02:30+00:00",
    "pushed_at": "2024-05-10T07:38:24+00:00",
    "contributors_count": 11,
    "readme_length": 94113,
    "readme_content": "<center>\n    <img style=\"border-radius: 0.3125em;\n    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);\" \n    src=\"./data/.logo\u56fe\u7247/.img.jpg\"width=\"180\">\n    <br>\n    <div style=\"color:orange; border-bottom: 1px solid #d9d9d9;\n    display: inline-block;\n    color: #999;\n    padding: 2px;\">NLP\u6c11\u5de5\u7684\u4e50\u56ed</div>\n</center>\n<br>\n\n[![](https://img.shields.io/github/stars/fighting41love/funnlp?style=social)](https://github.com/fighting41love/funnlp)\n[![](https://img.shields.io/badge/dynamic/json?color=blue&label=%E7%9F%A5%E4%B9%8E%E5%85%B3%E6%B3%A8&query=%24.data.totalSubs&url=https%3A%2F%2Fapi.spencerwoo.com%2Fsubstats%2F%3Fsource%3Dzhihu%26queryKey%3Dmountain-blue-64)](https://www.zhihu.com/people/mountain-blue-64)\n[![](data/.logo\u56fe\u7247/.\u6350\u8d60\u56fe\u7247/.Citations-487-red.svg)](https://scholar.google.com/citations?hl=en&user=aqZdfDUAAAAJ)\n\n[![](data/.logo\u56fe\u7247/.\u6350\u8d60\u56fe\u7247/.Home-%E4%BA%BA%E7%94%9F%E6%B5%AA%E8%B4%B9%E6%8C%87%E5%8D%97-brightgreen.svg)](http://fighting41love.github.io/archives/)\n[![](data/.logo\u56fe\u7247/.\u6350\u8d60\u56fe\u7247/.%E7%8C%8E%E9%80%81%E9%97%A8-CV-orange.svg)](http://fighting41love.github.io/)\n<!-- [![](https://img.shields.io/badge/dynamic/json?color=blueviolet&label=github%20followers&query=%24.data.totalSubs&url=https%3A%2F%2Fapi.spencerwoo.com%2Fsubstats%2F%3Fsource%3Dgithub%26queryKey%3Dfighting41love)](https://github.com/fighting41love) -->\n<!-- [![](https://img.shields.io/badge/Homepage-%E4%BA%BA%E7%94%9F%E6%B5%AA%E8%B4%B9%E6%8C%87%E5%8D%97-brightgreen)](http://fighting41love.github.io/archives/) -->\n\n### The Most Powerful NLP-Weapon Arsenal\n## NLP\u6c11\u5de5\u7684\u4e50\u56ed: \u51e0\u4e4e\u6700\u5168\u7684\u4e2d\u6587NLP\u8d44\u6e90\u5e93\n\u5728\u5165\u95e8\u5230\u719f\u6089NLP\u7684\u8fc7\u7a0b\u4e2d\uff0c\u7528\u5230\u4e86\u5f88\u591agithub\u4e0a\u7684\u5305\uff0c\u9042\u6574\u7406\u4e86\u4e00\u4e0b\uff0c\u5206\u4eab\u5728\u8fd9\u91cc\u3002\n\n\u5f88\u591a\u5305\u975e\u5e38\u6709\u8da3\uff0c\u503c\u5f97\u6536\u85cf\uff0c\u6ee1\u8db3\u5927\u5bb6\u7684\u6536\u96c6\u7656\uff01\n\u5982\u679c\u89c9\u5f97\u6709\u7528\uff0c\u8bf7\u5206\u4eab\u5e76star:star:\uff0c\u8c22\u8c22\uff01\n\n\u957f\u671f\u4e0d\u5b9a\u65f6\u66f4\u65b0\uff0c\u6b22\u8fcewatch\u548cfork\uff01:heart::heart::heart:\n\n|  :fire::fire::fire::fire::fire::fire::fire::fire::fire::fire:   &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |\n|  ----  |\n| * [\u7c7bChatGPT\u7684\u6a21\u578b\u8bc4\u6d4b\u5bf9\u6bd4](#\u7c7bChatGPT\u7684\u6a21\u578b\u8bc4\u6d4b\u5bf9\u6bd4)  <br> * [\u7c7bChatGPT\u7684\u8d44\u6599](#\u7c7bChatGPT\u7684\u8d44\u6599)  <br>* [\u7c7bChatGPT\u7684\u5f00\u6e90\u6846\u67b6](#\u7c7bChatGPT\u7684\u5f00\u6e90\u6846\u67b6)  <br>* [LLM\u7684\u8bad\u7ec3_\u63a8\u7406_\u4f4e\u8d44\u6e90_\u9ad8\u6548\u8bad\u7ec3](#LLM\u7684\u8bad\u7ec3_\u63a8\u7406_\u4f4e\u8d44\u6e90_\u9ad8\u6548\u8bad\u7ec3)   <br>* [\u63d0\u793a\u5de5\u7a0b](#\u63d0\u793a\u5de5\u7a0b)  <br>* [\u7c7bChatGPT\u7684\u6587\u6863\u95ee\u7b54](#\u7c7bChatGPT\u7684\u6587\u6863\u95ee\u7b54)  <br>* [\u7c7bChatGPT\u7684\u884c\u4e1a\u5e94\u7528](#\u7c7bChatGPT\u7684\u884c\u4e1a\u5e94\u7528)  <br>* [\u7c7bChatGPT\u7684\u8bfe\u7a0b\u8d44\u6599](#\u7c7bChatGPT\u7684\u8bfe\u7a0b\u8d44\u6599)  <br>* [LLM\u7684\u5b89\u5168\u95ee\u9898](#LLM\u7684\u5b89\u5168\u95ee\u9898)  <br>* [\u591a\u6a21\u6001LLM](#\u591a\u6a21\u6001LLM)  <br>* [LLM\u7684\u6570\u636e\u96c6](#LLM\u7684\u6570\u636e\u96c6)\n\n \n|  :eggplant: :cherries: :pear: :tangerine:   &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  |  :sunflower: :strawberry:  :melon: :tomato: :pineapple: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;|\n|  ----  | ----  |\n| * [\u8bed\u6599\u5e93](#\u8bed\u6599\u5e93)  <br> * [\u8bcd\u5e93\u53ca\u8bcd\u6cd5\u5de5\u5177](#\u8bcd\u5e93\u53ca\u8bcd\u6cd5\u5de5\u5177)  <br> * [\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b](#\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b)  <br>  * [\u62bd\u53d6](#\u62bd\u53d6)  <br> * [\u77e5\u8bc6\u56fe\u8c31](#\u77e5\u8bc6\u56fe\u8c31)  <br>   * [\u6587\u672c\u751f\u6210](#\u6587\u672c\u751f\u6210) <br>   * [\u6587\u672c\u6458\u8981](#\u6587\u672c\u6458\u8981)  <br>  * [\u667a\u80fd\u95ee\u7b54](#\u667a\u80fd\u95ee\u7b54) <br>  * [\u6587\u672c\u7ea0\u9519](#\u6587\u672c\u7ea0\u9519)  | * [\u6587\u6863\u5904\u7406](#\u6587\u6863\u5904\u7406) <br>   * [\u8868\u683c\u5904\u7406](#\u8868\u683c\u5904\u7406) <br>   * [\u6587\u672c\u5339\u914d](#\u6587\u672c\u5339\u914d)  <br>   * [\u6587\u672c\u6570\u636e\u589e\u5f3a](#\u6587\u672c\u6570\u636e\u589e\u5f3a) <br>   * [\u6587\u672c\u68c0\u7d22](#\u6587\u672c\u68c0\u7d22) <br>  * [\u9605\u8bfb\u7406\u89e3](#\u9605\u8bfb\u7406\u89e3) <br>  * [\u60c5\u611f\u5206\u6790](#\u60c5\u611f\u5206\u6790) <br>  * [\u5e38\u7528\u6b63\u5219\u8868\u8fbe\u5f0f](#\u5e38\u7528\u6b63\u5219\u8868\u8fbe\u5f0f) <br>   * [\u8bed\u97f3\u5904\u7406](#\u8bed\u97f3\u5904\u7406) |\n| * [\u5e38\u7528\u6b63\u5219\u8868\u8fbe\u5f0f](#\u5e38\u7528\u6b63\u5219\u8868\u8fbe\u5f0f) <br>  * [\u4e8b\u4ef6\u62bd\u53d6](#\u4e8b\u4ef6\u62bd\u53d6) <br> * [\u673a\u5668\u7ffb\u8bd1](#\u673a\u5668\u7ffb\u8bd1) <br> * [\u6570\u5b57\u8f6c\u6362](#\u6570\u5b57\u8f6c\u6362) <br>  * [\u6307\u4ee3\u6d88\u89e3](#\u6307\u4ee3\u6d88\u89e3) <br>  * [\u6587\u672c\u805a\u7c7b](#\u6587\u672c\u805a\u7c7b) <br>  * [\u6587\u672c\u5206\u7c7b](#\u6587\u672c\u5206\u7c7b) <br> * [\u77e5\u8bc6\u63a8\u7406](#\u77e5\u8bc6\u63a8\u7406) <br> * [\u53ef\u89e3\u91caNLP](#\u53ef\u89e3\u91ca\u81ea\u7136\u8bed\u8a00\u5904\u7406) <br> * [\u6587\u672c\u5bf9\u6297\u653b\u51fb](#\u6587\u672c\u5bf9\u6297\u653b\u51fb)  |  * [\u6587\u672c\u53ef\u89c6\u5316](#\u6587\u672c\u53ef\u89c6\u5316)  <br>  * [\u6587\u672c\u6807\u6ce8\u5de5\u5177](#\u6587\u672c\u6807\u6ce8\u5de5\u5177) <br>  * [\u7efc\u5408\u5de5\u5177](#\u7efc\u5408\u5de5\u5177) <br> * [\u6709\u8da3\u641e\u7b11\u5de5\u5177](#\u6709\u8da3\u641e\u7b11\u5de5\u5177) <br> * [\u8bfe\u7a0b\u62a5\u544a\u9762\u8bd5\u7b49](#\u8bfe\u7a0b\u62a5\u544a\u9762\u8bd5\u7b49) <br> * [\u6bd4\u8d5b](#\u6bd4\u8d5b) <br> * [\u91d1\u878dNLP](#\u91d1\u878d\u81ea\u7136\u8bed\u8a00\u5904\u7406) <br> * [\u533b\u7597NLP](#\u533b\u7597\u81ea\u7136\u8bed\u8a00\u5904\u7406) <br> * [\u6cd5\u5f8bNLP](#\u6cd5\u5f8b\u81ea\u7136\u8bed\u8a00\u5904\u7406) <br> * [\u6587\u672c\u751f\u6210\u56fe\u50cf](#\u6587\u672c\u751f\u6210\u56fe\u50cf) <br> * [\u5176\u4ed6](#\u5176\u4ed6)  |\n\n<!-- \n\u76ee\u5f55\uff08Table of contents\uff09\n=================\n<table border=\"0\">\n <tr>\n    <td><b style=\"font-size:30px\">:star:</b></td>\n    <td><b style=\"font-size:30px\">:star::star:</b></td>\n    <td><b style=\"font-size:30px\">:star::star::star:</b></td>\n    <td><b style=\"font-size:30px\">:star::star::star::star:</b></td>\n </tr>\n <tr>\n    <td>\n\n<!--ts-->\n   <!-- * [\u8bed\u6599\u5e93](#\u8bed\u6599\u5e93)\n   * [\u8bcd\u5e93\u53ca\u8bcd\u6cd5\u5de5\u5177](#\u8bcd\u5e93\u53ca\u8bcd\u6cd5\u5de5\u5177)\n   * [\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b](#\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b)\n   * [\u62bd\u53d6](#\u62bd\u53d6)\n   * [\u77e5\u8bc6\u56fe\u8c31](#\u77e5\u8bc6\u56fe\u8c31)\n   * [\u6587\u672c\u751f\u6210](#\u6587\u672c\u751f\u6210)\n   * [\u6587\u672c\u6458\u8981](#\u6587\u672c\u6458\u8981)\n   * [\u667a\u80fd\u95ee\u7b54](#\u667a\u80fd\u95ee\u7b54)\n   * [\u6587\u672c\u7ea0\u9519](#\u6587\u672c\u7ea0\u9519) -->\n\n\n<!--te-->\n\n  </td>\n\n  <td>\n\n<!--ts-->\n\n   <!-- * [\u6587\u6863\u5904\u7406](#\u6587\u6863\u5904\u7406)\n   * [\u8868\u683c\u5904\u7406](#\u8868\u683c\u5904\u7406)\n   * [\u6587\u672c\u5339\u914d](#\u6587\u672c\u5339\u914d)\n   * [\u6587\u672c\u6570\u636e\u589e\u5f3a](#\u6587\u672c\u6570\u636e\u589e\u5f3a)\n   * [\u6587\u672c\u68c0\u7d22](#\u6587\u672c\u68c0\u7d22)\n   * [\u9605\u8bfb\u7406\u89e3](#\u9605\u8bfb\u7406\u89e3)\n   * [\u60c5\u611f\u5206\u6790](#\u60c5\u611f\u5206\u6790)\n   * [\u5e38\u7528\u6b63\u5219\u8868\u8fbe\u5f0f](#\u5e38\u7528\u6b63\u5219\u8868\u8fbe\u5f0f)\n   * [\u8bed\u97f3\u5904\u7406](#\u8bed\u97f3\u5904\u7406) -->\n<!--te-->\n\n  </td>\n\n  <td>\n   \n<!--ts-->\n   <!-- * [\u5e38\u7528\u6b63\u5219\u8868\u8fbe\u5f0f](#\u5e38\u7528\u6b63\u5219\u8868\u8fbe\u5f0f)\n   * [\u4e8b\u4ef6\u62bd\u53d6](#\u4e8b\u4ef6\u62bd\u53d6)\n   * [\u673a\u5668\u7ffb\u8bd1](#\u673a\u5668\u7ffb\u8bd1)\n   * [\u6570\u5b57\u8f6c\u6362](#\u6570\u5b57\u8f6c\u6362)\n   * [\u6307\u4ee3\u6d88\u89e3](#\u6307\u4ee3\u6d88\u89e3)\n   * [\u6587\u672c\u805a\u7c7b](#\u6587\u672c\u805a\u7c7b)\n   * [\u6587\u672c\u5206\u7c7b](#\u6587\u672c\u5206\u7c7b)\n   * [\u77e5\u8bc6\u63a8\u7406](#\u77e5\u8bc6\u63a8\u7406)\n   * [\u53ef\u89e3\u91caNLP](#\u53ef\u89e3\u91ca\u81ea\u7136\u8bed\u8a00\u5904\u7406)\n   * [\u6587\u672c\u5bf9\u6297\u653b\u51fb](#\u6587\u672c\u5bf9\u6297\u653b\u51fb) -->\n\n<!--te-->\n    \n  </td>\n\n  <td>\n   \n<!--ts-->\n<!-- \n   * [\u6587\u672c\u53ef\u89c6\u5316](#\u6587\u672c\u53ef\u89c6\u5316)\n   * [\u6587\u672c\u6807\u6ce8\u5de5\u5177](#\u6587\u672c\u6807\u6ce8\u5de5\u5177)\n   * [\u7efc\u5408\u5de5\u5177](#\u7efc\u5408\u5de5\u5177)\n   * [\u6709\u8da3\u641e\u7b11\u5de5\u5177](#\u6709\u8da3\u641e\u7b11\u5de5\u5177)\n   * [\u8bfe\u7a0b\u62a5\u544a\u9762\u8bd5\u7b49](#\u8bfe\u7a0b\u62a5\u544a\u9762\u8bd5\u7b49)\n   * [\u6bd4\u8d5b](#\u6bd4\u8d5b)\n   * [\u91d1\u878dNLP](#\u91d1\u878d\u81ea\u7136\u8bed\u8a00\u5904\u7406)\n   * [\u533b\u7597NLP](#\u533b\u7597\u81ea\u7136\u8bed\u8a00\u5904\u7406)\n   * [\u6cd5\u5f8bNLP](#\u6cd5\u5f8b\u81ea\u7136\u8bed\u8a00\u5904\u7406)\n   * [\u5176\u4ed6](#\u5176\u4ed6) -->\n\n<!--te-->\n    \n  <!-- </td>\n\n </tr>\n</table> --> \n\n----\n# \u7c7bChatGPT\u7684\u6a21\u578b\u8bc4\u6d4b\u5bf9\u6bd4\n\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :----   |          :--- |\n|   ChatALL\uff1a\u53ef\u4ee5\u540c\u65f6\u4e0e\u591a\u4e2aAI\u804a\u5929\u673a\u5668\u4eba\uff08\u542b\u6e05\u534e\u3001\u8baf\u98de\u7684\u4ea7\u54c1\uff09    |    \u53ef\u4ee5\u540c\u65f6\u4e0e\u591a\u4e2aAI\u804a\u5929\u673a\u5668\u4eba\uff08\u5982ChatGPT\u3001Bing Chat\u3001Bard\u3001Alpaca\u3001Vincuna\u3001Claude\u3001ChatGLM\u3001MOSS\u3001iFlytek Spark\u3001ERNIE\u7b49\uff09\u8fdb\u884c\u5bf9\u8bdd\u7684\u5de5\u5177\u3002\u5b83\u53ef\u4ee5\u5e76\u884c\u53d1\u9001\u63d0\u793a\u7ed9\u4e0d\u540c\u7684AI\u673a\u5668\u4eba\uff0c\u5e2e\u52a9\u7528\u6237\u627e\u5230\u6700\u597d\u7684\u56de\u7b54   |  [github-ChatALL](https://github.com/sunner/ChatALL)  |\n|  Chatbot Arena    |  \u5b9e\u9645\u573a\u666f\u7528Elo rating\u5bf9 LLM \u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5 - \u4ecb\u7ecd\u4e86 Chatbot Arena\uff0c\u4e00\u79cd\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u57fa\u51c6\u5e73\u53f0\uff0c\u91c7\u7528\u533f\u540d\u3001\u968f\u673a\u7684\u65b9\u5f0f\u8fdb\u884c\u5bf9\u6297\u8bc4\u6d4b\uff0c\u8bc4\u6d4b\u65b9\u5f0f\u57fa\u4e8e\u56fd\u9645\u8c61\u68cb\u7b49\u7ade\u6280\u6e38\u620f\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7684 Elo rating system\u3002\u53d1\u5e03\u4e869\u4e2a\u6d41\u884c\u7684\u5f00\u6e90 LLM \u6a21\u578b\u7684 Elo rating \u5e76\u63a8\u51fa\u6392\u884c\u699c\u3002\u5e73\u53f0\u91c7\u7528 FastChat \u591a\u6a21\u578b\u670d\u52a1\u7cfb\u7edf\uff0c\u5728\u591a\u4e2a\u8bed\u8a00\u4e0b\u63d0\u4f9b\u4ea4\u4e92\u5f0f\u754c\u9762\uff0c\u6570\u636e\u6765\u6e90\u4e8e\u7528\u6237\u6295\u7968\u3002\u603b\u7ed3\u4e86 Chatbot Arena \u7684\u4f18\u70b9\u5e76\u8ba1\u5212\u63d0\u4f9b\u66f4\u597d\u7684\u91c7\u6837\u7b97\u6cd5\u3001\u6392\u540d\u548c\u670d\u52a1\u7cfb\u7edf     |   [\u622a\u6b622023\u5e745\u67083\u65e5](https://lmsys.org/blog/2023-05-03-arena/)    |\n|   \u7c7bChatGPT\u6a21\u578b\u8bc4\u6d4b\u603b\u7ed3   |   \u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\uff0c\u8fd9\u4e9b\u5f3a\u5927\u7684\u6a21\u578b\u80fd\u591f\u7406\u89e3\u590d\u6742\u7684\u4fe1\u606f\uff0c\u5e76\u5bf9\u5404\u79cd\u95ee\u9898\u63d0\u4f9b\u7c7b\u4eba\u7684\u56de\u5e94\u3002\u5176\u4e2dGPT-3\u548cGPT-4\u8868\u73b0\u6700\u597d\uff0cFlan-t5\u548cLit-LLaMA\u8868\u73b0\u4e5f\u4e0d\u9519\u3002\u4f46\u8981\u6ce8\u610f\uff0c\u6a21\u578b\u5546\u7528\u53ef\u80fd\u9700\u8981\u4ed8\u8d39\u548c\u6570\u636e\u5171\u4eab    |  [blog](https://lightning.ai/pages/community/community-discussions/the-ultimate-battle-of-language-models-lit-llama-vs-gpt3.5-vs-bloom-vs/)     |\n|   \u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5927\u76d8\u70b9   |       |   [blog](https://mp.weixin.qq.com/s/y81NvvWqyOnaBoKyV_1f6A)    |\n|   \u5927\u6a21\u578b\u8bc4\u6d4b\u65b9\u9762\u7684\u6700\u65b0\u7814\u7a76   |    \u957f\u6587\u672c\u5efa\u6a21\u4e00\u76f4\u662fChaGPT\u4ee4\u4eba\u60ca\u8273\u7684\u80fd\u529b\u4e4b\u4e00\uff0c\u6211\u4eec\u4ee5\u3010\u7bc7\u7ae0\u7ffb\u8bd1\u3011\u4e3a\u5b9e\u9a8c\u573a\u666f\uff0c\u5bf9\u5927\u6a21\u578b\u7684\u7bc7\u7ae0\u5efa\u6a21\u80fd\u529b\u8fdb\u884c\u5168\u9762\u3001\u7ec6\u7c92\u5ea6\u7684\u6d4b\u8bd5\u3002   |  [paper](https://arxiv.org/abs/2304.02210)     |\n|\u4e2d\u6587\u5927\u6a21\u578b\u8bc4\u6d4b\u5de5\u5177&\u6392\u884c\u699c|C-Eval\u662f\u4e00\u4e2a\u5168\u9762\u7684\u4e2d\u6587\u8bc4\u4f30\u5957\u4ef6\uff0c\u9002\u7528\u4e8e\u57fa\u7840\u6a21\u578b\u3002\u5b83\u5305\u542b13948\u4e2a\u591a\u9879\u9009\u62e9\u9898\uff0c\u6db5\u76d652\u4e2a\u4e0d\u540c\u7684\u5b66\u79d1\u548c\u56db\u4e2a\u96be\u5ea6\u7ea7\u522b\uff0c\u5177\u4f53\u5982\u4e0b\u6240\u793a\u3002\u8bf7\u8bbf\u95ee\u6211\u4eec\u7684\u7f51\u7ad9\u6216\u67e5\u9605\u6211\u4eec\u7684\u8bba\u6587\u83b7\u53d6\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\u3002|[github](https://github.com/SJTU-LIT/ceval)[paper](https://arxiv.org/abs/2305.08322)|\n|OpenCompass \u5927\u6a21\u578b\u8bc4\u6d4b|OpenCompass \u4e0a\u6d77\u4eba\u5de5\u667a\u80fd\u5b9e\u9a8c\u5ba4\u5f00\u53d1\u7684\u4e00\u6b3e\u5f00\u6e90\u3001\u9ad8\u6548\u3001\u5168\u9762\u7684\u8bc4\u6d4b\u5927\u6a21\u578b\u4f53\u7cfb\u53ca\u5f00\u653e\u5e73\u53f0\uff0c\u63d0\u4f9b\u5b8c\u6574\u5f00\u6e90\u53ef\u590d\u73b0\u7684\u8bc4\u6d4b\u6846\u67b6\uff0c\u652f\u6301\u5927\u8bed\u8a00\u6a21\u578b\u3001\u591a\u6a21\u6001\u6a21\u578b\u5404\u7c7b\u6a21\u578b\u7684\u4e00\u7ad9\u5f0f\u8bc4\u6d4b\u3002\u5229\u7528\u5206\u5e03\u5f0f\u6280\u672f\uff0c\u5373\u4f7f\u9762\u5bf9\u5343\u4ebf\u53c2\u6570\u6a21\u578b\u4e5f\u80fd\u5728\u6570\u5c0f\u65f6\u5185\u5b8c\u6210\u8bc4\u6d4b\u3002\u57fa\u4e8e\u591a\u4e2a\u4e0d\u540c\u7ef4\u5ea6\u7684\u9ad8\u8ba4\u53ef\u5ea6\u6570\u636e\u96c6\u5f00\u653e\u591a\u6837\u5316\u7684\u8bc4\u6d4b\u65b9\u5f0f\uff0c\u5305\u62ec\u96f6\u6837\u672c\u8bc4\u6d4b\u3001\u5c0f\u6837\u672c\u8bc4\u6d4b\u548c\u601d\u7ef4\u94fe\u8bc4\u6d4b\uff0c\u5168\u65b9\u4f4d\u91cf\u5316\u6a21\u578b\u5404\u4e2a\u7ef4\u5ea6\u80fd\u529b\u3002|[github](https://github.com/internLM/OpenCompass/)  [website](https://opencompass.org.cn/)|\n\n# \u7c7bChatGPT\u7684\u8d44\u6599\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :----   |          :--- |\n|  Open LLMs\uff1a\u53ef\u4f9b\u5546\u4e1a\u4f7f\u7528\u7684\u5f00\u653e\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)    |    A list of open LLMs available for commercial use   |   [github](https://github.com/eugeneyan/open-llms)    |\n|LLM Zoo: \u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6570\u636e\u3001\u6a21\u578b\u548c\u57fa\u51c6\u96c6\u5e02|LLM Zoo: democratizing ChatGPT - a project that provides data, models, and evaluation benchmark for large language models|[github](https://github.com/FreedomIntelligence/LLMZoo)|\n|   \u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u8d44\u6599\u5408\u96c6   |     \u76f8\u5173\u8bba\u6587\u5217\u8868\uff0c\u5305\u62ec\u6307\u5bfc\u3001\u63a8\u7406\u3001\u51b3\u7b56\u3001\u6301\u7eed\u6539\u8fdb\u548c\u81ea\u6211\u63d0\u5347\u7b49\u65b9\u9762\u7684\u7814\u7a76\u5de5\u4f5c  |    [LLM\u8d44\u6599\u5408\u96c6](https://github.com/floodsung/LLM-with-RL-papers)   |\n|DecryptPrompt|\u603b\u7ed3Prompt&LLM\u8bba\u6587\uff0c\u5f00\u6e90\u6570\u636e&\u6a21\u578b\uff0cAIGC\u5e94\u7528|[github](https://github.com/DSXiangLi/DecryptPrompt)|\n|   SmartGPT    |   \u65e8\u5728\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b(\u5c24\u5176\u662fGPT-3.5\u548cGPT-4)\u63d0\u4f9b\u5b8c\u6210\u590d\u6742\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u5c06\u5b83\u4eec\u5206\u89e3\u6210\u66f4\u5c0f\u7684\u95ee\u9898\uff0c\u5e76\u4f7f\u7528\u4e92\u8054\u7f51\u548c\u5176\u4ed6\u5916\u90e8\u6765\u6e90\u6536\u96c6\u4fe1\u606f\u3002\u7279\u70b9\u5305\u62ec\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u6613\u4e8e\u914d\u7f6e\uff0c\u4ee5\u53ca\u5bf9\u63d2\u4ef6\u7684\u9ad8\u5ea6\u652f\u6301\u3002SmartGPT\u7684\u8fd0\u4f5c\u57fa\u4e8e\"Autos\"\u7684\u6982\u5ff5\uff0c\u5305\u62ec\"Runner\"\u548c\"Assistant\"\u4e24\u79cd\u7c7b\u578b\uff0c\u90fd\u914d\u6709\u5904\u7406\u8ba1\u5212\u3001\u63a8\u7406\u548c\u4efb\u52a1\u6267\u884c\u7684LLM\u4ee3\u7406\u3002\u6b64\u5916\uff0cSmartGPT\u8fd8\u5177\u6709\u5185\u5b58\u7ba1\u7406\u7cfb\u7edf\uff0c\u4ee5\u53ca\u53ef\u4ee5\u5b9a\u4e49\u5404\u79cd\u547d\u4ee4\u7684\u63d2\u4ef6\u7cfb\u7edf     |  [github-SmartGPT](https://github.com/Cormanz/smartgpt)  |\n| OpenGPT      |   \u7528\u4e8e\u521b\u5efa\u57fa\u4e8e\u6307\u4ee4\u7684\u6570\u636e\u96c6\u5e76\u8bad\u7ec3\u5bf9\u8bdd\u9886\u57df\u4e13\u5bb6\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u7684\u6846\u67b6\u3002\u5df2\u7ecf\u6210\u529f\u5e94\u7528\u4e8e\u8bad\u7ec3\u5065\u5eb7\u62a4\u7406\u5bf9\u8bdd\u6a21\u578bNHS-LLM\uff0c\u5229\u7528\u6765\u81ea\u82f1\u56fd\u56fd\u5bb6\u536b\u751f\u670d\u52a1\u4f53\u7cfb(NHS)\u7f51\u7ad9\u7684\u6570\u636e\uff0c\u751f\u6210\u4e86\u5927\u91cf\u7684\u95ee\u7b54\u5bf9\u548c\u72ec\u7279\u5bf9\u8bdd  |          [github-OpenGPT](https://github.com/CogStack/OpenGPT) |\n|   PaLM 2\u6280\u672f\u62a5\u544a   |   Google\u6700\u65b0\u53d1\u5e03PaLM 2\uff0c\u4e00\u79cd\u65b0\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u5177\u6709\u66f4\u597d\u7684\u591a\u8bed\u8a00\u548c\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u6bd4\u5176\u524d\u8eabPaLM\u66f4\u8282\u7701\u8ba1\u7b97\u8d44\u6e90\u3002PaLM 2\u7efc\u5408\u4e86\u591a\u9879\u7814\u7a76\u8fdb\u5c55\uff0c\u5305\u62ec\u8ba1\u7b97\u6700\u4f18\u7684\u6a21\u578b\u548c\u6570\u636e\u89c4\u6a21\u3001\u66f4\u591a\u6837\u5316\u548c\u591a\u8bed\u8a00\u7684\u6570\u636e\u96c6\u3001\u4ee5\u53ca\u66f4\u6709\u6548\u7684\u6a21\u578b\u67b6\u6784\u548c\u76ee\u6807\u51fd\u6570\u3002PaLM 2\u5728\u591a\u79cd\u4efb\u52a1\u548c\u80fd\u529b\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5305\u62ec\u8bed\u8a00\u6c34\u5e73\u8003\u8bd5\u3001\u5206\u7c7b\u548c\u95ee\u7b54\u3001\u63a8\u7406\u3001\u7f16\u7a0b\u3001\u7ffb\u8bd1\u548c\u81ea\u7136\u8bed\u8a00\u751f\u6210\u7b49\u3002PaLM 2\u8fd8\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u591a\u8bed\u8a00\u80fd\u529b\uff0c\u80fd\u591f\u5904\u7406\u6570\u767e\u79cd\u8bed\u8a00\uff0c\u5e76\u5728\u4e0d\u540c\u8bed\u8a00\u4e4b\u95f4\u8fdb\u884c\u7ffb\u8bd1\u548c\u89e3\u91ca\u3002PaLM 2\u8fd8\u8003\u8651\u4e86\u8d1f\u8d23\u4efb\u7684\u4f7f\u7528\u95ee\u9898\uff0c\u5305\u62ec\u63a8\u7406\u65f6\u63a7\u5236\u6bd2\u6027\u3001\u51cf\u5c11\u8bb0\u5fc6\u5316\u3001\u8bc4\u4f30\u6f5c\u5728\u7684\u4f24\u5bb3\u548c\u504f\u89c1\u7b49    |    [PaLM 2 Technical Report](https://ai.google/static/documents/palm2techreport.pdf)   |\n|   DB-GPT   |    \u4e8evicuna-13b\u548cFastChat\u7684\u5f00\u6e90\u5b9e\u9a8c\u9879\u76ee\uff0c\u91c7\u7528\u4e86langchain\u548cllama-index\u6280\u672f\u8fdb\u884c\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u95ee\u7b54\u3002\u9879\u76ee\u5b8c\u5168\u672c\u5730\u5316\u90e8\u7f72\uff0c\u4fdd\u8bc1\u6570\u636e\u7684\u9690\u79c1\u5b89\u5168\uff0c\u80fd\u76f4\u63a5\u8fde\u63a5\u5230\u79c1\u6709\u6570\u636e\u5e93\u5904\u7406\u79c1\u6709\u6570\u636e\u3002\u5176\u529f\u80fd\u5305\u62ecSQL\u751f\u6210\u3001SQL\u8bca\u65ad\u3001\u6570\u636e\u5e93\u77e5\u8bc6\u95ee\u7b54\u7b49   |   [github-DB-GPT](https://github.com/csunny/DB-GPT)     |\n|  Transformers\u76f8\u5173\u6587\u732e\u8d44\u6e90\u5927\u5217\u8868    |   \u5305\u542b\u4e86\u5404\u79cd\u5404\u6837\u7684Transformer\u6a21\u578b\uff0c\u4f8b\u5982BERT\u3001GPT\u3001Transformer-XL\u7b49\uff0c\u8fd9\u4e9b\u6a21\u578b\u5df2\u7ecf\u5728\u8bb8\u591a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u5f97\u5230\u4e86\u5e7f\u6cdb\u5e94\u7528\u3002\u6b64\u5916\uff0c\u8be5\u5217\u8868\u8fd8\u63d0\u4f9b\u4e86\u8fd9\u4e9b\u6a21\u578b\u7684\u76f8\u5173\u8bba\u6587\u548c\u4ee3\u7801\u94fe\u63a5\uff0c\u4e3a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u7684\u7814\u7a76\u4eba\u5458\u548c\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u5f88\u597d\u7684\u53c2\u8003\u8d44\u6e90    |   [github](https://github.com/abacaj/awesome-transformers)    |\n|   GPT-4\u7ec8\u6781\u6307\u5357   |    \u4e00\u4efd\u5173\u4e8e\u5982\u4f55\u4f7f\u7528GPT3\u548cGPT4\u7684\u6307\u5357\uff0c\u5176\u4e2d\u5305\u62ec100\u591a\u4e2a\u8d44\u6e90\uff0c\u53ef\u4ee5\u5e2e\u52a9\u5b66\u4e60\u5982\u4f55\u7528\u5b83\u6765\u63d0\u9ad8\u751f\u6d3b\u6548\u7387\u3002\u5305\u62ec\u5982\u4f55\u5b66\u4e60ChatGPT\u57fa\u7840\u77e5\u8bc6\u3001\u5982\u4f55\u5b66\u4e60ChatGPT\u9ad8\u7ea7\u77e5\u8bc6\u3001\u5982\u4f55\u5728\u8bed\u8a00\u5b66\u4e60\u4e2d\u4f7f\u7528GPT-3\u3001\u5982\u4f55\u5728\u6559\u5b66\u4e2d\u4f7f\u7528GPT-3\u3001\u5982\u4f55\u4f7f\u7528GPT-4\u7b49\uff0c\u8fd8\u63d0\u4f9b\u4e86\u5982\u4f55\u5347\u7ea7\u5230ChatGPT+\u8ba1\u5212\u4ee5\u4f7f\u7528GPT-4\u4ee5\u53ca\u5982\u4f55\u514d\u8d39\u4f7f\u7528GPT-4\u7684\u65b9\u6cd5\u7b49\u5185\u5bb9\u3002\u540c\u65f6\uff0c\u8fd8\u63d0\u4f9b\u4e86\u5982\u4f55\u5728\u4e1a\u52a1\u3001\u751f\u4ea7\u529b\u3001\u53d7\u76ca\u3001\u91d1\u94b1\u7b49\u65b9\u9762\u4f7f\u7528ChatGPT\u7684\u6307\u5357   |   [link](https://doc.clickup.com/37456139/d/h/13q28b-324/e2a22b0c164b1f9)    |\n|  \u57fa\u4e8eLoRA\u7684LLM\u53c2\u6570\u9ad8\u6548\u5fae\u8c03    |       |   [link](https://sebastianraschka.com/blog/2023/llm-finetuning-lora.html)    |\n|  \u590d\u6742\u63a8\u7406\uff1a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5317\u6781\u661f\u80fd\u529b     |  \u5728 GPT-4 \u53d1\u5e03\u535a\u5ba2\u4e2d\uff0c\u4f5c\u8005\u5199\u9053\uff1a\u201c\u5728\u4e00\u6b21\u968f\u610f\u7684\u8c08\u8bdd\u4e2d\uff0cGPT-3.5 \u548c GPT-4 \u4e4b\u95f4\u7684\u533a\u522b\u53ef\u80fd\u662f\u5fae\u5999\u7684\u3002\u5f53\u4efb\u52a1\u7684\u590d\u6742\u7a0b\u5ea6\u8fbe\u5230\u8db3\u591f\u7684\u9608\u503c\u65f6\uff0c\u5dee\u5f02\u5c31\u4f1a\u663e\u73b0\u51fa\u6765\u3002\u201d\u8fd9\u610f\u5473\u7740\u590d\u6742\u4efb\u52a1\u5f88\u53ef\u80fd\u662f\u5927\u578b\u548c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5173\u952e\u5dee\u5f02\u56e0\u7d20\u3002\u5728\u8fd9\u7bc7\u6587\u7ae0\u4e2d\uff0c\u6211\u4eec\u5c06\u4ed4\u7ec6\u5206\u6790\u8ba8\u8bba\u5982\u4f55\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u62e5\u6709\u5f3a\u5927\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\u3002     |   [blog](https://yaofu.notion.site/6dafe3f8d11445ca9dcf8a2ca1c5b199)    |\n|   \u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6d8c\u73b0\u80fd\u529b\u662f\u5426\u662f\u6d77\u5e02\u8703\u697c\uff1f   |    \u5927\u8bed\u8a00\u6a21\u578b\u7684\u6d8c\u73b0\u80fd\u529b\u4e00\u76f4\u662f\u88ab\u5927\u5bb6\u89c6\u4f5c\u5f88\u795e\u5947\u7684\u73b0\u8c61\uff0c\u4f3c\u4e4e\u662f\u4e00\u79cd\u5927\u529b\u51fa\u5947\u8ff9\uff0c\u4f46\u8fd9\u7bc7\u8bba\u6587\u8ba4\u4e3a\u8fd9\u53ef\u80fd\u53ea\u662f\u4e00\u79cd\u9519\u89c9\u3002   |   [paper](https://arxiv.org/abs/2304.15004)    |\n|   \u5927\u8bed\u8a00\u6a21\u578b\u7684\u6982\u7387\u603b\u7ed3  |   \u975e\u5e38\u8be6\u5c3d\u7684LLM\u79d1\u5b66\u89e3\u91ca\u548c\u603b\u7ed3    |   [paper](https://wangxinyilinda.github.io/pdf/MAE_online.pdf)    |\n|  LLaMA \u6a21\u578b\u7b80\u53f2    |    LLaMA\u662fMeta\u53d1\u5e03\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u91c7\u7528Transformer\u67b6\u6784\uff0c\u6709\u591a\u4e2a\u7248\u672c\uff0c\u6700\u5927\u4e3a65B\u53c2\u6570\u3002\u4e0eGPT\u7c7b\u4f3c\uff0c\u53ef\u7528\u4e8e\u8fdb\u4e00\u6b65\u5fae\u8c03\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u3002\u4e0eGPT\u4e0d\u540c\u7684\u662f\uff0cLLaMA\u662f\u5f00\u6e90\u7684\uff0c\u53ef\u4ee5\u5728\u672c\u5730\u8fd0\u884c\u3002\u73b0\u6709\u7684LLaMA\u6a21\u578b\u5305\u62ec\uff1aAlpaca\u3001Vicuna\u3001Koala\u3001GPT4-x-Alpaca\u548cWizardLM\u3002\u6bcf\u4e2a\u6a21\u578b\u90fd\u6709\u4e0d\u540c\u7684\u8bad\u7ec3\u6570\u636e\u548c\u6027\u80fd\u8868\u73b0   |   [blog](https://agi-sphere.com/llama-models/)    |\n|  \u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u590d\u6742\u63a8\u7406     |   \u8ba8\u8bba\u4e86\u5982\u4f55\u8bad\u7ec3\u5177\u6709\u5f3a\u5927\u590d\u6742\u63a8\u7406\u80fd\u529b\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u63a2\u8ba8\u4e86\u5982\u4f55\u6709\u6548\u5730\u63d0\u793a\u6a21\u578b\u4ee5\u5145\u5206\u91ca\u653e\u5176\u6f5c\u529b\uff1b\u9488\u5bf9\u8bed\u8a00\u6a21\u578b\u548c\u7f16\u7a0b\u7684\u8bad\u7ec3\u76f8\u4f3c\u6027\uff0c\u63d0\u51fa\u4e86\u4e09\u9636\u6bb5\u7684\u8bad\u7ec3\uff1a\u6301\u7eed\u8bad\u7ec3\u3001\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff1b\u4ecb\u7ecd\u4e86\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u4e00\u5957\u4efb\u52a1\u96c6\u5408\uff1b\u8ba8\u8bba\u4e86\u5982\u4f55\u8fdb\u884c\u63d0\u793a\u5de5\u7a0b\uff0c\u901a\u8fc7\u63d0\u4f9b\u5404\u79cd\u5b66\u4e60\u673a\u4f1a\u4f7f\u6a21\u578b\u83b7\u5f97\u66f4\u597d\u7684\u5b66\u4e60\u6548\u679c\uff0c\u6700\u7ec8\u5b9e\u73b0\u667a\u80fd\u5316    |   [link](https://yaofu.notion.site/Towards-Complex-Reasoning-the-Polaris-of-Large-Language-Models-c2b4a51355b44764975f88e6a42d4e75)    |\n|   \u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u5316\u6811   |       |    [paper](https://arxiv.org/pdf/2304.13712.pdf)   |\n|\u674e\u5b8f\u6bc5\uff1a\u7a77\u4eba\u5982\u4f55\u4f4e\u8d44\u6e90\u590d\u523b\u81ea\u5df1\u7684ChatGPT||[blog](https://mp.weixin.qq.com/s/GAFYwlqY2SoTlCW7b4kOyA)|\n|   \u8bad\u7ec3ChatGPT\u7684\u5fc5\u5907\u8d44\u6e90\uff1a\u8bed\u6599\u3001\u6a21\u578b\u548c\u4ee3\u7801\u5e93\u5b8c\u5168\u6307\u5357   |       |    [\u8d44\u6e90\u94fe\u63a5](https://github.com/RUCAIBox/LLMSurvey)[\u8bba\u6587\u5730\u5740](https://arxiv.org/pdf/2303.18223.pdf)   |\n|  GitHub\u5b9d\u85cf\u5e93\uff0c\u91cc\u9762\u6574\u7406\u4e86GPT\u76f8\u5173\u7684\u5404\u79cd\u5f00\u6e90\u9879\u76ee    |       |    [github](https://github.com/EwingYangs/awesome-open-gpt)   |\n|  ChatGPT\u4e2d\u6587\u6307\u5357    |       |   [gitlab](https://gitlab.com/awesomeai/awesome-chatgpt-zh)    |\n|   \u63a2\u8ba8\u4e86ChatGPT\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u5e94\u7528\u3001\u4f18\u52bf\u3001\u9650\u5236\u4ee5\u53ca\u672a\u6765\u53d1\u5c55\u65b9\u5411   |   \u5f3a\u8c03\u4e86\u5728\u4f7f\u7528\u8be5\u6280\u672f\u65f6\u7684\u4f26\u7406\u9053\u5fb7\u8003\u91cf\u548c\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u3002    |    [paper](https://arxiv.org/abs/2304.02017)   |\n|\u5927\u578b\u8bed\u8a00\u6a21\u578b\u76f8\u5173\u6587\u732e\u8d44\u6e90\u5217\u8868||[github](https://github.com/RUCAIBox/LLMSurvey)|\n|\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6587\u732e\u7efc\u8ff0--\u4e2d\u6587\u7248||[github](https://github.com/fighting41love/funNLP/tree/master/data/paper/LLM_Survey_Chinese_0418.pdf)|\n|ChatGPT \u76f8\u5173\u8d44\u6e90\u5927\u5217\u8868||[github](https://github.com/OpenMindClub/awesome-chatgpt)|\n|Pre-Training to Learn in Context||[paper](https://arxiv.org/abs/2305.09137)|\n|Langchain\u67b6\u6784\u56fe||[image](https://pbs.twimg.com/media/Fv4hst2aIAAKypt?format=jpg&name=4096x4096)|\n|LLM\u5f00\u53d1\u4eba\u5458\u90fd\u5e94\u8be5\u77e5\u9053\u7684\u6570\u5b57||[github](https://github.com/ray-project/llm-numbers)|\n|\u5927\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u6784\u5efa\u5f3a\u5927\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b||[blog](https://zhuanlan.zhihu.com/p/626533715)|\n|LLMs\u4e5d\u5c42\u5996\u5854|\u5206\u4eab\u6253\u602a(ChatGLM\u3001Chinese-LLaMA-Alpaca\u3001MiniGPT-4\u3001FastChat\u3001LLaMA\u3001gpt4all\u7b49)\u5b9e\u6218\u4e0e\u7ecf\u9a8c|[github](https://github.com/km1994/LLMsNineStoryDemonTower)|\n\n# \u7c7bChatGPT\u7684\u5f00\u6e90\u6846\u67b6\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :----   |          :--- |\n|   LLM-As-Chatbot   |   \u8fd9\u4e2a\u9879\u76ee\u628a\u5e02\u9762\u4e0a\u6709\u7684LLM\u5168\u90e8\u505a\u6210\u4e86Chatbot\uff0c\u76f4\u63a5\u53ef\u4ee5\u5728google colab\u8fd0\u884c\uff0c\u4e0d\u9700\u8981\u81ea\u5df1\u642d\u5efa\uff0c\u975e\u5e38\u9002\u7528\u4e8e\u60f3\u4f53\u9a8cLLM\u7684\u670b\u53cb\u4eec\u3002\u6211\u521a\u8bd5\u4e86\uff0c\u771f\u7684\u8d85\u7b80\u5355\u3002\u6709\u4e9bLLM\u9700\u8981\u7684\u663e\u5b58\u6bd4\u8f83\u591a\uff0c\u6240\u4ee5\u6700\u597d\u662f\u8981\u6709colab pro\u8ba2\u9605\u3002    |  [github](https://github.com/deep-diver/LLM-As-Chatbot)     |\n|   OpenBuddy    |    \u4e00\u6b3e\u5f3a\u5927\u7684\u5f00\u6e90\u591a\u8bed\u8a00\u804a\u5929\u673a\u5668\u4eba\u6a21\u578b\uff0c\u76ee\u6807\u662f\u5168\u7403\u7528\u6237\uff0c\u91cd\u70b9\u662f\u5bf9\u8bddAI\u548c\u6d41\u7545\u7684\u591a\u8bed\u8a00\u652f\u6301\uff0c\u5305\u62ec\u82f1\u6587\u3001\u4e2d\u6587\u7b49\u591a\u79cd\u8bed\u8a00\u3002\u57fa\u4e8eFacebook\u7684LLAMA\u6a21\u578b\uff0c\u8fdb\u884c\u4e86\u5fae\u8c03\uff0c\u5305\u62ec\u6269\u5c55\u8bcd\u6c47\u8868\u3001\u589e\u52a0\u5e38\u7528\u5b57\u7b26\u548c\u589e\u5f3a\u7684token embeddings\u3002\u901a\u8fc7\u8fd9\u4e9b\u6539\u8fdb\u548c\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\u96c6\uff0cOpenBuddy\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u6a21\u578b\uff0c\u80fd\u56de\u7b54\u95ee\u9898\u5e76\u5728\u5404\u79cd\u8bed\u8a00\u4e4b\u95f4\u8fdb\u884c\u7ffb\u8bd1\u4efb\u52a1\u3002OpenBuddy\u7684\u4f7f\u547d\u662f\u63d0\u4f9b\u4e00\u4e2a\u514d\u8d39\u3001\u5f00\u653e\u4e14\u53ef\u79bb\u7ebf\u4f7f\u7528\u7684AI\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u53ef\u4ee5\u5728\u7528\u6237\u7684\u8bbe\u5907\u4e0a\u8fd0\u884c\uff0c\u65e0\u8bba\u4ed6\u4eec\u7684\u8bed\u8a00\u6216\u6587\u5316\u80cc\u666f\u5982\u4f55\u3002\u76ee\u524d\uff0cOpenBuddy-13B\u7684\u6f14\u793a\u7248\u672c\u53ef\u4ee5\u5728Discord\u670d\u52a1\u5668\u4e0a\u627e\u5230\u3002\u5176\u5173\u952e\u529f\u80fd\u5305\u62ec\u591a\u8bed\u8a00\u5bf9\u8bddAI(\u5305\u62ec\u4e2d\u6587\u3001\u82f1\u6587\u3001\u65e5\u6587\u3001\u97e9\u6587\u3001\u6cd5\u6587\u7b49)\u3001\u589e\u5f3a\u7684\u8bcd\u6c47\u8868\u548c\u5bf9\u5e38\u89c1CJK\u5b57\u7b26\u7684\u652f\u6301\uff0c\u4ee5\u53ca\u4e24\u79cd\u6a21\u578b\u7248\u672c\uff1a7B\u548c13B   |  [github-OpenBuddy](https://github.com/OpenBuddy/OpenBuddy)  |\n|   Panda: \u6d77\u5916\u4e2d\u6587\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b   |    \u57fa\u4e8e Llama-7B, -13B, -33B, -65B \u8fdb\u884c\u4e2d\u6587\u9886\u57df\u4e0a\u7684\u6301\u7eed\u9884\u8bad\u7ec3\uff0c\u4f7f\u7528\u4e86\u63a5\u8fd115M\u6761\u6570\u636e\uff0c\u5e76\u9488\u5bf9\u63a8\u7406\u80fd\u529b\u5728\u4e2d\u6587benchmark\u4e0a\u8fdb\u884c\u4e86\u8bc4\u6d4b   |   [github-PandaLM](https://github.com/dandelionsllm/pandallm)     |\n|  Dromedary\uff1a\u4e00\u4e2a\u5f00\u6e90\u7684\u81ea\u5bf9\u9f50\u8bed\u8a00\u6a21\u578b\uff0c\u53ea\u9700\u5c11\u91cf\u4eba\u5de5\u76d1\u7763\u5373\u53ef\u8fdb\u884c\u8bad\u7ec3    |       |    [github-Dromedary](https://github.com/IBM/Dromedary)   |\n|   LaMini-LM \u84b8\u998f\u7684\u5c0f\u578b\u3001\u9ad8\u6548\u7684\u8bed\u8a00\u6a21\u578b\u96c6\u5408  |   \u4ece ChatGPT \u84b8\u998f\u7684\u5c0f\u578b\u3001\u9ad8\u6548\u7684\u8bed\u8a00\u6a21\u578b\u96c6\u5408\uff0c\u57282.58 M \u6307\u4ee4\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3    |   [github](https://github.com/mbzuai-nlp/LaMini-LM)    |\n|   LLaMA-Adapter V2    |   \u4e0a\u6d77\u4eba\u5de5\u667a\u80fd\u5b9e\u9a8c\u5ba4 LLaMA-Adapter V2\uff0c\u4ec5\u6ce8\u516514M\u53c2\u6570\uff0c1\u5c0f\u65f6\u65f6\u95f4\u5373\u53ef\u5b8c\u6210\u8bad\u7ec3\uff0c\u5bf9\u6bd4\u8f83\u679c\u786e\u5b9e\u5f88\u60ca\u8273\uff0c\u4e14\u5177\u6709\u591a\u6a21\u6001\u529f\u80fd\uff08\u5bf9\u56fe\u50cf\u8fdb\u884c\u89e3\u91ca\u548c\u95ee\u7b54\uff09    |    [github](https://github.com/ZrrSkywalker/LLaMA-Adapter)   |\n|   HuggingChat   |   Hugging Face \u63a8\u51fa\u7b2c\u4e00\u4e2a ChatGPT \u5f00\u6e90\u66ff\u4ee3\u54c1\uff1aHuggingChat\u3002\u57fa\u4e8e Open Assistant  \u5927\u6a21\u578b\u642d\u5efa\uff0c\u652f\u6301\u4e2d\u6587\u5bf9\u8bdd\u4e0e\u7f16\u5199\u4ee3\u7801\uff0c\u4f46\u6682\u4e0d\u652f\u6301\u4e2d\u6587\u56de\u590d\u3002\u5e94\u7528\u5df2\u4e0a\u7ebf\uff0c\u65e0\u9700\u4ee3\u7406\uff0c\u6253\u5f00\u5373\u53ef\u8bbf\u95ee    |   [link](https://huggingface.co/chat/)    |\n| Open-Chinese-LLaMA     |   \u57fa\u4e8e LLaMA-7B \u7ecf\u8fc7 \u4e2d\u6587\u6570\u636e\u96c6\u589e\u91cf\u9884\u8bad\u7ec3 \u4ea7\u751f\u7684 \u4e2d\u6587\u5927\u8bed\u8a00\u6a21\u578b\u57fa\u5ea7    |   [github](https://github.com/OpenLMLab/OpenChineseLLaMA)    |\n|   OpenLLaMA   |   LLaMA\u6a21\u578b\u7684\u5f00\u6e90\u590d\u73b0\uff0c\u5728RedPajama\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u4f7f\u7528\u4e86\u4e0eLLaMA\u76f8\u540c\u7684\u9884\u5904\u7406\u6b65\u9aa4\u548c\u8d85\u53c2\u6570\uff0c\u6a21\u578b\u7ed3\u6784\uff0c\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u8bad\u7ec3\u6b65\u9aa4\uff0c\u5b66\u4e60\u7387\u8c03\u5ea6\u548c\u4f18\u5316\u5668\u3002OpenLLaMA\u7684PyTorch\u548cJax\u6743\u91cd\u53ef\u4ee5\u5728Huggingface Hub\u4e0a\u83b7\u5f97\u3002OpenLLaMA\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4e0eLLaMA\u548cGPT-J\u76f8\u4f3c\u7684\u8868\u73b0\uff0c\u90e8\u5206\u4efb\u52a1\u8868\u73b0\u4f18\u5f02    |  [github](https://github.com/openlm-research/open_llama)     |\n|  replit-code-v1-3b    |   BY-SA 4.0\u6388\u6743\u53d1\u5e03\uff0c\u8fd9\u610f\u5473\u7740\u5141\u8bb8\u5546\u4e1a\u4f7f\u7528    |  [link](https://huggingface.co/replit/replit-code-v1-3b)    |\n|   MOSS   |  MOSS\u662f\u4e00\u4e2a\u652f\u6301\u4e2d\u82f1\u53cc\u8bed\u548c\u591a\u79cd\u63d2\u4ef6\u7684\u5f00\u6e90\u5bf9\u8bdd\u8bed\u8a00\u6a21\u578b\uff0cmoss-moon\u7cfb\u5217\u6a21\u578b\u5177\u6709160\u4ebf\u53c2\u6570\uff0c\u5728FP16\u7cbe\u5ea6\u4e0b\u53ef\u5728\u5355\u5f20A100/A800\u6216\u4e24\u5f203090\u663e\u5361\u8fd0\u884c\uff0c\u5728INT4/8\u7cbe\u5ea6\u4e0b\u53ef\u5728\u5355\u5f203090\u663e\u5361\u8fd0\u884c\u3002MOSS\u57fa\u5ea7\u8bed\u8a00\u6a21\u578b\u5728\u7ea6\u4e03\u5343\u4ebf\u4e2d\u82f1\u6587\u4ee5\u53ca\u4ee3\u7801\u5355\u8bcd\u4e0a\u9884\u8bad\u7ec3\u5f97\u5230\uff0c\u540e\u7eed\u7ecf\u8fc7\u5bf9\u8bdd\u6307\u4ee4\u5fae\u8c03\u3001\u63d2\u4ef6\u589e\u5f3a\u5b66\u4e60\u548c\u4eba\u7c7b\u504f\u597d\u8bad\u7ec3\u5177\u5907\u591a\u8f6e\u5bf9\u8bdd\u80fd\u529b\u53ca\u4f7f\u7528\u591a\u79cd\u63d2\u4ef6\u7684\u80fd\u529b\u3002     |   [github](https://github.com/OpenLMLab/MOSS)   |\n|   RedPajama   |   1.2 \u4e07\u4ebftokens\u6570\u636e\u96c6    |  [link](https://www.together.xyz/blog/redpajama)     |\n|  chinese_llama_alpaca_lora \u62bd\u53d6\u6846\u67b6  |       |   [github](https://github.com/zhangnn520/chinese_llama_alpaca_lora)    |\n|   Scaling Transformer to 1M tokens and beyond with RMT   |  \u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a RMT \u7684\u65b0\u6280\u672f\uff0c\u6216\u8bb8\u53ef\u5c06 Transform \u7684 Token \u4e0a\u9650\u6269\u5c55\u81f3 100 \u4e07\uff0c\u751a\u81f3\u66f4\u591a\u3002     |     [github](arxiv.org/abs/2304.11062)  |\n|   Open Assistant   |   \u5305\u542b\u5927\u91cfAI\u751f\u6210\u7684\u3001\u4eba\u5de5\u6807\u6ce8\u7684\u8bed\u6599\u5e93\u548c\u5305\u62ec\u57fa\u4e8eLLaMA\u548c\u57fa\u4e8ePythia\u7684\u591a\u79cd\u6a21\u578b\u53ef\u9009\u3002\u53d1\u5e03\u7684\u6570\u636e\u96c6\u5305\u62ec\u8d85\u8fc7161K\u8f83\u9ad8\u8d28\u91cf\u7684\uff0c\u591a\u8fbe35\u79cd\u8bed\u8a00\u7684\u4eba\u5de5\u52a9\u624b\u578b\u4ea4\u4e92\u5bf9\u8bdd\u8bed\u6599\u5e93    |   [data](https://huggingface.co/datasets/OpenAssistant/oasst1) [model](https://huggingface.co/OpenAssistant)    |\n|   ChatGLM Efficient Tuning   |  \u57fa\u4e8e PEFT \u7684\u9ad8\u6548 ChatGLM \u5fae\u8c03     |   [github](https://github.com/hiyouga/ChatGLM-Efficient-Tuning)    |\n|  Dolly\u4ecb\u7ecd    |       |   [news](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html)    |\n|  Baize\uff1a\u4e00\u79cd\u5bf9\u81ea\u804a\u5929\u6570\u636e\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u8c03\u4f18\u7684\u5f00\u6e90\u804a\u5929\u6a21\u578b    |   Baize\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u804a\u5929\u6a21\u578b\uff0c\u53ef\u4ee5\u8fdb\u884c\u591a\u8f6e\u5bf9\u8bdd\u3002\u5b83\u662f\u901a\u8fc7\u4f7f\u7528ChatGPT\u81ea\u6211\u5bf9\u8bdd\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u591a\u8f6e\u804a\u5929\u8bed\u6599\u5e93\uff0c\u5e76\u4f7f\u7528\u53c2\u6570\u9ad8\u6548\u8c03\u6574\u6765\u589e\u5f3aLLaMA\uff08\u4e00\u4e2a\u5f00\u6e90\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff09\u800c\u521b\u5efa\u7684\u3002Baize\u6a21\u578b\u5728\u5177\u6709\u6700\u5c0f\u6f5c\u5728\u98ce\u9669\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u826f\u597d\u7684\u591a\u8f6e\u5bf9\u8bdd\u6027\u80fd\u3002\u5b83\u53ef\u4ee5\u5728\u5355\u4e2aGPU\u4e0a\u8fd0\u884c\uff0c\u4f7f\u66f4\u5e7f\u6cdb\u7684\u7814\u7a76\u4eba\u5458\u53ef\u4ee5\u4f7f\u7528\u5b83\u3002Baize\u6a21\u578b\u548c\u6570\u636e\u4ec5\u7528\u4e8e\u7814\u7a76\u76ee\u7684\u3002    |    [\u8bba\u6587\u5730\u5740](arxiv.org/abs/2304.01196)[\u6e90\u7801\u5730\u5740](https://github.com/project-baize/baize)   |\n|   GPTrillion--\u672a\u627e\u5230\u5f00\u6e90\u4ee3\u7801   |  \u5305\u542b1.5\u4e07\u4ebf\uff081.5T\uff09\u53c2\u6570\u7684\u5927\u6a21\u578bGPTrillion\u5f00\u6e90\u4e86\uff0c\u53f7\u79f0\u662f\u76ee\u524d\u4e16\u754c\u4e0a\u6700\u5927\u7684\u5f00\u6e90LLM    |    [google_doc](https://docs.google.com/document/d/1i9PivZcF9q2kQNBL-SurK_Hs5nFw24zGEWNcFrONCdo/edit)   |\n|Cerebras-GPT-13B(\u53ef\u5546\u7528)||[hugging_face](https://huggingface.co/cerebras/Cerebras-GPT-13B)|\n|Chinese-ChatLLaMA|\u4e2d\u6587ChatLLaMA\u5bf9\u8bdd\u6a21\u578b\uff1b\u9884\u8bad\u7ec3/\u6307\u4ee4\u5fae\u8c03\u6570\u636e\u96c6\uff0c\u57fa\u4e8e TencentPretrain \u591a\u6a21\u6001\u9884\u8bad\u7ec3\u6846\u67b6\u6784\u5efa\uff0c\u652f\u6301\u7b80\u7e41\u4f53\u4e2d\u6587\u3001\u82f1\u6587\u3001\u65e5\u6587\u7b49\u591a\u8bed\u8a00|[github](https://github.com/ydli-ai/Chinese-ChatLLaMA)|\n|Lit-LLaMA|\u57fa\u4e8eApache 2.0\u8bb8\u53ef\u8bc1\u5b8c\u5168\u5f00\u6e90\u7684LLaMA\u72ec\u7acb\u5b9e\u73b0\uff0c\u5efa\u7acb\u5728nanoGPT\u4e4b\u4e0a\uff0c\u65e8\u5728\u89e3\u51b3\u539f\u59cbLLaMA\u4ee3\u7801\u91c7\u7528GPL\u8bb8\u53ef\u8bc1\u7684\u9650\u5236\uff0c\u4ee5\u5b9e\u73b0\u66f4\u5e7f\u6cdb\u7684\u5b66\u672f\u548c\u5546\u4e1a\u5e94\u7528|[github](https://github.com/Lightning-AI/lit-llama)|\n|MosaicML|MPT-7B-StoryWriter\uff0c65K tokens\uff0c\u53ef\u4ee5\u628a\u300a\u4e86\u4e0d\u8d77\u7684\u76d6\u8328\u6bd4\u300b\u90fd\u4e00\u6b21\u6027\u6254\u8fdb\u53bb\u3002|[huggingface](https://huggingface.co/spaces/mosaicml/mpt-7b-storywriter)|\n|Langchain|\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6b63\u5728\u6210\u4e3a\u4e00\u9879\u5177\u6709\u53d8\u9769\u6027\u7684\u6280\u672f\uff0c\u4f7f\u5f00\u53d1\u8005\u80fd\u591f\u6784\u5efa\u4ee5\u524d\u65e0\u6cd5\u5b9e\u73b0\u7684\u5e94\u7528\u7a0b\u5e8f\u3002\u7136\u800c\uff0c\u4ec5\u4ec5\u4f7f\u7528\u8fd9\u4e9b\u72ec\u7acb\u7684LLMs\u901a\u5e38\u4e0d\u8db3\u4ee5\u521b\u5efa\u4e00\u4e2a\u771f\u6b63\u5f3a\u5927\u7684\u5e94\u7528\u7a0b\u5e8f - \u771f\u6b63\u7684\u529b\u91cf\u6765\u81ea\u4e8e\u80fd\u591f\u5c06\u5b83\u4eec\u4e0e\u5176\u4ed6\u8ba1\u7b97\u6216\u77e5\u8bc6\u6765\u6e90\u76f8\u7ed3\u5408\u3002|[github](https://github.com/hwchase17/langchain)|\n|Guidance|\u5f15\u5bfc\u80fd\u591f\u6bd4\u4f20\u7edf\u7684\u63d0\u793a\u6216\u94fe\u63a5\u66f4\u6709\u6548\u5730\u63a7\u5236\u73b0\u4ee3\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u4e14\u66f4\u9ad8\u6548\u3002\u5f15\u5bfc\u7a0b\u5e8f\u5141\u8bb8\u60a8\u5c06\u751f\u6210\u3001\u63d0\u793a\u548c\u903b\u8f91\u63a7\u5236\u4ea4\u9519\u5230\u5355\u4e00\u8fde\u7eed\u6d41\u4e2d\uff0c\u4e0e\u8bed\u8a00\u6a21\u578b\u5b9e\u9645\u5904\u7406\u6587\u672c\u7684\u65b9\u5f0f\u76f8\u5339\u914d\u3002\u50cf\"Chain of Thought\"\u53ca\u5176\u8bb8\u591a\u53d8\u4f53\uff08\u4f8b\u5982ART\u3001Auto-CoT\u7b49\uff09\u8fd9\u6837\u7684\u7b80\u5355\u8f93\u51fa\u7ed3\u6784\u5df2\u88ab\u8bc1\u660e\u80fd\u6539\u5584\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002\u66f4\u5f3a\u5927\u7684\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-4\uff09\u7684\u51fa\u73b0\u4f7f\u5f97\u66f4\u4e30\u5bcc\u7684\u7ed3\u6784\u6210\u4e3a\u53ef\u80fd\uff0c\u800c\u5f15\u5bfc\u5219\u4f7f\u5f97\u6784\u5efa\u8fd9\u79cd\u7ed3\u6784\u53d8\u5f97\u66f4\u52a0\u5bb9\u6613\u548c\u7ecf\u6d4e\u3002|[github](https://github.com/microsoft/guidance)|\n|WizardLM|\u8d4b\u4e88\u5927\u578b\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u9075\u5faa\u590d\u6742\u6307\u4ee4\u7684\u80fd\u529b\uff0c\u4f7f\u7528\u5b8c\u6574\u8fdb\u5316\u6307\u4ee4\uff08\u7ea6300k\uff09\u8bad\u7ec3\u7684WizardLM-7B\u6a21\u578b|[github](https://github.com/nlpxucan/WizardLM)|\n\n# LLM\u7684\u8bad\u7ec3_\u63a8\u7406_\u4f4e\u8d44\u6e90_\u9ad8\u6548\u8bad\u7ec3\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :----   |          :--- |\n|QLoRA--Guanaco|\u4e00\u79cd\u9ad8\u6548\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u5355\u4e2a48GB\u7684GPU\u4e0a\u5fae\u8c03\u4e00\u4e2a\u62e5\u670965B\u53c2\u6570\u7684\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u5b8c\u6574\u768416\u4f4d\u5fae\u8c03\u4efb\u52a1\u6027\u80fd\uff0c\u5e76\u901a\u8fc7QLoRA\u5c06\u68af\u5ea6\u53cd\u5411\u4f20\u64ad\u901a\u8fc7\u4e00\u4e2a\u51bb\u7ed3\u7684\u30014\u4f4d\u91cf\u5316\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5230\u4f4e\u79e9\u9002\u914d\u5668\uff08LoRA\uff09|[github](https://github.com/artidoro/qlora)|\n|Chinese-Guanaco|\u4e00\u4e2a\u4e2d\u6587\u4f4e\u8d44\u6e90\u7684\u91cf\u5316\u8bad\u7ec3/\u90e8\u7f72\u65b9\u6848|[github](https://github.com/jianzhnie/Chinese-Guanaco)|\n|   DeepSpeed Chat: \u4e00\u952e\u5f0fRLHF\u8bad\u7ec3  |       |   [github](http://github.com/microsoft/DeepSpeed/)    |\n|   LLMTune: \u5728\u6d88\u8d39\u7ea7GPU\u4e0a\u5fae\u8c03\u5927\u578b65B+LLM   |   \u53ef\u4ee5\u5728\u666e\u901a\u6d88\u8d39\u7ea7GPU\u4e0a\u8fdb\u884c4\u4f4d\u5fae\u8c03\uff0c\u4f8b\u5982\u6700\u5927\u768465B LLAMA\u6a21\u578b\u3002LLMTune\u8fd8\u5b9e\u73b0\u4e86LoRA\u7b97\u6cd5\u548cGPTQ\u7b97\u6cd5\u6765\u538b\u7f29\u548c\u91cf\u5316LLM\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u5e76\u884c\u5904\u7406\u5927\u578b\u6a21\u578b\u3002\u6b64\u5916\uff0cLLMTune\u63d0\u4f9b\u4e86\u547d\u4ee4\u884c\u754c\u9762\u548cPython\u5e93\u7684\u4f7f\u7528\u65b9\u5f0f    |  [github](https://github.com/kuleshov-group/llmtune)     |\n|  \u57fa\u4e8eChatGLM-6B+LoRA\u5728\u6307\u4ee4\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03    |  \u57fa\u4e8edeepspeed\u652f\u6301\u591a\u5361\u5fae\u8c03\uff0c\u901f\u5ea6\u76f8\u6bd4\u5355\u5361\u63d0\u53478-9\u500d\u5177\u4f53\u8bbe\u7f6e\u53ef\u89c1 \u5fae\u8c033 \u57fa\u4e8eDeepSpeed\u8fdb\u884cLora\u5fae\u8c03     |    [github](https://github.com/yanqiangmiffy/InstructGLM)   |\n|  \u5fae\u8f6f\u53d1\u5e03RLHF\u8bad\u7ec3\u5de5\u5177DeepSpeed Chat    |       |  [github](https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat)     |\n|  LlamaChat\uff1aMac\u4e0a\u57fa\u4e8eLLaMa\u7684\u804a\u5929\u673a\u5668\u4eba    |       |  [github](https://github.com/alexrozanski/LlamaChat)     |\n|   ChatGPT/GPT4\u5f00\u6e90\u201c\u5e73\u66ff\u201d\u4eec   |       |   [github](https://github.com/chenking2020/FindTheChatGPTer)    |\n|\u8bad\u7ec3\u5927\u578b\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u5b9e\u7528\u5efa\u8bae\u548c\u6280\u5de7|\u5e2e\u52a9\u60a8\u8bad\u7ec3\u5927\u578b\u6a21\u578b\uff08>1B \u53c2\u6570\uff09\u3001\u907f\u514d\u4e0d\u7a33\u5b9a\u6027\u3001\u4fdd\u5b58\u5f00\u59cb\u5931\u8d25\u7684\u5b9e\u9a8c\u800c\u4e0d\u4ece 0 \u91cd\u65b0\u5f00\u59cb|[link](https://wandb.ai/craiyon/report/reports/Recipe-Training-Large-Models--VmlldzozNjc4MzQz)|\n|  Instruction Tuning with GPT-4    |       |   [paper](https://arxiv.org/abs/2304.03277)    |\n|   xturing   |   \u4e00\u4e2aPython\u8f6f\u4ef6\u5305\uff0c\u7528\u4e8e\u9ad8\u6548\u3001\u5feb\u901f\u3001\u7b80\u5355\u5730\u5fae\u8c03LLM\u6a21\u578b\uff0c\u652f\u6301LLaMA\u3001GPT-J\u3001GPT-2\u7b49\u591a\u79cd\u6a21\u578b\uff0c\u53ef\u4f7f\u7528\u5355GPU\u548c\u591aGPU\u8bad\u7ec3\uff0c\u4f7f\u7528LoRA\u7b49\u9ad8\u6548\u5fae\u8c03\u6280\u672f\u53ef\u5c06\u786c\u4ef6\u6210\u672c\u964d\u4f4e\u9ad8\u8fbe90%\uff0c\u5e76\u5728\u77ed\u65f6\u95f4\u5185\u5b8c\u6210\u6a21\u578b\u8bad\u7ec3   |   [github]( github.com/stochasticai/xturing)    |\n|   GPT4All   |    \u4e00\u4e2a\u5141\u8bb8\u5728Macbook\u672c\u5730\u8fd0\u884cGPT\u7684\u5f00\u6e90\u9879\u76ee\u3002\u57fa\u4e8eLLaMa-7B\u5927\u8bed\u8a00\u6a21\u578b\u6253\u9020\uff0c\u5305\u62ec\u6570\u636e\u3001\u4ee3\u7801\u548cdemo\u90fd\u662f\u5f00\u6e90\u7684\uff0c\u5bf9\u8bdd\u98ce\u683c\u504f\u5411AI\u52a9\u7406   |    [github](https://github.com/nomic-ai/gpt4all)   |\n|   \u7528Alpaca-LoRA\u5fae\u8c03ChatGPT\u7c7b\u6a21\u578b   |       |  [link](https://replicate.com/blog/fine-tune-alpaca-with-lora)     |\n|  LMFlow    |   \u53ef\u6269\u5c55\u3001\u65b9\u4fbf\u6709\u6548\u7684\u5de5\u5177\u7bb1\uff0c\u7528\u4e8e\u5fae\u8c03\u5927\u578b\u673a\u5668\u5b66\u4e60\u6a21\u578b    |   [github](https://github.com/OptimalScale/LMFlow)    |\n|\u95fb\u8fbe\uff1a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8c03\u7528\u5e73\u53f0|\u76ee\u524d\u652f\u6301chatGLM-6B\u3001chatRWKV\u3001chatYuan\u548cchatGLM-6B\u6a21\u578b\u4e0b\u7684chatPDF\uff08\u81ea\u5efa\u77e5\u8bc6\u5e93\u67e5\u627e\uff09' |[github](https://github.com/l15y/wenda)|\n|Micro Agent|\u5c0f\u578b\u81ea\u4e3b\u667a\u80fd\u4f53\u5f00\u6e90\u9879\u76ee\uff0c\u7531LLM(OpenAI GPT-4)\u63d0\u4f9b\u52a8\u529b\uff0c\u53ef\u4ee5\u4e3a\u4f60\u7f16\u5199\u8f6f\u4ef6\uff0c\u53ea\u9700\u8bbe\u7f6e\u4e00\u4e2a\u201c\u76ee\u7684\u201d\uff0c\u8ba9\u5b83\u81ea\u5df1\u5de5\u4f5c|[github](https://github.com/pHaeusler/micro-agent )|\n|Llama-X|\u5f00\u6e90\u7684\u5b66\u672f\u7814\u7a76\u9879\u76ee\uff0c\u901a\u8fc7\u793e\u533a\u5171\u540c\u52aa\u529b\uff0c\u9010\u6b65\u5c06LLaMA\u7684\u6027\u80fd\u63d0\u9ad8\u5230SOTA LLM\u6c34\u5e73\uff0c\u8282\u7701\u91cd\u590d\u5de5\u4f5c\uff0c\u5171\u540c\u521b\u9020\u66f4\u591a\u3001\u66f4\u5feb\u7684\u589e\u91cf|[github](https://github.com/AetherCortex/Llama-X)|\n|Chinese-LLaMA-Alpaca|\u4e2d\u6587LLaMA&Alpaca\u5927\u8bed\u8a00\u6a21\u578b+\u672c\u5730\u90e8\u7f72 (Chinese LLaMA & Alpaca LLMs) - \u5f00\u6e90\u4e86\u7ecf\u8fc7\u4e2d\u6587\u6587\u672c\u6570\u636e\u9884\u8bad\u7ec3\u7684\u4e2d\u6587LLaMA\u5927\u6a21\u578b\uff1b\u5f00\u6e90\u4e86\u8fdb\u4e00\u6b65\u7ecf\u8fc7\u6307\u4ee4\u7cbe\u8c03\u7684\u4e2d\u6587Alpaca\u5927\u6a21\u578b\uff1b\u5feb\u901f\u5730\u4f7f\u7528\u7b14\u8bb0\u672c\u7535\u8111\uff08\u4e2a\u4ebaPC\uff09\u672c\u5730\u90e8\u7f72\u548c\u4f53\u9a8c\u91cf\u5316\u7248\u5927\u6a21\u578b| [github](https://github.com/ymcui/Chinese-LLaMA-Alpaca) |\n|Efficient Alpaca|\u57fa\u4e8eLLaMA\u5b9e\u73b0\u7684\u5f00\u6e90\u9879\u76ee\uff0c\u65e8\u5728\u901a\u8fc7\u5fae\u8c03 LLaMA-7B\u6a21\u578b\u5728\u8d44\u6e90\u6d88\u8017\u66f4\u5c11\u3001\u63a8\u7406\u901f\u5ea6\u66f4\u5feb\u3001\u66f4\u9002\u5408\u7814\u7a76\u8005\u4f7f\u7528\u65b9\u9762\u63d0\u9ad8Stanford Alpaca\u7684\u6027\u80fd|[github](https://github.com/dropreg/efficient_alpaca)|\n|ChatGLM-6B-Slim|\u88c1\u51cf\u638920K\u56fe\u7247Token\u7684ChatGLM-6B\uff0c\u5b8c\u5168\u4e00\u6837\u7684\u6027\u80fd\uff0c\u5360\u7528\u66f4\u5c0f\u7684\u663e\u5b58| [github](https://github.com/silverriver/ChatGLM-6B-Slim) |\n|Chinese-Vicuna|\u4e00\u4e2a\u4e2d\u6587\u4f4e\u8d44\u6e90\u7684llama+lora\u65b9\u6848| [github](https://github.com/Facico/Chinese-Vicuna ) |\n|Alpaca-LoRA|\u7528LoRA\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u590d\u73b0\u65af\u5766\u798fAlpaca\u7684\u7ed3\u679c|[github](https://ggithub.com/official-elinas/alpaca-lora-optimized)|\n|LLM Accelerator|\u8ba9\u57fa\u7840\u5927\u6a21\u578b\u66f4\u806a\u660e\u7684LLM Accelerator\u6765\u4e86\uff01\u57fa\u7840\u5927\u6a21\u578b\u6b63\u5728\u8bf8\u591a\u5e94\u7528\u4e2d\u53d1\u6325\u7740\u65e5\u76ca\u91cd\u8981\u7684\u4f5c\u7528\u3002\u5927\u591a\u6570\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u90fd\u662f\u91c7\u53d6\u81ea\u56de\u5f52\u7684\u65b9\u5f0f\u8fdb\u884c\u751f\u6210\uff0c\u867d\u7136\u81ea\u56de\u5f52\u6a21\u578b\u751f\u6210\u7684\u6587\u672c\u8d28\u91cf\u6709\u6240\u4fdd\u8bc1\uff0c\u4f46\u5374\u5bfc\u81f4\u4e86\u9ad8\u6602\u7684\u63a8\u7406\u6210\u672c\u548c\u957f\u65f6\u95f4\u7684\u5ef6\u8fdf\u3002\u7531\u4e8e\u5927\u6a21\u578b\u7684\u53c2\u6570\u91cf\u5de8\u5927\u3001\u63a8\u7406\u6210\u672c\u9ad8\uff0c\u56e0\u6b64\u5982\u4f55\u5728\u5927\u89c4\u6a21\u90e8\u7f72\u5927\u6a21\u578b\u7684\u8fc7\u7a0b\u4e2d\u964d\u4f4e\u6210\u672c\u3001\u51cf\u5c0f\u5ef6\u8fdf\u662f\u4e00\u4e2a\u5173\u952e\u8bfe\u9898\u3002\u9488\u5bf9\u6b64\u95ee\u9898\uff0c\u5fae\u8f6f\u4e9a\u6d32\u7814\u7a76\u9662\u7684\u7814\u7a76\u5458\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u53c2\u8003\u6587\u672c\u65e0\u635f\u52a0\u901f\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u65b9\u6cd5 LLM Accelerator\uff0c\u5728\u5927\u6a21\u578b\u5178\u578b\u7684\u5e94\u7528\u573a\u666f\u4e2d\u53ef\u4ee5\u53d6\u5f97\u4e24\u5230\u4e09\u500d\u7684\u52a0\u901f\u3002|[blog](https://weibo.com/ttarticle/p/show?id=2309404902475139252775)|\n|\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5fae\u8c03\u6280\u672f\u7b14\u8bb0||[github](https://github.com/ninehills/ninehills.github.io/issues/92)|\n|PyLLMs|\u7b80\u6d01\u7684 Python \u5e93\uff0c\u7528\u4e8e\u8fde\u63a5\u5404\u79cd LLM(OpenAI\u3001Anthropic\u3001Google\u3001AI21\u3001Cohere\u3001Aleph Alpha\u3001HuggingfaceHub)\uff0c\u5185\u7f6e\u6a21\u578b\u6027\u80fd\u57fa\u51c6\u3002\u975e\u5e38\u9002\u5408\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u548c\u8bc4\u4f30\u4e0d\u540c\u6a21\u578b\uff0c\u5177\u6709\u4ee5\u4e0b\u7279\u70b9\uff1a\u901a\u8fc7\u5c11\u91cf\u4ee3\u7801\u8fde\u63a5\u9876\u7ea7 LLM\uff1b\u54cd\u5e94\u5143\u6570\u636e\u5305\u62ec\u5904\u7406\u7684Token\u3001\u6210\u672c\u548c\u5ef6\u8fdf\uff0c\u5bf9\u5404\u4e2a\u6a21\u578b\u8fdb\u884c\u6807\u51c6\u5316\uff1b\u652f\u6301\u591a\u6a21\u578b\uff1a\u540c\u65f6\u4ece\u4e0d\u540c\u6a21\u578b\u83b7\u53d6\u8865\u5168\uff1bLLM \u57fa\u51c6\uff1a\u8bc4\u4f30\u6a21\u578b\u7684\u8d28\u91cf\u3001\u901f\u5ea6\u548c\u6210\u672c|[github](https://github.com/kagisearch/pyllms)|\n|\u7528\u6df7\u5408\u7cbe\u5ea6\u52a0\u901f\u5927\u578b\u8bed\u8a00\u6a21\u578b|\u901a\u8fc7\u4f7f\u7528\u4f4e\u7cbe\u5ea6\u6d6e\u70b9\u6570\u8fd0\u7b97\uff0c\u53ef\u4ee5\u5c06\u8bad\u7ec3\u548c\u63a8\u65ad\u901f\u5ea6\u63d0\u5347\u591a\u8fbe3\u500d\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u6a21\u578b\u51c6\u786e\u6027|[blog](https://sebastianraschka.com/blog/2023/llm-mixed-precision.html)|\n|\u65b0\u7684LLM\u8bad\u7ec3\u65b9\u6cd5 Federate|\u675c\u514b\u5927\u5b66\u548c\u5fae\u8f6f\u4e00\u8d77\u53d1\u5e03\u4e86\u4e00\u4e2a\u65b0\u7684LLM\u8bad\u7ec3\u65b9\u6cd5 Federated GPT\uff0c\u8fd9\u4e2a\u8bad\u7ec3\u65b9\u6cd5\u662f\u5c06\u539f\u672c\u4e2d\u5fc3\u5316\u7684\u8bad\u7ec3\u65b9\u6cd5\u5206\u6563\u5230\u4e0d\u540c\u7684\u8fb9\u7f18\u8bbe\u5907\u91cc\u9762\uff08edge device\uff09\uff0c\u7136\u540e\u8bad\u7ec3\u5b8c\u6210\u540e\uff0c\u518d\u4e0a\u4f20\u5230\u4e2d\u5fc3\u53bb\u5c06\u5404\u5b50\u6a21\u578b\u5408\u5e76\u3002|[github](https://github.com/JayZhang42/FederatedGPT-Shepherd)|\n\n# \u63d0\u793a\u5de5\u7a0b\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :----   |          :--- |\n|   OpenBuprompt-engineering-note  |   \u63d0\u793a\u5de5\u7a0b\u7b14\u8bb0(\u8bfe\u7a0b\u603b\u7ed3)\u300b\u4ecb\u7ecd\u4e86\u9762\u5411\u5f00\u53d1\u8005\u7684 ChatGPT Prompt Engineering Learning Notes \u8bfe\u7a0b\uff0c\u8be5\u8bfe\u7a0b\u63d0\u4f9b\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u5de5\u4f5c\u539f\u7406\u548c\u63d0\u793a\u5de5\u7a0b\u5b9e\u8df5\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u8bed\u8a00\u6a21\u578b API \u5e94\u7528\u4e8e\u5404\u79cd\u4efb\u52a1\u7684\u5e94\u7528\u7a0b\u5e8f\u4e2d\u3002\u8bfe\u7a0b\u5305\u62ec\u603b\u7ed3\u3001\u63a8\u65ad\u3001\u8f6c\u6362\u3001\u6269\u5c55\u548c\u6253\u9020\u804a\u5929\u673a\u5668\u4eba\u7b49\u65b9\u9762\u7684\u5185\u5bb9\uff0c\u5e76\u8bb2\u8ff0\u4e86\u5982\u4f55\u8bbe\u8ba1\u597d\u7684\u63d0\u793a\u548c\u6784\u5efa\u81ea\u5b9a\u4e49\u804a\u5929\u673a\u5668\u4eba\u3002  |  [github-OpenBuprompt](https://islinxu.github.io/prompt-engineering-note/)  |\n|  \u63d0\u793a\u5de5\u7a0b\u6307\u5357    |       |   [link](https://www.promptingguide.ai/zh)    |\n|  AIGC\u63d0\u793a\u5de5\u7a0b\u5b66\u4e60\u7ad9 Learn Prompt   |  ChatGPT/Midjourney/Runway     |  [link](https://www.learnprompt.pro/)     |\n|  Prompts \u7cbe\u9009 - ChatGPT \u4f7f\u7528\u6307\u5357    |   ChatGPT \u4f7f\u7528\u6307\u5357\uff0c\u63d0\u5347 ChatGPT \u53ef\u73a9\u6027\u548c\u53ef\u7528\u6027    |  [github](https://github.com/yzfly/wonderful-prompts)     |\n|   \u975e\u5b98\u65b9\u7684ChatGPT\u8d44\u6e90\u805a\u5408\u5217\u8868\uff0c\u65e8\u5728\u6c47\u603b\u4f7f\u7528ChatGPT   |    \u65e8\u5728\u6c47\u603b\u4f7f\u7528ChatGPT\u7684\u5e94\u7528\u3001Web\u5e94\u7528\u3001\u6d4f\u89c8\u5668\u6269\u5c55\u3001CLI\u5de5\u5177\u3001\u673a\u5668\u4eba\u3001\u96c6\u6210\u3001\u8f6f\u4ef6\u5305\u3001\u6587\u7ae0\u7b49\u8d44\u6e90   |   [github](https://github.com/sindresorhus/awesome-chatgpt)    |\n| Snack Prompt\uff1aChatGPT Prompt\u63d0\u793a\u5206\u4eab\u793e\u533a   |       |    [link](https://snackprompt.com/)   |\n|   ChatGPT\u63d0\u95ee\u6280\u5de7   |  \u5982\u4f55\u5411 ChatGPT \u63d0\u95ee\u4ee5\u83b7\u5f97\u9ad8\u8d28\u91cf\u7b54\u6848\uff1a\u63d0\u793a\u6280\u5de7\u5de5\u7a0b\u5b8c\u5168\u6307\u5357     |   [github](https://github.com/ORDINAND/The-Art-of-Asking-ChatGPT-for-High-Quality-Answers-A-complete-Guide-to-Prompt-Engineering-Technique )    |\n|   rompt-Engineering-Guide-Chinese - \u63d0\u793a\u5de5\u7a0b\u5e08\u6307\u5357   |     \u6e90\u81ea\u82f1\u6587\u7248\uff0c\u4f46\u589e\u52a0\u4e86AIGC\u7684prompt\u90e8\u5206  |   [github](https://github.com/wangxuqi/Prompt-Engineering-Guide-Chinese)    |\n|  OpenPrompt    |  \u4e00\u4e2a\u5f00\u653e\u7684\u5171\u4eabPrompt\u793e\u533a\uff0c\u5927\u5bb6\u4e00\u8d77\u63a8\u8350\u597d\u7528\u7684prompt     |  [github](https://github.com/timqian/openprompt.co)     |\n|  GPT-Prompts    |    \u6559\u4f60\u5982\u4f55\u7528GPT\u751f\u6210Prompts   | [github](https://github.com/jesselau76/GPT-Prompts)      |\n\n\n# \u7c7bChatGPT\u7684\u6587\u6863\u95ee\u7b54\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :----   |          :--- |\n|   privateGPT    |    \u57fa\u4e8eGPT4All-J\u7684\u79c1\u6709\u5316\u90e8\u7f72\u6587\u6863\u95ee\u7b54\u5e73\u53f0\uff0c\u65e0\u9700\u8054\u7f51\uff0c\u80fd100%\u4fdd\u8bc1\u7528\u6237\u7684\u9690\u79c1\u4e0d\u6cc4\u9732\u3002\u63d0\u4f9b\u4e86\u4e00\u4e2aAPI\uff0c\u7528\u6237\u53ef\u4ee5\u4f7f\u7528\u81ea\u5df1\u7684\u6587\u6863\u8fdb\u884c\u4ea4\u4e92\u5f0f\u95ee\u7b54\u548c\u751f\u6210\u6587\u672c\u3002\u6b64\u5916\uff0c\u5e73\u53f0\u652f\u6301\u81ea\u5b9a\u4e49\u8bad\u7ec3\u6570\u636e\u548c\u6a21\u578b\u53c2\u6570\uff0c\u4ee5\u6ee1\u8db3\u4e2a\u6027\u5316\u9700\u6c42    |  [github-privateGPT](https://github.com/imartinez/privateGPT)  |\n|  Auto-evaluator   |  \u6587\u6863\u95ee\u7b54\u7684\u81ea\u52a8\u8bc4\u4f30 \uff1b\u3001     |  [github](https://github.com/langchain-ai/auto-evaluator)    |\n|   PDF GP   |   \u4e00\u4e2a\u57fa\u4e8e GPT \u5b9e\u73b0\u7684\u5f00\u6e90 PDF \u6587\u6863\u804a\u5929\u65b9\u6848,\u4e3b\u8981\u5b9e\u73b0\u4ee5\u4e0b\u529f\u80fd\uff1a\u8ddf PDF \u6587\u6863\u8fdb\u884c\u4e00\u5bf9\u4e00\u5bf9\u8bdd\uff1b\u81ea\u52a8\u5207\u5272\u5185\u5bb9\uff0c\u5e76\u4f7f\u7528\u5f3a\u5927\u7684\u6df1\u5ea6\u5e73\u5747\u7f51\u7edc\u7f16\u7801\u5668\u6765\u751f\u6210\u5d4c\u5165\uff1b\u5bf9 PDF \u5185\u5bb9\u6267\u884c\u8bed\u4e49\u641c\u7d22\uff0c\u5e76\u5c06\u6700\u76f8\u5173\u7684\u5d4c\u5165\u4f20\u9012\u7ed9 Open AI\uff1b\u81ea\u5b9a\u4e49\u903b\u8f91\uff0c\u751f\u6210\u66f4\u7cbe\u786e\u7684\u54cd\u5e94\u4fe1\u606f\uff0c\u901f\u5ea6\u8981\u6bd4 OpenAI \u7684\u5feb\u3002    |    [github](https://github.com/bhaskatripathi/pdfGPT)   |\n|Redis-LLM-Document-Chat|\u7528LlamaIndex\u3001Redis\u548cOpenAI\u4e0ePDF\u6587\u6863\u8fdb\u884c\u4ea4\u4e92\uff0c\u5305\u542b\u4e00\u4e2aJupyter\u7b14\u8bb0\u672c\uff0c\u6f14\u793a\u4e86\u5982\u4f55\u4f7f\u7528Redis\u4f5c\u4e3a\u5411\u91cf\u6570\u636e\u5e93\u6765\u5b58\u50a8\u548c\u68c0\u7d22\u6587\u6863\u5411\u91cf\uff0c\u8fd8\u5c55\u793a\u4e86\u5982\u4f55\u4f7f\u7528LlamaIndex\u5728\u6587\u6863\u4e2d\u6267\u884c\u8bed\u4e49\u641c\u7d22\uff0c\u4ee5\u53ca\u5982\u4f55\u5229\u7528OpenAI\u63d0\u4f9b\u7c7b\u4f3c\u804a\u5929\u673a\u5668\u4eba\u7684\u4f53\u9a8c|[github](https://github.com/RedisVentures/LLM-Document-Chat)|\n|doc-chatbot|GPT-4 + Pinecone + LangChain + MongoDB\u5b9e\u73b0\u7684\u6587\u6863\u804a\u5929\u673a\u5668\u4eba\uff0c\u53ef\u591a\u6587\u4ef6\u3001\u591a\u8bdd\u9898\u548c\u591a\u7a97\u53e3\u804a\u5929\uff0c\u804a\u5929\u5386\u53f2\u7531MongoDB\u4fdd\u5b58|[github](https://github.com/dissorial/doc-chatbot )|\n|document.ai|\u57fa\u4e8e\u5411\u91cf\u6570\u636e\u5e93\u4e0eGPT3.5\u7684\u901a\u7528\u672c\u5730\u77e5\u8bc6\u5e93\u65b9\u6848(A universal local knowledge base solution based on vector database and GPT3.5)|[github](https://github.com/GanymedeNil/document.ai)|\n|DocsGPT|DocsGPT\u662f\u4e00\u79cd\u5c16\u7aef\u7684\u5f00\u6e90\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u4ee5\u7b80\u5316\u5728\u9879\u76ee\u6587\u6863\u4e2d\u67e5\u627e\u4fe1\u606f\u7684\u8fc7\u7a0b\u3002\u901a\u8fc7\u96c6\u6210\u5f3a\u5927\u7684GPT\u6a21\u578b\uff0c\u5f00\u53d1\u4eba\u5458\u53ef\u4ee5\u8f7b\u677e\u5730\u63d0\u51fa\u5173\u4e8e\u9879\u76ee\u7684\u95ee\u9898\u5e76\u83b7\u5f97\u51c6\u786e\u7684\u7b54\u6848\u3002|[github](https://github.com/arc53/DocsGPT)|\n|ChatGPT Retrieval Plugin|ChatGPT\u68c0\u7d22\u63d2\u4ef6\u5b58\u50a8\u5e93\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u4ee5\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u5bf9\u4e2a\u4eba\u6216\u7ec4\u7ec7\u6587\u6863\u8fdb\u884c\u8bed\u4e49\u641c\u7d22\u548c\u68c0\u7d22\u3002|[github](https://github.com/openai/chatgpt-retrieval-plugin)|\n|LamaIndex|lamaIndex\uff08GPT\u7d22\u5f15\uff09\u662f\u60a8\u7684LLM\u5e94\u7528\u7a0b\u5e8f\u7684\u6570\u636e\u6846\u67b6\u3002|[github](https://github.com/jerryjliu/llama_index)|\n|chatWeb|ChatWeb\u53ef\u4ee5\u722c\u53d6\u4efb\u610f\u7f51\u9875\u6216PDF\uff0cDOCX\uff0cTXT\u6587\u4ef6\u5e76\u63d0\u53d6\u6b63\u6587\uff0c\u53ef\u4ee5\u751f\u6210\u5d4c\u5165\u5f0f\u6982\u8981\uff0c\u53ef\u4ee5\u6839\u636e\u6b63\u6587\u5185\u5bb9\u56de\u7b54\u4f60\u7684\u95ee\u9898\u3002 \u57fa\u4e8egpt3.5\u7684chatAPI\u548cembeddingAPI\uff0c\u4ee5\u53ca\u5411\u91cf\u6570\u636e\u5e93\u5b9e\u73b0\u3002|[github](https://github.com/SkywalkerDarren/chatWeb)|\n\n# \u7c7bChatGPT\u7684\u884c\u4e1a\u5e94\u7528\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :----   |          :--- |\n|   \u65b0\u95fb\u62a5\u9053\u8fdb\u884c\u60c5\u611f\u5206\u6790    |  \u7528ChatGPT\u901a\u8fc7\u5bf9\u4e0a\u5e02\u516c\u53f8\u7684\u65b0\u95fb\u62a5\u9053\u8fdb\u884c\u60c5\u611f\u5206\u6790\uff0c\u572815\u4e2a\u6708\u65f6\u95f4\u5185\u5728\u80a1\u7968\u5e02\u573a(\u4ea4\u6613\u671f\u6743)\u4ea7\u751f\u4e86500%\u7684\u56de\u62a5\uff08\u5728\u5386\u53f2\u6570\u636e\u4e2d\u6d4b\u8bd5\u5f97\u51fa\u7684\u7ed3\u679c\uff09\u2014\u2014\u63a2\u8ba8\u4e86ChatGPT\u5728\u5229\u7528\u65b0\u95fb\u6807\u9898\u7684\u60c5\u611f\u5206\u6790\u6765\u9884\u6d4b\u80a1\u5e02\u56de\u62a5\u65b9\u9762\u7684\u6f5c\u529b\u3002\u53d1\u73b0ChatGPT\u7684\u60c5\u611f\u5206\u6790\u80fd\u529b\u8d85\u8fc7\u4e86\u4f20\u7edf\u7684\u65b9\u6cd5\uff0c\u5e76\u4e14\u4e0e\u80a1\u5e02\u56de\u62a5\u5448\u6b63\u76f8\u5173\u3002\u63d0\u51faChatGPT\u5728\u91d1\u878d\u7ecf\u6d4e\u9886\u57df\u6709\u5f88\u5927\u7684\u4ef7\u503c\uff0c\u5e76\u5bf9\u672a\u6765\u7684\u7814\u7a76\u548c\u5e94\u7528\u63d0\u51fa\u4e86\u4e00\u4e9b\u542f\u793a\u548c\u5efa\u8bae   |  [paper](https://arxiv.org/abs/2304.07619)  |\n|  \u7f16\u7a0b\u8bed\u8a00\u751f\u6210\u6a21\u578b StarCoder    |   BigCode\u662f ServiceNow Inc. \u548c Hugging Face Inc. \u5408\u4f5c\u6210\u7acb\u7684\u3002StarCoder \u6709\u591a\u4e2a\u7248\u672c\u3002\u6838\u5fc3\u7248\u672c StarCoderBase \u5177\u6709 155 \u4ebf\u4e2a\u53c2\u6570\uff0c\u652f\u630180\u591a\u79cd\u7f16\u7a0b\u8bed\u8a00\uff0c8192\u4e2atoken\u7684\u4e0a\u4e0b\u6587\u3002\u89c6\u9891\u4e3a\u5176vscode\u63d2\u4ef6\u6548\u679c    |    [github](https://github.com/bigcode-project/starcoder)   |\n|  CodeGen2: Lessons for Training LLMs on Programming and Natural Languages    |   code generation    |    [paper](https://arxiv.org/abs/2305.02309)   |\n|  MedicalGPT-zh\uff1a\u4e2d\u6587\u533b\u7597\u901a\u7528\u8bed\u8a00\u6a21\u578b    |   \u4e2d\u6587\u533b\u7597\u901a\u7528\u8bed\u8a00\u6a21\u578b\uff0c\u57fa\u4e8e28\u4e2a\u79d1\u5ba4\u7684\u533b\u7597\u5171\u8bc6\u4e0e\u4e34\u5e8a\u6307\u5357\u6587\u672c\uff0c\u63d0\u9ad8\u6a21\u578b\u7684\u533b\u7597\u9886\u57df\u77e5\u8bc6\u4e0e\u5bf9\u8bdd\u80fd\u529b    |    [github](https://github.com/MediaBrain-SJTU/MedicalGPT-zh)   |\n|  MagicSlides    |    \u4e0d\u5c11\u4eba\u68a6\u5bd0\u4ee5\u6c42\u7684AI\u81ea\u4f5cPPT\uff0c\u514d\u8d39\u7248\u6bcf\u6708\u80fd\u505a3\u4e2aPPT\uff0c\u652f\u63012500\u5b57\u8f93\u5165   |    [link](https://www.magicslides.app/)   |\n|   SalesGPT   |   \u4f7f\u7528LLM\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u9500\u552e\u52a9\u624b\uff0c\u53ef\u81ea\u52a8\u5316\u9500\u552e\u62d3\u5c55\u4ee3\u8868\u7684\u6d3b\u52a8\uff0c\u5982\u5916\u547c\u9500\u552e\u7535\u8bdd    |   [github](https://github.com/filip-michalsky/SalesGPT)    |\n|  \u534e\u9a7c(HuaTuo): \u57fa\u4e8e\u4e2d\u6587\u533b\u5b66\u77e5\u8bc6\u7684LLaMA\u5fae\u8c03\u6a21\u578b    |       |   [github](https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese)    |\n|  ai-code-translator    |  \u5e2e\u52a9\u4f60\u628a\u4ee3\u7801\u4ece\u4e00\u79cd\u8bed\u8a00\u7ffb\u8bd1\u6210\u53e6\u4e00\u79cd\u8bed\u8a00\uff0c\u8fd9\u4e8b\u5bf9ChatGPT\u6765\u8bf4\u7b80\u76f4\u592a\u64c5\u957f\u4e86\uff0c\u5c24\u5176\u662fGPT-4\uff0c\u7ffb\u8bd1\u8d28\u91cf\u76f8\u5f53\u9ad8\uff0c\u800c\u4e14tokens\u957f\u5ea6\u4e5f\u53ef\u4ee5\u66f4\u957f\u3002     |    [github](https://github.com/mckaywrigley/ai-code-translator)   |\n|   ChatGenTitle   |    \u4f7f\u7528\u767e\u4e07arXiv\u8bba\u6587\u4fe1\u606f\u5728LLaMA\u6a21\u578b\u4e0a\u8fdb\u884c\u5fae\u8c03\u7684\u8bba\u6587\u9898\u76ee\u751f\u6210\u6a21\u578b   |    [github](https://github.com/WangRongsheng/ChatGenTitle)   |\n|   Regex.ai    |    \u4e00\u6b3e\u6240\u89c1\u5373\u6240\u5f97\u7684\uff0c\u57fa\u4e8e AI \u7684\u6b63\u5219\u8868\u8fbe\u5f0f\u81ea\u52a8\u751f\u6210\u5de5\u5177\uff0c\u53ea\u9700\u8981\u9009\u62e9\u51fa\u6570\u636e\uff0c\u5b83\u5c31\u80fd\u5e2e\u4f60\u5199\u6b63\u5219\u8868\u8fbe\u5f0f\uff0c\u5e76\u63d0\u4f9b\u591a\u79cd\u63d0\u53d6\u6570\u636e\u7684\u65b9\u5f0f   |    [video](https://weibo.com/tv/show/1034:4885032649818161?from=old_pc_videoshow)   |\n|   ChatDoctor   |  \u4e00\u4e2a\u57fa\u4e8e\u533b\u5b66\u9886\u57df\u77e5\u8bc6\u5fae\u8c03LLaMA\u7684\u533b\u5b66\u804a\u5929\u6a21\u578b\uff0c\u5176\u4e2d\u533b\u5b66\u6570\u636e\u5305\u542b\u5927\u7ea6700\u79cd\u75be\u75c5\u7684\u6570\u636e\u3001\u4ee5\u53ca\u5927\u7ea65000\u6bb5\u533b\u751f\u548c\u75c5\u4eba\u7684\u5bf9\u8bdd\u8bb0\u5f55     |   [paper](https://arxiv.org/abs/2303.14070)    |\n|CodeGPT|\u63d0\u9ad8\u7f16\u7a0b\u80fd\u529b\u7684\u5173\u952e\u5728\u4e8e\u6570\u636e\u3002CodeGPT\u662f\u901a\u8fc7GPT\u751f\u6210\u7684\u7528\u4e8eGPT\u7684\u4ee3\u7801\u5bf9\u8bdd\u6570\u636e\u96c6\u3002\u73b0\u5728\u516c\u5f00\u4e8632K\u6761\u4e2d\u6587\u6570\u636e\uff0c\u8ba9\u6a21\u578b\u66f4\u64c5\u957f\u7f16\u7a0b|[github](https://github.com/zxx000728/CodeGPT)|\n|LaWGPT |\u4e00\u7cfb\u5217\u57fa\u4e8e\u4e2d\u6587\u6cd5\u5f8b\u77e5\u8bc6\u7684\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b|[github](https://github.com/pengxiao-song/LawGPT)|\n|LangChain-ChatGLM-Webui|\u53d7langchain-ChatGLM\u542f\u53d1, \u5229\u7528LangChain\u548cChatGLM-6B\u7cfb\u5217\u6a21\u578b\u5236\u4f5c\u7684Webui, \u63d0\u4f9b\u57fa\u4e8e\u672c\u5730\u77e5\u8bc6\u7684\u5927\u6a21\u578b\u5e94\u7528.\u76ee\u524d\u652f\u6301\u4e0a\u4f20 txt\u3001docx\u3001md\u3001pdf\u7b49\u6587\u672c\u683c\u5f0f\u6587\u4ef6, \u63d0\u4f9b\u5305\u62ecChatGLM-6B\u7cfb\u5217\u3001Belle\u7cfb\u5217\u7b49\u6a21\u578b\u6587\u4ef6\u4ee5\u53caGanymedeNil/text2vec-large-chinese\u3001nghuyong/ernie-3.0-base-zh\u3001nghuyong/ernie-3.0-nano-zh\u7b49Embedding\u6a21\u578b.|[github](https://github.com/thomas-yanxin/LangChain-ChatGLM-Webui)|\n\n# \u7c7bChatGPT\u7684\u8bfe\u7a0b\u8d44\u6599\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :----   |          :--- |\n|   Databricks   |   \uff08Dolly\u6a21\u578b\u7684\u4f5c\u8005\uff09\u5728edX\u53d1\u5e03\u4e86\u4e24\u4e2a\u514d\u8d39\u8bfe\u7a0b\u7a0b\uff0c\u5176\u4e2d\u7b2c\u4e8c\u4e2a\u662f\u5173\u4e8eLLM\u662f\u5982\u4f55\u6784\u5efa\u7684\u3002    |   [link](www.edx.org/course/large-language-models-foundation-models-from-the-ground-up)    |\n|   \u5927\u8bed\u8a00\u6a21\u578b\u6280\u672f\u5206\u4eab\u7cfb\u5217   |   \u4e1c\u5317\u5927\u5b66\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5b9e\u9a8c\u5ba4    |   [video](https://weibo.com/l/wblive/p/show/1022:2321324895201478181292)    |\n|  GPT-4\u662f\u5982\u4f55\u5de5\u4f5c\u7684\uff1f\u5982\u4f55\u5229\u7528GPT-4\u6253\u9020\u667a\u80fd\u7a0b\u5e8f\uff1f    |  \u54c8\u4f5b\u5927\u5b66CS50\u516c\u5f00\u8bfe     |   [video](https://weibo.com/tv/show/1034:4897967430107165?from=old_pc_videoshow)    |\n|  \u63d0\u793a\u5de5\u7a0b\u6700\u4f73\u5b9e\u8df5\uff1aAndrew Ng \u63d0\u793a\u5de5\u7a0b\u65b0\u8bfe\u6458\u8981+LangChain\u7ecf\u9a8c\u603b\u7ed3    |       |   [medium_blog](https://sophiamyang.medium.com/best-practices-in-prompt-engineering-a18d6bab904b)    |\n|   \u5fae\u8c03LLM\u6a21\u578b   |   \u5982\u679c\u4f60\u5bf9\u5fae\u8c03LLM\u6a21\u578b\u611f\u5174\u8da3\uff0c\u4e00\u5b9a\u8981\u5173\u6ce8\u8fd9\u4e2a\u6cb9\u7ba1\u535a\u4e3b\uff0c\u4ed6\u628a\u51e0\u4e4e\u4e16\u9762\u4e0a\u6240\u6709\u7684LLM\u6a21\u578b\u90fd\u516c\u5f00\u4e86\u5fae\u8c03\u7684\u65b9\u6cd5\u3002    |   \u6cb9\u7ba1\u535a\u4e3b Sam Witteveen    |\n|Transformer\u7684\u67b6\u6784\u89e3\u8bfb|\u901a\u4fd7\u6613\u61c2\u7684\u4ecb\u7ecd|[youtube1](https://www.youtube.com/watch?v=dichIcUZfOw)[youtube2](https://www.youtube.com/watch?v=mMa2PmYJlCo) [youtube3](https://www.youtube.com/watch?v=gJ9kaJsE78k&t=1s)|\n|Transformer multi head\u673a\u5236\u7684\u89c6\u9891|\u5982\u679c\u60f3\u8981\u771f\u6b63\u7406\u89e3\u6574\u4e2aTransform\u7684\u6bcf\u4e00\u4e2a\u7ec6\u8282\uff0c\u5305\u62ec\u91cc\u9762\u7684\u6570\u5b66\u539f\u7406\uff0c\u53ef\u4ee5\u770b\u4e00\u4e0b\u8fd9\u4e2a\u89c6\u9891\uff0c\u771f\u7684\u662f\u5256\u6790\u5730\u975e\u5e38\u8be6\u7ec6|[youtube](https://www.youtube.com/watch?v=hjesn5pCEYc)|\n|Introduction to Large Language Models | \u5927\u8bed\u8a00\u6a21\u578b\u4ecb\u7ecd|\u4ecb\u7ecd\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08Large Language Models\uff0cLLMs\uff09\u7684\u6982\u5ff5\u3001\u4f7f\u7528\u573a\u666f\u3001\u63d0\u793a\u8c03\u6574\u4ee5\u53caGoogle\u7684Gen AI\u5f00\u53d1\u5de5\u5177\u3002|[youtube](https://www.youtube.com/watch?v=zizonToFXDs)|\n\n# LLM\u7684\u5b89\u5168\u95ee\u9898\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :----   |          :--- |\n|   LLM\u6a21\u578b\u5b89\u5168\u7814\u7a76   |       |   [link](https://www.docdroid.net/KfwKd1y/llm-redteaming-pdf)    |\n|  Chatbot Injections & Exploit    | \u6536\u96c6\u4e86\u4e00\u4e9bChatbot\u6ce8\u5165\u548c\u6f0f\u6d1e\u7684\u4f8b\u5b50\uff0c\u4ee5\u5e2e\u52a9\u4eba\u4eec\u4e86\u89e3Chatbot\u7684\u6f5c\u5728\u6f0f\u6d1e\u548c\u8106\u5f31\u6027\u3002\u6ce8\u5165\u548c\u653b\u51fb\u7684\u65b9\u5f0f\u5305\u62ec\u547d\u4ee4\u6ce8\u5165\u3001\u5b57\u7b26\u7f16\u7801\u3001\u793e\u4ea4\u5de5\u7a0b\u3001\u8868\u60c5\u7b26\u53f7\u3001Unicode\u7b49\u3002\u4ed3\u5e93\u63d0\u4f9b\u4e86\u4e00\u4e9b\u793a\u4f8b\uff0c\u5176\u4e2d\u4e00\u4e9b\u5305\u62ec\u53ef\u7528\u4e8e\u653b\u51fbChatbot\u7684\u8868\u60c5\u7b26\u53f7\u5217\u8868      | [github](https://github.com/Cranot/chatbot-injections-exploits)      |\n|   GPTSecurity   |    \u4e00\u4e2a\u6db5\u76d6\u4e86\u524d\u6cbf\u5b66\u672f\u7814\u7a76\u548c\u5b9e\u8df5\u7ecf\u9a8c\u5206\u4eab\u7684\u793e\u533a\uff0c\u96c6\u6210\u4e86\u751f\u6210\u9884\u8bad\u7ec3 Transformer\uff08GPT\uff09\u3001\u4eba\u5de5\u667a\u80fd\u751f\u6210\u5185\u5bb9\uff08AIGC\uff09\u4ee5\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7b49\u5b89\u5168\u9886\u57df\u5e94\u7528\u7684\u77e5\u8bc6\u3002\u5728\u8fd9\u91cc\uff0c\u60a8\u53ef\u4ee5\u627e\u5230\u5173\u4e8eGPT/AIGC/LLM\u6700\u65b0\u7684\u7814\u7a76\u8bba\u6587\u3001\u535a\u5ba2\u6587\u7ae0\u3001\u5b9e\u7528\u7684\u5de5\u5177\u548c\u9884\u8bbe\u6307\u4ee4\uff08Prompts\uff09\u3002   |   [github](https://github.com/mo-xiaoxi/GPTSecurity)    |\n\n\n# \u591a\u6a21\u6001LLM\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :----   |          :--- |\n|  DeepFloyd IF    |  \u9ad8\u5ea6\u903c\u771f\u4e14\u5177\u6709\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u7684\u6700\u65b0\u5f00\u6e90\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\uff0c\u7531\u4e00\u4e2a\u51bb\u7ed3\u6587\u672c\u7f16\u7801\u5668\u548c\u4e09\u4e2a\u8fde\u7eed\u7684\u50cf\u7d20\u6269\u6563\u6a21\u5757\u7ec4\u6210\uff0c\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u6a21\u578b\uff0c\u6027\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u5728COCO\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u7684FID\u5f97\u5206\u4e3a6.66     |    [github](https://github.com/deep-floyd/IF)   |\n|  Multi-modal GPT    |   \u7528\u591a\u6a21\u6001GPT\u8bad\u7ec3\u4e00\u4e2a\u80fd\u540c\u65f6\u63a5\u6536\u89c6\u89c9\u548c\u8bed\u8a00\u6307\u4ee4\u7684\u804a\u5929\u673a\u5668\u4eba\u3002\u57fa\u4e8eOpenFlamingo\u591a\u6a21\u6001\u6a21\u578b\uff0c\u4f7f\u7528\u5404\u79cd\u5f00\u653e\u6570\u636e\u96c6\u521b\u5efa\u5404\u79cd\u89c6\u89c9\u6307\u5bfc\u6570\u636e\uff0c\u8054\u5408\u8bad\u7ec3\u89c6\u89c9\u548c\u8bed\u8a00\u6307\u5bfc\uff0c\u6709\u6548\u63d0\u9ad8\u6a21\u578b\u6027\u80fd    |   [github](https://github.com/open-mmlab/Multimodal-GPT)    |\n|   AudioGPT   |   Understanding and Generating Speech, Music, Sound, and Talking Head' by AIGC-Audio    |   [github](https://github.com/AIGC-Audio/AudioGPT)    |\n|   text2image-prompt-generator   |    \u57fa\u4e8eGPT-2\u752825\u4e07\u6761Midjourney\u7684promps\u8bad\u7ec3\u51fa\u6765\u7684\u5c0f\u6a21\u578b\uff0c\u53ef\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u7684Midjourney  prompt   |    [link](https://huggingface.co/succinctly/text2image-prompt-generator)  [data](https://huggingface.co/datasets/succinctly/midjourney-prompts) |\n|  \u6c47\u603b6\u4e2aMidjourney\u4ee5\u5916\u7684\u514d\u8d39\u4ee5\u6587\u751f\u56fe\u670d\u52a1\uff1a    |       |   [Bing Image Creator](http://t.cn/A6C1cnVg) [Playground AI](http://t.cn/A6CtFmLN) [DreamStudio](http://t.cn/A6NSI6la) [Pixlr](http://t.cn/A6NSI6li)  [Leonardo AI ](http://t.cn/A6NSI6lS)[Craiyon](http://t.cn/A6NSI6lX)    |\n| BARK   |   \u4e00\u4e2a\u975e\u5e38\u5f3a\u5927\u7684TTS\uff08\u6587\u5b57\u8f6c\u8bed\u97f3\uff09\u9879\u76ee\uff0c\u8fd9\u4e2a\u9879\u76ee\u7684\u7279\u70b9\u662f\uff0c\u5b83\u53ef\u4ee5\u5728\u6587\u5b57\u4e2d\u52a0\u5165\u63d0\u793a\u8bcd\uff0c\u6bd4\u5982\u201c\u5927\u7b11\u201d\u3002\u8fd9\u4e2a\u63d0\u793a\u8bcd\u4f1a\u53d8\u6210\u7b11\u7684\u58f0\u97f3\uff0c\u7136\u540e\u5408\u6210\u5230\u8bed\u97f3\u91cc\u53bb\u3002\u5b83\u4e5f\u53ef\u4ee5\u6df7\u5408\u201c\u7537\u58f0\u201d\uff0c\u201c\u5973\u58f0\u201d\uff0c\u8fd9\u6837\u518d\u505a\u5c31\u53ef\u4ee5\u4e0d\u7528\u518d\u505a\u62fc\u63a5\u64cd\u4f5c\u4e86    |   [github](https://github.com/suno-ai/bark)    |\n|  whisper    |   \u5728\u8bed\u97f3\u8f6c\u6587\u5b57\uff08STT\uff0c\u4e5f\u79f0ASR\uff09\u65b9\u9762\uff0cwhisper\u662f\u6211\u7528\u8fc7\u7684\u6700\u597d\u7684\uff0c\u6700\u5feb\u7684\u5e93\u3002\u6ca1\u60f3\u5230\uff0c\u8fd9\u4e48\u5feb\u7684\u6a21\u578b\uff0c\u8fd8\u80fd70x\u7684\u4f18\u5316\u7a7a\u95f4\u3002\u6211\u51c6\u5907\u90e8\u7f72\u8fd9\u4e2a\u6a21\u578b\uff0c\u5e76\u5f00\u653e\u7ed9\u5927\u5bb6\u4f7f\u7528\uff0c\u53ef\u4ee5\u7528\u6765\u8f6c\u5f55\u5927\u7684\u8bed\u97f3\u6587\u4ef6\uff0c\u548c\u8fdb\u884c\u7ffb\u8bd1\u3002\u8fd9\u4e2a\u6a21\u578b\u662f\u591a\u8bed\u8a00\u7684\uff0c\u800c\u4e14\u80fd\u81ea\u52a8\u8bc6\u522b\u662f\u4ec0\u4e48\u8bed\u8a00\uff0c\u771f\u7684\u975e\u5e38\u5f3a\u5927    |    [github](https://github.com/sanchit-gandhi/whisper-jax)   |\n|  OFA-Chinese\uff1a\u4e2d\u6587\u591a\u6a21\u6001\u7edf\u4e00\u9884\u8bad\u7ec3\u6a21\u578b    |  transformers\u7ed3\u6784\u7684\u4e2d\u6587OFA\u6a21\u578b     |    [github](https://github.com/yangjianxin1/OFA-Chinese)   |\n|\u6587\u751f\u56fe\u5f00\u6e90\u6a21\u578b\u8bd5\u70bc\u573a|\u53ef\u6839\u636e\u8f93\u5165\u6587\u5b57\u540c\u65f6\u7528stable-diffusion 1.5\u3001stable-diffusion 2.1\u3001DALL-E\u3001kandinsky-2\u7b49\u6a21\u578b\u751f\u6210\u56fe\u50cf\uff0c\u65b9\u4fbf\u6d4b\u8bd5\u6bd4\u8f83|[link](https://zoo.replicate.dev/?id=a-still-life-of-birds-analytical-art-by-ludwig-knaus-wfsbarr)|\n|LLMScore|LLMScore\u662f\u4e00\u79cd\u5168\u65b0\u7684\u6846\u67b6\uff0c\u80fd\u591f\u63d0\u4f9b\u5177\u6709\u591a\u7c92\u5ea6\u7ec4\u5408\u6027\u7684\u8bc4\u4f30\u5206\u6570\u3002\u5b83\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6765\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u3002\u9996\u5148\uff0c\u5c06\u56fe\u50cf\u8f6c\u5316\u4e3a\u56fe\u50cf\u7ea7\u522b\u548c\u5bf9\u8c61\u7ea7\u522b\u7684\u89c6\u89c9\u63cf\u8ff0\uff0c\u7136\u540e\u5c06\u8bc4\u4f30\u6307\u4ee4\u8f93\u5165\u5230LLM\u4e2d\uff0c\u4ee5\u8861\u91cf\u5408\u6210\u56fe\u50cf\u4e0e\u6587\u672c\u7684\u5bf9\u9f50\u7a0b\u5ea6\uff0c\u5e76\u6700\u7ec8\u751f\u6210\u4e00\u4e2a\u8bc4\u5206\u548c\u89e3\u91ca\u3002\u6211\u4eec\u7684\u5927\u91cf\u5206\u6790\u663e\u793a\uff0cLLMScore\u5728\u4f17\u591a\u6570\u636e\u96c6\u4e0a\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u76f8\u5173\u6027\u6700\u9ad8\uff0c\u660e\u663e\u4f18\u4e8e\u5e38\u7528\u7684\u6587\u672c-\u56fe\u50cf\u5339\u914d\u5ea6\u91cf\u6307\u6807CLIP\u548cBLIP\u3002|[paper](https://arxiv.org/abs/2305.11116)[github](https://github.com/YujieLu10/LLMScore)|\n|VisualGLM-6B|VisualGLM-6B \u662f\u4e00\u4e2a\u5f00\u6e90\u7684\uff0c\u652f\u6301\u56fe\u50cf\u3001\u4e2d\u6587\u548c\u82f1\u6587\u7684\u591a\u6a21\u6001\u5bf9\u8bdd\u8bed\u8a00\u6a21\u578b\uff0c\u8bed\u8a00\u6a21\u578b\u57fa\u4e8e ChatGLM-6B\uff0c\u5177\u6709 62 \u4ebf\u53c2\u6570\uff1b\u56fe\u50cf\u90e8\u5206\u901a\u8fc7\u8bad\u7ec3 BLIP2-Qformer \u6784\u5efa\u8d77\u89c6\u89c9\u6a21\u578b\u4e0e\u8bed\u8a00\u6a21\u578b\u7684\u6865\u6881\uff0c\u6574\u4f53\u6a21\u578b\u517178\u4ebf\u53c2\u6570\u3002|[github](https://github.com/THUDM/VisualGLM-6B)|\n\n# LLM\u7684\u6570\u636e\u96c6\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :----   |          :--- |\n|   \u6b67\u4e49\u6570\u636e\u96c6   |   \u80fd\u5426\u6b63\u786e\u7684\u6d88\u9664\u6b67\u4e49\u662f\u8861\u91cf\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e00\u4e2a\u91cd\u8981\u6307\u6807\u3002\u4e0d\u8fc7\u4e00\u76f4\u6ca1\u6709\u4e00\u4e2a\u6807\u51c6\u5316\u7684\u8861\u91cf\u65b9\u6cd5\uff0c\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b1,645\u4e2a\u5177\u6709\u4e0d\u540c\u79cd\u7c7b\u6b67\u4e49\u7684\u6570\u636e\u96c6\u53ca\u5bf9\u5e94\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002    |   [github](https://github.com/alisawuffles/ambient) [paper](arxiv.org/abs/2304.14399)   |\n|   thu\u6307\u4ee4\u8bad\u7ec3\u6570\u636e   |    \u8bbe\u8ba1\u4e86\u4e00\u5957\u6d41\u7a0b\u6765\u81ea\u52a8\u4ea7\u751f\u591a\u6837\u5316\u9ad8\u8d28\u91cf\u7684\u591a\u8f6e\u6307\u4ee4\u5bf9\u8bdd\u6570\u636eUltraChat\uff0c\u5e76\u8fdb\u884c\u4e86\u7ec6\u81f4\u7684\u4eba\u5de5\u540e\u5904\u7406\u3002\u73b0\u5df2\u5c06\u82f1\u6587\u6570\u636e\u5168\u90e8\u5f00\u6e90\uff0c\u5171\u8ba1150\u4f59\u4e07\u6761\uff0c\u662f\u5f00\u6e90\u793e\u533a\u6570\u91cf\u6700\u591a\u7684\u9ad8\u8d28\u91cf\u6307\u4ee4\u6570\u636e\u4e4b\u4e00   |   [github](https://github.com/thunlp/UltraChat/)    |\n|   \u591a\u6a21\u6001\u6570\u636e\u96c6MMC4   |    5.8\u4ebf\u56fe\u7247\uff0c1\u4ebf\u6587\u6863\uff0c400\u4ebftoken   |   [github](https://github.com/allenai/mmc4)    |\n|  EleutherAI \u6570\u636e   |   800g\u7684\u6587\u672c\u8bed\u6599\u7ed9\u4f60\u6574\u5408\u597d\u4e86\u514d\u8d39\u4e0b\u8f7d\uff0c\u4e0d\u77e5\u9053trian\u51fa\u6765\u7684model\u8d28\u91cf\u5982\u4f55\uff0c\u6253\u7b97\u8bd5\u8bd5\uff1a    |    [pile data](huggingface.co/datasets/EleutherAI/the_pile)    [paper](http://t.cn/A6NqJ2Zl)   |\n|UltraChat|\u5927\u89c4\u6a21\u3001\u4fe1\u606f\u4e30\u5bcc\u3001\u591a\u6837\u5316\u7684\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e|[github](https://github.com/thunlp/UltraChat)|\n|ConvFinQA\u91d1\u878d\u6570\u636e\u95ee\u7b54||[github](https://robustfin.github.io/2023/shared_task)|\n|   The botbots dataset   |  \u4e00\u4e2a\u5305\u542b\u5bf9\u8bdd\u5185\u5bb9\u7684\u6570\u636e\u96c6\uff0c\u5bf9\u8bdd\u5185\u5bb9\u6765\u81ea\u4e8e\u4e24\u4e2aChatGPT\u5b9e\u4f8b(gpt-3.5-turbo)\uff0cCLT\u547d\u4ee4\u548c\u5bf9\u8bdd\u63d0\u793a\u6765\u81eaGPT-4\uff0c\u8986\u76d6\u591a\u79cd\u60c5\u5883\u548c\u4efb\u52a1\uff0c\u751f\u6210\u6210\u672c\u7ea6\u4e3a35\u7f8e\u5143\uff0c\u53ef\u7528\u4e8e\u7814\u7a76\u548c\u8bad\u7ec3\u66f4\u5c0f\u7684\u5bf9\u8bdd\u6a21\u578b(\u5982Alpaca)     |   [github](https://github.com/radi-cho/botbots)    |\n|  alpaca_chinese_dataset - \u4eba\u5de5\u7cbe\u8c03\u7684\u4e2d\u6587\u5bf9\u8bdd\u6570\u636e\u96c6    |       |   [github](https://github.com/hikariming/alpaca_chinese_dataset)    |\n|CodeGPT-data|\u63d0\u9ad8\u7f16\u7a0b\u80fd\u529b\u7684\u5173\u952e\u5728\u4e8e\u6570\u636e\u3002CodeGPT\u662f\u901a\u8fc7GPT\u751f\u6210\u7684\u7528\u4e8eGPT\u7684\u4ee3\u7801\u5bf9\u8bdd\u6570\u636e\u96c6\u3002\u73b0\u5728\u516c\u5f00\u4e8632K\u6761\u4e2d\u6587\u6570\u636e\uff0c\u8ba9\u6a21\u578b\u66f4\u64c5\u957f\u7f16\u7a0b|[github](https://github.com/zxx000728/CodeGPT)|\n\n----\n\n# \u8bed\u6599\u5e93\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :----   |          :--- |\n|   \u4eba\u540d\u8bed\u6599\u5e93    |        |  [wainshine/Chinese-Names-Corpus](https://github.com/wainshine/Chinese-Names-Corpus)  |\n|   Chinese-Word-Vectors    |  \u5404\u79cd\u4e2d\u6587\u8bcd\u5411\u91cf      |  [github repo](https://github.com/Embedding/Chinese-Word-Vectors)  |\n|    \u4e2d\u6587\u804a\u5929\u8bed\u6599    |   \u8be5\u5e93\u641c\u96c6\u4e86\u5305\u542b\u8c46\u74e3\u591a\u8f6e, PTT\u516b\u5366\u8bed\u6599, \u9752\u4e91\u8bed\u6599, \u7535\u89c6\u5267\u5bf9\u767d\u8bed\u6599, \u8d34\u5427\u8bba\u575b\u56de\u5e16\u8bed\u6599,\u5fae\u535a\u8bed\u6599,\u5c0f\u9ec4\u9e21\u8bed\u6599     |  [link](https://github.com/codemayq/chaotbot_corpus_Chinese)  |\n|    \u4e2d\u6587\u8c23\u8a00\u6570\u636e    |     \u8be5\u6570\u636e\u6587\u4ef6\u4e2d\uff0c\u6bcf\u4e00\u884c\u4e3a\u4e00\u6761json\u683c\u5f0f\u7684\u8c23\u8a00\u6570\u636e   |   [github](https://github.com/thunlp/Chinese_Rumor_Dataset)  |\n|     \u4e2d\u6587\u95ee\u7b54\u6570\u636e\u96c6   |        |  [\u94fe\u63a5](https://pan.baidu.com/s/1QUsKcFWZ7Tg1dk_AbldZ1A) \u63d0\u53d6\u7801 2dva  |\n|    \u5fae\u4fe1\u516c\u4f17\u53f7\u8bed\u6599   |   3G\u8bed\u6599\uff0c\u5305\u542b\u90e8\u5206\u7f51\u7edc\u6293\u53d6\u7684\u5fae\u4fe1\u516c\u4f17\u53f7\u7684\u6587\u7ae0\uff0c\u5df2\u7ecf\u53bb\u9664HTML\uff0c\u53ea\u5305\u542b\u4e86\u7eaf\u6587\u672c\u3002\u6bcf\u884c\u4e00\u7bc7\uff0c\u662fJSON\u683c\u5f0f\uff0cname\u662f\u5fae\u4fe1\u516c\u4f17\u53f7\u540d\u5b57\uff0caccount\u662f\u5fae\u4fe1\u516c\u4f17\u53f7ID\uff0ctitle\u662f\u9898\u76ee\uff0ccontent\u662f\u6b63\u6587     | [github](https://github.com/nonamestreet/weixin_public_corpus)    |\n|    \u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406 \u8bed\u6599\u3001\u6570\u636e\u96c6   |        |  [github](https://github.com/SophonPlus/ChineseNlpCorpus)  |\n|    \u4efb\u52a1\u578b\u5bf9\u8bdd\u82f1\u6587\u6570\u636e\u96c6    |     \u3010\u6700\u5168\u4efb\u52a1\u578b\u5bf9\u8bdd\u6570\u636e\u96c6\u3011\u4e3b\u8981\u4ecb\u7ecd\u4e86\u4e00\u4efd\u4efb\u52a1\u578b\u5bf9\u8bdd\u6570\u636e\u96c6\u5927\u5168\uff0c\u8fd9\u4efd\u6570\u636e\u96c6\u5927\u5168\u6db5\u76d6\u4e86\u5230\u76ee\u524d\u5728\u4efb\u52a1\u578b\u5bf9\u8bdd\u9886\u57df\u7684\u6240\u6709\u5e38\u7528\u6570\u636e\u96c6\u7684\u4e3b\u8981\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u5e2e\u52a9\u7814\u7a76\u8005\u66f4\u597d\u7684\u628a\u63e1\u9886\u57df\u8fdb\u5c55\u7684\u8109\u7edc\uff0c\u6211\u4eec\u4ee5Leaderboard\u7684\u5f62\u5f0f\u7ed9\u51fa\u4e86\u51e0\u4e2a\u6570\u636e\u96c6\u4e0a\u7684State-of-the-art\u5b9e\u9a8c\u7ed3\u679c\u3002   |   [github](https://github.com/AtmaHou/Task-Oriented-Dialogue-Dataset-Survey)     |\n|    \u8bed\u97f3\u8bc6\u522b\u8bed\u6599\u751f\u6210\u5de5\u5177    |    \u4ece\u5177\u6709\u97f3\u9891/\u5b57\u5e55\u7684\u5728\u7ebf\u89c6\u9891\u521b\u5efa\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b(ASR)\u8bed\u6599\u5e93    |   [github](https://github.com/yc9701/pansori)  |\n|     LitBankNLP\u6570\u636e\u96c6   |   \u652f\u6301\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u8ba1\u7b97\u4eba\u6587\u5b66\u79d1\u4efb\u52a1\u7684100\u90e8\u5e26\u6807\u8bb0\u82f1\u6587\u5c0f\u8bf4\u8bed\u6599     |   [github](https://github.com/dbamman/litbank) |\n|    \u4e2d\u6587ULMFiT  |    \u60c5\u611f\u5206\u6790 \u6587\u672c\u5206\u7c7b \u8bed\u6599\u53ca\u6a21\u578b    |   [github](https://github.com/bigboNed3/chinese_ulmfit)  |\n|    \u7701\u5e02\u533a\u9547\u884c\u653f\u533a\u5212\u6570\u636e\u5e26\u62fc\u97f3\u6807\u6ce8    |        |   [github](https://github.com/xiangyuecn/AreaCity-JsSpider-StatsGov)  |\n|    \u6559\u80b2\u884c\u4e1a\u65b0\u95fb \u81ea\u52a8\u6587\u6458 \u8bed\u6599\u5e93    |        |   [github](https://github.com/wonderfulsuccess/chinese_abstractive_corpus)  |\n|    \u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6570\u636e\u96c6    |        |  [github](https://github.com/InsaneLife/ChineseNLPCorpus)   |\n|     \u7ef4\u57fa\u5927\u89c4\u6a21\u5e73\u884c\u6587\u672c\u8bed\u6599   |  85\u79cd\u8bed\u8a00\u30011620\u79cd\u8bed\u8a00\u5bf9\u3001135M\u5bf9\u7167\u53e5  |  [github](https://github.com/facebookresearch/LASER/tree/master/tasks/WikiMatrix) \n|   \u53e4\u8bd7\u8bcd\u5e93     |        |  [github repo](https://github.com/panhaiqi/AncientPoetry) <br>[\u66f4\u5168\u7684\u53e4\u8bd7\u8bcd\u5e93](https://github.com/chinese-poetry/chinese-poetry)\n|   \u4f4e\u5185\u5b58\u52a0\u8f7d\u7ef4\u57fa\u767e\u79d1\u6570\u636e    |     \u7528\u65b0\u7248nlp\u5e93\u52a0\u8f7d17GB+\u82f1\u6587\u7ef4\u57fa\u8bed\u6599\u53ea\u5360\u75289MB\u5185\u5b58\u904d\u5386\u901f\u5ea62-3 Gbit/s    |   [github](https://gistgithub.com/thomwolf/13ca2b2b172b2d17ac66685aa2eeba62) |\n|    \u5bf9\u8054\u6570\u636e    |   700,000 couplets, \u8d85\u8fc770\u4e07\u5bf9\u5bf9\u8054     |   [github](https://github.com/wb14123/couplet-dataset)  |\n|   \u300a\u914d\u8272\u8f9e\u5178\u300b\u6570\u636e\u96c6     |        |  [github](https://github.com/mattdesl/dictionary-of-colour-combinations)   |\n|    42GB\u7684JD\u5ba2\u670d\u5bf9\u8bdd\u6570\u636e(CSDD)    |        |   [github](https://github.com/jd-aig/nlp_baai/tree/master/pretrained_models_and_embeddings)  |\n|  70\u4e07\u5bf9\u8054\u6570\u636e       |        | [link](https://github.com/wb14123/couplet-dataset)   |\n|   \u7528\u6237\u540d\u9ed1\u540d\u5355\u5217\u8868    |        |   [github](https://github.com/marteinn/The-Big-Username-Blacklist)  |\n|     \u4f9d\u5b58\u53e5\u6cd5\u5206\u6790\u8bed\u6599   |    4\u4e07\u53e5\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e    |  [Homepage](http//hlt.suda.edu.cn/indexphp/Nlpcc-2019-shared-task)  |\n|      \u4eba\u6c11\u65e5\u62a5\u8bed\u6599\u5904\u7406\u5de5\u5177\u96c6  |        |  [github](https://github.com/howl-anderson/tools_for_corpus_of_people_daily)   |\n|  \u865a\u5047\u65b0\u95fb\u6570\u636e\u96c6 fake news corpus      |        |   [github](https://github.com/several27/FakeNewsCorpus)  |\n|    \u8bd7\u6b4c\u8d28\u91cf\u8bc4\u4ef7/\u7ec6\u7c92\u5ea6\u60c5\u611f\u8bd7\u6b4c\u8bed\u6599\u5e93    |        |  [github](https://github.com/THUNLP-AIPoet/Datasets)   |\n|    \u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406\u76f8\u5173\u7684\u5f00\u653e\u4efb\u52a1    |  \u6570\u636e\u96c6\u4ee5\u53ca\u5f53\u524d\u6700\u4f73\u7ed3\u679c     |    [github](https://github.com/didi/ChineseNLP) |\n|    \u4e2d\u6587\u7f29\u5199\u6570\u636e\u96c6    |        |   [github](https://github.com/zhangyics/Chinese-abbreviation-dataset)  |\n|    \u4e2d\u6587\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bc4     |    \u4ee3\u8868\u6027\u7684\u6570\u636e\u96c6-\u57fa\u51c6(\u9884\u8bad\u7ec3)\u6a21\u578b-\u8bed\u6599\u5e93-baseline-\u5de5\u5177\u5305-\u6392\u884c\u699c    |   [github](https://github.com/CLUEbenchmark/CLUE)  |\n|   \u4e2d\u6587\u8c23\u8a00\u6570\u636e\u5e93    |        |  [github](https://github.com/thunlp/Chinese_Rumor_Dataset)   |\n|     CLUEDatasetSearch    |   \u4e2d\u82f1\u6587NLP\u6570\u636e\u96c6\u641c\u7d22\u6240\u6709\u4e2d\u6587NLP\u6570\u636e\u96c6\uff0c\u9644\u5e38\u7528\u82f1\u6587NLP\u6570\u636e\u96c6     |   [github](https://github.com/CLUEbenchmark/CLUEDatasetSearch)  |\n|    \u591a\u6587\u6863\u6458\u8981\u6570\u636e\u96c6    |        |  [github](https://github.com/complementizer/wcep-mds-dataset)   |\n|    \u8ba9\u4eba\u4eba\u90fd\u53d8\u5f97\u201c\u5f6c\u5f6c\u6709\u793c\u201d\u793c\u8c8c\u8fc1\u79fb\u4efb\u52a1   |  \u5728\u4fdd\u7559\u610f\u4e49\u7684\u540c\u65f6\u5c06\u975e\u793c\u8c8c\u8bed\u53e5\u8f6c\u6362\u4e3a\u793c\u8c8c\u8bed\u53e5\uff0c\u63d0\u4f9b\u5305\u542b139M + \u5b9e\u4f8b\u7684\u6570\u636e\u96c6       |   [paper and code](https://arxiv.org/abs/200414257)  |\n|    \u7ca4\u8bed/\u82f1\u8bed\u4f1a\u8bdd\u53cc\u8bed\u8bed\u6599\u5e93    |        |   [github](https://github.com/khiajohnson/SpiCE-Corpus)  |\n|     \u4e2d\u6587NLP\u6570\u636e\u96c6\u5217\u8868   |        |   [github](https://github.com/OYE93/Chinese-NLP-Corpus)  |\n|   \u7c7b\u4eba\u540d/\u5730\u540d/\u7ec4\u7ec7\u673a\u6784\u540d\u7684\u547d\u540d\u4f53\u8bc6\u522b\u6570\u636e\u96c6     |        |  [github](https://github.com/LG-1/video_music_book_datasets)  |\n|    \u4e2d\u6587\u8bed\u8a00\u7406\u89e3\u6d4b\u8bc4\u57fa\u51c6    |    \u5305\u62ec\u4ee3\u8868\u6027\u7684\u6570\u636e\u96c6&\u57fa\u51c6\u6a21\u578b&\u8bed\u6599\u5e93&\u6392\u884c\u699c   |   [github](https://github.com/brightmart/ChineseGLUE)  |\n|    OpenCLaP\u591a\u9886\u57df\u5f00\u6e90\u4e2d\u6587\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4ed3\u5e93    |   \u6c11\u4e8b\u6587\u4e66\u3001\u5211\u4e8b\u6587\u4e66\u3001\u767e\u5ea6\u767e\u79d1 |    [github](https://github.com/thunlp/OpenCLaP) |\n|   \u4e2d\u6587\u5168\u8bcd\u8986\u76d6BERT\u53ca\u4e24\u4efd\u9605\u8bfb\u7406\u89e3\u6570\u636e     |      DRCD\u6570\u636e\u96c6\uff1a\u7531\u4e2d\u56fd\u53f0\u6e7e\u53f0\u8fbe\u7814\u7a76\u9662\u53d1\u5e03\uff0c\u5176\u5f62\u5f0f\u4e0eSQuAD\u76f8\u540c\uff0c\u662f\u57fa\u4e8e\u7e41\u4f53\u4e2d\u6587\u7684\u62bd\u53d6\u5f0f\u9605\u8bfb\u7406\u89e3\u6570\u636e\u96c6\u3002<br>CMRC 2018\u6570\u636e\u96c6:\u54c8\u5de5\u5927\u8baf\u98de\u8054\u5408\u5b9e\u9a8c\u5ba4\u53d1\u5e03\u7684\u4e2d\u6587\u673a\u5668\u9605\u8bfb\u7406\u89e3\u6570\u636e\u3002\u6839\u636e\u7ed9\u5b9a\u95ee\u9898\uff0c\u7cfb\u7edf\u9700\u8981\u4ece\u7bc7\u7ae0\u4e2d\u62bd\u53d6\u51fa\u7247\u6bb5\u4f5c\u4e3a\u7b54\u6848\uff0c\u5f62\u5f0f\u4e0eSQuAD\u76f8\u540c\u3002|    [github](https://github.com/ymcui/Chinese-BERT-wwm) |\n|  Dakshina\u6570\u636e\u96c6     |    \u5341\u4e8c\u79cd\u5357\u4e9a\u8bed\u8a00\u7684\u62c9\u4e01/\u672c\u5730\u6587\u5b57\u5e73\u884c\u6570\u636e\u96c6\u5408     |   [github](https://github.com/google-research-datasets/dakshina)  |\n|    OPUS-100    |   \u4ee5\u82f1\u6587\u4e3a\u4e2d\u5fc3\u7684\u591a\u8bed(100\u79cd)\u5e73\u884c\u8bed\u6599     |   [github](https://github.com/EdinburghNLP/opus-100-corpus)  |\n|      \u4e2d\u6587\u9605\u8bfb\u7406\u89e3\u6570\u636e\u96c6  |        |   [github](https://github.com/ymcui/Chinese-RC-Datasets)  |\n|    \u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5411\u91cf\u5408\u96c6    |        |   [github](https://github.com/liuhuanyong/ChineseEmbedding)  |\n|    \u4e2d\u6587\u8bed\u8a00\u7406\u89e3\u6d4b\u8bc4\u57fa\u51c6    |\u5305\u62ec\u4ee3\u8868\u6027\u7684\u6570\u636e\u96c6\u3001\u57fa\u51c6(\u9884\u8bad\u7ec3)\u6a21\u578b\u3001\u8bed\u6599\u5e93\u3001\u6392\u884c\u699c       |  [github](https://github.com/CLUEbenchmark/CLUE)   |\n|  NLP\u6570\u636e\u96c6/\u57fa\u51c6\u4efb\u52a1\u5927\u5217\u8868     |        |  [github](https://quantumstatcom/dataset/datasethtml)   |\n|   LitBankNLP\u6570\u636e\u96c6     |   \u652f\u6301\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u8ba1\u7b97\u4eba\u6587\u5b66\u79d1\u4efb\u52a1\u7684100\u90e8\u5e26\u6807\u8bb0\u82f1\u6587\u5c0f\u8bf4\u8bed\u6599     | [github](https://github.com/dbamman/litbank)    |\n|70\u4e07\u5bf9\u8054\u6570\u636e||[github](https://github.com/wb14123/couplet-dataset)|\n|\u6587\u8a00\u6587\uff08\u53e4\u6587\uff09-\u73b0\u4ee3\u6587\u5e73\u884c\u8bed\u6599|\u77ed\u7bc7\u7ae0\u4e2d\u5305\u62ec\u4e86\u300a\u8bba\u8bed\u300b\u3001\u300a\u5b5f\u5b50\u300b\u3001\u300a\u5de6\u4f20\u300b\u7b49\u7bc7\u5e45\u8f83\u77ed\u7684\u53e4\u7c4d\uff0c\u5df2\u548c\u300a\u8d44\u6cbb\u901a\u9274\u300b\u5408\u5e76|[github](https://github.com/NiuTrans/Classical-Modern)|\n|COLDDateset\uff0c\u4e2d\u6587\u5192\u72af\u6027\u8bed\u8a00\u68c0\u6d4b\u6570\u636e\u96c6|\u6db5\u76d6\u4e86\u79cd\u65cf\u3001\u6027\u522b\u548c\u5730\u533a\u7b49\u8bdd\u9898\u5185\u5bb9\uff0c\u6570\u636e\u5f85\u8bba\u6587\u53d1\u8868\u540e\u653e\u51fa|[paper](https://arxiv.org/pdf/2201.06025.pdf)|\n|GAOKAO-bench\uff1a\u4ee5\u4e2d\u56fd\u9ad8\u8003\u9898\u76ee\u4f5c\u4e3a\u6570\u636e\u96c6|\u4ee5\u4e2d\u56fd\u9ad8\u8003\u9898\u76ee\u4f5c\u4e3a\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u548c\u903b\u8f91\u63a8\u7406\u80fd\u529b\u7684\u6d4b\u8bc4\u6846\u67b6\uff0c\u5305\u542b1781\u9053\u9009\u62e9\u9898\u3001218\u9053\u586b\u7a7a\u9898\u548c812\u9053\u89e3\u7b54\u9898|[github](https://github.com/OpenLMLab/GAOKAO-Bench)|\n|zero to nlp - \u4e2d\u6587nlp\u5e94\u7528\u6570\u636e\u3001\u6a21\u578b\u3001\u8bad\u7ec3\u3001\u63a8\u7406||[github](https://github.com/yuanzhoulvpi2017/zero_nlp)|\n\n# \u8bcd\u5e93\u53ca\u8bcd\u6cd5\u5de5\u5177\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n|  textfilter     |    \u4e2d\u82f1\u6587\u654f\u611f\u8bcd\u8fc7\u6ee4    |  [observerss/textfilter](https://github.com/observerss/textfilter)  |\n|   \u4eba\u540d\u62bd\u53d6\u529f\u80fd    |   \u4e2d\u6587\uff08\u73b0\u4ee3\u3001\u53e4\u4ee3\uff09\u540d\u5b57\u3001\u65e5\u6587\u540d\u5b57\u3001\u4e2d\u6587\u7684\u59d3\u548c\u540d\u3001\u79f0\u547c\uff08\u5927\u59e8\u5988\u3001\u5c0f\u59e8\u5988\u7b49\uff09\u3001\u82f1\u6587->\u4e2d\u6587\u540d\u5b57\uff08\u674e\u7ea6\u7ff0\uff09\u3001\u6210\u8bed\u8bcd\u5178   |  [cocoNLP](https://github.com/fighting41love/cocoNLP)  |\n|   \u4e2d\u6587\u7f29\u5199\u5e93    | \u5168\u56fd\u4eba\u5927: \u5168\u56fd \u4eba\u6c11 \u4ee3\u8868\u5927\u4f1a; \u4e2d\u56fd: \u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd;\u5973\u7f51\u8d5b: \u5973\u5b50/n \u7f51\u7403/n \u6bd4\u8d5b/vn  |  [github](https://github.com/zhangyics/Chinese-abbreviation-dataset/blob/master/dev_set.txt)  |\n|   \u6c49\u8bed\u62c6\u5b57\u8bcd\u5178    |  \u6f22\u5b57\t\u62c6\u6cd5 (\u4e00)\t\u62c6\u6cd5 (\u4e8c)\t\u62c6\u6cd5 (\u4e09) \u62c6\t\u624b \u65a5\t\u624c \u65a5\t\u624d \u65a5    |  [kfcd/chaizi](https://github.com/kfcd/chaizi)  |\n|    \u8bcd\u6c47\u60c5\u611f\u503c   |    \u5c71\u6cc9\u6c34:0.400704566541 <br>  \u5145\u6c9b:\t0.37006739587   |   [rainarch/SentiBridge](https://github.com/rainarch/SentiBridge/blob/master/Entity_Emotion_Express/CCF_data/pair_mine_result) |\n|   \u4e2d\u6587\u8bcd\u5e93\u3001\u505c\u7528\u8bcd\u3001\u654f\u611f\u8bcd    |        |  [dongxiexidian/Chinese](https://github.com/fighting41love/Chinese_from_dongxiexidian)  |\n|   python-pinyin    |   \u6c49\u5b57\u8f6c\u62fc\u97f3     |  [mozillazg/python-pinyin](https://github.com/mozillazg/python-pinyin)  |\n|   zhtools   |   \u4e2d\u6587\u7e41\u7b80\u4f53\u4e92\u8f6c     |  [skydark/nstools](https://github.com/skydark/nstools/tree/master/zhtools)  |\n|   \u82f1\u6587\u6a21\u62df\u4e2d\u6587\u53d1\u97f3\u5f15\u64ce    |    say wo i ni #\u8bf4\uff1a\u6211\u7231\u4f60    |  [tinyfool/ChineseWithEnglish](https://github.com/tinyfool/ChineseWithEnglish)  |\n|  chinese_dictionary     |    \u540c\u4e49\u8bcd\u5e93\u3001\u53cd\u4e49\u8bcd\u5e93\u3001\u5426\u5b9a\u8bcd\u5e93    |  [guotong1988/chinese_dictionary](https://github.com/guotong1988/chinese_dictionary)  |\n|   wordninja    |   \u65e0\u7a7a\u683c\u82f1\u6587\u4e32\u5206\u5272\u3001\u62bd\u53d6\u5355\u8bcd     | [wordninja](https://github.com/keredson/wordninja)  |\n|   \u6c7d\u8f66\u54c1\u724c\u3001\u6c7d\u8f66\u96f6\u4ef6\u76f8\u5173\u8bcd\u6c47    |        |  [data](https://github.com/fighting41love/funNLP/tree/master/data)|     \u516c\u53f8\u540d\u5b57\u5927\u5168   |        |   [github repo](https://github.com/wainshine/Company-Names-Corpus)\n|   THU\u6574\u7406\u7684\u8bcd\u5e93   | IT\u8bcd\u5e93\u3001\u8d22\u7ecf\u8bcd\u5e93\u3001\u6210\u8bed\u8bcd\u5e93\u3001\u5730\u540d\u8bcd\u5e93\u3001\u5386\u53f2\u540d\u4eba\u8bcd\u5e93\u3001\u8bd7\u8bcd\u8bcd\u5e93\u3001\u533b\u5b66\u8bcd\u5e93\u3001\u996e\u98df\u8bcd\u5e93\u3001\u6cd5\u5f8b\u8bcd\u5e93\u3001\u6c7d\u8f66\u8bcd\u5e93\u3001\u52a8\u7269\u8bcd\u5e93    | [link](http://thuctc.thunlp.org/)   |\n|   \u7f6a\u540d\u6cd5\u52a1\u540d\u8bcd\u53ca\u5206\u7c7b\u6a21\u578b    |    \u5305\u542b856\u9879\u7f6a\u540d\u77e5\u8bc6\u56fe\u8c31, \u57fa\u4e8e280\u4e07\u7f6a\u540d\u8bad\u7ec3\u5e93\u7684\u7f6a\u540d\u9884\u6d4b,\u57fa\u4e8e20W\u6cd5\u52a1\u95ee\u7b54\u5bf9\u768413\u7c7b\u95ee\u9898\u5206\u7c7b\u4e0e\u6cd5\u5f8b\u8d44\u8baf\u95ee\u7b54\u529f\u80fd    |    [github](https://github.com/liuhuanyong/CrimeKgAssitant)     |\n|   \u5206\u8bcd\u8bed\u6599\u5e93+\u4ee3\u7801    |        |  [\u767e\u5ea6\u7f51\u76d8\u94fe\u63a5](https://pan.baidu.com/s/1MXZONaLgeaw0_TxZZDAIYQ)     - \u63d0\u53d6\u7801 pea6  |\n|  \u57fa\u4e8eBi-LSTM + CRF\u7684\u4e2d\u6587\u5206\u8bcd+\u8bcd\u6027\u6807\u6ce8     |   keras\u5b9e\u73b0     |  [link](https://github.com/GlassyWing/bi-lstm-crf)  |\n| \u57fa\u4e8eUniversal Transformer + CRF \u7684\u4e2d\u6587\u5206\u8bcd\u548c\u8bcd\u6027\u6807\u6ce8    |        |  [link](https://github.com/GlassyWing/transformer-word-segmenter)  |\n| \u5feb\u901f\u795e\u7ecf\u7f51\u7edc\u5206\u8bcd\u5305     |    java version     |   [](https://github.com/yaoguangluo/NeroParser) |\n|   chinese-xinhua      |    \u4e2d\u534e\u65b0\u534e\u5b57\u5178\u6570\u636e\u5e93\u53caapi\uff0c\u5305\u62ec\u5e38\u7528\u6b47\u540e\u8bed\u3001\u6210\u8bed\u3001\u8bcd\u8bed\u548c\u6c49\u5b57    |   [github](https://github.com/pwxcoo/chinese-xinhua)  |\n|   SpaCy \u4e2d\u6587\u6a21\u578b     |   \u5305\u542bParser, NER, \u8bed\u6cd5\u6811\u7b49\u529f\u80fd\u3002\u6709\u4e00\u4e9b\u82f1\u6587package\u4f7f\u7528spacy\u7684\u82f1\u6587\u6a21\u578b\u7684\uff0c\u5982\u679c\u8981\u9002\u914d\u4e2d\u6587\uff0c\u53ef\u80fd\u9700\u8981\u4f7f\u7528spacy\u4e2d\u6587\u6a21\u578b\u3002     |   [github](https://github.com/howl-anderson/Chinese_models_for_SpaCy)    |\n|    \u4e2d\u6587\u5b57\u7b26\u6570\u636e    |        |  [github](https://github.com/skishore/makemeahanzi)   |\n|    Synonyms\u4e2d\u6587\u8fd1\u4e49\u8bcd\u5de5\u5177\u5305    |        |   [github](https://github.com/huyingxi/Synonyms)  |\n|   HarvestText     |   \u9886\u57df\u81ea\u9002\u5e94\u6587\u672c\u6316\u6398\u5de5\u5177\uff08\u65b0\u8bcd\u53d1\u73b0-\u60c5\u611f\u5206\u6790-\u5b9e\u4f53\u94fe\u63a5\u7b49\uff09     |   [github](https://github.com/blmoistawinde/HarvestText)   |\n|    word2word    |    \u65b9\u4fbf\u6613\u7528\u7684\u591a\u8bed\u8a00\u8bcd-\u8bcd\u5bf9\u96c662\u79cd\u8bed\u8a00/3,564\u4e2a\u591a\u8bed\u8a00\u5bf9    |   [github](https://github.com/Kyubyong/word2word)  |\n|   \u591a\u97f3\u5b57\u8bcd\u5178\u6570\u636e\u53ca\u4ee3\u7801     |        |  [github](https://github.com/mozillazg/phrase-pinyin-data)   |\n|    \u6c49\u5b57\u3001\u8bcd\u8bed\u3001\u6210\u8bed\u67e5\u8be2\u63a5\u53e3    |        |   [github](https://github.com/netnr/zidian/tree/206028e5ce9a608afc583820df8dc2d1d4b61781)  |\n|    103976\u4e2a\u82f1\u8bed\u5355\u8bcd\u5e93\u5305    |    \uff08sql\u7248\uff0ccsv\u7248\uff0cExcel\u7248\uff09    |  [github](https://github.com/1eez/103976)   |\n|    \u82f1\u6587\u810f\u8bdd\u5927\u5217\u8868    |        |   [github](https://github.com/zacanger/profane-words)  |\n|      \u8bcd\u8bed\u62fc\u97f3\u6570\u636e  |        |   [github](https://github.com/mozillazg/phrase-pinyin-data)  |\n|   186\u79cd\u8bed\u8a00\u7684\u6570\u5b57\u53eb\u6cd5\u5e93     |        |   [github](https://github.com/google/UniNum)  |\n|    \u4e16\u754c\u5404\u56fd\u5927\u89c4\u6a21\u4eba\u540d\u5e93    |        |   [github](https://github.com/philipperemy/name-dataset)  |\n|   \u6c49\u5b57\u5b57\u7b26\u7279\u5f81\u63d0\u53d6\u5668 (featurizer)     |   \u63d0\u53d6\u6c49\u5b57\u7684\u7279\u5f81\uff08\u53d1\u97f3\u7279\u5f81\u3001\u5b57\u5f62\u7279\u5f81\uff09\u7528\u505a\u6df1\u5ea6\u5b66\u4e60\u7684\u7279\u5f81     |   [github](https://github.com/howl-anderson/hanzi_char_featurizer)  |\n|     char_featurizer - \u6c49\u5b57\u5b57\u7b26\u7279\u5f81\u63d0\u53d6\u5de5\u5177   |        |    [github](https://github.com/charlesXu86/char_featurizer) |\n|   \u4e2d\u65e5\u97e9\u5206\u8bcd\u5e93mecab\u7684Python\u63a5\u53e3\u5e93     |        |   [github](https://github.com/jeongukjae/python-mecab)  |\n|    g2pC\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u6c49\u8bed\u8bfb\u97f3\u81ea\u52a8\u6807\u8bb0\u6a21\u5757    |        |   [github](https://github.com/Kyubyong/g2pC)  |\n|     ssc, Sound Shape Code   | \u97f3\u5f62\u7801 - \u57fa\u4e8e\u201c\u97f3\u5f62\u7801\u201d\u7684\u4e2d\u6587\u5b57\u7b26\u4e32\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u65b9\u6cd5      | [version 1](https://github.com/qingyujean/ssc)<br>[version 2](https://github.com/wenyangchou/SimilarCharactor)<br>[blog/introduction](https://blogcsdnnet/chndata/article/details/41114771)   |\n|    \u57fa\u4e8e\u767e\u79d1\u77e5\u8bc6\u5e93\u7684\u4e2d\u6587\u8bcd\u8bed\u591a\u8bcd\u4e49/\u4e49\u9879\u83b7\u53d6\u4e0e\u7279\u5b9a\u53e5\u5b50\u8bcd\u8bed\u8bed\u4e49\u6d88\u6b67    |        |    [github](https://github.com/liuhuanyong/WordMultiSenseDisambiguation) |\n|   Tokenizer\u5feb\u901f\u3001\u53ef\u5b9a\u5236\u7684\u6587\u672c\u8bcd\u6761\u5316\u5e93     |        |   [github](https://github.com/OpenNMT/Tokenizer)  |\n|   Tokenizers     |  \u6ce8\u91cd\u6027\u80fd\u4e0e\u591a\u529f\u80fd\u6027\u7684\u6700\u5148\u8fdb\u5206\u8bcd\u5668      |    [github](https://github.com/huggingface/tokenizers)|\n|    \u901a\u8fc7\u540c\u4e49\u8bcd\u66ff\u6362\u5b9e\u73b0\u6587\u672c\u201c\u53d8\u8138\u201d    |        |    [github](https://github.com/paubric/python-sirajnet) |\n|    token2index\u4e0ePyTorch/Tensorflow\u517c\u5bb9\u7684\u5f3a\u5927\u8f7b\u91cf\u8bcd\u6761\u7d22\u5f15\u5e93    |        |  [github](https://github.com/Kaleidophon/token2index)   |\n|    \u7e41\u7b80\u4f53\u8f6c\u6362    |        |   [github](https://github.com/berniey/hanziconv)  |\n| \u7ca4\u8bedNLP\u5de5\u5177|       |   [github](https://github.com/jacksonllee/pycantonese)|\n|\u9886\u57df\u8bcd\u5178\u5e93|\u6db5\u76d668\u4e2a\u9886\u57df\u3001\u5171\u8ba1916\u4e07\u8bcd\u7684\u4e13\u4e1a\u8bcd\u5178\u77e5\u8bc6\u5e93|[github](https://github.com/liuhuanyong/DomainWordsDict)|\n\n\n\n# \u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b&\u5927\u6a21\u578b\n   \n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n|BMList|\u5927\u6a21\u578b\u5927\u5217\u8868|[github](https://github.com/OpenBMB/BMList)|\n| bert\u8bba\u6587\u4e2d\u6587\u7ffb\u8bd1     |        | [link](https://github.com/yuanxiaosc/BERT_Paper_Chinese_Translation)   |\n|    bert\u539f\u4f5c\u8005\u7684slides  |    |  [link](https://pan.baidu.com/s/1OSPsIu2oh1iJ-bcXoDZpJQ)  |\n| \u6587\u672c\u5206\u7c7b\u5b9e\u8df5     |        |  [github](https://github.com/NLPScott/bert-Chinese-classification-task)  |\n|  bert tutorial\u6587\u672c\u5206\u7c7b\u6559\u7a0b     |        | [github](https://github.com/Socialbird-AILab/BERT-Classification-Tutorial) |\n| bert pytorch\u5b9e\u73b0       |        |  [github](https://github.com/huggingface/pytorch-pretrained-BERT)  |\n|   bert pytorch\u5b9e\u73b0      |        |  [github](https://github.com/huggingface/pytorch-pretrained-BERT)  |\n|  BERT\u751f\u6210\u53e5\u5411\u91cf\uff0cBERT\u505a\u6587\u672c\u5206\u7c7b\u3001\u6587\u672c\u76f8\u4f3c\u5ea6\u8ba1\u7b97     |        | [github](https://github.com/terrifyzhao/bert-utils)   |\n|  bert\u3001ELMO\u7684\u56fe\u89e3     |        |  [github](https://jalammargithubio/illustrated-bert/)  |\n|  BERT Pre-trained models and downstream applications     |        |  [github](https://github.com/asyml/texar/tree/master/examples/bert)  |\n|  \u8bed\u8a00/\u77e5\u8bc6\u8868\u793a\u5de5\u5177BERT & ERNIE      |        |   [github](https://github.com/PaddlePaddle/LARK)  |\n|    Kashgari\u4e2d\u4f7f\u7528gpt-2\u8bed\u8a00\u6a21\u578b    |        |  [github](https://github.com/BrikerMan/Kashgari)   |\n|     Facebook LAMA   |    \u7528\u4e8e\u5206\u6790\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e2d\u5305\u542b\u7684\u4e8b\u5b9e\u548c\u5e38\u8bc6\u77e5\u8bc6\u7684\u63a2\u9488\u3002\u8bed\u8a00\u6a21\u578b\u5206\u6790\uff0c\u63d0\u4f9bTransformer-XL/BERT/ELMo/GPT\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u7edf\u4e00\u8bbf\u95ee\u63a5\u53e3    |  [github](https://github.com/facebookresearch/LAMA)   |\n|    \u4e2d\u6587\u7684GPT2\u8bad\u7ec3\u4ee3\u7801    |        |    [github](https://github.com/Morizeyao/GPT2-Chinese) |\n|   XLMFacebook\u7684\u8de8\u8bed\u8a00\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b     |        |   [github](https://github.com/facebookresearch/XLM)  |\n|    \u6d77\u91cf\u4e2d\u6587\u9884\u8bad\u7ec3ALBERT\u6a21\u578b    |        |  [github](https://github.com/brightmart/albert_zh)   |\n|    Transformers 20    |    \u652f\u6301TensorFlow 20 \u548c PyTorch \u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b(BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet\u2026) 8\u79cd\u67b6\u6784/33\u79cd\u9884\u8bad\u7ec3\u6a21\u578b/102\u79cd\u8bed\u8a00   | [github](https://github.com/huggingface/transformers)    |\n|    8\u7bc7\u8bba\u6587\u68b3\u7406BERT\u76f8\u5173\u6a21\u578b\u8fdb\u5c55\u4e0e\u53cd\u601d    |        |    [github](https://wwwmsracn/zh-cn/news/features/bert) |\n|    \u6cd5\u6587RoBERTa\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b    |    \u7528138GB\u8bed\u6599\u8bad\u7ec3\u7684\u6cd5\u6587RoBERTa\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b    |   [link](https://camembert-model.fr/)  |\n|     \u4e2d\u6587\u9884\u8bad\u7ec3 ELECTREA \u6a21\u578b    |    \u57fa\u4e8e\u5bf9\u6297\u5b66\u4e60 pretrain Chinese Model    |   [github](https://github.com/CLUEbenchmark/ELECTRA)  |\n|   albert-chinese-ner     |   \u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578bALBERT\u505a\u4e2d\u6587NER    |   [github](https://github.com/ProHiryu/albert-chinese-ner)  |\n|    \u5f00\u6e90\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5408\u96c6    |        |  [github](https://github.com/ZhuiyiTechnology/pretrained-models)   |\n|   \u4e2d\u6587ELECTRA\u9884\u8bad\u7ec3\u6a21\u578b     |        |  [github](https://github.com/ymcui/Chinese-ELECTRA)   |\n|    \u7528Transformers(BERT, XLNet, Bart, Electra, Roberta, XLM-Roberta)\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8bcd(\u6a21\u578b\u6bd4\u8f83)    |        |  [github](https://github.com/renatoviolin/next_word_prediction)   |\n|   TensorFlow Hub     |    40+\u79cd\u8bed\u8a00\u7684\u65b0\u8bed\u8a00\u6a21\u578b(\u5305\u62ec\u4e2d\u6587)    |  [link](https://tfhub.dev/google/collections/wiki40b-lm/1)   |\n|   UER     | \u57fa\u4e8e\u4e0d\u540c\u8bed\u6599\u3001\u7f16\u7801\u5668\u3001\u76ee\u6807\u4efb\u52a1\u7684\u4e2d\u6587\u9884\u8bad\u7ec3\u6a21\u578b\u4ed3\u5e93\uff08\u5305\u62ecBERT\u3001GPT\u3001ELMO\u7b49\uff09       |  [github](https://github.com/dbiir/UER-py)    |\n|    \u5f00\u6e90\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5408\u96c6    |        |  [github](https://github.com/ZhuiyiTechnology/pretrained-models)   |\n|   \u591a\u8bed\u8a00\u53e5\u5411\u91cf\u5305     |        |  [github](https://github.com/yannvgn/laserembeddings)   |\n|Language Model as a Service (LMaaS)|\u8bed\u8a00\u6a21\u578b\u5373\u670d\u52a1|[github](https://github.com/txsun1997/LMaaS-Papers)|\n|\u5f00\u6e90\u8bed\u8a00\u6a21\u578bGPT-NeoX-20B|200\u4ebf\u53c2\u6570\uff0c\u662f\u76ee\u524d\u6700\u5927\u7684\u53ef\u516c\u5f00\u8bbf\u95ee\u7684\u9884\u8bad\u7ec3\u901a\u7528\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b|[github](https://github.com/EleutherAI/gpt-neox)|\n|\u4e2d\u6587\u79d1\u5b66\u6587\u732e\u6570\u636e\u96c6\uff08CSL\uff09|\u5305\u542b 396,209 \u7bc7\u4e2d\u6587\u6838\u5fc3\u671f\u520a\u8bba\u6587\u5143\u4fe1\u606f \uff08\u6807\u9898\u3001\u6458\u8981\u3001\u5173\u952e\u8bcd\u3001\u5b66\u79d1\u3001\u95e8\u7c7b\uff09\u3002CSL \u6570\u636e\u96c6\u53ef\u4ee5\u4f5c\u4e3a\u9884\u8bad\u7ec3\u8bed\u6599\uff0c\u4e5f\u53ef\u4ee5\u6784\u5efa\u8bb8\u591aNLP\u4efb\u52a1\uff0c\u4f8b\u5982\u6587\u672c\u6458\u8981\uff08\u6807\u9898\u9884\u6d4b\uff09\u3001 \u5173\u952e\u8bcd\u751f\u6210\u548c\u6587\u672c\u5206\u7c7b\u7b49\u3002|[github](https://github.com/ydli-ai/CSL)|\n|\u5927\u6a21\u578b\u5f00\u53d1\u795e\u5668||[github](https://github.com/hpcaitech/ColossalAI)|\n\n# \u62bd\u53d6\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n|   \u65f6\u95f4\u62bd\u53d6   |    \u5df2\u96c6\u6210\u5230 python package [cocoNLP](https://github.com/fighting41love/cocoNLP)\u4e2d\uff0c\u6b22\u8fce\u8bd5\u7528    | [java version]( https://github.com/shinyke/Time-NLP)<br>[python version](https://github.com/zhanzecheng/Time_NLP) |\n|    \u795e\u7ecf\u7f51\u7edc\u5173\u7cfb\u62bd\u53d6 pytorch    |    \u6682\u4e0d\u652f\u6301\u4e2d\u6587     |    [github](https://github.com/ShulinCao/OpenNRE-PyTorch)    |\n|    \u57fa\u4e8ebert\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b pytorch    |  \u6682\u4e0d\u652f\u6301\u4e2d\u6587      |   [github](https://github.com/Kyubyong/bert_ner)     |\n|   \u5173\u952e\u8bcd(Keyphrase)\u62bd\u53d6\u5305 pke     |        |   [github](https://github.com/boudinfl/pke)     |\n|    BLINK\u6700\u5148\u8fdb\u7684\u5b9e\u4f53\u94fe\u63a5\u5e93    |        |   [github](https://github.com/facebookresearch/BLINK)  |\n|   BERT/CRF\u5b9e\u73b0\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b     |        |   [github](https://github.com/Louis-udm/NER-BERT-CRF)  |\n|    \u652f\u6301\u6279\u5e76\u884c\u7684LatticeLSTM\u4e2d\u6587\u547d\u540d\u5b9e\u4f53\u8bc6\u522b    |        |  [github](https://github.com/LeeSureman/Batch_Parallel_LatticeLSTM)   |\n|    \u6784\u5efa\u533b\u7597\u5b9e\u4f53\u8bc6\u522b\u7684\u6a21\u578b |   \u5305\u542b\u8bcd\u5178\u548c\u8bed\u6599\u6807\u6ce8\uff0c\u57fa\u4e8epython    |  [github](https://github.com/yixiu00001/LSTM-CRF-medical)   |\n|    \u57fa\u4e8eTensorFlow\u548cBERT\u7684\u7ba1\u9053\u5f0f\u5b9e\u4f53\u53ca\u5173\u7cfb\u62bd\u53d6    |       - Entity and Relation Extraction Based on TensorFlow and BERT \u57fa\u4e8eTensorFlow\u548cBERT\u7684\u7ba1\u9053\u5f0f\u5b9e\u4f53\u53ca\u5173\u7cfb\u62bd\u53d6\uff0c2019\u8bed\u8a00\u4e0e\u667a\u80fd\u6280\u672f\u7ade\u8d5b\u4fe1\u606f\u62bd\u53d6\u4efb\u52a1\u89e3\u51b3\u65b9\u6848\u3002Schema based Knowledge Extraction, SKE 2019   |   [github](https://github.com/yuanxiaosc/Entity-Relation-Extraction)  |\n| \u4e2d\u6587\u547d\u540d\u5b9e\u4f53\u8bc6\u522bNeuroNER vs BertNER       |        |    [github](https://github.com/EOA-AILab/NER-Chinese) |\n|  \u57fa\u4e8eBERT\u7684\u4e2d\u6587\u547d\u540d\u5b9e\u4f53\u8bc6\u522b      |        |  [github](https://github.com/lonePatient/BERT-NER-Pytorch)   |\n|    \u4e2d\u6587\u5173\u952e\u77ed\u8bed\u62bd\u53d6\u5de5\u5177    |        |  [github](https://github.com/dongrixinyu/chinese_keyphrase_extractor)   |\n| bert      |     \u7528\u4e8e\u4e2d\u6587\u547d\u540d\u5b9e\u4f53\u8bc6\u522b tensorflow\u7248\u672c   |   [github](https://github.com/macanv/BERT-BiLSTM-CRF-NER)  |\n|   bert-Kashgari     |    \u57fa\u4e8e keras \u7684\u5c01\u88c5\u5206\u7c7b\u6807\u6ce8\u6846\u67b6 Kashgari\uff0c\u51e0\u5206\u949f\u5373\u53ef\u642d\u5efa\u4e00\u4e2a\u5206\u7c7b\u6216\u8005\u5e8f\u5217\u6807\u6ce8\u6a21\u578b     |  [github](https://github.com/BrikerMan/Kashgari)  |\n|    cocoNLP    |  \u4eba\u540d\u3001\u5730\u5740\u3001\u90ae\u7bb1\u3001\u624b\u673a\u53f7\u3001\u624b\u673a\u5f52\u5c5e\u5730 \u7b49\u4fe1\u606f\u7684\u62bd\u53d6\uff0crake\u77ed\u8bed\u62bd\u53d6\u7b97\u6cd5\u3002  |   [github](https://github.com/fighting41love/cocoNLP)|\n|    Microsoft\u591a\u8bed\u8a00\u6570\u5b57/\u5355\u4f4d/\u5982\u65e5\u671f\u65f6\u95f4\u8bc6\u522b\u5305    |        |  [github](https://github.com/Microsoft/Recognizers-Text)   |\n| \u767e\u5ea6\u5f00\u6e90\u7684\u57fa\u51c6\u4fe1\u606f\u62bd\u53d6\u7cfb\u7edf       |        |   [github](https://github.com/baidu/information-extraction)  |\n|    \u4e2d\u6587\u5730\u5740\u5206\u8bcd\uff08\u5730\u5740\u5143\u7d20\u8bc6\u522b\u4e0e\u62bd\u53d6\uff09\uff0c\u901a\u8fc7\u5e8f\u5217\u6807\u6ce8\u8fdb\u884cNER    |        |  [github](https://github.com/yihenglu/chinese-address-segment)   |\n|    \u57fa\u4e8e\u4f9d\u5b58\u53e5\u6cd5\u7684\u5f00\u653e\u57df\u6587\u672c\u77e5\u8bc6\u4e09\u5143\u7ec4\u62bd\u53d6\u548c\u77e5\u8bc6\u5e93\u6784\u5efa    |        |   [github](https://github.com/lemonhu/open-entity-relation-extraction)  |\n|   \u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4e2d\u6587\u5173\u952e\u8bcd\u62bd\u53d6\u65b9\u6cd5     |        |   [github](https://github.com/sunyilgdx/SIFRank_zh)  |\n|  chinese_keyphrase_extractor (CKPE)      |  A tool for chinese keyphrase extraction \u4e00\u4e2a\u5feb\u901f\u4ece\u81ea\u7136\u8bed\u8a00\u6587\u672c\u4e2d\u63d0\u53d6\u548c\u8bc6\u522b\u5173\u952e\u77ed\u8bed\u7684\u5de5\u5177    |   [github](https://github.com/dongrixinyu/chinese_keyphrase_extractor)  |\n|    \u7b80\u5355\u7684\u7b80\u5386\u89e3\u6790\u5668\uff0c\u7528\u6765\u4ece\u7b80\u5386\u4e2d\u63d0\u53d6\u5173\u952e\u4fe1\u606f    |        |   [github](https://github.com/OmkarPathak/pyresparser)  |\n|   BERT-NER-Pytorch\u4e09\u79cd\u4e0d\u540c\u6a21\u5f0f\u7684BERT\u4e2d\u6587NER\u5b9e\u9a8c    |        |  [github](https://github.com/lonePatient/BERT-NER-Pytorch)   |\n\n\n\n\n# \u77e5\u8bc6\u56fe\u8c31\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n|    \u6e05\u534e\u5927\u5b66XLORE\u4e2d\u82f1\u6587\u8de8\u8bed\u8a00\u767e\u79d1\u77e5\u8bc6\u56fe\u8c31    |   \u767e\u5ea6\u3001\u4e2d\u6587\u7ef4\u57fa\u3001\u82f1\u6587\u7ef4\u57fa    |  [link](https://xlore.org/downloadhtml)     |\n|    \u6587\u6863\u56fe\u8c31\u81ea\u52a8\u751f\u6210    |        |   [github](https://github.com/liuhuanyong/TextGrapher)    |\n|     \u57fa\u4e8e\u533b\u7597\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\u7684\u95ee\u7b54\u7cfb\u7edf   |        |   [github](https://github.com/zhihao-chen/QASystemOnMedicalGraph) <br>\u8be5repo\u53c2\u8003\u4e86[github](https://github.com/liuhuanyong/QASystemOnMedicalKG)   |\n|    \u4e2d\u6587\u4eba\u7269\u5173\u7cfb\u77e5\u8bc6\u56fe\u8c31\u9879\u76ee    |        |   [github](https://github.com/liuhuanyong/PersonRelationKnowledgeGraph)  |\n|    AmpliGraph \u77e5\u8bc6\u56fe\u8c31\u8868\u793a\u5b66\u4e60(Python)\u5e93\u77e5\u8bc6\u56fe\u8c31\u6982\u5ff5\u94fe\u63a5\u9884\u6d4b    |        |  [github](https://github.com/Accenture/AmpliGraph)    |\n|    \u4e2d\u6587\u77e5\u8bc6\u56fe\u8c31\u8d44\u6599\u3001\u6570\u636e\u53ca\u5de5\u5177    |        |    [github](https://github.com/husthuke/awesome-knowledge-graph) |\n|    \u57fa\u4e8e\u767e\u5ea6\u767e\u79d1\u7684\u4e2d\u6587\u77e5\u8bc6\u56fe\u8c31  |     \u62bd\u53d6\u4e09\u5143\u7ec4\u4fe1\u606f\uff0c\u6784\u5efa\u4e2d\u6587\u77e5\u8bc6\u56fe\u8c31     |   [github](https://github.com/lixiang0/WEB_KG)  |\n|    Zincbase \u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u5de5\u5177\u5305    |        |  [github](https://github.com/tomgrek/zincbase)   |\n|    \u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u95ee\u7b54\u7cfb\u7edf    |        |  [github](https://github.com/WenRichard/KBQA-BERT)   |\n|    \u77e5\u8bc6\u56fe\u8c31\u6df1\u5ea6\u5b66\u4e60\u76f8\u5173\u8d44\u6599\u6574\u7406    |        |   [github](https://github.com/lihanghang/Knowledge-Graph)  |\n|   \u4e1c\u5357\u5927\u5b66\u300a\u77e5\u8bc6\u56fe\u8c31\u300b\u7814\u7a76\u751f\u8bfe\u7a0b(\u8d44\u6599)     |        |  [github](https://github.com/npubird/KnowledgeGraphCourse)   |\n|    \u77e5\u8bc6\u56fe\u8c31\u8f66\u97f3\u5de5\u4f5c\u9879\u76ee    |        |   [github](https://github.com/qiu997018209/KnowledgeGraph)  |\n|     \u300a\u6d77\u8d3c\u738b\u300b\u77e5\u8bc6\u56fe\u8c31   |        |   [github](https://github.com/mrbulb/ONEPIECE-KG)  |\n|    132\u4e2a\u77e5\u8bc6\u56fe\u8c31\u7684\u6570\u636e\u96c6    |    \u6db5\u76d6\u5e38\u8bc6\u3001\u57ce\u5e02\u3001\u91d1\u878d\u3001\u519c\u4e1a\u3001\u5730\u7406\u3001\u6c14\u8c61\u3001\u793e\u4ea4\u3001\u7269\u8054\u7f51\u3001\u533b\u7597\u3001\u5a31\u4e50\u3001\u751f\u6d3b\u3001\u5546\u4e1a\u3001\u51fa\u884c\u3001\u79d1\u6559    |   [link](http//openkg.cn)  |\n|    \u5927\u89c4\u6a21\u3001\u7ed3\u6784\u5316\u3001\u4e2d\u82f1\u6587\u53cc\u8bed\u7684\u65b0\u51a0\u77e5\u8bc6\u56fe\u8c31(COKG-19)    |        |   [link](http://www.openkg.cn/dataset?q=COKG-19)  |\n|    \u57fa\u4e8e\u4f9d\u5b58\u53e5\u6cd5\u4e0e\u8bed\u4e49\u89d2\u8272\u6807\u6ce8\u7684\u4e8b\u4ef6\u4e09\u5143\u7ec4\u62bd\u53d6    |        |    [github](https://github.com/liuhuanyong/EventTriplesExtraction)   |\n|     \u62bd\u8c61\u77e5\u8bc6\u56fe\u8c31  |   \u76ee\u524d\u89c4\u6a2150\u4e07\uff0c\u652f\u6301\u540d\u8bcd\u6027\u5b9e\u4f53\u3001\u72b6\u6001\u6027\u63cf\u8ff0\u3001\u4e8b\u4ef6\u6027\u52a8\u4f5c\u8fdb\u884c\u62bd\u8c61      |   [github](https://github.com/liuhuanyong/AbstractKnowledgeGraph)  |\n|    \u5927\u89c4\u6a21\u4e2d\u6587\u77e5\u8bc6\u56fe\u8c31\u6570\u636e14\u4ebf\u5b9e\u4f53    |        |   [github](https://github.com/ownthink/KnowledgeGraphData)  |\n|    Jiagu\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177     |    \u4ee5BiLSTM\u7b49\u6a21\u578b\u4e3a\u57fa\u7840\uff0c\u63d0\u4f9b\u77e5\u8bc6\u56fe\u8c31\u5173\u7cfb\u62bd\u53d6 \u4e2d\u6587\u5206\u8bcd \u8bcd\u6027\u6807\u6ce8 \u547d\u540d\u5b9e\u4f53\u8bc6\u522b \u60c5\u611f\u5206\u6790 \u65b0\u8bcd\u53d1\u73b0 \u5173\u952e\u8bcd \u6587\u672c\u6458\u8981 \u6587\u672c\u805a\u7c7b\u7b49\u529f\u80fd    |  [github](https://github.com/ownthink/Jiagu)   |\n|     medical_NER - \u4e2d\u6587\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u547d\u540d\u5b9e\u4f53\u8bc6\u522b   |        |    [github](https://github.com/pumpkinduo/KnowledgeGraph_NER) |\n|   \u77e5\u8bc6\u56fe\u8c31\u76f8\u5173\u5b66\u4e60\u8d44\u6599/\u6570\u636e\u96c6/\u5de5\u5177\u8d44\u6e90\u5927\u5217\u8868     |        |  [github](https://github.com/totogo/awesome-knowledge-graph)   |\n|    LibKGE\u9762\u5411\u53ef\u590d\u73b0\u7814\u7a76\u7684\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u5e93    |        |  [github](https://github.com/uma-pi1/kge)   |\n|   \u57fa\u4e8emongodb\u5b58\u50a8\u7684\u519b\u4e8b\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u9879\u76ee    |    \u5305\u62ec\u98de\u884c\u5668\u3001\u592a\u7a7a\u88c5\u5907\u7b498\u5927\u7c7b\uff0c100\u4f59\u5c0f\u7c7b\uff0c\u5171\u8ba15800\u9879\u7684\u519b\u4e8b\u6b66\u5668\u77e5\u8bc6\u5e93\uff0c\u8be5\u9879\u76ee\u4e0d\u4f7f\u7528\u56fe\u6570\u636e\u5e93\u8fdb\u884c\u5b58\u50a8\uff0c\u901a\u8fc7jieba\u8fdb\u884c\u95ee\u53e5\u89e3\u6790\uff0c\u95ee\u53e5\u5b9e\u4f53\u9879\u8bc6\u522b\uff0c\u57fa\u4e8e\u67e5\u8be2\u6a21\u677f\u5b8c\u6210\u591a\u7c7b\u95ee\u9898\u7684\u67e5\u8be2\uff0c\u4e3b\u8981\u662f\u63d0\u4f9b\u4e00\u79cd\u5de5\u4e1a\u754c\u7684\u95ee\u7b54\u601d\u60f3demo\u3002    |   [github](https://github.com/liuhuanyong/QAonMilitaryKG)  |\n|     \u4eac\u4e1c\u5546\u54c1\u77e5\u8bc6\u56fe\u8c31   |        |   [github](https://github.com/liuhuanyong/ProductKnowledgeGraph)  |\n|    \u57fa\u4e8e\u8fdc\u76d1\u7763\u7684\u4e2d\u6587\u5173\u7cfb\u62bd\u53d6    |        |    [github](https://github.com/xiaolalala/Distant-Supervised-Chinese-Relation-Extraction) |\n|  \u57fa\u4e8e\u533b\u836f\u77e5\u8bc6\u56fe\u8c31\u7684\u667a\u80fd\u95ee\u7b54\u7cfb\u7edf      |        |   [github](https://github.com/YeYzheng/KGQA-Based-On-medicine) |\n|    BLINK\u6700\u5148\u8fdb\u7684\u5b9e\u4f53\u94fe\u63a5\u5e93    |        |   [github](https://github.com/facebookresearch/BLINK)  |\n|   \u4e00\u4e2a\u5c0f\u578b\u7684\u8bc1\u5238\u77e5\u8bc6\u56fe\u8c31/\u77e5\u8bc6\u5e93     |        |    [github](https://github.com/lemonhu/stock-knowledge-graph) |\n|   dstlr\u975e\u7ed3\u6784\u5316\u6587\u672c\u53ef\u6269\u5c55\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u5e73\u53f0     |        |   [github](https://github.com/dstlry/dstlr)  |\n|  \u767e\u5ea6\u767e\u79d1\u4eba\u7269\u8bcd\u6761\u5c5e\u6027\u62bd\u53d6    |  \u7528\u57fa\u4e8eBERT\u7684\u5fae\u8c03\u548c\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u6765\u8fdb\u884c\u77e5\u8bc6\u56fe\u8c31      |   [github](https://github.com/sakuranew/BERT-AttributeExtraction)|\n|   \u65b0\u51a0\u80ba\u708e\u76f8\u5173\u6570\u636e     |  \u65b0\u51a0\u53ca\u5176\u4ed6\u7c7b\u578b\u80ba\u708e\u4e2d\u6587\u533b\u7597\u5bf9\u8bdd\u6570\u636e\u96c6\uff1b\u6e05\u534e\u5927\u5b66\u7b49\u673a\u6784\u7684\u5f00\u653e\u6570\u636e\u6e90\uff08COVID-19\uff09   | [github](https://www.aminer.cn/data-covid19/)<br>  [github](https://github.com/UCSD-AI4H/COVID-Dialogue) |\n|   DGL-KE \u56fe\u5d4c\u5165\u8868\u793a\u5b66\u4e60\u7b97\u6cd5     |        |   [github](https://github.com/awslabs/dgl-ke)  |\n|\u56e0\u679c\u5173\u7cfb\u56fe\u8c31||[method](https://github.com/liuhuanyong/CausalityEventExtraction) [data](https://github.com/fighting41love/CausalDataset)|\n|\u57fa\u4e8e\u591a\u9886\u57df\u6587\u672c\u6570\u636e\u96c6\u7684\u56e0\u679c\u4e8b\u4ef6\u5bf9||[link](http://thuctc.thunlp.org/)|\n\n# \u6587\u672c\u751f\u6210\n \n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n|   Texar    |   Toolkit for Text Generation and Beyond     |  [github](https://github.com/asyml/texar)  |\n|   Ehud Reiter\u6559\u6388\u7684\u535a\u5ba2    |        | [link](https://ehudreiter.com)  \u5317\u5927\u4e07\u5c0f\u519b\u6559\u6388\u5f3a\u529b\u63a8\u8350\uff0c\u8be5\u535a\u5ba2\u5bf9NLG\u6280\u672f\u3001\u8bc4\u4ef7\u4e0e\u5e94\u7528\u8fdb\u884c\u4e86\u6df1\u5165\u7684\u63a2\u8ba8\u4e0e\u53cd\u601d\u3002   |\n|   \u6587\u672c\u751f\u6210\u76f8\u5173\u8d44\u6e90\u5927\u5217\u8868    |        | [github](https://github.com/ChenChengKuan/awesome-text-generation)     |\n|  \u5f00\u653e\u57df\u5bf9\u8bdd\u751f\u6210\u53ca\u5728\u5fae\u8f6f\u5c0f\u51b0\u4e2d\u7684\u5b9e\u8df5      |   \u81ea\u7136\u8bed\u8a00\u751f\u6210\u8ba9\u673a\u5668\u638c\u63e1\u81ea\u52a8\u521b\u4f5c\u7684\u672c\u9886    |   [link](https://drive.google.com/file/d/1Mdna3q986k6OoJNsfAHznTtnMAEVzv5z/view)  |\n|    \u6587\u672c\u751f\u6210\u63a7\u5236   |        |  [github](https://github.com/harvardnlp/Talk-Latent/blob/master/mainpdf)    |\n|    \u81ea\u7136\u8bed\u8a00\u751f\u6210\u76f8\u5173\u8d44\u6e90\u5927\u5217\u8868   |        |   [github](https://github.com/tokenmill/awesome-nlg)  |\n|    \u7528BLEURT\u8bc4\u4ef7\u81ea\u7136\u8bed\u8a00\u751f\u6210   |        |  [link](https://ai.googleblog.com/2020/05/evaluating-natural-language-generation.html)  |\n|   \u81ea\u52a8\u5bf9\u8054\u6570\u636e\u53ca\u673a\u5668\u4eba    |        | [\u4ee3\u7801 link](https://github.com/wb14123/seq2seq-couplet) <br> [70\u4e07\u5bf9\u8054\u6570\u636e](https://github.com/wb14123/couplet-dataset)     |\n|   \u81ea\u52a8\u751f\u6210\u8bc4\u8bba     |   \u7528Transformer\u7f16\u89e3\u7801\u6a21\u578b\u5b9e\u73b0\u7684\u6839\u636eHacker News\u6587\u7ae0\u6807\u9898\u751f\u6210\u8bc4\u8bba     |   [github](https://github.com/leod/hncynic)  |\n|    \u81ea\u7136\u8bed\u8a00\u751f\u6210SQL\u8bed\u53e5\uff08\u82f1\u6587\uff09    |        |   [github](https://github.com/paulfitz/mlsql)  |\n|    \u81ea\u7136\u8bed\u8a00\u751f\u6210\u8d44\u6e90\u5927\u5168    |        |   [github](https://github.com/tokenmill/awesome-nlg)  |\n|    \u4e2d\u6587\u751f\u6210\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bc4    |        |  [github](https://github.com/CLUEbenchmark/CLGE)   |\n|     \u57fa\u4e8eGPT2\u7684\u7279\u5b9a\u4e3b\u9898\u6587\u672c\u751f\u6210/\u6587\u672c\u589e\u5e7f   |        |   [github](https://github.com/prakhar21/TextAugmentation-GPT2)  |\n|     \u7f16\u7801\u3001\u6807\u8bb0\u548c\u5b9e\u73b0\u4e00\u79cd\u53ef\u63a7\u9ad8\u6548\u7684\u6587\u672c\u751f\u6210\u65b9\u6cd5   |        |   [github](https://github.com/yannvgn/laserembeddings)  |\n|    TextFooler\u9488\u5bf9\u6587\u672c\u5206\u7c7b/\u63a8\u7406\u7684\u5bf9\u6297\u6587\u672c\u751f\u6210\u6a21\u5757    |        |   [github](https://github.com/jind11/TextFooler)  |\n|    SimBERT     |\u57fa\u4e8eUniLM\u601d\u60f3\u3001\u878d\u68c0\u7d22\u4e0e\u751f\u6210\u4e8e\u4e00\u4f53\u7684BERT\u6a21\u578b        |   [github](https://github.com/ZhuiyiTechnology/simbert)  |\n|    \u65b0\u8bcd\u751f\u6210\u53ca\u9020\u53e5    |    \u4e0d\u5b58\u5728\u7684\u8bcd\u7528GPT-2\u53d8\u4f53\u4ece\u5934\u751f\u6210\u65b0\u8bcd\u53ca\u5176\u5b9a\u4e49\u3001\u4f8b\u53e5    |    [github](https://github.com/turtlesoupy/this-word-does-not-exist) |\n|   \u7531\u6587\u672c\u81ea\u52a8\u751f\u6210\u591a\u9879\u9009\u62e9\u9898     |        |   [github](https://github.com/KristiyanVachev/Question-Generation)  |\n|     \u5408\u6210\u6570\u636e\u751f\u6210\u57fa\u51c6   |        |  [github](https://github.com/sdv-dev/SDGym)   |\n|       |        |    |\n\n# \u6587\u672c\u6458\u8981\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n|   \u4e2d\u6587\u6587\u672c\u6458\u8981/\u5173\u952e\u8bcd\u63d0\u53d6     |        |    [github](https://github.com/letiantian/TextRank4ZH) |\n|    \u57fa\u4e8e\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7684\u7b80\u5386\u81ea\u52a8\u6458\u8981    |        |   [github](https://github.com/DataTurks-Engg/Entity-Recognition-In-Resumes-SpaCy)  |\n|    \u6587\u672c\u81ea\u52a8\u6458\u8981\u5e93TextTeaser     |  \u4ec5\u652f\u6301\u82f1\u6587      |   [github](https://github.com/IndigoResearch/textteaser)  |\n|    \u57fa\u4e8eBERT\u7b49\u6700\u65b0\u8bed\u8a00\u6a21\u578b\u7684\u62bd\u53d6\u5f0f\u6458\u8981\u63d0\u53d6    |        |   [github](https://github.com/Hellisotherpeople/CX_DB8)  |\n|   Python\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u8fdb\u884c\u6587\u672c\u6458\u8981\u7684\u7efc\u5408\u6307\u5357     |        |   [link](https://mp.weixin.qq.com/s/gDZyTbM1nw3fbEnU--y3nQ)  |\n|   (Colab)\u62bd\u8c61\u6587\u672c\u6458\u8981\u5b9e\u73b0\u96c6\u9526(\u6559\u7a0b     |        |   [github](https://github.com/theamrzaki/text_summurization_abstractive_methods)  |\n\n\n# \u667a\u80fd\u95ee\u7b54\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n|    \u4e2d\u6587\u804a\u5929\u673a\u5668\u4eba    |  \u6839\u636e\u81ea\u5df1\u7684\u8bed\u6599\u8bad\u7ec3\u51fa\u81ea\u5df1\u60f3\u8981\u7684\u804a\u5929\u673a\u5668\u4eba\uff0c\u53ef\u4ee5\u7528\u4e8e\u667a\u80fd\u5ba2\u670d\u3001\u5728\u7ebf\u95ee\u7b54\u3001\u667a\u80fd\u804a\u5929\u7b49\u573a\u666f      |   [github](https://github.com/Doragd/Chinese-Chatbot-PyTorch-Implementation)  |\n|   \u6709\u8da3\u7684\u60c5\u8da3robot qingyun      |    qingyun \u8bad\u7ec3\u51fa\u6765\u7684\u4e2d\u6587\u804a\u5929\u673a\u5668\u4eba     |   [github](https://github.com/Doragd/Chinese-Chatbot-PyTorch-Implementation)  |\n|     \u5f00\u653e\u4e86\u5bf9\u8bdd\u673a\u5668\u4eba\u3001\u77e5\u8bc6\u56fe\u8c31\u3001\u8bed\u4e49\u7406\u89e3\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177\u53ca\u6570\u636e   |        |   [github](https://wwwownthinkcom/#header-n30)  |\n|  qa\u5bf9\u7684\u673a\u5668\u4eba      |    Amodel-for-Retrivalchatbot - \u5ba2\u670d\u673a\u5668\u4eba\uff0cChinese Retreival chatbot\uff08\u4e2d\u6587\u68c0\u7d22\u5f0f\u673a\u5668\u4eba\uff09    | [git](https://github.com/WenRichard/QAmodel-for-Retrievalchatbot)   |\n|  ConvLab\u5f00\u6e90\u591a\u57df\u7aef\u5230\u7aef\u5bf9\u8bdd\u7cfb\u7edf\u5e73\u53f0      |        |  [github](https://github.com/ConvLab/ConvLab)   |\n|   \u57fa\u4e8e\u6700\u65b0\u7248\u672crasa\u642d\u5efa\u7684\u5bf9\u8bdd\u7cfb\u7edf     |        |   [github](https://github.com/GaoQ1/rasa_chatbot_cn)  |\n|   \u57fa\u4e8e\u91d1\u878d-\u53f8\u6cd5\u9886\u57df(\u517c\u6709\u95f2\u804a\u6027\u8d28)\u7684\u804a\u5929\u673a\u5668\u4eba     |        |   [github](https://github.com/charlesXu86/Chatbot_CN)  |\n|    \u7aef\u5230\u7aef\u7684\u5c01\u95ed\u57df\u5bf9\u8bdd\u7cfb\u7edf    |        |  [github](https://github.com/cdqa-suite/cdQA)   |\n|     MiningZhiDaoQACorpus    |    580\u4e07\u767e\u5ea6\u77e5\u9053\u95ee\u7b54\u6570\u636e\u6316\u6398\u9879\u76ee\uff0c\u767e\u5ea6\u77e5\u9053\u95ee\u7b54\u8bed\u6599\u5e93\uff0c\u5305\u62ec\u8d85\u8fc7580\u4e07\u7684\u95ee\u9898\uff0c\u6bcf\u4e2a\u95ee\u9898\u5e26\u6709\u95ee\u9898\u6807\u7b7e\u3002\u57fa\u4e8e\u8be5\u95ee\u7b54\u8bed\u6599\u5e93\uff0c\u53ef\u652f\u6301\u591a\u79cd\u5e94\u7528\uff0c\u5982\u903b\u8f91\u6316\u6398    |    [github]() |\n|   \u7528\u4e8e\u4e2d\u6587\u95f2\u804a\u7684GPT2\u6a21\u578bGPT2-chitchat     |        |    [github](https://github.com/yangjianxin1/GPT2-chitchat) |\n|    \u57fa\u4e8e\u68c0\u7d22\u804a\u5929\u673a\u5668\u4eba\u591a\u8f6e\u54cd\u5e94\u9009\u62e9\u76f8\u5173\u8d44\u6e90\u5217\u8868(Leaderboards\u3001Datasets\u3001Papers)    |        |   [github](https://github.com/JasonForJoy/Leaderboards-for-Multi-Turn-Response-Selection)  |\n|   \u5fae\u8f6f\u5bf9\u8bdd\u673a\u5668\u4eba\u6846\u67b6     |        |    [github](https://github.com/microsoft/botframework) |\n|      chatbot-list  |   \u884c\u4e1a\u5185\u5173\u4e8e\u667a\u80fd\u5ba2\u670d\u3001\u804a\u5929\u673a\u5668\u4eba\u7684\u5e94\u7528\u548c\u67b6\u6784\u3001\u7b97\u6cd5\u5206\u4eab\u548c\u4ecb\u7ecd    |  [github](https://github.com/lizhe2004/chatbot-list)   |\n|     Chinese medical dialogue data \u4e2d\u6587\u533b\u7597\u5bf9\u8bdd\u6570\u636e\u96c6   |        |   [github](https://github.com/Toyhom/Chinese-medical-dialogue-data)  |\n|    \u4e00\u4e2a\u5927\u89c4\u6a21\u533b\u7597\u5bf9\u8bdd\u6570\u636e\u96c6    |   \u5305\u542b110\u4e07\u533b\u5b66\u54a8\u8be2\uff0c400\u4e07\u6761\u533b\u60a3\u5bf9\u8bdd    |    [github](https://github.com/UCSD-AI4H/Medical-Dialogue-System) |\n|    \u5927\u89c4\u6a21\u8de8\u9886\u57df\u4e2d\u6587\u4efb\u52a1\u5bfc\u5411\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\u96c6\u53ca\u6a21\u578bCrossWOZ    |        |  [paper & data](https://arxiv.org/pdf/200211893pdf)   |\n|   \u5f00\u6e90\u5bf9\u8bdd\u5f0f\u4fe1\u606f\u641c\u7d22\u5e73\u53f0     |        |    [github](https://github.com/microsoft/macaw) |\n|      \u60c5\u5883\u4e92\u52a8\u591a\u6a21\u6001\u5bf9\u8bdd\u6311\u62182020(DSTC9 2020)  |        |  [github](https://github.com/facebookresearch/simmc)   |\n|    \u7528Quora\u95ee\u9898\u5bf9\u8bad\u7ec3\u7684T5\u95ee\u9898\u610f\u8bd1(Paraphrase)    |        |   [github](https://github.com/renatoviolin/T5-paraphrase-generation)  |\n|    Google\u53d1\u5e03Taskmaster-2\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u5bf9\u8bdd\u6570\u636e\u96c6    |        |   [github](https://github.com/google-research-datasets/Taskmaster/tree/master/TM-2-2020)  |\n|    Haystack\u7075\u6d3b\u3001\u5f3a\u5927\u7684\u53ef\u6269\u5c55\u95ee\u7b54(QA)\u6846\u67b6    |        |   [github](https://github.com/deepset-ai/haystack)  |\n|    \u7aef\u5230\u7aef\u7684\u5c01\u95ed\u57df\u5bf9\u8bdd\u7cfb\u7edf    |        |   [github](https://github.com/cdqa-suite/cdQA)  |\n|   Amazon\u53d1\u5e03\u57fa\u4e8e\u77e5\u8bc6\u7684\u4eba-\u4eba\u5f00\u653e\u9886\u57df\u5bf9\u8bdd\u6570\u636e\u96c6     |        |   [github](https://github.com/alexa/alexa-prize-topical-chat-dataset/)  |\n|    \u57fa\u4e8e\u767e\u5ea6webqa\u4e0edureader\u6570\u636e\u96c6\u8bad\u7ec3\u7684Albert Large QA\u6a21\u578b    |        |   [github](https://github.com/wptoux/albert-chinese-large-webqa/tree/master)  |\n|   CommonsenseQA\u9762\u5411\u5e38\u8bc6\u7684\u82f1\u6587QA\u6311\u6218     |        |   [link](https://www.tau-nlp.org/commonsenseqa)  |\n|   MedQuAD(\u82f1\u6587)\u533b\u5b66\u95ee\u7b54\u6570\u636e\u96c6     |        |  [github](https://github.com/abachaa/MedQuAD)   |\n|    \u57fa\u4e8eAlbert\u3001Electra\uff0c\u7528\u7ef4\u57fa\u767e\u79d1\u6587\u672c\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u7684\u95ee\u7b54\u5f15\u64ce    |        |   [github](https://github.com/renatoviolin/Question-Answering-Albert-Electra)  |\n|   \u57fa\u4e8e14W\u6b4c\u66f2\u77e5\u8bc6\u5e93\u7684\u95ee\u7b54\u5c1d\u8bd5    |     \u529f\u80fd\u5305\u62ec\u6b4c\u8bcd\u63a5\u9f99\uff0c\u5df2\u77e5\u6b4c\u8bcd\u627e\u6b4c\u66f2\u4ee5\u53ca\u6b4c\u66f2\u6b4c\u624b\u6b4c\u8bcd\u4e09\u89d2\u5173\u7cfb\u7684\u95ee\u7b54   |    [github](https://github.com/liuhuanyong/MusicLyricChatbot) |\n\n\n# \u6587\u672c\u7ea0\u9519\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n|  \u4e2d\u6587\u6587\u672c\u7ea0\u9519\u6a21\u5757\u4ee3\u7801      |        |  [github](https://github.com/zedom1/error-detection)   |\n|    \u82f1\u6587\u62fc\u5199\u68c0\u67e5\u5e93    |        |   [github](https://github.com/barrust/pyspellchecker)  |\n|  python\u62fc\u5199\u68c0\u67e5\u5e93      |        |  [github](https://github.com/barrust/pyspellchecker)   |\n|    GitHub Typo Corpus\u5927\u89c4\u6a21GitHub\u591a\u8bed\u8a00\u62fc\u5199\u9519\u8bef/\u8bed\u6cd5\u9519\u8bef\u6570\u636e\u96c6    |        |   [github](https://github.com/mhagiwara/github-typo-corpus)  |\n|    BertPunc\u57fa\u4e8eBERT\u7684\u6700\u5148\u8fdb\u6807\u70b9\u4fee\u590d\u6a21\u578b    |        |   [github](https://github.com/nkrnrnk/BertPunc)  |\n|    \u4e2d\u6587\u5199\u4f5c\u6821\u5bf9\u5de5\u5177    |        |  [github](https://xiezuocat.com/#/)   |\n|\u6587\u672c\u7ea0\u9519\u6587\u732e\u5217\u8868| Chinese Spell Checking (CSC) and Grammatical Error Correction (GEC)|[github](https://github.com/nghuyong/text-correction-papers)|\n|\u6587\u672c\u667a\u80fd\u6821\u5bf9\u5927\u8d5b\u51a0\u519b\u65b9\u6848|\u5df2\u843d\u5730\u5e94\u7528\uff0c\u6765\u81ea\u82cf\u5dde\u5927\u5b66\u3001\u8fbe\u6469\u9662\u56e2\u961f|[link](https://mp.weixin.qq.com/s/2TjpmoYnt2BUTQVLi26AFA)|\n\n\n# \u591a\u6a21\u6001\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n|\u4e2d\u6587\u591a\u6a21\u6001\u6570\u636e\u96c6\u300c\u609f\u7a7a\u300d|\u534e\u4e3a\u8bfa\u4e9a\u65b9\u821f\u5b9e\u9a8c\u5ba4\u5f00\u6e90\u5927\u578b\uff0c\u5305\u542b1\u4ebf\u56fe\u6587\u5bf9|[github](https://wukong-dataset.github.io/wukong-dataset/)|\n|\u4e2d\u6587\u56fe\u6587\u8868\u5f81\u9884\u8bad\u7ec3\u6a21\u578bChinese-CLIP|\u4e2d\u6587\u7248\u672cCLIP\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5f00\u6e90\u591a\u4e2a\u6a21\u578b\u89c4\u6a21\uff0c\u51e0\u884c\u4ee3\u7801\u641e\u5b9a\u4e2d\u6587\u56fe\u6587\u8868\u5f81\u63d0\u53d6 & \u56fe\u6587\u68c0\u7d22|[github](https://github.com/OFA-Sys/Chinese-CLIP)|\n\n\n# \u8bed\u97f3\u5904\u7406\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n|    ASR \u8bed\u97f3\u6570\u636e\u96c6 + \u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4e2d\u6587\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf    |        |    [github](https://github.com/nl8590687/ASRT_SpeechRecognition)  |\n|   \u6e05\u534e\u5927\u5b66THCHS30\u4e2d\u6587\u8bed\u97f3\u6570\u636e\u96c6    |        |  [data_thchs30tgz-OpenSLR\u56fd\u5185\u955c\u50cf](<http//cn-mirroropenslrorg/resources/18/data_thchs30tgz>)<br>[data_thchs30tgz](<http//wwwopenslrorg/resources/18/data_thchs30tgz>) <br>[test-noisetgz-OpenSLR\u56fd\u5185\u955c\u50cf](<http//cn-mirroropenslrorg/resources/18/test-noisetgz>)[test-noisetgz](<http//wwwopenslrorg/resources/18/test-noisetgz>) <br>[resourcetgz-OpenSLR\u56fd\u5185\u955c\u50cf](<http//cn-mirroropenslrorg/resources/18/resourcetgz>)<br>[resourcetgz](<http//wwwopenslrorg/resources/18/resourcetgz>)<br>[Free ST Chinese Mandarin Corpus](<http//cn-mirroropenslrorg/resources/38/ST-CMDS-20170001_1-OStargz>)<br>[Free ST Chinese Mandarin Corpus](<http//wwwopenslrorg/resources/38/ST-CMDS-20170001_1-OStargz>)<br>[AIShell-1 \u5f00\u6e90\u7248\u6570\u636e\u96c6-OpenSLR\u56fd\u5185\u955c\u50cf](<http//cn-mirroropenslrorg/resources/33/data_aishelltgz>)<br>[AIShell-1 \u5f00\u6e90\u7248\u6570\u636e\u96c6](<http//wwwopenslrorg/resources/33/data_aishelltgz>)<br>[Primewords Chinese Corpus Set 1-OpenSLR\u56fd\u5185\u955c\u50cf](<http//cn-mirroropenslrorg/resources/47/primewords_md_2018_set1targz>)<br>[Primewords Chinese Corpus Set 1](<http//wwwopenslrorg/resources/47/primewords_md_2018_set1targz>) |\n|    \u7b11\u58f0\u68c0\u6d4b\u5668    |        |    [github](https://github.com/ideo/LaughDetection)  |\n|    Common Voice\u8bed\u97f3\u8bc6\u522b\u6570\u636e\u96c6\u65b0\u7248    |  \u5305\u62ec\u6765\u81ea42,000\u540d\u8d21\u732e\u8005\u8d85\u8fc71,400\u5c0f\u65f6\u7684\u8bed\u97f3\u6837\u672c\uff0c\u6db5github     |   [link](https://voice.mozilla.org/en/datasets)     |\n|    speech-aligner    |  \u4ece\u201c\u4eba\u58f0\u8bed\u97f3\u201d\u53ca\u5176\u201c\u8bed\u8a00\u6587\u672c\u201d\uff0c\u4ea7\u751f\u97f3\u7d20\u7ea7\u522b\u65f6\u95f4\u5bf9\u9f50\u6807\u6ce8\u7684\u5de5\u5177       |   [github](https://github.com/open-speech/speech-aligner)  |\n|   ASR\u8bed\u97f3\u5927\u8f9e\u5178/\u8bcd\u5178     |        |   [github](hhttps://github.com/aishell-foundation/DaCiDian)  |\n|     \u8bed\u97f3\u60c5\u611f\u5206\u6790   |        |   [github](https://github.com/MITESHPUTHRANNEU/Speech-Emotion-Analyzer)  |\n|    masr     | \u4e2d\u6587\u8bed\u97f3\u8bc6\u522b\uff0c\u63d0\u4f9b\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u9ad8\u8bc6\u522b\u7387       |   [github](https://github.com/lukhy/masr)  |\n|    \u9762\u5411\u8bed\u97f3\u8bc6\u522b\u7684\u4e2d\u6587\u6587\u672c\u89c4\u8303\u5316    |        |  [github](https://github.com/speech-io/chinese_text_normalization)   |\n|      \u8bed\u97f3\u8d28\u91cf\u8bc4\u4ef7\u6307\u6807(MOSNet, BSSEval, STOI, PESQ, SRMR)  |        |   [github](https://github.com/aliutkus/speechmetrics)  |\n|    \u9762\u5411\u8bed\u97f3\u8bc6\u522b\u7684\u4e2d\u6587/\u82f1\u6587\u53d1\u97f3\u8f9e\u5178    |        |   [github](https://github.com/speech-io/BigCiDian)  |\n|    CoVoSTFacebook\u53d1\u5e03\u7684\u591a\u8bed\u79cd\u8bed\u97f3-\u6587\u672c\u7ffb\u8bd1\u8bed\u6599\u5e93    | \u5305\u62ec11\u79cd\u8bed\u8a00(\u6cd5\u8bed\u3001\u5fb7\u8bed\u3001\u8377\u5170\u8bed\u3001\u4fc4\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u610f\u5927\u5229\u8bed\u3001\u571f\u8033\u5176\u8bed\u3001\u6ce2\u65af\u8bed\u3001\u745e\u5178\u8bed\u3001\u8499\u53e4\u8bed\u548c\u4e2d\u6587)\u7684\u8bed\u97f3\u3001\u6587\u5b57\u8f6c\u5f55\u53ca\u82f1\u6587\u8bd1\u6587      |    [github](https://github.com/facebookresearch/covost) |\n|    Parakeet\u57fa\u4e8ePaddlePaddle\u7684\u6587\u672c-\u8bed\u97f3\u5408\u6210    |        |   [github](https://github.com/PaddlePaddle/Parakeet)  |\n|     (Java)\u51c6\u786e\u7684\u8bed\u97f3\u81ea\u7136\u8bed\u8a00\u68c0\u6d4b\u5e93   |        |   [github](https://github.com/pemistahl/lingua)  |\n|   CoVoSTFacebook\u53d1\u5e03\u7684\u591a\u8bed\u79cd\u8bed\u97f3-\u6587\u672c\u7ffb\u8bd1\u8bed\u6599\u5e93     |        |   [github](https://github.com/facebookresearch/covost)  |\n|   TensorFlow 2 \u5b9e\u73b0\u7684\u6587\u672c\u8bed\u97f3\u5408\u6210     |        |   [github](https://github.com/as-ideas/TransformerTTS)  |\n|    Python\u97f3\u9891\u7279\u5f81\u63d0\u53d6\u5305    |        |  [github](https://github.com/novoic/surfboard)   |\n|   ViSQOL\u97f3\u9891\u8d28\u91cf\u611f\u77e5\u5ba2\u89c2\u3001\u5b8c\u6574\u53c2\u8003\u6307\u6807\uff0c\u5206\u97f3\u9891\u3001\u8bed\u97f3\u4e24\u79cd\u6a21\u5f0f     |        |  [github](https://github.com/google/visqol)   |\n|    zhrtvc    |     \u597d\u7528\u7684\u4e2d\u6587\u8bed\u97f3\u514b\u9686\u517c\u4e2d\u6587\u8bed\u97f3\u5408\u6210\u7cfb\u7edf     |  [github](https://github.com/KuangDD/zhrtvc)  |\n|      aukit    |  \u597d\u7528\u7684\u8bed\u97f3\u5904\u7406\u5de5\u5177\u7bb1\uff0c\u5305\u542b\u8bed\u97f3\u964d\u566a\u3001\u97f3\u9891\u683c\u5f0f\u8f6c\u6362\u3001\u7279\u5f81\u9891\u8c31\u751f\u6210\u7b49\u6a21\u5757       |  [github](https://github.com/KuangDD/aukit)  |\n|      phkit    |   \u597d\u7528\u7684\u97f3\u7d20\u5904\u7406\u5de5\u5177\u7bb1\uff0c\u5305\u542b\u4e2d\u6587\u97f3\u7d20\u3001\u82f1\u6587\u97f3\u7d20\u3001\u6587\u672c\u8f6c\u62fc\u97f3\u3001\u6587\u672c\u6b63\u5219\u5316\u7b49\u6a21\u5757     |  [github](https://github.com/KuangDD/phkit)   |\n|     zhvoice     |   \u4e2d\u6587\u8bed\u97f3\u8bed\u6599\uff0c\u8bed\u97f3\u66f4\u52a0\u6e05\u6670\u81ea\u7136\uff0c\u5305\u542b8\u4e2a\u5f00\u6e90\u6570\u636e\u96c6\uff0c3200\u4e2a\u8bf4\u8bdd\u4eba\uff0c900\u5c0f\u65f6\u8bed\u97f3\uff0c1300\u4e07\u5b57     |  [github](https://github.com/KuangDD/zhvoice)   |\n|   audio\u9762\u5411\u8bed\u97f3\u884c\u4e3a\u68c0\u6d4b     |  \u3001\u4e8c\u503c\u5316\u3001\u8bf4\u8bdd\u4eba\u8bc6\u522b\u3001\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u3001\u60c5\u611f\u8bc6\u522b\u7b49\u4efb\u52a1\u7684\u97f3\u9891\u6807\u6ce8\u5de5\u5177      |  [github](https://github.com/midas-research/audino)   |\n|     \u6df1\u5ea6\u5b66\u4e60\u60c5\u611f\u6587\u672c\u8bed\u97f3\u5408\u6210   |        |  [github](https://github.com/Emotional-Text-to-Speech/dl-for-emo-tts)   |\n|   Python\u97f3\u9891\u6570\u636e\u589e\u5e7f\u5e93     |        |   [github](https://github.com/iver56/audiomentations)  |\n|   \u57fa\u4e8e\u5927\u89c4\u6a21\u97f3\u9891\u6570\u636e\u96c6Audioset\u7684\u97f3\u9891\u589e\u5f3a     |        |    [github](https://github.com/AppleHolic/audioset_augmentor) |\n|    \u8bed\u58f0\u8fc1\u79fb    |        |   [github](https://github.com/fighting41love/become-yukarin)  |\n\n\n\n# \u6587\u6863\u5904\u7406\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n|LayoutLM-v3\u6587\u6863\u7406\u89e3\u6a21\u578b||[github](https://github.com/microsoft/unilm/tree/master/layoutlmv3)|\n|   PyLaia\u9762\u5411\u624b\u5199\u6587\u6863\u5206\u6790\u7684\u6df1\u5ea6\u5b66\u4e60\u5de5\u5177\u5305     |        |   [github](https://github.com/jpuigcerver/PyLaia)  |\n|    \u5355\u6587\u6863\u975e\u76d1\u7763\u7684\u5173\u952e\u8bcd\u62bd\u53d6    |        |  [github](https://github.com/LIAAD/yake)   |\n|      DocSearch\u514d\u8d39\u6587\u6863\u641c\u7d22\u5f15\u64ce  |        |   [github](https://github.com/algolia/docsearch)  |\n|  fdfgen       |    \u80fd\u591f\u81ea\u52a8\u521b\u5efapdf\u6587\u6863\uff0c\u5e76\u586b\u5199\u4fe1\u606f     | [link](https://github.com/ccnmtl/fdfgen)   |\n| pdfx       |   \u81ea\u52a8\u62bd\u53d6\u51fa\u5f15\u7528\u53c2\u8003\u6587\u732e\uff0c\u5e76\u4e0b\u8f7d\u5bf9\u5e94\u7684pdf\u6587\u4ef6 | [link](https://github.com/metachris/pdfx)   |\n|     invoice2data   |   \u53d1\u7968pdf\u4fe1\u606f\u62bd\u53d6     |  [invoice2data](https://github.com/invoice-x/invoice2data)  |\n|   pdf\u6587\u6863\u4fe1\u606f\u62bd\u53d6    |        |  [github](https://github.com/jstockwin/py-pdf-parser)   |\n|PDFMiner     |     PDFMiner\u80fd\u83b7\u53d6\u9875\u9762\u4e2d\u6587\u672c\u7684\u51c6\u786e\u4f4d\u7f6e\uff0c\u4ee5\u53ca\u5b57\u4f53\u6216\u884c\u7b49\u5176\u4ed6\u4fe1\u606f\u3002\u5b83\u8fd8\u6709\u4e00\u4e2aPDF\u8f6c\u6362\u5668\uff0c\u53ef\u4ee5\u5c06PDF\u6587\u4ef6\u8f6c\u6362\u6210\u5176\u4ed6\u6587\u672c\u683c\u5f0f(\u5982HTML)\u3002\u8fd8\u6709\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u89e3\u6790\u5668PDF\uff0c\u53ef\u4ee5\u7528\u4e8e\u6587\u672c\u5206\u6790\u4ee5\u5916\u7684\u5176\u4ed6\u7528\u9014\u3002   |  [link](https://github.com/euske/pdfminer)  |\n|  PyPDF2      |    PyPDF 2\u662f\u4e00\u4e2apython PDF\u5e93\uff0c\u80fd\u591f\u5206\u5272\u3001\u5408\u5e76\u3001\u88c1\u526a\u548c\u8f6c\u6362PDF\u6587\u4ef6\u7684\u9875\u9762\u3002\u5b83\u8fd8\u53ef\u4ee5\u5411PDF\u6587\u4ef6\u4e2d\u6dfb\u52a0\u81ea\u5b9a\u4e49\u6570\u636e\u3001\u67e5\u770b\u9009\u9879\u548c\u5bc6\u7801\u3002\u5b83\u53ef\u4ee5\u4ecePDF\u68c0\u7d22\u6587\u672c\u548c\u5143\u6570\u636e\uff0c\u8fd8\u53ef\u4ee5\u5c06\u6574\u4e2a\u6587\u4ef6\u5408\u5e76\u5728\u4e00\u8d77\u3002    |  [link](https://github.com/mstamy2/PyPDF2)   |\n|   PyPDF2     |     PyPDF 2\u662f\u4e00\u4e2apython PDF\u5e93\uff0c\u80fd\u591f\u5206\u5272\u3001\u5408\u5e76\u3001\u88c1\u526a\u548c\u8f6c\u6362PDF\u6587\u4ef6\u7684\u9875\u9762\u3002\u5b83\u8fd8\u53ef\u4ee5\u5411PDF\u6587\u4ef6\u4e2d\u6dfb\u52a0\u81ea\u5b9a\u4e49\u6570\u636e\u3001\u67e5\u770b\u9009\u9879\u548c\u5bc6\u7801\u3002\u5b83\u53ef\u4ee5\u4ecePDF\u68c0\u7d22\u6587\u672c\u548c\u5143\u6570\u636e\uff0c\u8fd8\u53ef\u4ee5\u5c06\u6574\u4e2a\u6587\u4ef6\u5408\u5e76\u5728\u4e00\u8d77\u3002  |   [link](https://github.com/mstamy2/PyPDF2)  |\n|    ReportLab   |      ReportLab\u80fd\u5feb\u901f\u521b\u5efaPDF \u6587\u6863\u3002\u7ecf\u8fc7\u65f6\u95f4\u8bc1\u660e\u7684\u3001\u8d85\u597d\u7528\u7684\u5f00\u6e90\u9879\u76ee\uff0c\u7528\u4e8e\u521b\u5efa\u590d\u6742\u7684\u3001\u6570\u636e\u9a71\u52a8\u7684PDF\u6587\u6863\u548c\u81ea\u5b9a\u4e49\u77e2\u91cf\u56fe\u5f62\u3002\u5b83\u662f\u514d\u8d39\u7684\uff0c\u5f00\u6e90\u7684\uff0c\u7528Python\u7f16\u5199\u7684\u3002\u8be5\u8f6f\u4ef6\u5305\u6bcf\u6708\u4e0b\u8f7d5\u4e07\u591a\u6b21\uff0c\u662f\u6807\u51c6Linux\u53d1\u884c\u7248\u7684\u4e00\u90e8\u5206\uff0c\u5d4c\u5165\u5230\u8bb8\u591a\u4ea7\u54c1\u4e2d\uff0c\u5e76\u88ab\u9009\u4e2d\u4e3aWikipedia\u7684\u6253\u5370/\u5bfc\u51fa\u529f\u80fd\u63d0\u4f9b\u52a8\u529b\u3002  | [link](https://www.reportlab.com/opensource/)   |\n|    SIMPdfPython\u5199\u7684\u7b80\u5355PDF\u6587\u4ef6\u6587\u5b57\u7f16\u8f91\u5668    |        |   [github](https://github.com/shashanoid/Simpdf)  |\n|pdf-diff |PDF\u6587\u4ef6diff\u5de5\u5177 \u53ef\u663e\u793a\u4e24\u4e2apdf\u6587\u6863\u7684\u5dee\u522b| [github](https://github.com/serhack/pdf-diff)|\n\n\n# \u8868\u683c\u5904\u7406\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n|  \u7528unet\u5b9e\u73b0\u5bf9\u6587\u6863\u8868\u683c\u7684\u81ea\u52a8\u68c0\u6d4b\uff0c\u8868\u683c\u91cd\u5efa      |        |   [github](https://github.com/chineseocr/table-ocr)  |\n|   pdftabextract    |  \u7528\u4e8eOCR\u8bc6\u522b\u540e\u7684\u8868\u683c\u4fe1\u606f\u89e3\u6790\uff0c\u5f88\u5f3a\u5927      |    [link](https://github.com/WZBSocialScienceCenter/pdftabextract)   |\n| tabula-py    |     \u76f4\u63a5\u5c06pdf\u4e2d\u7684\u8868\u683c\u4fe1\u606f\u8f6c\u6362\u4e3apandas\u7684dataframe\uff0c\u6709java\u548cpython\u4e24\u79cd\u7248\u672c\u4ee3\u7801   |    [](https://github.com/chezou/tabula-py) |\n|   camelot     |   pdf\u8868\u683c\u89e3\u6790       |  [link](https://github.com/atlanhq/camelot)  |\n| pdfplumber      |   pdf\u8868\u683c\u89e3\u6790     | [](https://github.com/jsvine/pdfplumber)   |\n|   PubLayNet   |     \u80fd\u591f\u5212\u5206\u6bb5\u843d\u3001\u8bc6\u522b\u8868\u683c\u3001\u56fe\u7247   |  [link](https://github.com/ibm-aur-nlp/PubTabNet)  |\n|    \u4ece\u8bba\u6587\u4e2d\u63d0\u53d6\u8868\u683c\u6570\u636e |        |    [github](https://github.com/paperswithcode/axcell) |\n|    \u7528BERT\u5728\u8868\u683c\u4e2d\u5bfb\u627e\u7b54\u6848    |        |  [github](https://github.com/google-research/tapas)   |\n|    \u8868\u683c\u95ee\u7b54\u7684\u7cfb\u5217\u6587\u7ae0    |        |  [\u7b80\u4ecb](https://mp.weixin.qq.com/s?__biz=MzAxMDk0OTI3Ng==&mid=2247484103&idx=2&sn=4a5b50557ab9178270866d812bcfc87f&chksm=9b49c534ac3e4c22de7c53ae5d986fac60a7641c0c072d4038d9d4efd6beb24a22df9f859d08&scene=21#wechat_redirect)<br>[\u6a21\u578b](https://mp.weixin.qq.com/s?__biz=MzAxMDk0OTI3Ng==&mid=2247484103&idx=1&sn=73f37fbc1dbd5fdc2d4ad54f58693ef3&chksm=9b49c534ac3e4c222f6a320674b3728cf8567b9a16e6d66b8fdcf06703b05a16a9c9ed9d79a3&scene=21#wechat_redirect)<br>[\u5b8c\u7ed3\u7bc7](https://mp.weixin.qq.com/s/ee1DG_vO2qblqFC6zO97pA)  |\n|     \u4f7f\u7528GAN\u751f\u6210\u8868\u683c\u6570\u636e\uff08\u4ec5\u652f\u6301\u82f1\u6587\uff09   |        |   [github](https://github.com/Diyago/GAN-for-tabular-data)  |\n|  carefree-learn(PyTorch)      |    \u8868\u683c\u6570\u636e\u96c6\u81ea\u52a8\u5316\u673a\u5668\u5b66\u4e60(AutoML)\u5305    |  [github](https://github.com/carefree0910/carefree-learn)   |\n|   \u5c01\u95ed\u57df\u5fae\u8c03\u8868\u683c\u68c0\u6d4b     |        |  [github](https://github.com/holms-ur/fine-tuning)   |\n|   PDF\u8868\u683c\u6570\u636e\u63d0\u53d6\u5de5\u5177     |        |   [github](https://github.com/camelot-dev/camelot)  |\n|     TaBERT\u7406\u89e3\u8868\u683c\u6570\u636e\u67e5\u8be2\u7684\u65b0\u6a21\u578b   |        |  [paper](https://scontent-hkt1-1xxfbcdnnet/v/t398562-6/106708899_597765107810230_1899215558892880563_npdf?_nc_cat=107&_nc_sid=ae5e01&_nc_ohc=4sN3TJwewSIAX8iliBD&_nc_ht=scontent-hkt1-1xx&oh=eccb9795f027ff63be61ff4a5e337c02&oe=5F316505)   |\n| \u8868\u683c\u5904\u7406 | Awesome-Table-Recognition | [github](https://github.com/cv-small-snails/Awesome-Table-Recognition)|\n\n\n\n# \u6587\u672c\u5339\u914d\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n|    \u53e5\u5b50\u3001QA\u76f8\u4f3c\u5ea6\u5339\u914dMatchZoo    |   \u6587\u672c\u76f8\u4f3c\u5ea6\u5339\u914d\u7b97\u6cd5\u7684\u96c6\u5408\uff0c\u5305\u542b\u591a\u4e2a\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u503c\u5f97\u5c1d\u8bd5\u3002    |    [github](https://github.com/NTMC-Community/MatchZoo)  |\n|    \u4e2d\u6587\u95ee\u9898\u53e5\u5b50\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u6bd4\u8d5b\u53ca\u65b9\u6848\u6c47\u603b    |        |   [github](https://github.com/ShuaichiLi/Chinese-sentence-similarity-task)  |\n|    similarity\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u5de5\u5177\u5305    |   java\u7f16\u5199,\u7528\u4e8e\u8bcd\u8bed\u3001\u77ed\u8bed\u3001\u53e5\u5b50\u3001\u8bcd\u6cd5\u5206\u6790\u3001\u60c5\u611f\u5206\u6790\u3001\u8bed\u4e49\u5206\u6790\u7b49\u76f8\u5173\u7684\u76f8\u4f3c\u5ea6\u8ba1\u7b97    |  [github](https://github.com/shibing624/similarity)   |\n|    \u4e2d\u6587\u8bcd\u8bed\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u65b9\u6cd5    |  \u7efc\u5408\u4e86\u540c\u4e49\u8bcd\u8bcd\u6797\u6269\u5c55\u7248\u4e0e\u77e5\u7f51\uff08Hownet\uff09\u7684\u8bcd\u8bed\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u65b9\u6cd5\uff0c\u8bcd\u6c47\u8986\u76d6\u66f4\u591a\u3001\u7ed3\u679c\u66f4\u51c6\u786e\u3002      |    [gihtub](https://github.com/yaleimeng/Final_word_Similarity) |\n|    Python\u5b57\u7b26\u4e32\u76f8\u4f3c\u6027\u7b97\u6cd5\u5e93    |        |    [github](https://github.com/luozhouyang/python-string-similarity) |\n|    \u57fa\u4e8eSiamese bilstm\u6a21\u578b\u7684\u76f8\u4f3c\u53e5\u5b50\u5224\u5b9a\u6a21\u578b,\u63d0\u4f9b\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u6d4b\u8bd5\u6570\u636e\u96c6    |    \u63d0\u4f9b\u4e8610\u4e07\u4e2a\u8bad\u7ec3\u6837\u672c    | [github](https://github.com/liuhuanyong/SiameseSentenceSimilarity)    |\n\n\n# \u6587\u672c\u6570\u636e\u589e\u5f3a\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n|    \u4e2d\u6587NLP\u6570\u636e\u589e\u5f3a\uff08EDA\uff09\u5de5\u5177  |        |    [github](https://github.com/zhanlaoban/eda_nlp_for_Chinese) |\n|   \u82f1\u6587NLP\u6570\u636e\u589e\u5f3a\u5de5\u5177     |        |  [github](https://github.com/makcedward/nlpaug)   |\n|    \u4e00\u952e\u4e2d\u6587\u6570\u636e\u589e\u5f3a\u5de5\u5177    |        | [github](https://github.com/425776024/nlpcda)   |\n|    \u6570\u636e\u589e\u5f3a\u5728\u673a\u5668\u7ffb\u8bd1\u53ca\u5176\u4ed6nlp\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u53ca\u6548\u679c    |        |  [link](https://mp.weixin.qq.com/s/_aVwSWuYho_7MUT0LuFgVA)   |\n|    NLP\u6570\u636e\u589e\u5e7f\u8d44\u6e90\u96c6    |        |   [github](https://github.com/quincyliang/nlp-data-augmentation)  |\n\n\n# \u5e38\u7528\u6b63\u5219\u8868\u8fbe\u5f0f\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n|   \u62bd\u53d6email\u7684\u6b63\u5219\u8868\u8fbe\u5f0f    |    |  \u5df2\u96c6\u6210\u5230 python package [cocoNLP](https://github.com/fighting41love/cocoNLP)\u4e2d\uff0c\u6b22\u8fce\u8bd5\u7528  |\n|   \u62bd\u53d6phone_number    |     | \u5df2\u96c6\u6210\u5230 python package [cocoNLP](https://github.com/fighting41love/cocoNLP)\u4e2d\uff0c\u6b22\u8fce\u8bd5\u7528   |\n|    \u62bd\u53d6\u8eab\u4efd\u8bc1\u53f7\u7684\u6b63\u5219\u8868\u8fbe\u5f0f   |  IDCards_pattern = r'^([1-9]\\d{5}[12]\\d{3}(0[1-9]\\|1[012])(0[1-9]\\|[12][0-9]\\|3[01])\\d{3}[0-9xX])<br>IDs = re.findall(IDCards_pattern, text, flags=0)|   \n  IP\u5730\u5740\u6b63\u5219\u8868\u8fbe\u5f0f|(25[0-5]\\|  2[0-4]\\d\\|  [0-1]\\d{2}\\|  [1-9]?\\d)\\.(25[0-5]\\|  2[0-4]\\d\\|  [0-1]\\d{2}\\|  [1-9]?\\d)\\.(25[0-5]\\|  2[0-4]\\d\\|  [0-1]\\d{2}\\|  [1-9]?\\d)\\.(25[0-5]\\|  2[0-4]\\d\\|  [0-1]\\d{2}\\|  [1-9]?\\d)||\n|  \u817e\u8bafQQ\u53f7\u6b63\u5219\u8868\u8fbe\u5f0f     |   \\[1-9]([0-9]{5,11})     |    |\n|   \u56fd\u5185\u56fa\u8bdd\u53f7\u7801\u6b63\u5219\u8868\u8fbe\u5f0f    |      [0-9-()\uff08\uff09]{7,18}  |    |\n|   \u7528\u6237\u540d\u6b63\u5219\u8868\u8fbe\u5f0f    |  [A-Za-z0-9_\\-\\u4e00-\\u9fa5]+      |    |\n|    \u56fd\u5185\u7535\u8bdd\u53f7\u7801\u6b63\u5219\u5339\u914d\uff08\u4e09\u5927\u8fd0\u8425\u5546+\u865a\u62df\u7b49\uff09    |        |   [github](https://github.com/VincentSit/ChinaMobilePhoneNumberRegex)  |\n|     \u6b63\u5219\u8868\u8fbe\u5f0f\u6559\u7a0b   |        |  [github](https://github.com/ziishaned/learn-regex/blob/master/translations/README-cnmd)   |\n\n\n# \u6587\u672c\u68c0\u7d22\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n|   \u9ad8\u6548\u6a21\u7cca\u641c\u7d22\u5de5\u5177     |        |  [github](https://github.com/Yggdroot/LeaderF)   |\n|  \u9762\u5411\u5404\u8bed\u79cd/\u4efb\u52a1\u7684BERT\u6a21\u578b\u5927\u5217\u8868/\u641c\u7d22\u5f15\u64ce      |        |    [link](https://bertlang.unibocconi.it/) |\n|    Deepmatch\u9488\u5bf9\u63a8\u8350\u3001\u5e7f\u544a\u548c\u641c\u7d22\u7684\u6df1\u5ea6\u5339\u914d\u6a21\u578b\u5e93    |        |   [github](https://github.com/shenweichen/DeepMatch)  |\n|    wwsearch\u662f\u4f01\u4e1a\u5fae\u4fe1\u540e\u53f0\u81ea\u7814\u7684\u5168\u6587\u68c0\u7d22\u5f15\u64ce    |        |   [github](https://github.com/Tencent/wwsearch)  |\n|   aili - the fastest in-memory index in the East \u4e1c\u534a\u7403\u6700\u5feb\u5e76\u53d1\u7d22\u5f15     |        |    [github](https://github.com/UncP/aili) |\n|\u9ad8\u6548\u7684\u5b57\u7b26\u4e32\u5339\u914d\u5de5\u5177 RapidFuzz|a fast string matching library for Python and C++, which is using the string similarity calculations from FuzzyWuzzy|[github](https://github.com/maxbachmann/rapidfuzz)|\n\n# \u9605\u8bfb\u7406\u89e3\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n|   \u9ad8\u6548\u6a21\u7cca\u641c\u7d22\u5de5\u5177     |        |  [github](https://github.com/Yggdroot/LeaderF)   |\n|  \u9762\u5411\u5404\u8bed\u79cd/\u4efb\u52a1\u7684BERT\u6a21\u578b\u5927\u5217\u8868/\u641c\u7d22\u5f15\u64ce      |        |    [link](https://bertlang.uniboc.coni.it) |\n|    Deepmatch\u9488\u5bf9\u63a8\u8350\u3001\u5e7f\u544a\u548c\u641c\u7d22\u7684\u6df1\u5ea6\u5339\u914d\u6a21\u578b\u5e93    |        |   [github](https://github.com/shenweichen/DeepMatch)  |\n|   allennlp\u9605\u8bfb\u7406\u89e3\u652f\u6301\u591a\u79cd\u6570\u636e\u548c\u6a21     |        |  [github](https://github.com/allenai/allennlp-reading-comprehension)   |\n\n\n\n# \u60c5\u611f\u5206\u6790\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n|     \u65b9\u9762\u60c5\u611f\u5206\u6790\u5305   |        |   [github](https://github.com/ScalaConsultants/Aspect-Based-Sentiment-Analysis)  |\n|    awesome-nlp-sentiment-analysis    |    \u60c5\u611f\u5206\u6790\u3001\u60c5\u7eea\u539f\u56e0\u8bc6\u522b\u3001\u8bc4\u4ef7\u5bf9\u8c61\u548c\u8bc4\u4ef7\u8bcd\u62bd\u53d6   |  [github](https://github.com/haiker2011/awesome-nlp-sentiment-analysis)   |\n|    \u60c5\u611f\u5206\u6790\u6280\u672f\u8ba9\u667a\u80fd\u5ba2\u670d\u66f4\u61c2\u4eba\u7c7b\u60c5\u611f    |        |   [github](https://developeraliyuncom/article/761513?utm_content=g_1000124809)  |\n\n\n# \u4e8b\u4ef6\u62bd\u53d6\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n|   \u4e2d\u6587\u4e8b\u4ef6\u62bd\u53d6    |        |  [github](https://github.com/liuhuanyong/ComplexEventExtraction)  |\n|     NLP\u4e8b\u4ef6\u63d0\u53d6\u6587\u732e\u8d44\u6e90\u5217\u8868   |        |  [github](https://github.com/BaptisteBlouin/EventExtractionPapers)   |\n|    PyTorch\u5b9e\u73b0\u7684BERT\u4e8b\u4ef6\u62bd\u53d6(ACE 2005 corpus)    |         | [github](https://github.com/nlpcl-lab/bert-event-extraction)   |\n|  \u65b0\u95fb\u4e8b\u4ef6\u7ebf\u7d22\u62bd\u53d6      |        |  [github](https://github.com/liuhuanyong/ImportantEventExtractor)   |\n\n\n# \u673a\u5668\u7ffb\u8bd1\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n|   \u65e0\u9053\u8bcd\u5178     |  \u6709\u9053\u8bcd\u5178\u7684\u547d\u4ee4\u884c\u7248\u672c\uff0c\u652f\u6301\u82f1\u6c49\u4e92\u67e5\u548c\u5728\u7ebf\u67e5\u8be2      |  [github](https://github.com/ChestnutHeng/Wudao-dict)   |\n|NLLB|\u652f\u6301200+\u79cd\u8bed\u8a00\u4efb\u610f\u4e92\u8bd1\u7684\u8bed\u8a00\u6a21\u578bNLLB|[link](https://openbmb.github.io/BMList/list/)|\n|Easy-Translate|\u5728\u672c\u5730\u7ffb\u8bd1\u5927\u6587\u672c\u6587\u4ef6\u7684\u811a\u672c\uff0c\u57fa\u4e8eFacebook/Meta AI\u7684 M2M100\u6a21\u578b\u548cNLLB200\u6a21\u578b\uff0c\u652f\u6301200+\u79cd\u8bed\u8a00|[github](https://github.com/ikergarcia1996/Easy-Translate/fork)|\n\n# \u6570\u5b57\u8f6c\u6362\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n|   \u6700\u597d\u7684\u6c49\u5b57\u6570\u5b57(\u4e2d\u6587\u6570\u5b57)-\u963f\u62c9\u4f2f\u6570\u5b57\u8f6c\u6362\u5de5\u5177     |        |   [github](https://github.com/Wall-ee/chinese2digits)  |\n|  \u5feb\u901f\u8f6c\u5316\u300c\u4e2d\u6587\u6570\u5b57\u300d\u548c\u300c\u963f\u62c9\u4f2f\u6570\u5b57\u300d      |        |    [github](https://github.com/HaveTwoBrush/cn2an) |\n|    \u5c06\u81ea\u7136\u8bed\u8a00\u6570\u5b57\u4e32\u89e3\u6790\u8f6c\u6362\u4e3a\u6574\u6570\u548c\u6d6e\u70b9\u6570    |        |   [github](https://github.com/jaidevd/numerizer)  |\n\n\n# \u6307\u4ee3\u6d88\u89e3\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n|    \u4e2d\u6587\u6307\u4ee3\u6d88\u89e3\u6570\u636e    |        |   [github](https://github.com/CLUEbenchmark/CLUEWSC2020) <br>[baidu ink](https://pan.baidu.com/s/1gKP_Mj-7KVfFWpjYvSvAAA)  code a0qq |\n\n\n# \u6587\u672c\u805a\u7c7b\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n|     TextCluster\u77ed\u6587\u672c\u805a\u7c7b\u9884\u5904\u7406\u6a21\u5757 Short text cluster   |        |    [github](https://github.com/RandyPen/TextCluster) |\n\n\n# \u6587\u672c\u5206\u7c7b\n\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n|    NeuralNLP-NeuralClassifier\u817e\u8baf\u5f00\u6e90\u6df1\u5ea6\u5b66\u4e60\u6587\u672c\u5206\u7c7b\u5de5\u5177    |        |    [github](https://github.com/Tencent/NeuralNLP-NeuralClassifier) |\n\n\n# \u77e5\u8bc6\u63a8\u7406\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n|   GraphbrainAI\u5f00\u6e90\u8f6f\u4ef6\u5e93\u548c\u79d1\u7814\u5de5\u5177\uff0c\u76ee\u7684\u662f\u4fc3\u8fdb\u81ea\u52a8\u610f\u4e49\u63d0\u53d6\u548c\u6587\u672c\u7406\u89e3\u4ee5\u53ca\u77e5\u8bc6\u7684\u63a2\u7d22\u548c\u63a8\u65ad     |        |    [github](https://github.com/graphbrain/graphbrain) |\n|    (\u54c8\u4f5b)\u8bb2\u56e0\u679c\u63a8\u7406\u7684\u514d\u8d39\u4e66    |        |  [pdf](https://cdn1sphharvardedu/wp-content/uploads/sites/1268/2019/10/ci_hernanrobins_23oct19pdf)   |\n\n# \u53ef\u89e3\u91ca\u81ea\u7136\u8bed\u8a00\u5904\u7406\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n|   \u6587\u672c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6700\u5148\u8fdb\u89e3\u91ca\u5668\u5e93     |        |  [github](https://github.com/interpretml/interpret-text)   |\n\n\n# \u6587\u672c\u653b\u51fb\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n|     TextAttack\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6a21\u578b\u5bf9\u6297\u6027\u653b\u51fb\u6846\u67b6   |        |    [github](https://github.com/QData/TextAttack) |\n|OpenBackdoor: \u6587\u672c\u540e\u95e8\u653b\u9632\u5de5\u5177\u5305|       OpenBackdoor\u57fa\u4e8ePython\u548cPyTorch\u5f00\u53d1\uff0c\u53ef\u7528\u4e8e\u590d\u73b0\u3001\u8bc4\u4f30\u548c\u5f00\u53d1\u6587\u672c\u540e\u95e8\u653b\u9632\u7684\u76f8\u5173\u7b97\u6cd5     |    [github](https://github.com/thunlp/OpenBackdoor)|\n\n# \u6587\u672c\u53ef\u89c6\u5316\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n|     Scattertext \u6587\u672c\u53ef\u89c6\u5316(python)   |        |   [github](https://github.com/JasonKessler/scattertext)  |\n|     whatlies\u8bcd\u5411\u91cf\u4ea4\u4e92\u53ef\u89c6\u5316   |        | [spacy\u5de5\u5177](https://spacyio/universe/project/whatlies)  |\n|   PySS3\u9762\u5411\u53ef\u89e3\u91caAI\u7684SS3\u6587\u672c\u5206\u7c7b\u5668\u673a\u5668\u53ef\u89c6\u5316\u5de5\u5177     |        |   [github](https://github.com/sergioburdisso/pyss3)  |\n|     \u7528\u8bb0\u4e8b\u672c\u6e32\u67d33D\u56fe\u50cf   |        | [github](https://github.com/khalladay/render-with-notepad)    |\n|    attnvisGPT2\u3001BERT\u7b49transformer\u8bed\u8a00\u6a21\u578b\u6ce8\u610f\u529b\u4ea4\u4e92\u53ef\u89c6\u5316    |        |   [github](https://github.com/SIDN-IAP/attnvis)  |\n|    Texthero\u6587\u672c\u6570\u636e\u9ad8\u6548\u5904\u7406\u5305    |   \u5305\u62ec\u9884\u5904\u7406\u3001\u5173\u952e\u8bcd\u63d0\u53d6\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3001\u5411\u91cf\u7a7a\u95f4\u5206\u6790\u3001\u6587\u672c\u53ef\u89c6\u5316\u7b49     |  [github](https://github.com/jbesomi/texthero)   |\n\n# \u6587\u672c\u6807\u6ce8\u5de5\u5177\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n|   NLP\u6807\u6ce8\u5e73\u53f0\u7efc\u8ff0     |        |   [github](https://github.com/alvations/annotate-questionnaire)  |\n|   brat rapid annotation tool \u5e8f\u5217\u6807\u6ce8\u5de5\u5177     |        |   [link](http://brat.nlplab.org/index.html)  |\n|     Poplar\u7f51\u9875\u7248\u81ea\u7136\u8bed\u8a00\u6807\u6ce8\u5de5\u5177   |        |    [github](https://github.com/synyi/poplar) |\n|   LIDA\u8f7b\u91cf\u4ea4\u4e92\u5f0f\u5bf9\u8bdd\u6807\u6ce8\u5de5\u5177     |        |  [github](https://github.com/Wluper/lida)   |\n|    doccano\u57fa\u4e8e\u7f51\u9875\u7684\u5f00\u6e90\u534f\u540c\u591a\u8bed\u8a00\u6587\u672c\u6807\u6ce8\u5de5\u5177    |        |   [github](https://github.com/doccano/doccano)  |\n|     Datasaurai \u5728\u7ebf\u6570\u636e\u6807\u6ce8\u5de5\u4f5c\u6d41\u7ba1\u7406\u5de5\u5177   |        |    [link](https://datasaurai.gitbook.io/datasaur/) |\n\n# \u8bed\u8a00\u68c0\u6d4b\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n|  langid     |   97\u79cd\u8bed\u8a00\u68c0\u6d4b     |  [https://github.com/saffsd/langid.py](https://github.com/saffsd/langid.py)  |\n|   langdetect    |   \u8bed\u8a00\u68c0\u6d4b     |  [https://code.google.com/archive/p/language-detection/](https://code.google.com/archive/p/language-detection/)  |\n\n# \u7efc\u5408\u5de5\u5177\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n|   jieba    |        |  [jieba](https://github.com/fxsjy/jieba)  |\n|  hanlp     |        |   [hanlp](https://github.com/hankcs/pyhanlp) |\n|    nlp4han    |  \u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177\u96c6(\u65ad\u53e5/\u5206\u8bcd/\u8bcd\u6027\u6807\u6ce8/\u7ec4\u5757/\u53e5\u6cd5\u5206\u6790/\u8bed\u4e49\u5206\u6790/NER/N\u5143\u8bed\u6cd5/HMM/\u4ee3\u8bcd\u6d88\u89e3/\u60c5\u611f\u5206\u6790/\u62fc\u5199\u68c0    |   [github](https://github.com/kidden/nlp4han)  |\n|    \u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u8fdb\u5c55    |        |   [link](https://ai.facebook.com/blog/ai-advances-to-better-detect-hate-speech)  |\n|   \u57fa\u4e8ePytorch\u7684Bert\u5e94\u7528    |    \u5305\u62ec\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3001\u60c5\u611f\u5206\u6790\u3001\u6587\u672c\u5206\u7c7b\u4ee5\u53ca\u6587\u672c\u76f8\u4f3c\u5ea6\u7b49    |  [github](https://github.com/rsanshierli/EasyBert)   |\n|    nlp4han\u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177\u96c6    |   \u65ad\u53e5/\u5206\u8bcd/\u8bcd\u6027\u6807\u6ce8/\u7ec4\u5757/\u53e5\u6cd5\u5206\u6790/\u8bed\u4e49\u5206\u6790/NER/N\u5143\u8bed\u6cd5/HMM/\u4ee3\u8bcd\u6d88\u89e3/\u60c5\u611f\u5206\u6790/\u62fc\u5199\u68c0\u67e5     |   [github](https://github.com/kidden/nlp4han)  |\n|    \u4e00\u4e9b\u5173\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u57fa\u672c\u6a21\u578b    |        |  [github](https://github.com/lpty/nlp_base)   |\n|     \u7528BERT\u8fdb\u884c\u5e8f\u5217\u6807\u8bb0\u548c\u6587\u672c\u5206\u7c7b\u7684\u6a21\u677f\u4ee3\u7801   |        |  [github](https://github.com/yuanxiaosc/BERT-for-Sequence-Labeling-and-Text-Classification)|\n|   jieba_fast \u52a0\u901f\u7248\u7684jieba     |        |  [github](https://github.com/deepcs233/jieba_fast)   |\n|    StanfordNLP     |   \u7eafPython\u7248\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5305     |  [link](https://stanford.nlp.github.io/stanfordnlp/)   |\n|     Python\u53e3\u8bed\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177\u96c6(\u82f1\u6587)   |        |  [github](https://github.com/gooofy/py-nltools)   |\n|    PreNLP\u81ea\u7136\u8bed\u8a00\u9884\u5904\u7406\u5e93    |        |  [github](https://github.com/lyeoni/prenlp)   |\n|    nlp\u76f8\u5173\u7684\u4e00\u4e9b\u8bba\u6587\u53ca\u4ee3\u7801    |  \u5305\u62ec\u4e3b\u9898\u6a21\u578b\u3001\u8bcd\u5411\u91cf(Word Embedding)\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b(NER)\u3001\u6587\u672c\u5206\u7c7b(Text Classificatin)\u3001\u6587\u672c\u751f\u6210(Text Generation)\u3001\u6587\u672c\u76f8\u4f3c\u6027(Text Similarity)\u8ba1\u7b97\u7b49\uff0c\u6d89\u53ca\u5230\u5404\u79cd\u4e0enlp\u76f8\u5173\u7684\u7b97\u6cd5\uff0c\u57fa\u4e8ekeras\u548ctensorflow      |    [github](https://github.com/msgi/nlp-journey) |\n|  Python\u6587\u672c\u6316\u6398/NLP\u5b9e\u6218\u793a\u4f8b      |        |   [github](https://github.com/kavgan/nlp-in-practice)  |\n|   Forte\u7075\u6d3b\u5f3a\u5927\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406pipeline\u5de5\u5177\u96c6     |        |    [github](https://github.com/asyml/forte) |\n|   stanza\u65af\u5766\u798f\u56e2\u961fNLP\u5de5\u5177     |  \u53ef\u5904\u7406\u516d\u5341\u591a\u79cd\u8bed\u8a00   |    [github](https://github.com/stanfordnlp/stanza) |\n|   Fancy-NLP\u7528\u4e8e\u5efa\u8bbe\u5546\u54c1\u753b\u50cf\u7684\u6587\u672c\u77e5\u8bc6\u6316\u6398\u5de5\u5177     |        |   [github](https://github.com/boat-group/fancy-nlp)  |\n|    \u5168\u9762\u7b80\u4fbf\u7684\u4e2d\u6587 NLP \u5de5\u5177\u5305    |        |   [github](https://github.com/dongrixinyu/JioNLP)  |\n|   \u5de5\u4e1a\u754c\u5e38\u7528\u57fa\u4e8eDSSM\u5411\u91cf\u5316\u53ec\u56depipeline\u590d\u73b0     |        |   [github](https://github.com/wangzhegeek/DSSM-Lookalike)  |\n|    Texthero\u6587\u672c\u6570\u636e\u9ad8\u6548\u5904\u7406\u5305    |   \u5305\u62ec\u9884\u5904\u7406\u3001\u5173\u952e\u8bcd\u63d0\u53d6\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3001\u5411\u91cf\u7a7a\u95f4\u5206\u6790\u3001\u6587\u672c\u53ef\u89c6\u5316\u7b49     |  [github](https://github.com/jbesomi/texthero)   |\n|    nlpgnn\u56fe\u795e\u7ecf\u7f51\u7edc\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177\u7bb1    |        |  [github](https://github.com/kyzhouhzau/NLPGNN)   |\n|    Macadam   |  \u4ee5Tensorflow(Keras)\u548cbert4keras\u4e3a\u57fa\u7840\uff0c\u4e13\u6ce8\u4e8e\u6587\u672c\u5206\u7c7b\u3001\u5e8f\u5217\u6807\u6ce8\u548c\u5173\u7cfb\u62bd\u53d6\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177\u5305     |    [github](https://github.com/yongzhuo/Macadam) |\n|    LineFlow\u9762\u5411\u6240\u6709\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u7684NLP\u6570\u636e\u9ad8\u6548\u52a0\u8f7d\u5668    |        |   [github](https://github.com/tofunlp/lineflow)  |\n|Arabica\uff1aPython\u6587\u672c\u6570\u636e\u63a2\u7d22\u6027\u5206\u6790\u5de5\u5177\u5305||[github](https://github.com/PetrKorab/Arabica)|\n|Python \u538b\u529b\u6d4b\u8bd5\u5de5\u5177\uff1aSMSBoom||[github](https://github.com/WhaleFell/SMSBoom)|\n\n# \u6709\u8da3\u641e\u7b11\u5de5\u5177\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n|   \u6c6a\u5cf0\u6b4c\u8bcd\u751f\u6210\u5668    |        |  [phunterlau/wangfeng-rnn](https://github.com/phunterlau/wangfeng-rnn)  |\n|  \u5973\u53cb \u60c5\u611f\u6ce2\u52a8\u5206\u6790     |        |  [github](https://github.com/CasterWx/python-girlfriend-mood/)  |\n|   NLP\u592a\u96be\u4e86\u7cfb\u5217    |        |   [github](https://github.com/fighting41love/hardNLP)  |\n|  \u53d8\u91cf\u547d\u540d\u795e\u5668     |        |  [github](https://github.com/unbug/codelf) [link](https://unbug.github.io/codelf/)  |\n|     \u56fe\u7247\u6587\u5b57\u53bb\u9664\uff0c\u53ef\u7528\u4e8e\u6f2b\u753b\u7ffb\u8bd1   |        |  [github](https://github.com/yu45020/Text_Segmentation_Image_Inpainting)   |\n|     CoupletAI - \u5bf9\u8054\u751f\u6210   |   \u57fa\u4e8eCNN+Bi-LSTM+Attention \u7684\u81ea\u52a8\u5bf9\u5bf9\u8054\u7cfb\u7edf     |  [github](https://github.com/WiseDoge/CoupletAI)   |\n|   \u7528\u795e\u7ecf\u7f51\u7edc\u7b26\u53f7\u63a8\u7406\u6c42\u89e3\u590d\u6742\u6570\u5b66\u65b9\u7a0b     |        |    [github](https://ai.facebook.com/blog/using-neural-networks-to-solve-advanced-mathematics-equations/) |\n|   \u57fa\u4e8e14W\u6b4c\u66f2\u77e5\u8bc6\u5e93\u7684\u95ee\u7b54\u673a\u5668\u4eba    |     \u529f\u80fd\u5305\u62ec\u6b4c\u8bcd\u63a5\u9f99\uff0c\u5df2\u77e5\u6b4c\u8bcd\u627e\u6b4c\u66f2\u4ee5\u53ca\u6b4c\u66f2\u6b4c\u624b\u6b4c\u8bcd\u4e09\u89d2\u5173\u7cfb\u7684\u95ee\u7b54   |    [github](https://github.com/liuhuanyong/MusicLyricChatbot) |\n|    COPE - \u683c\u5f8b\u8bd7\u7f16\u8f91\u7a0b\u5e8f    |        |  [github](https://github.com/LingDong-/cope)   |\n|Paper2GUI | \u4e00\u6b3e\u9762\u5411\u666e\u901a\u4eba\u7684AI\u684c\u9762APP\u5de5\u5177\u7bb1\uff0c\u514d\u5b89\u88c5\u5373\u5f00\u5373\u7528\uff0c\u5df2\u652f\u630118+AI\u6a21\u578b\uff0c\u5185\u5bb9\u6db5\u76d6\u8bed\u97f3\u5408\u6210\u3001\u89c6\u9891\u8865\u5e27\u3001\u89c6\u9891\u8d85\u5206\u3001\u76ee\u6807\u68c0\u6d4b\u3001\u56fe\u7247\u98ce\u683c\u5316\u3001OCR\u8bc6\u522b\u7b49\u9886\u57df |   [github](https://github.com/Baiyuetribe/paper2gui) |  \n|\u793c\u8c8c\u7a0b\u5ea6\u4f30\u7b97\u5668\uff08\u4f7f\u7528\u65b0\u6d6a\u5fae\u535a\u6570\u636e\u8bad\u7ec3\uff09|| [github](https://github.com/tslmy/politeness-estimator) [paper](https://dl.acm.org/doi/abs/10.1145/3415190)|\n|\u8349\u87d2\uff08Python \u4e2d\u6587\u7248\uff09\u5165\u95e8\u6307\u5357|\u4e2d\u6587\u7f16\u7a0b\u8bed\u8a00|[homepage](https://www.grasspy.cn/zwdocs/grasspy-start/day1/)  [gitee](https://gitee.com/laowu2019_admin/zwdocs)|\n\n\n\n# \u8bfe\u7a0b\u62a5\u544a\u9762\u8bd5\u7b49\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n|   \u81ea\u7136\u8bed\u8a00\u5904\u7406\u62a5\u544a     |        |  [link](https://static.aminer.cn/misc/article/nlppdf)  |\n|    \u77e5\u8bc6\u56fe\u8c31\u62a5\u544a   |        |   [link](https://www.aminer.cn/research_report/5c3d5a8709%20e961951592a49d?download=true&pathname=knowledgegraphpdf) |\n|   \u6570\u636e\u6316\u6398\u62a5\u544a   |        |  [link](https://www.aminer.cn/research_report/5c3d5a5cecb160952fa10b76?download=true&pathname=dataminingpdf)  |\n|   \u81ea\u52a8\u9a7e\u9a76\u62a5\u544a    |        |  [link](https://static.aminer.cn/misc/article/selfdrivingpdf)  |\n|   \u673a\u5668\u7ffb\u8bd1\u62a5\u544a    |        |  [link](https://static.aminer.cn/misc/article/translationpdf)  |\n|    \u533a\u5757\u94fe\u62a5\u544a   |        |  [link](https://static.aminer.cn/misc/article/blockchain_publicpdf)  |\n|   \u673a\u5668\u4eba\u62a5\u544a    |        | [link](https://static.aminer.cn/misc/article/robotics_betapdf)   |\n|   \u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u62a5\u544a    |        |  [link](https://static.aminer.cn/misc/article/cgpdf)  |\n|  3D\u6253\u5370\u62a5\u544a    |        |  [link](https://static.aminer.cn/misc/article/3dpdf)  |\n|   \u4eba\u8138\u8bc6\u522b\u62a5\u544a    |        |  [link](https://static.aminer.cn/misc/article/facerecognitionpdf)  |\n|   \u4eba\u5de5\u667a\u80fd\u82af\u7247\u62a5\u544a    |        |  [link](https://static.aminer.cn/misc/article/aichippdf)  |\n|   cs224n\u6df1\u5ea6\u5b66\u4e60\u81ea\u7136\u8bed\u8a00\u5904\u7406\u8bfe\u7a0b    |        |  [link](http//web.stanford.edu/class/cs224n/) \u8bfe\u7a0b\u4e2d\u6a21\u578b\u7684pytorch\u5b9e\u73b0 [link](https://github.com/DSKSD/DeepNLP-models-Pytorch)   |\n|   \u9762\u5411\u6df1\u5ea6\u5b66\u4e60\u7814\u7a76\u4eba\u5458\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5b9e\u4f8b\u6559\u7a0b     |        |  [github](https://github.com/graykode/nlp-tutorial)  |\n|   \u300aNatural Language Processing\u300bby Jacob Eisenstein     |        |   [github](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notespdf)  |\n|     ML-NLP    | \u673a\u5668\u5b66\u4e60(Machine Learning)\u3001NLP\u9762\u8bd5\u4e2d\u5e38\u8003\u5230\u7684\u77e5\u8bc6\u70b9\u548c\u4ee3\u7801\u5b9e\u73b0       |    [github](https://github.com/NLP-LOVE/ML-NLP) |\n|      NLP\u4efb\u52a1\u793a\u4f8b\u9879\u76ee\u4ee3\u7801\u96c6  |        |   [github](https://github.com/explosion/projects)  |\n|     2019\u5e74NLP\u4eae\u70b9\u56de\u987e   |        |   [download](https://pan.baidu.com/s/1h5gEPUhvY1HkUVc32eeX4w)  |\n|   nlp-recipes\u5fae\u8f6f\u51fa\u54c1--\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6700\u4f73\u5b9e\u8df5\u548c\u8303\u4f8b     |        |   [github](https://github.com/microsoft/nlp-recipes)  |\n|    \u9762\u5411\u6df1\u5ea6\u5b66\u4e60\u7814\u7a76\u4eba\u5458\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5b9e\u4f8b\u6559\u7a0b    |        |   [github](https://github.com/graykode/nlp-tutorial)  |\n|   Transfer Learning in Natural Language Processing (NLP)     |        |    [youtube](https://www.youtube.com/watch?v=ly0TRNr7I_M) |\n|\u300a\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u300b\u56fe\u4e66|  |  [link](https://openmlsys.github.io/)  [github](https://github.com/fighting41love/openmlsys-zh) |\n\n\n# \u6bd4\u8d5b\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n| NLPer-Arsenal | NLP\u7ade\u8d5b\uff0c\u542b\u5f53\u524d\u8d5b\u4e8b\u4fe1\u606f\u3001\u8fc7\u5f80\u7ade\u8d5b\u65b9\u6848\u7b49\uff0c\u6301\u7eed\u66f4\u65b0\u4e2d | [github](https://github.com/TingFree/NLPer-Arsenal) |\n|    \u590d\u76d8\u6240\u6709NLP\u6bd4\u8d5b\u7684TOP\u65b9\u6848    |        |   [github](https://github.com/zhpmatrix/nlp-competitions-list-review)  |\n|   2019\u5e74\u767e\u5ea6\u7684\u4e09\u5143\u7ec4\u62bd\u53d6\u6bd4\u8d5b\uff0c\u201c\u79d1\u5b66\u7a7a\u95f4\u961f\u201d\u6e90\u7801(\u7b2c7\u540d)     |        |   [github](https://github.com/bojone/kg-2019)  |\n\n\n# \u91d1\u878d\u81ea\u7136\u8bed\u8a00\u5904\u7406\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n|   BDCI2019\u91d1\u878d\u8d1f\u9762\u4fe1\u606f\u5224\u5b9a     |        |   [github](https://github.com/A-Rain/BDCI2019-Negative_Finance_Info_Judge)  |\n|     \u5f00\u6e90\u7684\u91d1\u878d\u6295\u8d44\u6570\u636e\u63d0\u53d6\u5de5\u5177    |        |    [github](https://github.com/PKUJohnson/OpenData) |\n|    \u91d1\u878d\u9886\u57df\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7814\u7a76\u8d44\u6e90\u5927\u5217\u8868    |        |    [github](https://github.com/icoxfog417/awesome-financial-nlp) |\n|   \u57fa\u4e8e\u91d1\u878d-\u53f8\u6cd5\u9886\u57df(\u517c\u6709\u95f2\u804a\u6027\u8d28)\u7684\u804a\u5929\u673a\u5668\u4eba     |        |   [github](https://github.com/charlesXu86/Chatbot_CN)  |\n|\u5c0f\u578b\u91d1\u878d\u77e5\u8bc6\u56fe\u8c31\u6784\u6d41\u7a0b\u793a\u8303| |[github](https://github.com/jm199504/Financial-Knowledge-Graphs)|\n\n# \u533b\u7597\u81ea\u7136\u8bed\u8a00\u5904\u7406\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n|   \u4e2d\u6587\u533b\u5b66NLP\u516c\u5f00\u8d44\u6e90\u6574\u7406     |        |   [github](https://github.com/GanjinZero/awesome_Chinese_medical_NLP)  |\n|    spaCy \u533b\u5b66\u6587\u672c\u6316\u6398\u4e0e\u4fe1\u606f\u63d0\u53d6    |        |  [github](https://github.com/NLPatVCU/medaCy)   |\n|    \u6784\u5efa\u533b\u7597\u5b9e\u4f53\u8bc6\u522b\u7684\u6a21\u578b |   \u5305\u542b\u8bcd\u5178\u548c\u8bed\u6599\u6807\u6ce8\uff0c\u57fa\u4e8epython    |  [github](https://github.com/yixiu00001/LSTM-CRF-medical)   |\n|     \u57fa\u4e8e\u533b\u7597\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\u7684\u95ee\u7b54\u7cfb\u7edf   |        |   [github](https://github.com/zhihao-chen/QASystemOnMedicalGraph) \u8be5repo\u53c2\u8003\u4e86[github](https://github.com/liuhuanyong/QASystemOnMedicalKG)   |\n|     Chinese medical dialogue data \u4e2d\u6587\u533b\u7597\u5bf9\u8bdd\u6570\u636e\u96c6   |        |   [github](https://github.com/Toyhom/Chinese-medical-dialogue-data)  |\n|    \u4e00\u4e2a\u5927\u89c4\u6a21\u533b\u7597\u5bf9\u8bdd\u6570\u636e\u96c6    |   \u5305\u542b110\u4e07\u533b\u5b66\u54a8\u8be2\uff0c400\u4e07\u6761\u533b\u60a3\u5bf9\u8bdd    |    [github](https://github.com/UCSD-AI4H/Medical-Dialogue-System) |\n|   \u65b0\u51a0\u80ba\u708e\u76f8\u5173\u6570\u636e     |  \u65b0\u51a0\u53ca\u5176\u4ed6\u7c7b\u578b\u80ba\u708e\u4e2d\u6587\u533b\u7597\u5bf9\u8bdd\u6570\u636e\u96c6\uff1b\u6e05\u534e\u5927\u5b66\u7b49\u673a\u6784\u7684\u5f00\u653e\u6570\u636e\u6e90\uff08COVID-19\uff09   | [github](https://www\u3002aminer\u3002cn/data-covid19/)<br>  [github](https://github.com/UCSD-AI4H/COVID-Dialogue) |\n\n\n# \u6cd5\u5f8b\u81ea\u7136\u8bed\u8a00\u5904\u7406\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n|    Blackstone\u9762\u5411\u975e\u7ed3\u6784\u5316\u6cd5\u5f8b\u6587\u672c\u7684spaCy pipeline\u548cNLP\u6a21\u578b    |        |    [github](https://github.com/ICLRandD/Blackstone) |\n|   \u6cd5\u52a1\u667a\u80fd\u6587\u732e\u8d44\u6e90\u5217\u8868     |        |  [github](https://github.com/thunlp/LegalPapers)   |\n|   \u57fa\u4e8e\u91d1\u878d-\u53f8\u6cd5\u9886\u57df(\u517c\u6709\u95f2\u804a\u6027\u8d28)\u7684\u804a\u5929\u673a\u5668\u4eba     |        |   [github](https://github.com/charlesXu86/Chatbot_CN)  |\n|   \u7f6a\u540d\u6cd5\u52a1\u540d\u8bcd\u53ca\u5206\u7c7b\u6a21\u578b    |    \u5305\u542b856\u9879\u7f6a\u540d\u77e5\u8bc6\u56fe\u8c31, \u57fa\u4e8e280\u4e07\u7f6a\u540d\u8bad\u7ec3\u5e93\u7684\u7f6a\u540d\u9884\u6d4b,\u57fa\u4e8e20W\u6cd5\u52a1\u95ee\u7b54\u5bf9\u768413\u7c7b\u95ee\u9898\u5206\u7c7b\u4e0e\u6cd5\u5f8b\u8d44\u8baf\u95ee\u7b54\u529f\u80fd    |    [github](https://github.com/liuhuanyong/CrimeKgAssitant)     |\n|\u6cd5\u5f8bNLP\u76f8\u5173\u8d44\u6e90\u5927\u5217\u8868||[github](https://github.com/maastrichtlawtech/awesome-legal-nlp)|\n\n# \u6587\u672c\u751f\u6210\u56fe\u50cf\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n| Dalle-mini|\u6839\u636e\u6587\u672c\u63d0\u793a\u751f\u6210\u56fe\u7247\u7684\u8ff7\u4f60\u7248DALL\u00b7E|[github](https://github.com/borisdayma/dalle-mini)|\n\n# \u5176\u4ed6\n\n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n|  phone     |     \u4e2d\u56fd\u624b\u673a\u5f52\u5c5e\u5730\u67e5\u8be2    |  [ls0f/phone](https://github.com/ls0f/phone)  |\n|   phone    |    \u56fd\u9645\u624b\u673a\u3001\u7535\u8bdd\u5f52\u5c5e\u5730\u67e5\u8be2    |   [AfterShip/phone](https://github.com/AfterShip/phone) |\n|    ngender   |   \u6839\u636e\u540d\u5b57\u5224\u65ad\u6027\u522b     |  [observerss/ngender](https://github.com/observerss/ngender)  |\n|    \u4e2d\u6587\u5bf9\u6bd4\u82f1\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406NLP\u7684\u533a\u522b\u7efc\u8ff0  |        |   [link](https://mp.weixin.qq.com/s/LQU_HJ4q74lL5oCIk7w5RA)  |\n|  \u5404\u5927\u516c\u53f8\u5185\u90e8\u91cc\u5927\u725b\u5206\u4eab\u7684\u6280\u672f\u6587\u6863 PDF \u6216\u8005 PPT      |        |   [github](https://github.com/0voice/from_coder_to_expert)  |\n|   comparxiv \u7528\u4e8e\u6bd4\u8f83arXiv\u4e0a\u4e24\u63d0\u4ea4\u7248\u672c\u5dee\u5f02\u7684\u547d\u4ee4     |        |  [pypi](https://pypiorg/project/comparxiv/)   |\n|     CHAMELEON\u6df1\u5ea6\u5b66\u4e60\u65b0\u95fb\u63a8\u8350\u7cfb\u7edf\u5143\u67b6\u6784   |        | [github](https://github.com/gabrielspmoreira/chameleon_recsys)    |\n|    \u7b80\u5386\u81ea\u52a8\u7b5b\u9009\u7cfb\u7edf    |        |   [github](https://github.com/JAIJANYANI/Automated-Resume-Screening-System)  |\n|    Python\u5b9e\u73b0\u7684\u591a\u79cd\u6587\u672c\u53ef\u8bfb\u6027\u8bc4\u4ef7\u6307\u6807    |        |    [github](https://github.com/cdimascio/py-readability-metrics) |\n\n\n\n\n<!-- # \u5907\u6ce8\n\n\u6d89\u53ca\u5185\u5bb9\u5305\u62ec\u4f46\u4e0d\u9650\u4e8e\uff1a**\u4e2d\u82f1\u6587\u654f\u611f\u8bcd\u3001\u8bed\u8a00\u68c0\u6d4b\u3001\u4e2d\u5916\u624b\u673a/\u7535\u8bdd\u5f52\u5c5e\u5730/\u8fd0\u8425\u5546\u67e5\u8be2\u3001\u540d\u5b57\u63a8\u65ad\u6027\u522b\u3001\u624b\u673a\u53f7\u62bd\u53d6\u3001\u8eab\u4efd\u8bc1\u62bd\u53d6\u3001\u90ae\u7bb1\u62bd\u53d6\u3001\u4e2d\u65e5\u6587\u4eba\u540d\u5e93\u3001\u4e2d\u6587\u7f29\u5199\u5e93\u3001\u62c6\u5b57\u8bcd\u5178\u3001\u8bcd\u6c47\u60c5\u611f\u503c\u3001\u505c\u7528\u8bcd\u3001\u53cd\u52a8\u8bcd\u8868\u3001\u66b4\u6050\u8bcd\u8868\u3001\u7e41\u7b80\u4f53\u8f6c\u6362\u3001\u82f1\u6587\u6a21\u62df\u4e2d\u6587\u53d1\u97f3\u3001\u6c6a\u5cf0\u6b4c\u8bcd\u751f\u6210\u5668\u3001\u804c\u4e1a\u540d\u79f0\u8bcd\u5e93\u3001\u540c\u4e49\u8bcd\u5e93\u3001\u53cd\u4e49\u8bcd\u5e93\u3001\u5426\u5b9a\u8bcd\u5e93\u3001\u6c7d\u8f66\u54c1\u724c\u8bcd\u5e93\u3001\u6c7d\u8f66\u96f6\u4ef6\u8bcd\u5e93\u3001\u8fde\u7eed\u82f1\u6587\u5207\u5272\u3001\u5404\u79cd\u4e2d\u6587\u8bcd\u5411\u91cf\u3001\u516c\u53f8\u540d\u5b57\u5927\u5168\u3001\u53e4\u8bd7\u8bcd\u5e93\u3001IT\u8bcd\u5e93\u3001\u8d22\u7ecf\u8bcd\u5e93\u3001\u6210\u8bed\u8bcd\u5e93\u3001\u5730\u540d\u8bcd\u5e93\u3001\u5386\u53f2\u540d\u4eba\u8bcd\u5e93\u3001\u8bd7\u8bcd\u8bcd\u5e93\u3001\u533b\u5b66\u8bcd\u5e93\u3001\u996e\u98df\u8bcd\u5e93\u3001\u6cd5\u5f8b\u8bcd\u5e93\u3001\u6c7d\u8f66\u8bcd\u5e93\u3001\u52a8\u7269\u8bcd\u5e93\u3001\u4e2d\u6587\u804a\u5929\u8bed\u6599\u3001\u4e2d\u6587\u8c23\u8a00\u6570\u636e\u3001\u767e\u5ea6\u4e2d\u6587\u95ee\u7b54\u6570\u636e\u96c6\u3001\u53e5\u5b50\u76f8\u4f3c\u5ea6\u5339\u914d\u7b97\u6cd5\u96c6\u5408\u3001bert\u8d44\u6e90\u3001\u6587\u672c\u751f\u6210&\u6458\u8981\u76f8\u5173\u5de5\u5177\u3001cocoNLP\u4fe1\u606f\u62bd\u53d6\u5de5\u5177\u3001\u56fd\u5185\u7535\u8bdd\u53f7\u7801\u6b63\u5219\u5339\u914d\u3001\u6e05\u534e\u5927\u5b66XLORE:\u4e2d\u82f1\u6587\u8de8\u8bed\u8a00\u767e\u79d1\u77e5\u8bc6\u56fe\u8c31\u3001\u6e05\u534e\u5927\u5b66\u4eba\u5de5\u667a\u80fd\u6280\u672f\u7cfb\u5217\u62a5\u544a\u3001\u81ea\u7136\u8bed\u8a00\u751f\u6210\u3001NLU\u592a\u96be\u4e86\u7cfb\u5217\u3001\u81ea\u52a8\u5bf9\u8054\u6570\u636e\u53ca\u673a\u5668\u4eba\u3001\u7528\u6237\u540d\u9ed1\u540d\u5355\u5217\u8868\u3001\u7f6a\u540d\u6cd5\u52a1\u540d\u8bcd\u53ca\u5206\u7c7b\u6a21\u578b\u3001\u5fae\u4fe1\u516c\u4f17\u53f7\u8bed\u6599\u3001cs224n\u6df1\u5ea6\u5b66\u4e60\u81ea\u7136\u8bed\u8a00\u5904\u7406\u8bfe\u7a0b\u3001\u4e2d\u6587\u624b\u5199\u6c49\u5b57\u8bc6\u522b\u3001\u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406 \u8bed\u6599/\u6570\u636e\u96c6\u3001\u53d8\u91cf\u547d\u540d\u795e\u5668\u3001\u5206\u8bcd\u8bed\u6599\u5e93+\u4ee3\u7801\u3001\u4efb\u52a1\u578b\u5bf9\u8bdd\u82f1\u6587\u6570\u636e\u96c6\u3001ASR \u8bed\u97f3\u6570\u636e\u96c6 + \u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4e2d\u6587\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u3001\u7b11\u58f0\u68c0\u6d4b\u5668\u3001Microsoft\u591a\u8bed\u8a00\u6570\u5b57/\u5355\u4f4d/\u5982\u65e5\u671f\u65f6\u95f4\u8bc6\u522b\u5305\u3001\u4e2d\u534e\u65b0\u534e\u5b57\u5178\u6570\u636e\u5e93\u53caapi(\u5305\u62ec\u5e38\u7528\u6b47\u540e\u8bed\u3001\u6210\u8bed\u3001\u8bcd\u8bed\u548c\u6c49\u5b57)\u3001\u6587\u6863\u56fe\u8c31\u81ea\u52a8\u751f\u6210\u3001SpaCy \u4e2d\u6587\u6a21\u578b\u3001Common Voice\u8bed\u97f3\u8bc6\u522b\u6570\u636e\u96c6\u65b0\u7248\u3001\u795e\u7ecf\u7f51\u7edc\u5173\u7cfb\u62bd\u53d6\u3001\u57fa\u4e8ebert\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3001\u5173\u952e\u8bcd(Keyphrase)\u62bd\u53d6\u5305pke\u3001\u57fa\u4e8e\u533b\u7597\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\u7684\u95ee\u7b54\u7cfb\u7edf\u3001\u57fa\u4e8e\u4f9d\u5b58\u53e5\u6cd5\u4e0e\u8bed\u4e49\u89d2\u8272\u6807\u6ce8\u7684\u4e8b\u4ef6\u4e09\u5143\u7ec4\u62bd\u53d6\u3001\u4f9d\u5b58\u53e5\u6cd5\u5206\u67904\u4e07\u53e5\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u3001cnocr\uff1a\u7528\u6765\u505a\u4e2d\u6587OCR\u7684Python3\u5305\u3001\u4e2d\u6587\u4eba\u7269\u5173\u7cfb\u77e5\u8bc6\u56fe\u8c31\u9879\u76ee\u3001\u4e2d\u6587nlp\u7ade\u8d5b\u9879\u76ee\u53ca\u4ee3\u7801\u6c47\u603b\u3001\u4e2d\u6587\u5b57\u7b26\u6570\u636e\u3001speech-aligner: \u4ece\u201c\u4eba\u58f0\u8bed\u97f3\u201d\u53ca\u5176\u201c\u8bed\u8a00\u6587\u672c\u201d\u4ea7\u751f\u97f3\u7d20\u7ea7\u522b\u65f6\u95f4\u5bf9\u9f50\u6807\u6ce8\u7684\u5de5\u5177\u3001AmpliGraph: \u77e5\u8bc6\u56fe\u8c31\u8868\u793a\u5b66\u4e60(Python)\u5e93\uff1a\u77e5\u8bc6\u56fe\u8c31\u6982\u5ff5\u94fe\u63a5\u9884\u6d4b\u3001Scattertext \u6587\u672c\u53ef\u89c6\u5316(python)\u3001\u8bed\u8a00/\u77e5\u8bc6\u8868\u793a\u5de5\u5177\uff1aBERT & ERNIE\u3001\u4e2d\u6587\u5bf9\u6bd4\u82f1\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406NLP\u7684\u533a\u522b\u7efc\u8ff0\u3001Synonyms\u4e2d\u6587\u8fd1\u4e49\u8bcd\u5de5\u5177\u5305\u3001HarvestText\u9886\u57df\u81ea\u9002\u5e94\u6587\u672c\u6316\u6398\u5de5\u5177\uff08\u65b0\u8bcd\u53d1\u73b0-\u60c5\u611f\u5206\u6790-\u5b9e\u4f53\u94fe\u63a5\u7b49\uff09\u3001word2word\uff1a(Python)\u65b9\u4fbf\u6613\u7528\u7684\u591a\u8bed\u8a00\u8bcd-\u8bcd\u5bf9\u96c6\uff1a62\u79cd\u8bed\u8a00/3,564\u4e2a\u591a\u8bed\u8a00\u5bf9\u3001\u8bed\u97f3\u8bc6\u522b\u8bed\u6599\u751f\u6210\u5de5\u5177\uff1a\u4ece\u5177\u6709\u97f3\u9891/\u5b57\u5e55\u7684\u5728\u7ebf\u89c6\u9891\u521b\u5efa\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b(ASR)\u8bed\u6599\u5e93\u3001\u6784\u5efa\u533b\u7597\u5b9e\u4f53\u8bc6\u522b\u7684\u6a21\u578b\uff08\u5305\u542b\u8bcd\u5178\u548c\u8bed\u6599\u6807\u6ce8\uff09\u3001\u5355\u6587\u6863\u975e\u76d1\u7763\u7684\u5173\u952e\u8bcd\u62bd\u53d6\u3001Kashgari\u4e2d\u4f7f\u7528gpt-2\u8bed\u8a00\u6a21\u578b\u3001\u5f00\u6e90\u7684\u91d1\u878d\u6295\u8d44\u6570\u636e\u63d0\u53d6\u5de5\u5177\u3001\u6587\u672c\u81ea\u52a8\u6458\u8981\u5e93TextTeaser: \u4ec5\u652f\u6301\u82f1\u6587\u3001\u4eba\u6c11\u65e5\u62a5\u8bed\u6599\u5904\u7406\u5de5\u5177\u96c6\u3001\u4e00\u4e9b\u5173\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u57fa\u672c\u6a21\u578b\u3001\u57fa\u4e8e14W\u6b4c\u66f2\u77e5\u8bc6\u5e93\u7684\u95ee\u7b54\u5c1d\u8bd5--\u529f\u80fd\u5305\u62ec\u6b4c\u8bcd\u63a5\u9f99and\u5df2\u77e5\u6b4c\u8bcd\u627e\u6b4c\u66f2\u4ee5\u53ca\u6b4c\u66f2\u6b4c\u624b\u6b4c\u8bcd\u4e09\u89d2\u5173\u7cfb\u7684\u95ee\u7b54\u3001\u57fa\u4e8eSiamese bilstm\u6a21\u578b\u7684\u76f8\u4f3c\u53e5\u5b50\u5224\u5b9a\u6a21\u578b\u5e76\u63d0\u4f9b\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u6d4b\u8bd5\u6570\u636e\u96c6\u3001\u7528Transformer\u7f16\u89e3\u7801\u6a21\u578b\u5b9e\u73b0\u7684\u6839\u636eHacker News\u6587\u7ae0\u6807\u9898\u81ea\u52a8\u751f\u6210\u8bc4\u8bba\u3001\u7528BERT\u8fdb\u884c\u5e8f\u5217\u6807\u8bb0\u548c\u6587\u672c\u5206\u7c7b\u7684\u6a21\u677f\u4ee3\u7801\u3001LitBank\uff1aNLP\u6570\u636e\u96c6\u2014\u2014\u652f\u6301\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u8ba1\u7b97\u4eba\u6587\u5b66\u79d1\u4efb\u52a1\u7684100\u90e8\u5e26\u6807\u8bb0\u82f1\u6587\u5c0f\u8bf4\u8bed\u6599\u3001\u767e\u5ea6\u5f00\u6e90\u7684\u57fa\u51c6\u4fe1\u606f\u62bd\u53d6\u7cfb\u7edf\u3001\u865a\u5047\u65b0\u95fb\u6570\u636e\u96c6\u3001Facebook: LAMA\u8bed\u8a00\u6a21\u578b\u5206\u6790\uff0c\u63d0\u4f9bTransformer-XL/BERT/ELMo/GPT\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u7edf\u4e00\u8bbf\u95ee\u63a5\u53e3\u3001CommonsenseQA\uff1a\u9762\u5411\u5e38\u8bc6\u7684\u82f1\u6587QA\u6311\u6218\u3001\u4e2d\u6587\u77e5\u8bc6\u56fe\u8c31\u8d44\u6599\u3001\u6570\u636e\u53ca\u5de5\u5177\u3001\u5404\u5927\u516c\u53f8\u5185\u90e8\u91cc\u5927\u725b\u5206\u4eab\u7684\u6280\u672f\u6587\u6863 PDF \u6216\u8005 PPT\u3001\u81ea\u7136\u8bed\u8a00\u751f\u6210SQL\u8bed\u53e5\uff08\u82f1\u6587\uff09\u3001\u4e2d\u6587NLP\u6570\u636e\u589e\u5f3a\uff08EDA\uff09\u5de5\u5177\u3001\u82f1\u6587NLP\u6570\u636e\u589e\u5f3a\u5de5\u5177 \u3001\u57fa\u4e8e\u533b\u836f\u77e5\u8bc6\u56fe\u8c31\u7684\u667a\u80fd\u95ee\u7b54\u7cfb\u7edf\u3001\u4eac\u4e1c\u5546\u54c1\u77e5\u8bc6\u56fe\u8c31\u3001\u57fa\u4e8emongodb\u5b58\u50a8\u7684\u519b\u4e8b\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u9879\u76ee\u3001\u57fa\u4e8e\u8fdc\u76d1\u7763\u7684\u4e2d\u6587\u5173\u7cfb\u62bd\u53d6\u3001\u8bed\u97f3\u60c5\u611f\u5206\u6790\u3001\u4e2d\u6587ULMFiT-\u60c5\u611f\u5206\u6790-\u6587\u672c\u5206\u7c7b-\u8bed\u6599\u53ca\u6a21\u578b\u3001\u4e00\u4e2a\u62cd\u7167\u505a\u9898\u7a0b\u5e8f\u3001\u4e16\u754c\u5404\u56fd\u5927\u89c4\u6a21\u4eba\u540d\u5e93\u3001\u4e00\u4e2a\u5229\u7528\u6709\u8da3\u4e2d\u6587\u8bed\u6599\u5e93 qingyun \u8bad\u7ec3\u51fa\u6765\u7684\u4e2d\u6587\u804a\u5929\u673a\u5668\u4eba\u3001\u4e2d\u6587\u804a\u5929\u673a\u5668\u4ebaseqGAN\u3001\u7701\u5e02\u533a\u9547\u884c\u653f\u533a\u5212\u6570\u636e\u5e26\u62fc\u97f3\u6807\u6ce8\u3001\u6559\u80b2\u884c\u4e1a\u65b0\u95fb\u8bed\u6599\u5e93\u5305\u542b\u81ea\u52a8\u6587\u6458\u529f\u80fd\u3001\u5f00\u653e\u4e86\u5bf9\u8bdd\u673a\u5668\u4eba-\u77e5\u8bc6\u56fe\u8c31-\u8bed\u4e49\u7406\u89e3-\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177\u53ca\u6570\u636e\u3001\u4e2d\u6587\u77e5\u8bc6\u56fe\u8c31\uff1a\u57fa\u4e8e\u767e\u5ea6\u767e\u79d1\u4e2d\u6587\u9875\u9762-\u62bd\u53d6\u4e09\u5143\u7ec4\u4fe1\u606f-\u6784\u5efa\u4e2d\u6587\u77e5\u8bc6\u56fe\u8c31\u3001masr: \u4e2d\u6587\u8bed\u97f3\u8bc6\u522b-\u63d0\u4f9b\u9884\u8bad\u7ec3\u6a21\u578b-\u9ad8\u8bc6\u522b\u7387\u3001Python\u97f3\u9891\u6570\u636e\u589e\u5e7f\u5e93\u3001\u4e2d\u6587\u5168\u8bcd\u8986\u76d6BERT\u53ca\u4e24\u4efd\u9605\u8bfb\u7406\u89e3\u6570\u636e\u3001ConvLab\uff1a\u5f00\u6e90\u591a\u57df\u7aef\u5230\u7aef\u5bf9\u8bdd\u7cfb\u7edf\u5e73\u53f0\u3001\u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6570\u636e\u96c6\u3001\u57fa\u4e8e\u6700\u65b0\u7248\u672crasa\u642d\u5efa\u7684\u5bf9\u8bdd\u7cfb\u7edf\u3001\u57fa\u4e8eTensorFlow\u548cBERT\u7684\u7ba1\u9053\u5f0f\u5b9e\u4f53\u53ca\u5173\u7cfb\u62bd\u53d6\u3001\u4e00\u4e2a\u5c0f\u578b\u7684\u8bc1\u5238\u77e5\u8bc6\u56fe\u8c31/\u77e5\u8bc6\u5e93\u3001\u590d\u76d8\u6240\u6709NLP\u6bd4\u8d5b\u7684TOP\u65b9\u6848\u3001OpenCLaP\uff1a\u591a\u9886\u57df\u5f00\u6e90\u4e2d\u6587\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4ed3\u5e93\u3001UER\uff1a\u57fa\u4e8e\u4e0d\u540c\u8bed\u6599+\u7f16\u7801\u5668+\u76ee\u6807\u4efb\u52a1\u7684\u4e2d\u6587\u9884\u8bad\u7ec3\u6a21\u578b\u4ed3\u5e93\u3001\u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5411\u91cf\u5408\u96c6\u3001\u57fa\u4e8e\u91d1\u878d-\u53f8\u6cd5\u9886\u57df(\u517c\u6709\u95f2\u804a\u6027\u8d28)\u7684\u804a\u5929\u673a\u5668\u4eba\u3001g2pC\uff1a\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u6c49\u8bed\u8bfb\u97f3\u81ea\u52a8\u6807\u8bb0\u6a21\u5757\u3001Zincbase \u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u5de5\u5177\u5305\u3001\u8bd7\u6b4c\u8d28\u91cf\u8bc4\u4ef7/\u7ec6\u7c92\u5ea6\u60c5\u611f\u8bd7\u6b4c\u8bed\u6599\u5e93\u3001\u5feb\u901f\u8f6c\u5316\u300c\u4e2d\u6587\u6570\u5b57\u300d\u548c\u300c\u963f\u62c9\u4f2f\u6570\u5b57\u300d\u3001\u767e\u5ea6\u77e5\u9053\u95ee\u7b54\u8bed\u6599\u5e93\u3001\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u95ee\u7b54\u7cfb\u7edf\u3001jieba_fast \u52a0\u901f\u7248\u7684jieba\u3001\u6b63\u5219\u8868\u8fbe\u5f0f\u6559\u7a0b\u3001\u4e2d\u6587\u9605\u8bfb\u7406\u89e3\u6570\u636e\u96c6\u3001\u57fa\u4e8eBERT\u7b49\u6700\u65b0\u8bed\u8a00\u6a21\u578b\u7684\u62bd\u53d6\u5f0f\u6458\u8981\u63d0\u53d6\u3001Python\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u8fdb\u884c\u6587\u672c\u6458\u8981\u7684\u7efc\u5408\u6307\u5357\u3001\u77e5\u8bc6\u56fe\u8c31\u6df1\u5ea6\u5b66\u4e60\u76f8\u5173\u8d44\u6599\u6574\u7406\u3001\u7ef4\u57fa\u5927\u89c4\u6a21\u5e73\u884c\u6587\u672c\u8bed\u6599\u3001StanfordNLP 0.2.0\uff1a\u7eafPython\u7248\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5305\u3001NeuralNLP-NeuralClassifier\uff1a\u817e\u8baf\u5f00\u6e90\u6df1\u5ea6\u5b66\u4e60\u6587\u672c\u5206\u7c7b\u5de5\u5177\u3001\u7aef\u5230\u7aef\u7684\u5c01\u95ed\u57df\u5bf9\u8bdd\u7cfb\u7edf\u3001\u4e2d\u6587\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff1aNeuroNER vs. BertNER\u3001\u65b0\u95fb\u4e8b\u4ef6\u7ebf\u7d22\u62bd\u53d6\u30012019\u5e74\u767e\u5ea6\u7684\u4e09\u5143\u7ec4\u62bd\u53d6\u6bd4\u8d5b\uff1a\u201c\u79d1\u5b66\u7a7a\u95f4\u961f\u201d\u6e90\u7801\u3001\u57fa\u4e8e\u4f9d\u5b58\u53e5\u6cd5\u7684\u5f00\u653e\u57df\u6587\u672c\u77e5\u8bc6\u4e09\u5143\u7ec4\u62bd\u53d6\u548c\u77e5\u8bc6\u5e93\u6784\u5efa\u3001\u4e2d\u6587\u7684GPT2\u8bad\u7ec3\u4ee3\u7801\u3001ML-NLP - \u673a\u5668\u5b66\u4e60(Machine Learning)NLP\u9762\u8bd5\u4e2d\u5e38\u8003\u5230\u7684\u77e5\u8bc6\u70b9\u548c\u4ee3\u7801\u5b9e\u73b0\u3001nlp4han:\u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177\u96c6(\u65ad\u53e5/\u5206\u8bcd/\u8bcd\u6027\u6807\u6ce8/\u7ec4\u5757/\u53e5\u6cd5\u5206\u6790/\u8bed\u4e49\u5206\u6790/NER/N\u5143\u8bed\u6cd5/HMM/\u4ee3\u8bcd\u6d88\u89e3/\u60c5\u611f\u5206\u6790/\u62fc\u5199\u68c0\u67e5\u3001XLM\uff1aFacebook\u7684\u8de8\u8bed\u8a00\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u3001\u7528\u57fa\u4e8eBERT\u7684\u5fae\u8c03\u548c\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u6765\u8fdb\u884c\u77e5\u8bc6\u56fe\u8c31\u767e\u5ea6\u767e\u79d1\u4eba\u7269\u8bcd\u6761\u5c5e\u6027\u62bd\u53d6\u3001\u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406\u76f8\u5173\u7684\u5f00\u653e\u4efb\u52a1-\u6570\u636e\u96c6-\u5f53\u524d\u6700\u4f73\u7ed3\u679c\u3001CoupletAI - \u57fa\u4e8eCNN+Bi-LSTM+Attention \u7684\u81ea\u52a8\u5bf9\u5bf9\u8054\u7cfb\u7edf\u3001\u62bd\u8c61\u77e5\u8bc6\u56fe\u8c31\u3001MiningZhiDaoQACorpus - 580\u4e07\u767e\u5ea6\u77e5\u9053\u95ee\u7b54\u6570\u636e\u6316\u6398\u9879\u76ee\u3001brat rapid annotation tool: \u5e8f\u5217\u6807\u6ce8\u5de5\u5177\u3001\u5927\u89c4\u6a21\u4e2d\u6587\u77e5\u8bc6\u56fe\u8c31\u6570\u636e\uff1a1.4\u4ebf\u5b9e\u4f53\u3001\u6570\u636e\u589e\u5f3a\u5728\u673a\u5668\u7ffb\u8bd1\u53ca\u5176\u4ed6nlp\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u53ca\u6548\u679c\u3001allennlp\u9605\u8bfb\u7406\u89e3:\u652f\u6301\u591a\u79cd\u6570\u636e\u548c\u6a21\u578b\u3001PDF\u8868\u683c\u6570\u636e\u63d0\u53d6\u5de5\u5177 \u3001 Graphbrain\uff1aAI\u5f00\u6e90\u8f6f\u4ef6\u5e93\u548c\u79d1\u7814\u5de5\u5177\uff0c\u76ee\u7684\u662f\u4fc3\u8fdb\u81ea\u52a8\u610f\u4e49\u63d0\u53d6\u548c\u6587\u672c\u7406\u89e3\u4ee5\u53ca\u77e5\u8bc6\u7684\u63a2\u7d22\u548c\u63a8\u65ad\u3001\u7b80\u5386\u81ea\u52a8\u7b5b\u9009\u7cfb\u7edf\u3001\u57fa\u4e8e\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7684\u7b80\u5386\u81ea\u52a8\u6458\u8981\u3001\u4e2d\u6587\u8bed\u8a00\u7406\u89e3\u6d4b\u8bc4\u57fa\u51c6\uff0c\u5305\u62ec\u4ee3\u8868\u6027\u7684\u6570\u636e\u96c6&\u57fa\u51c6\u6a21\u578b&\u8bed\u6599\u5e93&\u6392\u884c\u699c\u3001\u6811\u6d1e OCR \u6587\u5b57\u8bc6\u522b \u3001\u4ece\u5305\u542b\u8868\u683c\u7684\u626b\u63cf\u56fe\u7247\u4e2d\u8bc6\u522b\u8868\u683c\u548c\u6587\u5b57\u3001\u8bed\u58f0\u8fc1\u79fb\u3001Python\u53e3\u8bed\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177\u96c6(\u82f1\u6587)\u3001 similarity\uff1a\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u5de5\u5177\u5305\uff0cjava\u7f16\u5199\u3001\u6d77\u91cf\u4e2d\u6587\u9884\u8bad\u7ec3ALBERT\u6a21\u578b \u3001Transformers 2.0 \u3001\u57fa\u4e8e\u5927\u89c4\u6a21\u97f3\u9891\u6570\u636e\u96c6Audioset\u7684\u97f3\u9891\u589e\u5f3a \u3001Poplar\uff1a\u7f51\u9875\u7248\u81ea\u7136\u8bed\u8a00\u6807\u6ce8\u5de5\u5177\u3001\u56fe\u7247\u6587\u5b57\u53bb\u9664\uff0c\u53ef\u7528\u4e8e\u6f2b\u753b\u7ffb\u8bd1 \u3001186\u79cd\u8bed\u8a00\u7684\u6570\u5b57\u53eb\u6cd5\u5e93\u3001Amazon\u53d1\u5e03\u57fa\u4e8e\u77e5\u8bc6\u7684\u4eba-\u4eba\u5f00\u653e\u9886\u57df\u5bf9\u8bdd\u6570\u636e\u96c6 \u3001\u4e2d\u6587\u6587\u672c\u7ea0\u9519\u6a21\u5757\u4ee3\u7801\u3001\u7e41\u7b80\u4f53\u8f6c\u6362 \u3001 Python\u5b9e\u73b0\u7684\u591a\u79cd\u6587\u672c\u53ef\u8bfb\u6027\u8bc4\u4ef7\u6307\u6807\u3001\u7c7b\u4f3c\u4e8e\u4eba\u540d/\u5730\u540d/\u7ec4\u7ec7\u673a\u6784\u540d\u7684\u547d\u540d\u4f53\u8bc6\u522b\u6570\u636e\u96c6 \u3001\u4e1c\u5357\u5927\u5b66\u300a\u77e5\u8bc6\u56fe\u8c31\u300b\u7814\u7a76\u751f\u8bfe\u7a0b(\u8d44\u6599)\u3001. \u82f1\u6587\u62fc\u5199\u68c0\u67e5\u5e93 \u3001 wwsearch\u662f\u4f01\u4e1a\u5fae\u4fe1\u540e\u53f0\u81ea\u7814\u7684\u5168\u6587\u68c0\u7d22\u5f15\u64ce\u3001CHAMELEON\uff1a\u6df1\u5ea6\u5b66\u4e60\u65b0\u95fb\u63a8\u8350\u7cfb\u7edf\u5143\u67b6\u6784 \u3001 8\u7bc7\u8bba\u6587\u68b3\u7406BERT\u76f8\u5173\u6a21\u578b\u8fdb\u5c55\u4e0e\u53cd\u601d\u3001DocSearch\uff1a\u514d\u8d39\u6587\u6863\u641c\u7d22\u5f15\u64ce\u3001 LIDA\uff1a\u8f7b\u91cf\u4ea4\u4e92\u5f0f\u5bf9\u8bdd\u6807\u6ce8\u5de5\u5177 \u3001aili - the fastest in-memory index in the East \u4e1c\u534a\u7403\u6700\u5feb\u5e76\u53d1\u7d22\u5f15 \u3001\u77e5\u8bc6\u56fe\u8c31\u8f66\u97f3\u5de5\u4f5c\u9879\u76ee\u3001\u81ea\u7136\u8bed\u8a00\u751f\u6210\u8d44\u6e90\u5927\u5168 \u3001\u4e2d\u65e5\u97e9\u5206\u8bcd\u5e93mecab\u7684Python\u63a5\u53e3\u5e93\u3001\u4e2d\u6587\u6587\u672c\u6458\u8981/\u5173\u952e\u8bcd\u63d0\u53d6\u3001\u6c49\u5b57\u5b57\u7b26\u7279\u5f81\u63d0\u53d6\u5668 (featurizer)\uff0c\u63d0\u53d6\u6c49\u5b57\u7684\u7279\u5f81\uff08\u53d1\u97f3\u7279\u5f81\u3001\u5b57\u5f62\u7279\u5f81\uff09\u7528\u505a\u6df1\u5ea6\u5b66\u4e60\u7684\u7279\u5f81\u3001\u4e2d\u6587\u751f\u6210\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bc4 \u3001\u4e2d\u6587\u7f29\u5199\u6570\u636e\u96c6\u3001\u4e2d\u6587\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bc4 - \u4ee3\u8868\u6027\u7684\u6570\u636e\u96c6-\u57fa\u51c6(\u9884\u8bad\u7ec3)\u6a21\u578b-\u8bed\u6599\u5e93-baseline-\u5de5\u5177\u5305-\u6392\u884c\u699c\u3001PySS3\uff1a\u9762\u5411\u53ef\u89e3\u91caAI\u7684SS3\u6587\u672c\u5206\u7c7b\u5668\u673a\u5668\u53ef\u89c6\u5316\u5de5\u5177 \u3001\u4e2d\u6587NLP\u6570\u636e\u96c6\u5217\u8868\u3001COPE - \u683c\u5f8b\u8bd7\u7f16\u8f91\u7a0b\u5e8f\u3001doccano\uff1a\u57fa\u4e8e\u7f51\u9875\u7684\u5f00\u6e90\u534f\u540c\u591a\u8bed\u8a00\u6587\u672c\u6807\u6ce8\u5de5\u5177 \u3001PreNLP\uff1a\u81ea\u7136\u8bed\u8a00\u9884\u5904\u7406\u5e93\u3001\u7b80\u5355\u7684\u7b80\u5386\u89e3\u6790\u5668\uff0c\u7528\u6765\u4ece\u7b80\u5386\u4e2d\u63d0\u53d6\u5173\u952e\u4fe1\u606f\u3001\u7528\u4e8e\u4e2d\u6587\u95f2\u804a\u7684GPT2\u6a21\u578b\uff1aGPT2-chitchat\u3001\u57fa\u4e8e\u68c0\u7d22\u804a\u5929\u673a\u5668\u4eba\u591a\u8f6e\u54cd\u5e94\u9009\u62e9\u76f8\u5173\u8d44\u6e90\u5217\u8868(Leaderboards\u3001Datasets\u3001Papers)\u3001(Colab)\u62bd\u8c61\u6587\u672c\u6458\u8981\u5b9e\u73b0\u96c6\u9526(\u6559\u7a0b \u3001\u8bcd\u8bed\u62fc\u97f3\u6570\u636e\u3001\u9ad8\u6548\u6a21\u7cca\u641c\u7d22\u5de5\u5177\u3001NLP\u6570\u636e\u589e\u5e7f\u8d44\u6e90\u96c6\u3001\u5fae\u8f6f\u5bf9\u8bdd\u673a\u5668\u4eba\u6846\u67b6 \u3001 GitHub Typo Corpus\uff1a\u5927\u89c4\u6a21GitHub\u591a\u8bed\u8a00\u62fc\u5199\u9519\u8bef/\u8bed\u6cd5\u9519\u8bef\u6570\u636e\u96c6\u3001TextCluster\uff1a\u77ed\u6587\u672c\u805a\u7c7b\u9884\u5904\u7406\u6a21\u5757 Short text cluster\u3001\u9762\u5411\u8bed\u97f3\u8bc6\u522b\u7684\u4e2d\u6587\u6587\u672c\u89c4\u8303\u5316\u3001BLINK\uff1a\u6700\u5148\u8fdb\u7684\u5b9e\u4f53\u94fe\u63a5\u5e93\u3001BertPunc\uff1a\u57fa\u4e8eBERT\u7684\u6700\u5148\u8fdb\u6807\u70b9\u4fee\u590d\u6a21\u578b\u3001Tokenizer\uff1a\u5feb\u901f\u3001\u53ef\u5b9a\u5236\u7684\u6587\u672c\u8bcd\u6761\u5316\u5e93\u3001\u4e2d\u6587\u8bed\u8a00\u7406\u89e3\u6d4b\u8bc4\u57fa\u51c6\uff0c\u5305\u62ec\u4ee3\u8868\u6027\u7684\u6570\u636e\u96c6\u3001\u57fa\u51c6(\u9884\u8bad\u7ec3)\u6a21\u578b\u3001\u8bed\u6599\u5e93\u3001\u6392\u884c\u699c\u3001spaCy \u533b\u5b66\u6587\u672c\u6316\u6398\u4e0e\u4fe1\u606f\u63d0\u53d6 \u3001 NLP\u4efb\u52a1\u793a\u4f8b\u9879\u76ee\u4ee3\u7801\u96c6\u3001 python\u62fc\u5199\u68c0\u67e5\u5e93\u3001chatbot-list - \u884c\u4e1a\u5185\u5173\u4e8e\u667a\u80fd\u5ba2\u670d\u3001\u804a\u5929\u673a\u5668\u4eba\u7684\u5e94\u7528\u548c\u67b6\u6784\u3001\u7b97\u6cd5\u5206\u4eab\u548c\u4ecb\u7ecd\u3001\u8bed\u97f3\u8d28\u91cf\u8bc4\u4ef7\u6307\u6807(MOSNet, BSSEval, STOI, PESQ, SRMR)\u3001 \u7528138GB\u8bed\u6599\u8bad\u7ec3\u7684\u6cd5\u6587RoBERTa\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b \u3001BERT-NER-Pytorch\uff1a\u4e09\u79cd\u4e0d\u540c\u6a21\u5f0f\u7684BERT\u4e2d\u6587NER\u5b9e\u9a8c\u3001\u65e0\u9053\u8bcd\u5178 - \u6709\u9053\u8bcd\u5178\u7684\u547d\u4ee4\u884c\u7248\u672c\uff0c\u652f\u6301\u82f1\u6c49\u4e92\u67e5\u548c\u5728\u7ebf\u67e5\u8be2\u30012019\u5e74NLP\u4eae\u70b9\u56de\u987e\u3001 Chinese medical dialogue data \u4e2d\u6587\u533b\u7597\u5bf9\u8bdd\u6570\u636e\u96c6 \u3001\u6700\u597d\u7684\u6c49\u5b57\u6570\u5b57(\u4e2d\u6587\u6570\u5b57)-\u963f\u62c9\u4f2f\u6570\u5b57\u8f6c\u6362\u5de5\u5177\u3001 \u57fa\u4e8e\u767e\u79d1\u77e5\u8bc6\u5e93\u7684\u4e2d\u6587\u8bcd\u8bed\u591a\u8bcd\u4e49/\u4e49\u9879\u83b7\u53d6\u4e0e\u7279\u5b9a\u53e5\u5b50\u8bcd\u8bed\u8bed\u4e49\u6d88\u6b67\u3001awesome-nlp-sentiment-analysis - \u60c5\u611f\u5206\u6790\u3001\u60c5\u7eea\u539f\u56e0\u8bc6\u522b\u3001\u8bc4\u4ef7\u5bf9\u8c61\u548c\u8bc4\u4ef7\u8bcd\u62bd\u53d6\u3001LineFlow\uff1a\u9762\u5411\u6240\u6709\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u7684NLP\u6570\u636e\u9ad8\u6548\u52a0\u8f7d\u5668\u3001\u4e2d\u6587\u533b\u5b66NLP\u516c\u5f00\u8d44\u6e90\u6574\u7406 \u3001MedQuAD\uff1a(\u82f1\u6587)\u533b\u5b66\u95ee\u7b54\u6570\u636e\u96c6\u3001\u5c06\u81ea\u7136\u8bed\u8a00\u6570\u5b57\u4e32\u89e3\u6790\u8f6c\u6362\u4e3a\u6574\u6570\u548c\u6d6e\u70b9\u6570\u3001Transfer Learning in Natural Language Processing (NLP) \u3001\u9762\u5411\u8bed\u97f3\u8bc6\u522b\u7684\u4e2d\u6587/\u82f1\u6587\u53d1\u97f3\u8f9e\u5178\u3001Tokenizers\uff1a\u6ce8\u91cd\u6027\u80fd\u4e0e\u591a\u529f\u80fd\u6027\u7684\u6700\u5148\u8fdb\u5206\u8bcd\u5668\u3001CLUENER \u7ec6\u7c92\u5ea6\u547d\u540d\u5b9e\u4f53\u8bc6\u522b Fine Grained Named Entity Recognition\u3001 \u57fa\u4e8eBERT\u7684\u4e2d\u6587\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3001\u4e2d\u6587\u8c23\u8a00\u6570\u636e\u5e93\u3001NLP\u6570\u636e\u96c6/\u57fa\u51c6\u4efb\u52a1\u5927\u5217\u8868\u3001nlp\u76f8\u5173\u7684\u4e00\u4e9b\u8bba\u6587\u53ca\u4ee3\u7801, \u5305\u62ec\u4e3b\u9898\u6a21\u578b\u3001\u8bcd\u5411\u91cf(Word Embedding)\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b(NER)\u3001\u6587\u672c\u5206\u7c7b(Text Classificatin)\u3001\u6587\u672c\u751f\u6210(Text Generation)\u3001\u6587\u672c\u76f8\u4f3c\u6027(Text Similarity)\u8ba1\u7b97\u7b49\uff0c\u6d89\u53ca\u5230\u5404\u79cd\u4e0enlp\u76f8\u5173\u7684\u7b97\u6cd5\uff0c\u57fa\u4e8ekeras\u548ctensorflow \u3001Python\u6587\u672c\u6316\u6398/NLP\u5b9e\u6218\u793a\u4f8b\u3001 Blackstone\uff1a\u9762\u5411\u975e\u7ed3\u6784\u5316\u6cd5\u5f8b\u6587\u672c\u7684spaCy pipeline\u548cNLP\u6a21\u578b\u901a\u8fc7\u540c\u4e49\u8bcd\u66ff\u6362\u5b9e\u73b0\u6587\u672c\u201c\u53d8\u8138\u201d \u3001\u4e2d\u6587 \u9884\u8bad\u7ec3 ELECTREA \u6a21\u578b: \u57fa\u4e8e\u5bf9\u6297\u5b66\u4e60 pretrain Chinese Model \u3001albert-chinese-ner - \u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578bALBERT\u505a\u4e2d\u6587NER \u3001\u57fa\u4e8eGPT2\u7684\u7279\u5b9a\u4e3b\u9898\u6587\u672c\u751f\u6210/\u6587\u672c\u589e\u5e7f\u3001\u5f00\u6e90\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5408\u96c6\u3001\u591a\u8bed\u8a00\u53e5\u5411\u91cf\u5305\u3001\u7f16\u7801\u3001\u6807\u8bb0\u548c\u5b9e\u73b0\uff1a\u4e00\u79cd\u53ef\u63a7\u9ad8\u6548\u7684\u6587\u672c\u751f\u6210\u65b9\u6cd5\u3001 \u82f1\u6587\u810f\u8bdd\u5927\u5217\u8868 \u3001attnvis\uff1aGPT2\u3001BERT\u7b49transformer\u8bed\u8a00\u6a21\u578b\u6ce8\u610f\u529b\u4ea4\u4e92\u53ef\u89c6\u5316\u3001CoVoST\uff1aFacebook\u53d1\u5e03\u7684\u591a\u8bed\u79cd\u8bed\u97f3-\u6587\u672c\u7ffb\u8bd1\u8bed\u6599\u5e93\uff0c\u5305\u62ec11\u79cd\u8bed\u8a00(\u6cd5\u8bed\u3001\u5fb7\u8bed\u3001\u8377\u5170\u8bed\u3001\u4fc4\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u610f\u5927\u5229\u8bed\u3001\u571f\u8033\u5176\u8bed\u3001\u6ce2\u65af\u8bed\u3001\u745e\u5178\u8bed\u3001\u8499\u53e4\u8bed\u548c\u4e2d\u6587)\u7684\u8bed\u97f3\u3001\u6587\u5b57\u8f6c\u5f55\u53ca\u82f1\u6587\u8bd1\u6587\u3001Jiagu\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177 - \u4ee5BiLSTM\u7b49\u6a21\u578b\u4e3a\u57fa\u7840\uff0c\u63d0\u4f9b\u77e5\u8bc6\u56fe\u8c31\u5173\u7cfb\u62bd\u53d6 \u4e2d\u6587\u5206\u8bcd \u8bcd\u6027\u6807\u6ce8 \u547d\u540d\u5b9e\u4f53\u8bc6\u522b \u60c5\u611f\u5206\u6790 \u65b0\u8bcd\u53d1\u73b0 \u5173\u952e\u8bcd \u6587\u672c\u6458\u8981 \u6587\u672c\u805a\u7c7b\u7b49\u529f\u80fd\u3001\u7528unet\u5b9e\u73b0\u5bf9\u6587\u6863\u8868\u683c\u7684\u81ea\u52a8\u68c0\u6d4b\uff0c\u8868\u683c\u91cd\u5efa\u3001NLP\u4e8b\u4ef6\u63d0\u53d6\u6587\u732e\u8d44\u6e90\u5217\u8868 \u3001 \u91d1\u878d\u9886\u57df\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7814\u7a76\u8d44\u6e90\u5927\u5217\u8868\u3001CLUEDatasetSearch - \u4e2d\u82f1\u6587NLP\u6570\u636e\u96c6\uff1a\u641c\u7d22\u6240\u6709\u4e2d\u6587NLP\u6570\u636e\u96c6\uff0c\u9644\u5e38\u7528\u82f1\u6587NLP\u6570\u636e\u96c6 \u3001medical_NER - \u4e2d\u6587\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u547d\u540d\u5b9e\u4f53\u8bc6\u522b \u3001(\u54c8\u4f5b)\u8bb2\u56e0\u679c\u63a8\u7406\u7684\u514d\u8d39\u4e66\u3001\u77e5\u8bc6\u56fe\u8c31\u76f8\u5173\u5b66\u4e60\u8d44\u6599/\u6570\u636e\u96c6/\u5de5\u5177\u8d44\u6e90\u5927\u5217\u8868\u3001Forte\uff1a\u7075\u6d3b\u5f3a\u5927\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406pipeline\u5de5\u5177\u96c6 \u3001Python\u5b57\u7b26\u4e32\u76f8\u4f3c\u6027\u7b97\u6cd5\u5e93\u3001PyLaia\uff1a\u9762\u5411\u624b\u5199\u6587\u6863\u5206\u6790\u7684\u6df1\u5ea6\u5b66\u4e60\u5de5\u5177\u5305\u3001TextFooler\uff1a\u9488\u5bf9\u6587\u672c\u5206\u7c7b/\u63a8\u7406\u7684\u5bf9\u6297\u6587\u672c\u751f\u6210\u6a21\u5757\u3001Haystack\uff1a\u7075\u6d3b\u3001\u5f3a\u5927\u7684\u53ef\u6269\u5c55\u95ee\u7b54(QA)\u6846\u67b6\u3001\u4e2d\u6587\u5173\u952e\u77ed\u8bed\u62bd\u53d6\u5de5\u5177**\u3002 -->\n\n<!-- \n| \u8d44\u6e90\u540d\uff08Name\uff09      | \u63cf\u8ff0\uff08Description\uff09 | \u94fe\u63a5     |\n| :---        |    :---  |          :--- |\n|       |        |    |\n|       |        |    |\n|       |        |    |\n|       |        |    |\n|       |        |    |\n|       |        |    |\n|       |        |    |\n|       |        |    |\n|       |        |    |\n|       |        |    |\n|       |        |    |\n|       |        |    |\n|       |        |    |\n|       |        |    |\n|       |        |    |\n|       |        |    | -->\n\n\n<!-- <img align=\"right\" src=\"https://github-readme-stats.vercel.app/api?username=fighting41love&show_icons=true&icon_color=CE1D2D&text_color=718096&bg_color=ffffff&hide_title=true\" /> -->\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 21872392,
    "name": "awesome-machine-learning",
    "full_name": "josephmisiti/awesome-machine-learning",
    "description": "A curated list of awesome Machine Learning frameworks, libraries and software.",
    "html_url": "https://github.com/josephmisiti/awesome-machine-learning",
    "clone_url": "https://github.com/josephmisiti/awesome-machine-learning.git",
    "owner_login": "josephmisiti",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/246302?v=4",
    "stargazers_count": 69221,
    "watchers_count": 69221,
    "forks_count": 15035,
    "open_issues_count": 5,
    "size": 2827,
    "language": "Python",
    "languages": {
      "Python": 1150
    },
    "topics": [],
    "license_name": "Other",
    "created_at": "2014-07-15T19:11:19+00:00",
    "updated_at": "2025-08-06T00:44:51+00:00",
    "pushed_at": "2025-06-25T14:00:11+00:00",
    "contributors_count": 100,
    "readme_length": 202485,
    "readme_content": "# Awesome Machine Learning [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome) [![Track Awesome List](https://www.trackawesomelist.com/badge.svg)](https://www.trackawesomelist.com/josephmisiti/awesome-machine-learning/)\n\nA curated list of awesome machine learning frameworks, libraries and software (by language). Inspired by `awesome-php`.\n\n_If you want to contribute to this list (please do), send me a pull request or contact me [@josephmisiti](https://twitter.com/josephmisiti)._\nAlso, a listed repository should be deprecated if:\n\n* Repository's owner explicitly says that \"this library is not maintained\".\n* Not committed for a long time (2~3 years).\n\nFurther resources:\n\n* For a list of free machine learning books available for download, go [here](https://github.com/josephmisiti/awesome-machine-learning/blob/master/books.md).\n\n* For a list of professional machine learning events, go [here](https://github.com/josephmisiti/awesome-machine-learning/blob/master/events.md).\n\n* For a list of (mostly) free machine learning courses available online, go [here](https://github.com/josephmisiti/awesome-machine-learning/blob/master/courses.md).\n\n* For a list of blogs and newsletters on data science and machine learning, go [here](https://github.com/josephmisiti/awesome-machine-learning/blob/master/blogs.md).\n\n* For a list of free-to-attend meetups and local events, go [here](https://github.com/josephmisiti/awesome-machine-learning/blob/master/meetups.md).\n\n## Table of Contents\n\n### Frameworks and Libraries\n<!-- MarkdownTOC depth=4 -->\n<!-- Contents-->\n- [Awesome Machine Learning ![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](#awesome-machine-learning-)\n  - [Table of Contents](#table-of-contents)\n    - [Frameworks and Libraries](#frameworks-and-libraries)\n    - [Tools](#tools)\n  - [APL](#apl)\n      - [General-Purpose Machine Learning](#apl-general-purpose-machine-learning)\n  - [C](#c)\n      - [General-Purpose Machine Learning](#c-general-purpose-machine-learning)\n      - [Computer Vision](#c-computer-vision)\n  - [C++](#cpp)\n      - [Computer Vision](#cpp-computer-vision)\n      - [General-Purpose Machine Learning](#cpp-general-purpose-machine-learning)\n      - [Natural Language Processing](#cpp-natural-language-processing)\n      - [Speech Recognition](#cpp-speech-recognition)\n      - [Sequence Analysis](#cpp-sequence-analysis)\n      - [Gesture Detection](#cpp-gesture-detection)\n      - [Reinforcement Learning](#cpp-reinforcement-learning)\n  - [Common Lisp](#common-lisp)\n      - [General-Purpose Machine Learning](#common-lisp-general-purpose-machine-learning)\n  - [Clojure](#clojure)\n      - [Natural Language Processing](#clojure-natural-language-processing)\n      - [General-Purpose Machine Learning](#clojure-general-purpose-machine-learning)\n      - [Deep Learning](#clojure-deep-learning)\n      - [Data Analysis](#clojure-data-analysis--data-visualization)\n      - [Data Visualization](#clojure-data-visualization)\n      - [Interop](#clojure-interop)\n      - [Misc](#clojure-misc)\n      - [Extra](#clojure-extra)\n  - [Crystal](#crystal)\n      - [General-Purpose Machine Learning](#crystal-general-purpose-machine-learning)\n  - [Elixir](#elixir)\n      - [General-Purpose Machine Learning](#elixir-general-purpose-machine-learning)\n      - [Natural Language Processing](#elixir-natural-language-processing)\n  - [Erlang](#erlang)\n      - [General-Purpose Machine Learning](#erlang-general-purpose-machine-learning)\n  - [Fortran](#fortran)\n      - [General-Purpose Machine Learning](#fortran-general-purpose-machine-learning)\n      - [Data Analysis / Data Visualization](#fortran-data-analysis--data-visualization)\n  - [Go](#go)\n      - [Natural Language Processing](#go-natural-language-processing)\n      - [General-Purpose Machine Learning](#go-general-purpose-machine-learning)\n      - [Spatial analysis and geometry](#go-spatial-analysis-and-geometry)\n      - [Data Analysis / Data Visualization](#go-data-analysis--data-visualization)\n      - [Computer vision](#go-computer-vision)\n      - [Reinforcement learning](#go-reinforcement-learning)\n  - [Haskell](#haskell)\n      - [General-Purpose Machine Learning](#haskell-general-purpose-machine-learning)\n  - [Java](#java)\n      - [Natural Language Processing](#java-natural-language-processing)\n      - [General-Purpose Machine Learning](#java-general-purpose-machine-learning)\n      - [Speech Recognition](#java-speech-recognition)\n      - [Data Analysis / Data Visualization](#java-data-analysis--data-visualization)\n      - [Deep Learning](#java-deep-learning)\n  - [Javascript](#javascript)\n      - [Natural Language Processing](#javascript-natural-language-processing)\n      - [Data Analysis / Data Visualization](#javascript-data-analysis--data-visualization)\n      - [General-Purpose Machine Learning](#javascript-general-purpose-machine-learning)\n      - [Misc](#javascript-misc)\n      - [Demos and Scripts](#javascript-demos-and-scripts)\n  - [Julia](#julia)\n      - [General-Purpose Machine Learning](#julia-general-purpose-machine-learning)\n      - [Natural Language Processing](#julia-natural-language-processing)\n      - [Data Analysis / Data Visualization](#julia-data-analysis--data-visualization)\n      - [Misc Stuff / Presentations](#julia-misc-stuff--presentations)\n  - [Kotlin](#kotlin)\n      - [Deep Learning](#kotlin-deep-learning)\n  - [Lua](#lua)\n      - [General-Purpose Machine Learning](#lua-general-purpose-machine-learning)\n      - [Demos and Scripts](#lua-demos-and-scripts)\n  - [Matlab](#matlab)\n      - [Computer Vision](#matlab-computer-vision)\n      - [Natural Language Processing](#matlab-natural-language-processing)\n      - [General-Purpose Machine Learning](#matlab-general-purpose-machine-learning)\n      - [Data Analysis / Data Visualization](#matlab-data-analysis--data-visualization)\n  - [.NET](#net)\n      - [Computer Vision](#net-computer-vision)\n      - [Natural Language Processing](#net-natural-language-processing)\n      - [General-Purpose Machine Learning](#net-general-purpose-machine-learning)\n      - [Data Analysis / Data Visualization](#net-data-analysis--data-visualization)\n  - [Objective C](#objective-c)\n    - [General-Purpose Machine Learning](#objective-c-general-purpose-machine-learning)\n  - [OCaml](#ocaml)\n    - [General-Purpose Machine Learning](#ocaml-general-purpose-machine-learning)\n  - [OpenCV](#opencv)\n    - [Computer Vision](#opencv-Computer-Vision)\n    - [Text-Detection](#Text-Character-Number-Detection)\n  - [Perl](#perl)\n    - [Data Analysis / Data Visualization](#perl-data-analysis--data-visualization)\n    - [General-Purpose Machine Learning](#perl-general-purpose-machine-learning)\n  - [Perl 6](#perl-6)\n    - [Data Analysis / Data Visualization](#perl-6-data-analysis--data-visualization)\n    - [General-Purpose Machine Learning](#perl-6-general-purpose-machine-learning)\n  - [PHP](#php)\n    - [Natural Language Processing](#php-natural-language-processing)\n    - [General-Purpose Machine Learning](#php-general-purpose-machine-learning)\n  - [Python](#python)\n      - [Computer Vision](#python-computer-vision)\n      - [Natural Language Processing](#python-natural-language-processing)\n      - [General-Purpose Machine Learning](#python-general-purpose-machine-learning)\n      - [Data Analysis / Data Visualization](#python-data-analysis--data-visualization)\n      - [Misc Scripts / iPython Notebooks / Codebases](#python-misc-scripts--ipython-notebooks--codebases)\n      - [Neural Networks](#python-neural-networks)\n      - [Survival Analysis](#python-survival-analysis)\n      - [Federated Learning](#python-federated-learning)\n      - [Kaggle Competition Source Code](#python-kaggle-competition-source-code)\n      - [Reinforcement Learning](#python-reinforcement-learning)\n      - [Speech Recognition](#python-speech-recognition)\n  - [Ruby](#ruby)\n      - [Natural Language Processing](#ruby-natural-language-processing)\n      - [General-Purpose Machine Learning](#ruby-general-purpose-machine-learning)\n      - [Data Analysis / Data Visualization](#ruby-data-analysis--data-visualization)\n      - [Misc](#ruby-misc)\n  - [Rust](#rust)\n      - [General-Purpose Machine Learning](#rust-general-purpose-machine-learning)\n      - [Deep Learning](#rust-deep-learning)\n      - [Natural Language Processing](#rust-natural-language-processing)\n  - [R](#r)\n      - [General-Purpose Machine Learning](#r-general-purpose-machine-learning)\n      - [Data Analysis / Data Visualization](#r-data-analysis--data-visualization)\n  - [SAS](#sas)\n      - [General-Purpose Machine Learning](#sas-general-purpose-machine-learning)\n      - [Data Analysis / Data Visualization](#sas-data-analysis--data-visualization)\n      - [Natural Language Processing](#sas-natural-language-processing)\n      - [Demos and Scripts](#sas-demos-and-scripts)\n  - [Scala](#scala)\n      - [Natural Language Processing](#scala-natural-language-processing)\n      - [Data Analysis / Data Visualization](#scala-data-analysis--data-visualization)\n      - [General-Purpose Machine Learning](#scala-general-purpose-machine-learning)\n  - [Scheme](#scheme)\n      - [Neural Networks](#scheme-neural-networks)\n  - [Swift](#swift)\n      - [General-Purpose Machine Learning](#swift-general-purpose-machine-learning)\n  - [TensorFlow](#tensorflow)\n      - [General-Purpose Machine Learning](#tensorflow-general-purpose-machine-learning)\n\n### [Tools](#tools-1)\n\n- [Neural Networks](#tools-neural-networks)\n- [Misc](#tools-misc)\n\n\n[Credits](#credits)\n\n<!-- /MarkdownTOC -->\n\n<a name=\"apl\"></a>\n## APL\n\n<a name=\"apl-general-purpose-machine-learning\"></a>\n#### General-Purpose Machine Learning\n* [naive-apl](https://github.com/mattcunningham/naive-apl) - Naive Bayesian Classifier implementation in APL. **[Deprecated]**\n\n<a name=\"c\"></a>\n## C\n\n<a name=\"c-general-purpose-machine-learning\"></a>\n#### General-Purpose Machine Learning\n* [Darknet](https://github.com/pjreddie/darknet) - Darknet is an open source neural network framework written in C and CUDA. It is fast, easy to install, and supports CPU and GPU computation.\n* [Recommender](https://github.com/GHamrouni/Recommender) - A C library for product recommendations/suggestions using collaborative filtering (CF).\n* [Hybrid Recommender System](https://github.com/SeniorSA/hybrid-rs-trainner) - A hybrid recommender system based upon scikit-learn algorithms. **[Deprecated]**\n* [neonrvm](https://github.com/siavashserver/neonrvm) - neonrvm is an open source machine learning library based on RVM technique. It's written in C programming language and comes with Python programming language bindings.\n* [cONNXr](https://github.com/alrevuelta/cONNXr) - An `ONNX` runtime written in pure C (99) with zero dependencies focused on small embedded devices. Run inference on your machine learning models no matter which framework you train it with. Easy to install and compiles everywhere, even in very old devices.\n* [libonnx](https://github.com/xboot/libonnx) - A lightweight, portable pure C99 onnx inference engine for embedded devices with hardware acceleration support.\n\n<a name=\"c-computer-vision\"></a>\n#### Computer Vision\n\n* [CCV](https://github.com/liuliu/ccv) - C-based/Cached/Core Computer Vision Library, A Modern Computer Vision Library.\n* [VLFeat](http://www.vlfeat.org/) - VLFeat is an open and portable library of computer vision algorithms, which has a Matlab toolbox.\n\n<a name=\"cpp\"></a>\n## C++\n\n<a name=\"cpp-computer-vision\"></a>\n#### Computer Vision\n\n* [DLib](http://dlib.net/imaging.html) - DLib has C++ and Python interfaces for face detection and training general object detectors.\n* [EBLearn](http://eblearn.sourceforge.net/) - Eblearn is an object-oriented C++ library that implements various machine learning models **[Deprecated]**\n* [OpenCV](https://opencv.org) - OpenCV has C++, C, Python, Java and MATLAB interfaces and supports Windows, Linux, Android and Mac OS.\n* [VIGRA](https://github.com/ukoethe/vigra) - VIGRA is a genertic cross-platform C++ computer vision and machine learning library for volumes of arbitrary dimensionality with Python bindings.\n* [Openpose](https://github.com/CMU-Perceptual-Computing-Lab/openpose) - A real-time multi-person keypoint detection library for body, face, hands, and foot estimation\n\n<a name=\"cpp-general-purpose-machine-learning\"></a>\n#### General-Purpose Machine Learning\n\n* [Speedster](https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/speedster) -Automatically apply SOTA optimization techniques to achieve the maximum inference speed-up on your hardware. [DEEP LEARNING]\n* [BanditLib](https://github.com/jkomiyama/banditlib) - A simple Multi-armed Bandit library. **[Deprecated]**\n* [Caffe](https://github.com/BVLC/caffe) - A deep learning framework developed with cleanliness, readability, and speed in mind. [DEEP LEARNING]\n* [CatBoost](https://github.com/catboost/catboost) - General purpose gradient boosting on decision trees library with categorical features support out of the box. It is easy to install, contains fast inference implementation and supports CPU and GPU (even multi-GPU) computation.\n* [CNTK](https://github.com/Microsoft/CNTK) - The Computational Network Toolkit (CNTK) by Microsoft Research, is a unified deep-learning toolkit that describes neural networks as a series of computational steps via a directed graph.\n* [CUDA](https://code.google.com/p/cuda-convnet/) - This is a fast C++/CUDA implementation of convolutional [DEEP LEARNING]\n* [DeepDetect](https://github.com/jolibrain/deepdetect) - A machine learning API and server written in C++11. It makes state of the art machine learning easy to work with and integrate into existing applications.\n* [Distributed Machine learning Tool Kit (DMTK)](http://www.dmtk.io/) - A distributed machine learning (parameter server) framework by Microsoft. Enables training models on large data sets across multiple machines. Current tools bundled with it include: LightLDA and Distributed (Multisense) Word Embedding.\n* [DLib](http://dlib.net/ml.html) - A suite of ML tools designed to be easy to imbed in other applications.\n* [DSSTNE](https://github.com/amznlabs/amazon-dsstne) - A software library created by Amazon for training and deploying deep neural networks using GPUs which emphasizes speed and scale over experimental flexibility.\n* [DyNet](https://github.com/clab/dynet) - A dynamic neural network library working well with networks that have dynamic structures that change for every training instance. Written in C++ with bindings in Python.\n* [Fido](https://github.com/FidoProject/Fido) - A highly-modular C++ machine learning library for embedded electronics and robotics.\n* [FlexML](https://github.com/ozguraslank/flexml) - Easy-to-use and flexible AutoML library for Python.\n* [igraph](http://igraph.org/) - General purpose graph library.\n* [Intel\u00ae oneAPI Data Analytics Library](https://github.com/oneapi-src/oneDAL) - A high performance software library developed by Intel and optimized for Intel's architectures. Library provides algorithmic building blocks for all stages of data analytics and allows to process data in batch, online and distributed modes.\n* [LightGBM](https://github.com/Microsoft/LightGBM) - Microsoft's fast, distributed, high performance gradient boosting (GBDT, GBRT, GBM or MART) framework based on decision tree algorithms, used for ranking, classification and many other machine learning tasks.\n* [libfm](https://github.com/srendle/libfm) - A generic approach that allows to mimic most factorization models by feature engineering.\n* [MLDB](https://mldb.ai) - The Machine Learning Database is a database designed for machine learning. Send it commands over a RESTful API to store data, explore it using SQL, then train machine learning models and expose them as APIs.\n* [mlpack](https://www.mlpack.org/) - A scalable C++ machine learning library.\n* [MXNet](https://github.com/apache/incubator-mxnet) - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, JavaScript and more.\n* [N2D2](https://github.com/CEA-LIST/N2D2) - CEA-List's CAD framework for designing and simulating Deep Neural Network, and building full DNN-based applications on embedded platforms\n* [oneDNN](https://github.com/oneapi-src/oneDNN) - An open-source cross-platform performance library for deep learning applications.\n* [Opik](https://www.comet.com/site/products/opik/) - Open source engineering platform to debug, evaluate, and monitor your LLM applications, RAG systems, and agentic workflows with comprehensive tracing, automated evaluations, and production-ready dashboards. ([Source Code](https://github.com/comet-ml/opik/))\n* [ParaMonte](https://github.com/cdslaborg/paramonte) - A general-purpose library with C/C++ interface for Bayesian data analysis and visualization via serial/parallel Monte Carlo and MCMC simulations. Documentation can be found [here](https://www.cdslab.org/paramonte/).\n* [proNet-core](https://github.com/cnclabs/proNet-core) - A general-purpose network embedding framework: pair-wise representations optimization Network Edit.\n* [PyCaret](https://github.com/pycaret/pycaret) - An open-source, low-code machine learning library in Python that automates machine learning workflows.\n* [PyCUDA](https://mathema.tician.de/software/pycuda/) - Python interface to CUDA\n* [ROOT](https://root.cern.ch) - A modular scientific software framework. It provides all the functionalities needed to deal with big data processing, statistical analysis, visualization and storage.\n* [shark](http://image.diku.dk/shark/sphinx_pages/build/html/index.html) - A fast, modular, feature-rich open-source C++ machine learning library.\n* [Shogun](https://github.com/shogun-toolbox/shogun) - The Shogun Machine Learning Toolbox.\n* [sofia-ml](https://code.google.com/archive/p/sofia-ml) - Suite of fast incremental algorithms.\n* [Stan](http://mc-stan.org/) - A probabilistic programming language implementing full Bayesian statistical inference with Hamiltonian Monte Carlo sampling.\n* [Timbl](https://languagemachines.github.io/timbl/) - A software package/C++ library implementing several memory-based learning algorithms, among which IB1-IG, an implementation of k-nearest neighbor classification, and IGTree, a decision-tree approximation of IB1-IG. Commonly used for NLP.\n* [Vowpal Wabbit (VW)](https://github.com/VowpalWabbit/vowpal_wabbit) - A fast out-of-core learning system.\n* [Warp-CTC](https://github.com/baidu-research/warp-ctc) - A fast parallel implementation of Connectionist Temporal Classification (CTC), on both CPU and GPU.\n* [XGBoost](https://github.com/dmlc/xgboost) - A parallelized optimized general purpose gradient boosting library.\n* [ThunderGBM](https://github.com/Xtra-Computing/thundergbm) - A fast library for GBDTs and Random Forests on GPUs.\n* [ThunderSVM](https://github.com/Xtra-Computing/thundersvm) - A fast SVM library on GPUs and CPUs.\n* [LKYDeepNN](https://github.com/mosdeo/LKYDeepNN) - A header-only C++11 Neural Network library. Low dependency, native traditional chinese document.\n* [xLearn](https://github.com/aksnzhy/xlearn) - A high performance, easy-to-use, and scalable machine learning package, which can be used to solve large-scale machine learning problems. xLearn is especially useful for solving machine learning problems on large-scale sparse data, which is very common in Internet services such as online advertising and recommender systems.\n* [Featuretools](https://github.com/featuretools/featuretools) - A library for automated feature engineering. It excels at transforming transactional and relational datasets into feature matrices for machine learning using reusable feature engineering \"primitives\".\n* [skynet](https://github.com/Tyill/skynet) - A library for learning neural networks, has C-interface, net set in JSON. Written in C++ with bindings in Python, C++ and C#.\n* [Feast](https://github.com/gojek/feast) - A feature store for the management, discovery, and access of machine learning features. Feast provides a consistent view of feature data for both model training and model serving.\n* [Hopsworks](https://github.com/logicalclocks/hopsworks) - A data-intensive platform for AI with the industry's first open-source feature store. The Hopsworks Feature Store provides both a feature warehouse for training and batch based on Apache Hive and a feature serving database, based on MySQL Cluster, for online applications.\n* [Polyaxon](https://github.com/polyaxon/polyaxon) - A platform for reproducible and scalable machine learning and deep learning.\n* [QuestDB](https://questdb.io/) - A relational column-oriented database designed for real-time analytics on time series and event data.\n* [Phoenix](https://phoenix.arize.com) - Uncover insights, surface problems, monitor and fine tune your generative LLM, CV and tabular models.\n* [XAD](https://github.com/auto-differentiation/XAD) - Comprehensive backpropagation tool for C++.\n* [Truss](https://truss.baseten.co) - An open source framework for packaging and serving ML models.\n\n<a name=\"cpp-natural-language-processing\"></a>\n#### Natural Language Processing\n\n* [BLLIP Parser](https://github.com/BLLIP/bllip-parser) - BLLIP Natural Language Parser (also known as the Charniak-Johnson parser).\n* [colibri-core](https://github.com/proycon/colibri-core) - C++ library, command line tools, and Python binding for extracting and working with basic linguistic constructions such as n-grams and skipgrams in a quick and memory-efficient way.\n* [CRF++](https://taku910.github.io/crfpp/) - Open source implementation of Conditional Random Fields (CRFs) for segmenting/labeling sequential data & other Natural Language Processing tasks. **[Deprecated]**\n* [CRFsuite](http://www.chokkan.org/software/crfsuite/) - CRFsuite is an implementation of Conditional Random Fields (CRFs) for labeling sequential data. **[Deprecated]**\n* [frog](https://github.com/LanguageMachines/frog) - Memory-based NLP suite developed for Dutch: PoS tagger, lemmatiser, dependency parser, NER, shallow parser, morphological analyzer.\n* [libfolia](https://github.com/LanguageMachines/libfolia) - C++ library for the [FoLiA format](https://proycon.github.io/folia/)\n* [MeTA](https://github.com/meta-toolkit/meta) - [MeTA : ModErn Text Analysis](https://meta-toolkit.org/) is a C++ Data Sciences Toolkit that facilitates mining big text data.\n* [MIT Information Extraction Toolkit](https://github.com/mit-nlp/MITIE) - C, C++, and Python tools for named entity recognition and relation extraction\n* [ucto](https://github.com/LanguageMachines/ucto) - Unicode-aware regular-expression based tokenizer for various languages. Tool and C++ library. Supports FoLiA format.\n\n<a name=\"cpp-speech-recognition\"></a>\n#### Speech Recognition\n* [Kaldi](https://github.com/kaldi-asr/kaldi) - Kaldi is a toolkit for speech recognition written in C++ and licensed under the Apache License v2.0. Kaldi is intended for use by speech recognition researchers.\n\n<a name=\"cpp-sequence-analysis\"></a>\n#### Sequence Analysis\n* [ToPS](https://github.com/ayoshiaki/tops) - This is an object-oriented framework that facilitates the integration of probabilistic models for sequences over a user defined alphabet. **[Deprecated]**\n\n<a name=\"cpp-gesture-detection\"></a>\n#### Gesture Detection\n* [grt](https://github.com/nickgillian/grt) - The Gesture Recognition Toolkit (GRT) is a cross-platform, open-source, C++ machine learning library designed for real-time gesture recognition.\n\n<a name=\"cpp-reinforcement-learning\"></a>\n#### Reinforcement Learning\n* [RLtools](https://github.com/rl-tools/rl-tools) - The fastest deep reinforcement learning library for continuous control, implemented header-only in pure, dependency-free C++ (Python bindings available as well).\n\n<a name=\"common-lisp\"></a>\n## Common Lisp\n\n<a name=\"common-lisp-general-purpose-machine-learning\"></a>\n#### General-Purpose Machine Learning\n\n* [mgl](https://github.com/melisgl/mgl/) - Neural networks (boltzmann machines, feed-forward and recurrent nets), Gaussian Processes.\n* [mgl-gpr](https://github.com/melisgl/mgl-gpr/) - Evolutionary algorithms. **[Deprecated]**\n* [cl-libsvm](https://github.com/melisgl/cl-libsvm/) - Wrapper for the libsvm support vector machine library. **[Deprecated]**\n* [cl-online-learning](https://github.com/masatoi/cl-online-learning) - Online learning algorithms (Perceptron, AROW, SCW, Logistic Regression).\n* [cl-random-forest](https://github.com/masatoi/cl-random-forest) - Implementation of Random Forest in Common Lisp.\n\n<a name=\"clojure\"></a>\n## Clojure\n\n<a name=\"clojure-natural-language-processing\"></a>\n#### Natural Language Processing\n\n* [Clojure-openNLP](https://github.com/dakrone/clojure-opennlp) - Natural Language Processing in Clojure (opennlp).\n* [Infections-clj](https://github.com/r0man/inflections-clj) - Rails-like inflection library for Clojure and ClojureScript.\n\n<a name=\"clojure-general-purpose-machine-learning\"></a>\n#### General-Purpose Machine Learning\n\n* [scicloj.ml](https://github.com/scicloj/scicloj.ml) -  A idiomatic Clojure machine learning library based on tech.ml.dataset with a unique approach for immutable data processing pipelines.\n* [clj-ml](https://github.com/joshuaeckroth/clj-ml/) - A machine learning library for Clojure built on top of Weka and friends.\n* [clj-boost](https://gitlab.com/alanmarazzi/clj-boost) - Wrapper for XGBoost\n* [Touchstone](https://github.com/ptaoussanis/touchstone) - Clojure A/B testing library.\n* [Clojush](https://github.com/lspector/Clojush) - The Push programming language and the PushGP genetic programming system implemented in Clojure.\n* [lambda-ml](https://github.com/cloudkj/lambda-ml) - Simple, concise implementations of machine learning techniques and utilities in Clojure.\n* [Infer](https://github.com/aria42/infer) - Inference and machine learning in Clojure. **[Deprecated]**\n* [Encog](https://github.com/jimpil/enclog) - Clojure wrapper for Encog (v3) (Machine-Learning framework that specializes in neural-nets). **[Deprecated]**\n* [Fungp](https://github.com/vollmerm/fungp) - A genetic programming library for Clojure. **[Deprecated]**\n* [Statistiker](https://github.com/clojurewerkz/statistiker) - Basic Machine Learning algorithms in Clojure. **[Deprecated]**\n* [clortex](https://github.com/htm-community/clortex) - General Machine Learning library using Numenta\u2019s Cortical Learning Algorithm. **[Deprecated]**\n* [comportex](https://github.com/htm-community/comportex) - Functionally composable Machine Learning library using Numenta\u2019s Cortical Learning Algorithm. **[Deprecated]**\n\n<a name=\"clojure-deep-learning\"></a>\n#### Deep Learning\n* [MXNet](https://mxnet.apache.org/versions/1.7.0/api/clojure) - Bindings to Apache MXNet - part of the MXNet project\n* [Deep Diamond](https://github.com/uncomplicate/deep-diamond) - A fast Clojure Tensor & Deep Learning library\n* [jutsu.ai](https://github.com/hswick/jutsu.ai) - Clojure wrapper for deeplearning4j with some added syntactic sugar.\n* [cortex](https://github.com/originrose/cortex) - Neural networks, regression and feature learning in Clojure.\n* [Flare](https://github.com/aria42/flare) - Dynamic Tensor Graph library in Clojure (think PyTorch, DynNet, etc.)\n* [dl4clj](https://github.com/yetanalytics/dl4clj) - Clojure wrapper for Deeplearning4j.\n\n<a name=\"clojure-data-analysis--data-visualization\"></a>\n#### Data Analysis\n* [tech.ml.dataset](https://github.com/techascent/tech.ml.dataset) - Clojure dataframe library and pipeline for data processing and machine learning\n* [Tablecloth](https://github.com/scicloj/tablecloth) - A dataframe grammar wrapping tech.ml.dataset, inspired by several R libraries\n* [Panthera](https://github.com/alanmarazzi/panthera) - Clojure API wrapping Python's Pandas library\n* [Incanter](http://incanter.org/) - Incanter is a Clojure-based, R-like platform for statistical computing and graphics.\n* [PigPen](https://github.com/Netflix/PigPen) - Map-Reduce for Clojure.\n* [Geni](https://github.com/zero-one-group/geni) - a Clojure dataframe library that runs on Apache Spark\n\n<a name=\"clojure-data-visualization\"></a>\n#### Data Visualization\n* [Hanami](https://github.com/jsa-aerial/hanami) : Clojure(Script) library and framework for creating interactive visualization applications based in Vega-Lite (VGL) and/or Vega (VG) specifications. Automatic framing and layouts along with a powerful templating system for abstracting visualization specs\n* [Saite](https://github.com/jsa-aerial/saite) -  Clojure(Script) client/server application for dynamic interactive explorations and the creation of live shareable documents capturing them using Vega/Vega-Lite, CodeMirror, markdown, and LaTeX\n* [Oz](https://github.com/metasoarous/oz) - Data visualisation using Vega/Vega-Lite and Hiccup, and a live-reload platform for literate-programming\n* [Envision](https://github.com/clojurewerkz/envision) - Clojure Data Visualisation library, based on Statistiker and D3.\n* [Pink Gorilla Notebook](https://github.com/pink-gorilla/gorilla-notebook) - A Clojure/Clojurescript notebook application/-library based on Gorilla-REPL\n* [clojupyter](https://github.com/clojupyter/clojupyter) -  A Jupyter kernel for Clojure - run Clojure code in Jupyter Lab, Notebook and Console.\n* [notespace](https://github.com/scicloj/notespace) - Notebook experience in your Clojure namespace\n* [Delight](https://github.com/datamechanics/delight) - A listener that streams your spark events logs to delight, a free and improved spark UI\n\n<a name=\"clojure-interop\"></a>\n#### Interop\n\n* [Java Interop](https://clojure.org/reference/java_interop) - Clojure has Native Java Interop from which Java's ML ecosystem can be accessed\n* [JavaScript Interop](https://clojurescript.org/reference/javascript-api) - ClojureScript has Native JavaScript Interop from which JavaScript's ML ecosystem can be accessed\n* [Libpython-clj](https://github.com/clj-python/libpython-clj) - Interop with Python\n* [ClojisR](https://github.com/scicloj/clojisr) - Interop with R and Renjin (R on the JVM)\n\n<a name=\"clojure-misc\"></a>\n#### Misc\n* [Neanderthal](https://neanderthal.uncomplicate.org/) - Fast Clojure Matrix Library (native CPU, GPU, OpenCL, CUDA)\n* [kixistats](https://github.com/MastodonC/kixi.stats) - A library of statistical distribution sampling and transducing functions\n* [fastmath](https://github.com/generateme/fastmath) - A collection of functions for mathematical and statistical computing, macine learning, etc., wrapping several JVM libraries\n* [matlib](https://github.com/atisharma/matlib) - A Clojure library of optimisation and control theory tools and convenience functions based on Neanderthal.\n\n<a name=\"clojure-extra\"></a>\n#### Extra\n* [Scicloj](https://scicloj.github.io/pages/libraries/) - Curated list of ML related resources for Clojure.\n\n<a name=\"crystal\"></a>\n## Crystal\n\n<a name=\"crystal-general-purpose-machine-learning\"></a>\n#### General-Purpose Machine Learning\n\n* [machine](https://github.com/mathieulaporte/machine) - Simple machine learning algorithm.\n* [crystal-fann](https://github.com/NeuraLegion/crystal-fann) - FANN (Fast Artificial Neural Network) binding.\n\n<a name=\"elixir\"></a>\n## Elixir\n\n<a name=\"elixir-general-purpose-machine-learning\"></a>\n#### General-Purpose Machine Learning\n\n* [Simple Bayes](https://github.com/fredwu/simple_bayes) - A Simple Bayes / Naive Bayes implementation in Elixir.\n* [emel](https://github.com/mrdimosthenis/emel) - A simple and functional machine learning library written in Elixir.\n* [Tensorflex](https://github.com/anshuman23/tensorflex) - Tensorflow bindings for the Elixir programming language.\n\n<a name=\"elixir-natural-language-processing\"></a>\n#### Natural Language Processing\n\n* [Stemmer](https://github.com/fredwu/stemmer) - An English (Porter2) stemming implementation in Elixir.\n\n<a name=\"erlang\"></a>\n## Erlang\n\n<a name=\"erlang-general-purpose-machine-learning\"></a>\n#### General-Purpose Machine Learning\n\n* [Disco](https://github.com/discoproject/disco/) - Map Reduce in Erlang. **[Deprecated]**\n\n<a name=\"fortran\"></a>\n## Fortran\n\n<a name=\"fortran-general-purpose-machine-learning\"></a>\n#### General-Purpose Machine Learning\n\n* [neural-fortran](https://github.com/modern-fortran/neural-fortran) - A parallel neural net microframework.\nRead the paper [here](https://arxiv.org/abs/1902.06714).\n\n<a name=\"fortran-data-analysis--data-visualization\"></a>\n#### Data Analysis / Data Visualization\n\n* [ParaMonte](https://github.com/cdslaborg/paramonte) - A general-purpose Fortran library for Bayesian data analysis and visualization via serial/parallel Monte Carlo and MCMC simulations. Documentation can be found [here](https://www.cdslab.org/paramonte/).\n\n<a name=\"go\"></a>\n## Go\n\n<a name=\"go-natural-language-processing\"></a>\n#### Natural Language Processing\n\n* [Cybertron](https://github.com/nlpodyssey/cybertron) - Cybertron: the home planet of the Transformers in Go.\n* [snowball](https://github.com/tebeka/snowball) - Snowball Stemmer for Go.\n* [word-embedding](https://github.com/ynqa/word-embedding) - Word Embeddings: the full implementation of word2vec, GloVe in Go.\n* [sentences](https://github.com/neurosnap/sentences) - Golang implementation of Punkt sentence tokenizer.\n* [go-ngram](https://github.com/Lazin/go-ngram) - In-memory n-gram index with compression. *[Deprecated]*\n* [paicehusk](https://github.com/Rookii/paicehusk) - Golang implementation of the Paice/Husk Stemming Algorithm. *[Deprecated]*\n* [go-porterstemmer](https://github.com/reiver/go-porterstemmer) - A native Go clean room implementation of the Porter Stemming algorithm. **[Deprecated]**\n\n<a name=\"go-general-purpose-machine-learning\"></a>\n#### General-Purpose Machine Learning\n\n* [Spago](https://github.com/nlpodyssey/spago) - Self-contained Machine Learning and Natural Language Processing library in Go.\n* [birdland](https://github.com/rlouf/birdland) - A recommendation library in Go.\n* [eaopt](https://github.com/MaxHalford/eaopt) - An evolutionary optimization library.\n* [leaves](https://github.com/dmitryikh/leaves) - A pure Go implementation of the prediction part of GBRTs, including XGBoost and LightGBM.\n* [gobrain](https://github.com/goml/gobrain) - Neural Networks written in Go.\n* [go-featureprocessing](https://github.com/nikolaydubina/go-featureprocessing) - Fast and convenient feature processing for low latency machine learning in Go.\n* [go-mxnet-predictor](https://github.com/songtianyi/go-mxnet-predictor) - Go binding for MXNet c_predict_api to do inference with a pre-trained model.\n* [go-ml-benchmarks](https://github.com/nikolaydubina/go-ml-benchmarks) \u2014 benchmarks of machine learning inference for Go.\n* [go-ml-transpiler](https://github.com/znly/go-ml-transpiler) - An open source Go transpiler for machine learning models.\n* [golearn](https://github.com/sjwhitworth/golearn) - Machine learning for Go.\n* [goml](https://github.com/cdipaolo/goml) - Machine learning library written in pure Go.\n* [gorgonia](https://github.com/gorgonia/gorgonia) - Deep learning in Go.\n* [goro](https://github.com/aunum/goro) - A high-level machine learning library in the vein of Keras.\n* [gorse](https://github.com/zhenghaoz/gorse) - An offline recommender system backend based on collaborative filtering written in Go.\n* [therfoo](https://github.com/therfoo/therfoo) - An embedded deep learning library for Go.\n* [neat](https://github.com/jinyeom/neat) - Plug-and-play, parallel Go framework for NeuroEvolution of Augmenting Topologies (NEAT). **[Deprecated]**\n* [go-pr](https://github.com/daviddengcn/go-pr) - Pattern recognition package in Go lang. **[Deprecated]**\n* [go-ml](https://github.com/alonsovidales/go_ml) - Linear / Logistic regression, Neural Networks, Collaborative Filtering and Gaussian Multivariate Distribution. **[Deprecated]**\n* [GoNN](https://github.com/fxsjy/gonn) - GoNN is an implementation of Neural Network in Go Language, which includes BPNN, RBF, PCN. **[Deprecated]**\n* [bayesian](https://github.com/jbrukh/bayesian) - Naive Bayesian Classification for Golang. **[Deprecated]**\n* [go-galib](https://github.com/thoj/go-galib) - Genetic Algorithms library written in Go / Golang. **[Deprecated]**\n* [Cloudforest](https://github.com/ryanbressler/CloudForest) - Ensembles of decision trees in Go/Golang. **[Deprecated]**\n* [go-dnn](https://github.com/sudachen/go-dnn) - Deep Neural Networks for Golang (powered by MXNet)\n\n<a name=\"go-spatial-analysis-and-geometry\"></a>\n#### Spatial analysis and geometry\n\n* [go-geom](https://github.com/twpayne/go-geom) - Go library to handle geometries.\n* [gogeo](https://github.com/golang/geo) - Spherical geometry in Go.\n\n<a name=\"go-data-analysis--data-visualization\"></a>\n#### Data Analysis / Data Visualization\n\n* [dataframe-go](https://github.com/rocketlaunchr/dataframe-go) - Dataframes for machine-learning and statistics (similar to pandas).\n* [gota](https://github.com/go-gota/gota) - Dataframes.\n* [gonum/mat](https://godoc.org/gonum.org/v1/gonum/mat) - A linear algebra package for Go.\n* [gonum/optimize](https://godoc.org/gonum.org/v1/gonum/optimize) - Implementations of optimization algorithms.\n* [gonum/plot](https://godoc.org/gonum.org/v1/plot) - A plotting library.\n* [gonum/stat](https://godoc.org/gonum.org/v1/gonum/stat) - A statistics library.\n* [SVGo](https://github.com/ajstarks/svgo) - The Go Language library for SVG generation.\n* [glot](https://github.com/arafatk/glot) - Glot is a plotting library for Golang built on top of gnuplot.\n* [globe](https://github.com/mmcloughlin/globe) - Globe wireframe visualization.\n* [gonum/graph](https://godoc.org/gonum.org/v1/gonum/graph) - General-purpose graph library.\n* [go-graph](https://github.com/StepLg/go-graph) - Graph library for Go/Golang language. **[Deprecated]**\n* [RF](https://github.com/fxsjy/RF.go) - Random forests implementation in Go. **[Deprecated]**\n\n<a name=\"go-computer-vision\"></a>\n#### Computer vision\n\n* [GoCV](https://github.com/hybridgroup/gocv) - Package for computer vision using OpenCV 4 and beyond.\n\n<a name=\"go-reinforcement-learning\"></a>\n#### Reinforcement learning\n\n* [gold](https://github.com/aunum/gold) - A reinforcement learning library.\n* [stable-baselines3](https://github.com/DLR-RM/stable-baselines3) - PyTorch implementations of Stable Baselines (deep) reinforcement learning algorithms.\n\n<a name=\"haskell\"></a>\n## Haskell\n\n<a name=\"haskell-general-purpose-machine-learning\"></a>\n#### General-Purpose Machine Learning\n* [haskell-ml](https://github.com/ajtulloch/haskell-ml) - Haskell implementations of various ML algorithms. **[Deprecated]**\n* [HLearn](https://github.com/mikeizbicki/HLearn) - a suite of libraries for interpreting machine learning models according to their algebraic structure. **[Deprecated]**\n* [hnn](https://github.com/alpmestan/HNN) - Haskell Neural Network library.\n* [hopfield-networks](https://github.com/ajtulloch/hopfield-networks) - Hopfield Networks for unsupervised learning in Haskell. **[Deprecated]**\n* [DNNGraph](https://github.com/ajtulloch/dnngraph) - A DSL for deep neural networks. **[Deprecated]**\n* [LambdaNet](https://github.com/jbarrow/LambdaNet) - Configurable Neural Networks in Haskell. **[Deprecated]**\n\n<a name=\"java\"></a>\n## Java\n\n<a name=\"java-natural-language-processing\"></a>\n#### Natural Language Processing\n* [Cortical.io](https://www.cortical.io/) - Retina: an API performing complex NLP operations (disambiguation, classification, streaming text filtering, etc...) as quickly and intuitively as the brain.\n* [IRIS](https://github.com/cortical-io/Iris) - [Cortical.io's](https://cortical.io) FREE NLP, Retina API Analysis Tool (written in JavaFX!) - [See the Tutorial Video](https://www.youtube.com/watch?v=CsF4pd7fGF0).\n* [CoreNLP](https://nlp.stanford.edu/software/corenlp.shtml) - Stanford CoreNLP provides a set of natural language analysis tools which can take raw English language text input and give the base forms of words.\n* [Stanford Parser](https://nlp.stanford.edu/software/lex-parser.shtml) - A natural language parser is a program that works out the grammatical structure of sentences.\n* [Stanford POS Tagger](https://nlp.stanford.edu/software/tagger.shtml) - A Part-Of-Speech Tagger (POS Tagger).\n* [Stanford Name Entity Recognizer](https://nlp.stanford.edu/software/CRF-NER.shtml) - Stanford NER is a Java implementation of a Named Entity Recognizer.\n* [Stanford Word Segmenter](https://nlp.stanford.edu/software/segmenter.shtml) - Tokenization of raw text is a standard pre-processing step for many NLP tasks.\n* [Tregex, Tsurgeon and Semgrex](https://nlp.stanford.edu/software/tregex.shtml) - Tregex is a utility for matching patterns in trees, based on tree relationships and regular expression matches on nodes (the name is short for \"tree regular expressions\").\n* [Stanford Phrasal: A Phrase-Based Translation System](https://nlp.stanford.edu/phrasal/)\n* [Stanford English Tokenizer](https://nlp.stanford.edu/software/tokenizer.shtml) - Stanford Phrasal is a state-of-the-art statistical phrase-based machine translation system, written in Java.\n* [Stanford Tokens Regex](https://nlp.stanford.edu/software/tokensregex.shtml) - A tokenizer divides text into a sequence of tokens, which roughly correspond to \"words\".\n* [Stanford Temporal Tagger](https://nlp.stanford.edu/software/sutime.shtml) - SUTime is a library for recognizing and normalizing time expressions.\n* [Stanford SPIED](https://nlp.stanford.edu/software/patternslearning.shtml) - Learning entities from unlabeled text starting with seed sets using patterns in an iterative fashion.\n* [Twitter Text Java](https://github.com/twitter/twitter-text/tree/master/java) - A Java implementation of Twitter's text processing library.\n* [MALLET](http://mallet.cs.umass.edu/) - A Java-based package for statistical natural language processing, document classification, clustering, topic modelling, information extraction, and other machine learning applications to text.\n* [OpenNLP](https://opennlp.apache.org/) - A machine learning based toolkit for the processing of natural language text.\n* [LingPipe](http://alias-i.com/lingpipe/index.html) - A tool kit for processing text using computational linguistics.\n* [ClearTK](https://github.com/ClearTK/cleartk) - ClearTK provides a framework for developing statistical natural language processing (NLP) components in Java and is built on top of Apache UIMA. **[Deprecated]**\n* [Apache cTAKES](https://ctakes.apache.org/) - Apache Clinical Text Analysis and Knowledge Extraction System (cTAKES) is an open-source natural language processing system for information extraction from electronic medical record clinical free-text.\n* [NLP4J](https://github.com/emorynlp/nlp4j) - The NLP4J project provides software and resources for natural language processing. The project started at the Center for Computational Language and EducAtion Research, and is currently developed by the Center for Language and Information Research at Emory University. **[Deprecated]**\n* [CogcompNLP](https://github.com/CogComp/cogcomp-nlp) - This project collects a number of core libraries for Natural Language Processing (NLP) developed in the University of Illinois' Cognitive Computation Group, for example `illinois-core-utilities` which provides a set of NLP-friendly data structures and a number of NLP-related utilities that support writing NLP applications, running experiments, etc, `illinois-edison` a library for feature extraction from illinois-core-utilities data structures and many other packages.\n\n<a name=\"java-general-purpose-machine-learning\"></a>\n#### General-Purpose Machine Learning\n\n* [aerosolve](https://github.com/airbnb/aerosolve) - A machine learning library by Airbnb designed from the ground up to be human friendly.\n* [AMIDST Toolbox](http://www.amidsttoolbox.com/) - A Java Toolbox for Scalable Probabilistic Machine Learning.\n* [Chips-n-Salsa](https://github.com/cicirello/Chips-n-Salsa) - A Java library for genetic algorithms, evolutionary computation, and stochastic local search, with a focus on self-adaptation / self-tuning, as well as parallel execution.\n* [Datumbox](https://github.com/datumbox/datumbox-framework) - Machine Learning framework for rapid development of Machine Learning and Statistical applications.\n* [ELKI](https://elki-project.github.io/) - Java toolkit for data mining. (unsupervised: clustering, outlier detection etc.)\n* [Encog](https://github.com/encog/encog-java-core) - An advanced neural network and machine learning framework. Encog contains classes to create a wide variety of networks, as well as support classes to normalize and process data for these neural networks. Encog trainings using multithreaded resilient propagation. Encog can also make use of a GPU to further speed processing time. A GUI based workbench is also provided to help model and train neural networks.\n* [FlinkML in Apache Flink](https://ci.apache.org/projects/flink/flink-docs-master/dev/libs/ml/index.html) - Distributed machine learning library in Flink.\n* [H2O](https://github.com/h2oai/h2o-3) - ML engine that supports distributed learning on Hadoop, Spark or your laptop via APIs in R, Python, Scala, REST/JSON.\n* [htm.java](https://github.com/numenta/htm.java) - General Machine Learning library using Numenta\u2019s Cortical Learning Algorithm.\n* [liblinear-java](https://github.com/bwaldvogel/liblinear-java) - Java version of liblinear.\n* [Mahout](https://github.com/apache/mahout) - Distributed machine learning.\n* [Meka](http://meka.sourceforge.net/) - An open source implementation of methods for multi-label classification and evaluation (extension to Weka).\n* [MLlib in Apache Spark](https://spark.apache.org/docs/latest/mllib-guide.html) - Distributed machine learning library in Spark.\n* [Hydrosphere Mist](https://github.com/Hydrospheredata/mist) - a service for deployment Apache Spark MLLib machine learning models as realtime, batch or reactive web services.\n* [Neuroph](http://neuroph.sourceforge.net/) - Neuroph is lightweight Java neural network framework.\n* [ORYX](https://github.com/oryxproject/oryx) - Lambda Architecture Framework using Apache Spark and Apache Kafka with a specialization for real-time large-scale machine learning.\n* [Samoa](https://samoa.incubator.apache.org/) SAMOA is a framework that includes distributed machine learning for data streams with an interface to plug-in different stream processing platforms.\n* [RankLib](https://sourceforge.net/p/lemur/wiki/RankLib/) - RankLib is a library of learning to rank algorithms. **[Deprecated]**\n* [rapaio](https://github.com/padreati/rapaio) - statistics, data mining and machine learning toolbox in Java.\n* [RapidMiner](https://rapidminer.com) - RapidMiner integration into Java code.\n* [Stanford Classifier](https://nlp.stanford.edu/software/classifier.shtml) - A classifier is a machine learning tool that will take data items and place them into one of k classes.\n* [Smile](https://haifengl.github.io/) - Statistical Machine Intelligence & Learning Engine.\n* [SystemML](https://github.com/apache/systemml) - flexible, scalable machine learning (ML) language.\n* [Tribou](https://tribuo.org) - A machine learning library written in Java by Oracle.\n* [Weka](https://www.cs.waikato.ac.nz/ml/weka/) - Weka is a collection of machine learning algorithms for data mining tasks.\n* [LBJava](https://github.com/CogComp/lbjava) - Learning Based Java is a modelling language for the rapid development of software systems, offers a convenient, declarative syntax for classifier and constraint definition directly in terms of the objects in the programmer's application.\n* [knn-java-library](https://github.com/felipexw/knn-java-library) - Just a simple implementation of K-Nearest Neighbors algorithm using with a bunch of similarity measures.\n\n<a name=\"java-speech-recognition\"></a>\n#### Speech Recognition\n* [CMU Sphinx](https://cmusphinx.github.io) - Open Source Toolkit For Speech Recognition purely based on Java speech recognition library.\n\n<a name=\"java-data-analysis--data-visualization\"></a>\n#### Data Analysis / Data Visualization\n\n* [Flink](https://flink.apache.org/) - Open source platform for distributed stream and batch data processing.\n* [Hadoop](https://github.com/apache/hadoop) - Hadoop/HDFS.\n* [Onyx](https://github.com/onyx-platform/onyx) - Distributed, masterless, high performance, fault tolerant data processing. Written entirely in Clojure.\n* [Spark](https://github.com/apache/spark) - Spark is a fast and general engine for large-scale data processing.\n* [Storm](https://storm.apache.org/) - Storm is a distributed realtime computation system.\n* [Impala](https://github.com/cloudera/impala) - Real-time Query for Hadoop.\n* [DataMelt](https://jwork.org/dmelt/) - Mathematics software for numeric computation, statistics, symbolic calculations, data analysis and data visualization.\n* [Dr. Michael Thomas Flanagan's Java Scientific Library.](https://www.ee.ucl.ac.uk/~mflanaga/java/) **[Deprecated]**\n\n<a name=\"java-deep-learning\"></a>\n#### Deep Learning\n\n* [Deeplearning4j](https://github.com/deeplearning4j/deeplearning4j) - Scalable deep learning for industry with parallel GPUs.\n* [Keras Beginner Tutorial](https://victorzhou.com/blog/keras-neural-network-tutorial/) - Friendly guide on using Keras to implement a simple Neural Network in Python.\n* [deepjavalibrary/djl](https://github.com/deepjavalibrary/djl) - Deep Java Library (DJL) is an open-source, high-level, engine-agnostic Java framework for deep learning, designed to be easy to get started with and simple to use for Java developers.\n\n<a name=\"javascript\"></a>\n## JavaScript\n\n<a name=\"javascript-natural-language-processing\"></a>\n#### Natural Language Processing\n\n* [Twitter-text](https://github.com/twitter/twitter-text) - A JavaScript implementation of Twitter's text processing library.\n* [natural](https://github.com/NaturalNode/natural) - General natural language facilities for node.\n* [Knwl.js](https://github.com/loadfive/Knwl.js) - A Natural Language Processor in JS.\n* [Retext](https://github.com/retextjs/retext) - Extensible system for analyzing and manipulating natural language.\n* [NLP Compromise](https://github.com/spencermountain/compromise) - Natural Language processing in the browser.\n* [nlp.js](https://github.com/axa-group/nlp.js) - An NLP library built in node over Natural, with entity extraction, sentiment analysis, automatic language identify, and so more.\n\n\n\n<a name=\"javascript-data-analysis--data-visualization\"></a>\n#### Data Analysis / Data Visualization\n\n* [D3.js](https://d3js.org/)\n* [High Charts](https://www.highcharts.com/)\n* [NVD3.js](http://nvd3.org/)\n* [dc.js](https://dc-js.github.io/dc.js/)\n* [chartjs](https://www.chartjs.org/)\n* [dimple](http://dimplejs.org/)\n* [amCharts](https://www.amcharts.com/)\n* [D3xter](https://github.com/NathanEpstein/D3xter) - Straight forward plotting built on D3. **[Deprecated]**\n* [statkit](https://github.com/rigtorp/statkit) - Statistics kit for JavaScript. **[Deprecated]**\n* [datakit](https://github.com/nathanepstein/datakit) - A lightweight framework for data analysis in JavaScript\n* [science.js](https://github.com/jasondavies/science.js/) - Scientific and statistical computing in JavaScript. **[Deprecated]**\n* [Z3d](https://github.com/NathanEpstein/Z3d) - Easily make interactive 3d plots built on Three.js **[Deprecated]**\n* [Sigma.js](http://sigmajs.org/) - JavaScript library dedicated to graph drawing.\n* [C3.js](https://c3js.org/) - customizable library based on D3.js for easy chart drawing.\n* [Datamaps](https://datamaps.github.io/) - Customizable SVG map/geo visualizations using D3.js. **[Deprecated]**\n* [ZingChart](https://www.zingchart.com/) - library written on Vanilla JS for big data visualization.\n* [cheminfo](https://www.cheminfo.org/) - Platform for data visualization and analysis, using the [visualizer](https://github.com/npellet/visualizer) project.\n* [Learn JS Data](http://learnjsdata.com/)\n* [AnyChart](https://www.anychart.com/)\n* [FusionCharts](https://www.fusioncharts.com/)\n* [Nivo](https://nivo.rocks) - built on top of the awesome d3 and Reactjs libraries\n\n\n<a name=\"javascript-general-purpose-machine-learning\"></a>\n#### General-Purpose Machine Learning\n\n* [Auto ML](https://github.com/ClimbsRocks/auto_ml) - Automated machine learning, data formatting, ensembling, and hyperparameter optimization for competitions and exploration- just give it a .csv file! **[Deprecated]**\n* [Convnet.js](https://cs.stanford.edu/people/karpathy/convnetjs/) - ConvNetJS is a JavaScript library for training Deep Learning models[DEEP LEARNING] **[Deprecated]**\n* [Creatify MCP](https://github.com/TSavo/creatify-mcp) - Model Context Protocol server that exposes Creatify AI's video generation capabilities to AI assistants, enabling natural language video creation workflows.\n* [Clusterfck](https://harthur.github.io/clusterfck/) - Agglomerative hierarchical clustering implemented in JavaScript for Node.js and the browser. **[Deprecated]**\n* [Clustering.js](https://github.com/emilbayes/clustering.js) - Clustering algorithms implemented in JavaScript for Node.js and the browser. **[Deprecated]**\n* [Decision Trees](https://github.com/serendipious/nodejs-decision-tree-id3) - NodeJS Implementation of Decision Tree using ID3 Algorithm. **[Deprecated]**\n* [DN2A](https://github.com/antoniodeluca/dn2a.js) - Digital Neural Networks Architecture. **[Deprecated]**\n* [figue](https://code.google.com/archive/p/figue) - K-means, fuzzy c-means and agglomerative clustering.\n* [Gaussian Mixture Model](https://github.com/lukapopijac/gaussian-mixture-model) - Unsupervised machine learning with multivariate Gaussian mixture model.\n* [Node-fann](https://github.com/rlidwka/node-fann) - FANN (Fast Artificial Neural Network Library) bindings for Node.js **[Deprecated]**\n* [Keras.js](https://github.com/transcranial/keras-js) - Run Keras models in the browser, with GPU support provided by WebGL 2.\n* [Kmeans.js](https://github.com/emilbayes/kMeans.js) - Simple JavaScript implementation of the k-means algorithm, for node.js and the browser. **[Deprecated]**\n* [LDA.js](https://github.com/primaryobjects/lda) - LDA topic modelling for Node.js\n* [Learning.js](https://github.com/yandongliu/learningjs) - JavaScript implementation of logistic regression/c4.5 decision tree **[Deprecated]**\n* [machinelearn.js](https://github.com/machinelearnjs/machinelearnjs) - Machine Learning library for the web, Node.js and developers\n* [mil-tokyo](https://github.com/mil-tokyo) - List of several machine learning libraries.\n* [Node-SVM](https://github.com/nicolaspanel/node-svm) - Support Vector Machine for Node.js\n* [Brain](https://github.com/harthur/brain) - Neural networks in JavaScript **[Deprecated]**\n* [Brain.js](https://github.com/BrainJS/brain.js) - Neural networks in JavaScript - continued community fork of [Brain](https://github.com/harthur/brain).\n* [Bayesian-Bandit](https://github.com/omphalos/bayesian-bandit.js) - Bayesian bandit implementation for Node and the browser. **[Deprecated]**\n* [Synaptic](https://github.com/cazala/synaptic) - Architecture-free neural network library for Node.js and the browser.\n* [kNear](https://github.com/NathanEpstein/kNear) - JavaScript implementation of the k nearest neighbors algorithm for supervised learning.\n* [NeuralN](https://github.com/totemstech/neuraln) - C++ Neural Network library for Node.js. It has advantage on large dataset and multi-threaded training. **[Deprecated]**\n* [kalman](https://github.com/itamarwe/kalman) - Kalman filter for JavaScript. **[Deprecated]**\n* [shaman](https://github.com/luccastera/shaman) - Node.js library with support for both simple and multiple linear regression. **[Deprecated]**\n* [ml.js](https://github.com/mljs/ml) - Machine learning and numerical analysis tools for Node.js and the Browser!\n* [ml5](https://github.com/ml5js/ml5-library) - Friendly machine learning for the web!\n* [Pavlov.js](https://github.com/NathanEpstein/Pavlov.js) - Reinforcement learning using Markov Decision Processes.\n* [MXNet](https://github.com/apache/incubator-mxnet) - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, JavaScript and more.\n* [TensorFlow.js](https://js.tensorflow.org/) - A WebGL accelerated, browser based JavaScript library for training and deploying ML models.\n* [JSMLT](https://github.com/jsmlt/jsmlt) - Machine learning toolkit with classification and clustering for Node.js; supports visualization (see [visualml.io](https://visualml.io)).\n* [xgboost-node](https://github.com/nuanio/xgboost-node) - Run XGBoost model and make predictions in Node.js.\n* [Netron](https://github.com/lutzroeder/netron) - Visualizer for machine learning models.\n* [tensor-js](https://github.com/Hoff97/tensorjs) - A deep learning library for the browser, accelerated by WebGL and WebAssembly.\n* [WebDNN](https://github.com/mil-tokyo/webdnn) - Fast Deep Neural Network JavaScript Framework. WebDNN uses next generation JavaScript API, WebGPU for GPU execution, and WebAssembly for CPU execution.\n* [WebNN](https://webnn.dev) - A new web standard that allows web apps and frameworks to accelerate deep neural networks with on-device hardware such as GPUs, CPUs, or purpose-built AI accelerators.\n\n<a name=\"javascript-misc\"></a>\n#### Misc\n\n* [stdlib](https://github.com/stdlib-js/stdlib) - A standard library for JavaScript and Node.js, with an emphasis on numeric computing. The library provides a collection of robust, high performance libraries for mathematics, statistics, streams, utilities, and more.\n* [sylvester](https://github.com/jcoglan/sylvester) - Vector and Matrix math for JavaScript. **[Deprecated]**\n* [simple-statistics](https://github.com/simple-statistics/simple-statistics) - A JavaScript implementation of descriptive, regression, and inference statistics. Implemented in literate JavaScript with no dependencies, designed to work in all modern browsers (including IE) as well as in Node.js.\n* [regression-js](https://github.com/Tom-Alexander/regression-js) - A javascript library containing a collection of least squares fitting methods for finding a trend in a set of data.\n* [Lyric](https://github.com/flurry/Lyric) - Linear Regression library. **[Deprecated]**\n* [GreatCircle](https://github.com/mwgg/GreatCircle) - Library for calculating great circle distance.\n* [MLPleaseHelp](https://github.com/jgreenemi/MLPleaseHelp) - MLPleaseHelp is a simple ML resource search engine. You can use this search engine right now at [https://jgreenemi.github.io/MLPleaseHelp/](https://jgreenemi.github.io/MLPleaseHelp/), provided via GitHub Pages.\n* [Pipcook](https://github.com/alibaba/pipcook) - A JavaScript application framework for machine learning and its engineering.\n\n<a name=\"javascript-demos-and-scripts\"></a>\n#### Demos and Scripts\n* [The Bot](https://github.com/sta-ger/TheBot) - Example of how the neural network learns to predict the angle between two points created with [Synaptic](https://github.com/cazala/synaptic).\n* [Half Beer](https://github.com/sta-ger/HalfBeer) - Beer glass classifier created with [Synaptic](https://github.com/cazala/synaptic).\n* [NSFWJS](http://nsfwjs.com) - Indecent content checker with TensorFlow.js\n* [Rock Paper Scissors](https://rps-tfjs.netlify.com/) - Rock Paper Scissors trained in the browser with TensorFlow.js\n* [Heroes Wear Masks](https://heroeswearmasks.fun/) - A fun TensorFlow.js-based oracle that tells, whether one wears a face mask or not. It can even tell when one wears the mask incorrectly.\n\n<a name=\"julia\"></a>\n## Julia\n\n<a name=\"julia-general-purpose-machine-learning\"></a>\n#### General-Purpose Machine Learning\n\n* [MachineLearning](https://github.com/benhamner/MachineLearning.jl) - Julia Machine Learning library. **[Deprecated]**\n* [MLBase](https://github.com/JuliaStats/MLBase.jl) - A set of functions to support the development of machine learning algorithms.\n* [PGM](https://github.com/JuliaStats/PGM.jl) - A Julia framework for probabilistic graphical models.\n* [DA](https://github.com/trthatcher/DiscriminantAnalysis.jl) - Julia package for Regularized Discriminant Analysis.\n* [Regression](https://github.com/lindahua/Regression.jl) - Algorithms for regression analysis (e.g. linear regression and logistic regression). **[Deprecated]**\n* [Local Regression](https://github.com/JuliaStats/Loess.jl) - Local regression, so smooooth!\n* [Naive Bayes](https://github.com/nutsiepully/NaiveBayes.jl) - Simple Naive Bayes implementation in Julia. **[Deprecated]**\n* [Mixed Models](https://github.com/dmbates/MixedModels.jl) - A Julia package for fitting (statistical) mixed-effects models.\n* [Simple MCMC](https://github.com/fredo-dedup/SimpleMCMC.jl) - basic MCMC sampler implemented in Julia. **[Deprecated]**\n* [Distances](https://github.com/JuliaStats/Distances.jl) - Julia module for Distance evaluation.\n* [Decision Tree](https://github.com/bensadeghi/DecisionTree.jl) - Decision Tree Classifier and Regressor.\n* [Neural](https://github.com/compressed/BackpropNeuralNet.jl) - A neural network in Julia.\n* [MCMC](https://github.com/doobwa/MCMC.jl) - MCMC tools for Julia. **[Deprecated]**\n* [Mamba](https://github.com/brian-j-smith/Mamba.jl) - Markov chain Monte Carlo (MCMC) for Bayesian analysis in Julia.\n* [GLM](https://github.com/JuliaStats/GLM.jl) - Generalized linear models in Julia.\n* [Gaussian Processes](https://github.com/STOR-i/GaussianProcesses.jl) - Julia package for Gaussian processes.\n* [Online Learning](https://github.com/lendle/OnlineLearning.jl) **[Deprecated]**\n* [GLMNet](https://github.com/simonster/GLMNet.jl) - Julia wrapper for fitting Lasso/ElasticNet GLM models using glmnet.\n* [Clustering](https://github.com/JuliaStats/Clustering.jl) - Basic functions for clustering data: k-means, dp-means, etc.\n* [SVM](https://github.com/JuliaStats/SVM.jl) - SVM for Julia. **[Deprecated]**\n* [Kernel Density](https://github.com/JuliaStats/KernelDensity.jl) - Kernel density estimators for Julia.\n* [MultivariateStats](https://github.com/JuliaStats/MultivariateStats.jl) - Methods for dimensionality reduction.\n* [NMF](https://github.com/JuliaStats/NMF.jl) - A Julia package for non-negative matrix factorization.\n* [ANN](https://github.com/EricChiang/ANN.jl) - Julia artificial neural networks. **[Deprecated]**\n* [Mocha](https://github.com/pluskid/Mocha.jl) - Deep Learning framework for Julia inspired by Caffe. **[Deprecated]**\n* [XGBoost](https://github.com/dmlc/XGBoost.jl) - eXtreme Gradient Boosting Package in Julia.\n* [ManifoldLearning](https://github.com/wildart/ManifoldLearning.jl) - A Julia package for manifold learning and nonlinear dimensionality reduction.\n* [MXNet](https://github.com/apache/incubator-mxnet) - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, JavaScript and more.\n* [Merlin](https://github.com/hshindo/Merlin.jl) - Flexible Deep Learning Framework in Julia.\n* [ROCAnalysis](https://github.com/davidavdav/ROCAnalysis.jl) - Receiver Operating Characteristics and functions for evaluation probabilistic binary classifiers.\n* [GaussianMixtures](https://github.com/davidavdav/GaussianMixtures.jl) - Large scale Gaussian Mixture Models.\n* [ScikitLearn](https://github.com/cstjean/ScikitLearn.jl) - Julia implementation of the scikit-learn API.\n* [Knet](https://github.com/denizyuret/Knet.jl) - Ko\u00e7 University Deep Learning Framework.\n* [Flux](https://fluxml.ai/) - Relax! Flux is the ML library that doesn't make you tensor\n* [MLJ](https://github.com/alan-turing-institute/MLJ.jl) - A Julia machine learning framework.\n* [CluGen](https://github.com/clugen/CluGen.jl/) - Multidimensional cluster generation in Julia.\n\n<a name=\"julia-natural-language-processing\"></a>\n#### Natural Language Processing\n\n* [Topic Models](https://github.com/slycoder/TopicModels.jl) - TopicModels for Julia. **[Deprecated]**\n* [Text Analysis](https://github.com/JuliaText/TextAnalysis.jl) - Julia package for text analysis.\n* [Word Tokenizers](https://github.com/JuliaText/WordTokenizers.jl) - Tokenizers for Natural Language Processing in Julia\n* [Corpus Loaders](https://github.com/JuliaText/CorpusLoaders.jl) - A Julia package providing a variety of loaders for various NLP corpora.\n* [Embeddings](https://github.com/JuliaText/Embeddings.jl) - Functions and data dependencies for loading various word embeddings\n* [Languages](https://github.com/JuliaText/Languages.jl) - Julia package for working with various human languages\n* [WordNet](https://github.com/JuliaText/WordNet.jl) - A Julia package for Princeton's WordNet\n\n<a name=\"julia-data-analysis--data-visualization\"></a>\n#### Data Analysis / Data Visualization\n\n* [Graph Layout](https://github.com/IainNZ/GraphLayout.jl) - Graph layout algorithms in pure Julia.\n* [LightGraphs](https://github.com/JuliaGraphs/LightGraphs.jl) - Graph modelling and analysis.\n* [Data Frames Meta](https://github.com/JuliaData/DataFramesMeta.jl) - Metaprogramming tools for DataFrames.\n* [Julia Data](https://github.com/nfoti/JuliaData) - library for working with tabular data in Julia. **[Deprecated]**\n* [Data Read](https://github.com/queryverse/ReadStat.jl) - Read files from Stata, SAS, and SPSS.\n* [Hypothesis Tests](https://github.com/JuliaStats/HypothesisTests.jl) - Hypothesis tests for Julia.\n* [Gadfly](https://github.com/GiovineItalia/Gadfly.jl) - Crafty statistical graphics for Julia.\n* [Stats](https://github.com/JuliaStats/StatsKit.jl) - Statistical tests for Julia.\n* [RDataSets](https://github.com/johnmyleswhite/RDatasets.jl) - Julia package for loading many of the data sets available in R.\n* [DataFrames](https://github.com/JuliaData/DataFrames.jl) - library for working with tabular data in Julia.\n* [Distributions](https://github.com/JuliaStats/Distributions.jl) - A Julia package for probability distributions and associated functions.\n* [Data Arrays](https://github.com/JuliaStats/DataArrays.jl) - Data structures that allow missing values. **[Deprecated]**\n* [Time Series](https://github.com/JuliaStats/TimeSeries.jl) - Time series toolkit for Julia.\n* [Sampling](https://github.com/lindahua/Sampling.jl) - Basic sampling algorithms for Julia.\n\n<a name=\"julia-misc-stuff--presentations\"></a>\n#### Misc Stuff / Presentations\n\n* [DSP](https://github.com/JuliaDSP/DSP.jl) - Digital Signal Processing (filtering, periodograms, spectrograms, window functions).\n* [JuliaCon Presentations](https://github.com/JuliaCon/presentations) - Presentations for JuliaCon.\n* [SignalProcessing](https://github.com/JuliaDSP/DSP.jl) - Signal Processing tools for Julia.\n* [Images](https://github.com/JuliaImages/Images.jl) - An image library for Julia.\n* [DataDeps](https://github.com/oxinabox/DataDeps.jl) - Reproducible data setup for reproducible science.\n\n<a name=\"kotlin\"></a>\n## Kotlin\n\n<a name=\"kotlin-deep-learning\"></a>\n#### Deep Learning\n* [KotlinDL](https://github.com/JetBrains/KotlinDL) - Deep learning framework written in Kotlin.\n\n<a name=\"lua\"></a>\n## Lua\n\n<a name=\"lua-general-purpose-machine-learning\"></a>\n#### General-Purpose Machine Learning\n\n* [Torch7](http://torch.ch/)\n  * [cephes](https://github.com/deepmind/torch-cephes) - Cephes mathematical functions library, wrapped for Torch. Provides and wraps the 180+ special mathematical functions from the Cephes mathematical library, developed by Stephen L. Moshier. It is used, among many other places, at the heart of SciPy. **[Deprecated]**\n  * [autograd](https://github.com/twitter/torch-autograd) - Autograd automatically differentiates native Torch code. Inspired by the original Python version.\n  * [graph](https://github.com/torch/graph) - Graph package for Torch. **[Deprecated]**\n  * [randomkit](https://github.com/deepmind/torch-randomkit) - Numpy's randomkit, wrapped for Torch. **[Deprecated]**\n  * [signal](https://github.com/soumith/torch-signal) - A signal processing toolbox for Torch-7. FFT, DCT, Hilbert, cepstrums, stft.\n  * [nn](https://github.com/torch/nn) - Neural Network package for Torch.\n  * [torchnet](https://github.com/torchnet/torchnet) - framework for torch which provides a set of abstractions aiming at encouraging code re-use as well as encouraging modular programming.\n  * [nngraph](https://github.com/torch/nngraph) - This package provides graphical computation for nn library in Torch7.\n  * [nnx](https://github.com/clementfarabet/lua---nnx) - A completely unstable and experimental package that extends Torch's builtin nn library.\n  * [rnn](https://github.com/Element-Research/rnn) - A Recurrent Neural Network library that extends Torch's nn. RNNs, LSTMs, GRUs, BRNNs, BLSTMs, etc.\n  * [dpnn](https://github.com/Element-Research/dpnn) - Many useful features that aren't part of the main nn package.\n  * [dp](https://github.com/nicholas-leonard/dp) - A deep learning library designed for streamlining research and development using the Torch7 distribution. It emphasizes flexibility through the elegant use of object-oriented design patterns. **[Deprecated]**\n  * [optim](https://github.com/torch/optim) - An optimization library for Torch. SGD, Adagrad, Conjugate-Gradient, LBFGS, RProp and more.\n  * [unsup](https://github.com/koraykv/unsup) - A package for unsupervised learning in Torch. Provides modules that are compatible with nn (LinearPsd, ConvPsd, AutoEncoder, ...), and self-contained algorithms (k-means, PCA). **[Deprecated]**\n  * [manifold](https://github.com/clementfarabet/manifold) - A package to manipulate manifolds.\n  * [svm](https://github.com/koraykv/torch-svm) - Torch-SVM library. **[Deprecated]**\n  * [lbfgs](https://github.com/clementfarabet/lbfgs) - FFI Wrapper for liblbfgs. **[Deprecated]**\n  * [vowpalwabbit](https://github.com/clementfarabet/vowpal_wabbit) - An old vowpalwabbit interface to torch. **[Deprecated]**\n  * [OpenGM](https://github.com/clementfarabet/lua---opengm) - OpenGM is a C++ library for graphical modelling, and inference. The Lua bindings provide a simple way of describing graphs, from Lua, and then optimizing them with OpenGM. **[Deprecated]**\n  * [spaghetti](https://github.com/MichaelMathieu/lua---spaghetti) - Spaghetti (sparse linear) module for torch7 by @MichaelMathieu **[Deprecated]**\n  * [LuaSHKit](https://github.com/ocallaco/LuaSHkit) - A Lua wrapper around the Locality sensitive hashing library SHKit **[Deprecated]**\n  * [kernel smoothing](https://github.com/rlowrance/kernel-smoothers) - KNN, kernel-weighted average, local linear regression smoothers. **[Deprecated]**\n  * [cutorch](https://github.com/torch/cutorch) - Torch CUDA Implementation.\n  * [cunn](https://github.com/torch/cunn) - Torch CUDA Neural Network Implementation.\n  * [imgraph](https://github.com/clementfarabet/lua---imgraph) - An image/graph library for Torch. This package provides routines to construct graphs on images, segment them, build trees out of them, and convert them back to images. **[Deprecated]**\n  * [videograph](https://github.com/clementfarabet/videograph) - A video/graph library for Torch. This package provides routines to construct graphs on videos, segment them, build trees out of them, and convert them back to videos. **[Deprecated]**\n  * [saliency](https://github.com/marcoscoffier/torch-saliency) - code and tools around integral images. A library for finding interest points based on fast integral histograms. **[Deprecated]**\n  * [stitch](https://github.com/marcoscoffier/lua---stitch) - allows us to use hugin to stitch images and apply same stitching to a video sequence. **[Deprecated]**\n  * [sfm](https://github.com/marcoscoffier/lua---sfm) - A bundle adjustment/structure from motion package. **[Deprecated]**\n  * [fex](https://github.com/koraykv/fex) - A package for feature extraction in Torch. Provides SIFT and dSIFT modules. **[Deprecated]**\n  * [OverFeat](https://github.com/sermanet/OverFeat) - A state-of-the-art generic dense feature extractor. **[Deprecated]**\n  * [wav2letter](https://github.com/facebookresearch/wav2letter) - a simple and efficient end-to-end Automatic Speech Recognition (ASR) system from Facebook AI Research.\n* [Numeric Lua](http://numlua.luaforge.net/)\n* [Lunatic Python](https://labix.org/lunatic-python)\n* [SciLua](http://scilua.org/)\n* [Lua - Numerical Algorithms](https://bitbucket.org/lucashnegri/lna) **[Deprecated]**\n* [Lunum](https://github.com/jzrake/lunum) **[Deprecated]**\n* [Keras GPT Copilot](https://github.com/fabprezja/keras-gpt-copilot) - A python package that integrates an LLM copilot inside the keras model development workflow.\n\n<a name=\"lua-demos-and-scripts\"></a>\n#### Demos and Scripts\n* [Core torch7 demos repository](https://github.com/e-lab/torch7-demos).\n  * linear-regression, logistic-regression\n  * face detector (training and detection as separate demos)\n  * mst-based-segmenter\n  * train-a-digit-classifier\n  * train-autoencoder\n  * optical flow demo\n  * train-on-housenumbers\n  * train-on-cifar\n  * tracking with deep nets\n  * kinect demo\n  * filter-bank visualization\n  * saliency-networks\n* [Training a Convnet for the Galaxy-Zoo Kaggle challenge(CUDA demo)](https://github.com/soumith/galaxyzoo)\n* [torch-datasets](https://github.com/rosejn/torch-datasets) - Scripts to load several popular datasets including:\n  * BSR 500\n  * CIFAR-10\n  * COIL\n  * Street View House Numbers\n  * MNIST\n  * NORB\n* [Atari2600](https://github.com/fidlej/aledataset) - Scripts to generate a dataset with static frames from the Arcade Learning Environment.\n\n\n\n<a name=\"matlab\"></a>\n## Matlab\n\n<a name=\"matlab-computer-vision\"></a>\n#### Computer Vision\n\n* [Contourlets](http://www.ifp.illinois.edu/~minhdo/software/contourlet_toolbox.tar) - MATLAB source code that implements the contourlet transform and its utility functions.\n* [Shearlets](https://www3.math.tu-berlin.de/numerik/www.shearlab.org/software) - MATLAB code for shearlet transform.\n* [Curvelets](http://www.curvelet.org/software.html) - The Curvelet transform is a higher dimensional generalization of the Wavelet transform designed to represent images at different scales and different angles.\n* [Bandlets](http://www.cmap.polytechnique.fr/~peyre/download/) - MATLAB code for bandlet transform.\n* [mexopencv](https://kyamagu.github.io/mexopencv/) - Collection and a development kit of MATLAB mex functions for OpenCV library.\n\n<a name=\"matlab-natural-language-processing\"></a>\n#### Natural Language Processing\n\n* [NLP](https://amplab.cs.berkeley.edu/an-nlp-library-for-matlab/) - A NLP library for Matlab.\n\n<a name=\"matlab-general-purpose-machine-learning\"></a>\n#### General-Purpose Machine Learning\n\n* [Training a deep autoencoder or a classifier\non MNIST digits](https://www.cs.toronto.edu/~hinton/MatlabForSciencePaper.html) - Training a deep autoencoder or a classifier\non MNIST digits[DEEP LEARNING].\n* [Convolutional-Recursive Deep Learning for 3D Object Classification](https://www.socher.org/index.php/Main/Convolutional-RecursiveDeepLearningFor3DObjectClassification) - Convolutional-Recursive Deep Learning for 3D Object Classification[DEEP LEARNING].\n* [Spider](https://people.kyb.tuebingen.mpg.de/spider/) - The spider is intended to be a complete object orientated environment for machine learning in Matlab.\n* [LibSVM](https://www.csie.ntu.edu.tw/~cjlin/libsvm/#matlab) - A Library for Support Vector Machines.\n* [ThunderSVM](https://github.com/Xtra-Computing/thundersvm) - An Open-Source SVM Library on GPUs and CPUs\n* [LibLinear](https://www.csie.ntu.edu.tw/~cjlin/liblinear/#download) - A Library for Large Linear Classification.\n* [Machine Learning Module](https://github.com/josephmisiti/machine-learning-module) - Class on machine w/ PDF, lectures, code\n* [Caffe](https://github.com/BVLC/caffe) - A deep learning framework developed with cleanliness, readability, and speed in mind.\n* [Pattern Recognition Toolbox](https://github.com/covartech/PRT) - A complete object-oriented environment for machine learning in Matlab.\n* [Pattern Recognition and Machine Learning](https://github.com/PRML/PRMLT) - This package contains the matlab implementation of the algorithms described in the book Pattern Recognition and Machine Learning by C. Bishop.\n* [Optunity](https://optunity.readthedocs.io/en/latest/) - A library dedicated to automated hyperparameter optimization with a simple, lightweight API to facilitate drop-in replacement of grid search. Optunity is written in Python but interfaces seamlessly with MATLAB.\n* [MXNet](https://github.com/apache/incubator-mxnet/) - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, JavaScript and more.\n* [Machine Learning in MatLab/Octave](https://github.com/trekhleb/machine-learning-octave) - Examples of popular machine learning algorithms (neural networks, linear/logistic regressions, K-Means, etc.) with code examples and mathematics behind them being explained.\n* [MOCluGen](https://github.com/clugen/MOCluGen/) - Multidimensional cluster generation in MATLAB/Octave.\n\n<a name=\"matlab-data-analysis--data-visualization\"></a>\n#### Data Analysis / Data Visualization\n\n* [ParaMonte](https://github.com/cdslaborg/paramonte) - A general-purpose MATLAB library for Bayesian data analysis and visualization via serial/parallel Monte Carlo and MCMC simulations. Documentation can be found [here](https://www.cdslab.org/paramonte/).\n* [matlab_bgl](https://www.cs.purdue.edu/homes/dgleich/packages/matlab_bgl/) - MatlabBGL is a Matlab package for working with graphs.\n* [gaimc](https://www.mathworks.com/matlabcentral/fileexchange/24134-gaimc---graph-algorithms-in-matlab-code) - Efficient pure-Matlab implementations of graph algorithms to complement MatlabBGL's mex functions.\n\n<a name=\"net\"></a>\n## .NET\n\n<a name=\"net-computer-vision\"></a>\n#### Computer Vision\n\n* [OpenCVDotNet](https://code.google.com/archive/p/opencvdotnet) - A wrapper for the OpenCV project to be used with .NET applications.\n* [Emgu CV](http://www.emgu.com/wiki/index.php/Main_Page) - Cross platform wrapper of OpenCV which can be compiled in Mono to be run on Windows, Linus, Mac OS X, iOS, and Android.\n* [AForge.NET](http://www.aforgenet.com/framework/) - Open source C# framework for developers and researchers in the fields of Computer Vision and Artificial Intelligence. Development has now shifted to GitHub.\n* [Accord.NET](http://accord-framework.net) - Together with AForge.NET, this library can provide image processing and computer vision algorithms to Windows, Windows RT and Windows Phone. Some components are also available for Java and Android.\n\n<a name=\"net-natural-language-processing\"></a>\n#### Natural Language Processing\n\n* [Stanford.NLP for .NET](https://github.com/sergey-tihon/Stanford.NLP.NET/) - A full port of Stanford NLP packages to .NET and also available precompiled as a NuGet package.\n\n<a name=\"net-general-purpose-machine-learning\"></a>\n#### General-Purpose Machine Learning\n\n* [Accord-Framework](http://accord-framework.net/) -The Accord.NET Framework is a complete framework for building machine learning, computer vision, computer audition, signal processing and statistical applications.\n* [Accord.MachineLearning](https://www.nuget.org/packages/Accord.MachineLearning/) - Support Vector Machines, Decision Trees, Naive Bayesian models, K-means, Gaussian Mixture models and general algorithms such as Ransac, Cross-validation and Grid-Search for machine-learning applications. This package is part of the Accord.NET Framework.\n* [DiffSharp](https://diffsharp.github.io/DiffSharp/) - An automatic differentiation (AD) library providing exact and efficient derivatives (gradients, Hessians, Jacobians, directional derivatives, and matrix-free Hessian- and Jacobian-vector products) for machine learning and optimization applications. Operations can be nested to any level, meaning that you can compute exact higher-order derivatives and differentiate functions that are internally making use of differentiation, for applications such as hyperparameter optimization.\n* [Encog](https://www.nuget.org/packages/encog-dotnet-core/) - An advanced neural network and machine learning framework. Encog contains classes to create a wide variety of networks, as well as support classes to normalize and process data for these neural networks. Encog trains using multithreaded resilient propagation. Encog can also make use of a GPU to further speed processing time. A GUI based workbench is also provided to help model and train neural networks.\n* [GeneticSharp](https://github.com/giacomelli/GeneticSharp) - Multi-platform genetic algorithm library for .NET Core and .NET Framework. The library has several implementations of GA operators, like: selection, crossover, mutation, reinsertion and termination.\n* [Infer.NET](https://dotnet.github.io/infer/) - Infer.NET is a framework for running Bayesian inference in graphical models. One can use Infer.NET to solve many different kinds of machine learning problems, from standard problems like classification, recommendation or clustering through customized solutions to domain-specific problems. Infer.NET has been used in a wide variety of domains including information retrieval, bioinformatics, epidemiology, vision, and many others.\n* [ML.NET](https://github.com/dotnet/machinelearning) - ML.NET is a cross-platform open-source machine learning framework which makes machine learning accessible to .NET developers. ML.NET was originally developed in Microsoft Research and evolved into a significant framework over the last decade and is used across many product groups in Microsoft like Windows, Bing, PowerPoint, Excel and more.\n* [Neural Network Designer](https://sourceforge.net/projects/nnd/) - DBMS management system and designer for neural networks. The designer application is developed using WPF, and is a user interface which allows you to design your neural network, query the network, create and configure chat bots that are capable of asking questions and learning from your feedback. The chat bots can even scrape the internet for information to return in their output as well as to use for learning.\n* [Synapses](https://github.com/mrdimosthenis/Synapses) - Neural network library in F#.\n* [Vulpes](https://github.com/fsprojects/Vulpes) - Deep belief and deep learning implementation written in F# and leverages CUDA GPU execution with Alea.cuBase.\n* [MxNet.Sharp](https://github.com/tech-quantum/MxNet.Sharp) - .NET Standard bindings for Apache MxNet with Imperative, Symbolic and Gluon Interface for developing, training and deploying Machine Learning models in C#. https://mxnet.tech-quantum.com/\n\n<a name=\"net-data-analysis--data-visualization\"></a>\n#### Data Analysis / Data Visualization\n\n* [numl](https://www.nuget.org/packages/numl/) - numl is a machine learning library intended to ease the use of using standard modelling techniques for both prediction and clustering.\n* [Math.NET Numerics](https://www.nuget.org/packages/MathNet.Numerics/) - Numerical foundation of the Math.NET project, aiming to provide methods and algorithms for numerical computations in science, engineering and everyday use. Supports .Net 4.0, .Net 3.5 and Mono on Windows, Linux and Mac; Silverlight 5, WindowsPhone/SL 8, WindowsPhone 8.1 and Windows 8 with PCL Portable Profiles 47 and 344; Android/iOS with Xamarin.\n* [Sho](https://www.microsoft.com/en-us/research/project/sho-the-net-playground-for-data/) - Sho is an interactive environment for data analysis and scientific computing that lets you seamlessly connect scripts (in IronPython) with compiled code (in .NET) to enable fast and flexible prototyping. The environment includes powerful and efficient libraries for linear algebra as well as data visualization that can be used from any .NET language, as well as a feature-rich interactive shell for rapid development.\n\n<a name=\"objective-c\"></a>\n## Objective C\n\n<a name=\"objective-c-general-purpose-machine-learning\"></a>\n### General-Purpose Machine Learning\n\n* [YCML](https://github.com/yconst/YCML) - A Machine Learning framework for Objective-C and Swift (OS X / iOS).\n* [MLPNeuralNet](https://github.com/nikolaypavlov/MLPNeuralNet) - Fast multilayer perceptron neural network library for iOS and Mac OS X. MLPNeuralNet predicts new examples by trained neural networks. It is built on top of the Apple's Accelerate Framework, using vectorized operations and hardware acceleration if available. **[Deprecated]**\n* [MAChineLearning](https://github.com/gianlucabertani/MAChineLearning) - An Objective-C multilayer perceptron library, with full support for training through backpropagation. Implemented using vDSP and vecLib, it's 20 times faster than its Java equivalent. Includes sample code for use from Swift.\n* [BPN-NeuralNetwork](https://github.com/Kalvar/ios-BPN-NeuralNetwork) - It implemented 3 layers of neural networks ( Input Layer, Hidden Layer and Output Layer ) and it was named Back Propagation Neural Networks (BPN). This network can be used in products recommendation, user behavior analysis, data mining and data analysis. **[Deprecated]**\n* [Multi-Perceptron-NeuralNetwork](https://github.com/Kalvar/ios-Multi-Perceptron-NeuralNetwork) - It implemented multi-perceptrons neural network (\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af) based on Back Propagation Neural Networks (BPN) and designed unlimited-hidden-layers.\n* [KRHebbian-Algorithm](https://github.com/Kalvar/ios-KRHebbian-Algorithm) - It is a non-supervisory and self-learning algorithm (adjust the weights) in the neural network of Machine Learning. **[Deprecated]**\n* [KRKmeans-Algorithm](https://github.com/Kalvar/ios-KRKmeans-Algorithm) - It implemented K-Means  clustering and classification algorithm. It could be used in data mining and image compression. **[Deprecated]**\n* [KRFuzzyCMeans-Algorithm](https://github.com/Kalvar/ios-KRFuzzyCMeans-Algorithm) - It implemented Fuzzy C-Means (FCM) the fuzzy clustering / classification algorithm on Machine Learning. It could be used in data mining and image compression. **[Deprecated]**\n\n<a name=\"ocaml\"></a>\n## OCaml\n\n<a name=\"ocaml-general-purpose-machine-learning\"></a>\n### General-Purpose Machine Learning\n\n* [Oml](https://github.com/rleonid/oml) - A general statistics and machine learning library.\n* [GPR](https://mmottl.github.io/gpr/) - Efficient Gaussian Process Regression in OCaml.\n* [Libra-Tk](https://libra.cs.uoregon.edu) - Algorithms for learning and inference with discrete probabilistic models.\n* [TensorFlow](https://github.com/LaurentMazare/tensorflow-ocaml) - OCaml bindings for TensorFlow.\n\n<a name=\"opencv\"></a>\n## OpenCV\n\n<a name=\"opencv-ComputerVision and Text Detection\"></a>\n### OpenSource-Computer-Vision\n\n* [OpenCV](https://github.com/opencv/opencv) - A OpenSource Computer Vision Library\n\n<a name=\"perl\"></a>\n## Perl\n\n<a name=\"perl-data-analysis--data-visualization\"></a>\n### Data Analysis / Data Visualization\n\n* [Perl Data Language](https://metacpan.org/pod/Paws::MachineLearning), a pluggable architecture for data and image processing, which can\nbe [used for machine learning](https://github.com/zenogantner/PDL-ML).\n\n<a name=\"perl-general-purpose-machine-learning\"></a>\n### General-Purpose Machine Learning\n\n* [MXnet for Deep Learning, in Perl](https://github.com/apache/incubator-mxnet/tree/master/perl-package),\nalso [released in CPAN](https://metacpan.org/pod/AI::MXNet).\n* [Perl Data Language](https://metacpan.org/pod/Paws::MachineLearning),\nusing AWS machine learning platform from Perl.\n* [Algorithm::SVMLight](https://metacpan.org/pod/Algorithm::SVMLight),\n  implementation of Support Vector Machines with SVMLight under it. **[Deprecated]**\n* Several machine learning and artificial intelligence models are\n  included in the [`AI`](https://metacpan.org/search?size=20&q=AI)\n  namespace. For instance, you can\n  find [Na\u00efve Bayes](https://metacpan.org/pod/AI::NaiveBayes).\n\n<a name=\"perl6\"></a>\n## Perl 6\n\n* [Support Vector Machines](https://github.com/titsuki/p6-Algorithm-LibSVM)\n* [Na\u00efve Bayes](https://github.com/titsuki/p6-Algorithm-NaiveBayes)\n\n<a name=\"perl-6-data-analysis--data-visualization\"></a>\n### Data Analysis / Data Visualization\n\n* [Perl Data Language](https://metacpan.org/pod/Paws::MachineLearning),\na pluggable architecture for data and image processing, which can\nbe\n[used for machine learning](https://github.com/zenogantner/PDL-ML).\n\n<a name=\"perl-6-general-purpose-machine-learning\"></a>\n### General-Purpose Machine Learning\n\n<a name=\"php\"></a>\n## PHP\n\n<a name=\"php-natural-language-processing\"></a>\n### Natural Language Processing\n\n* [jieba-php](https://github.com/fukuball/jieba-php) - Chinese Words Segmentation Utilities.\n\n<a name=\"php-general-purpose-machine-learning\"></a>\n### General-Purpose Machine Learning\n\n* [PHP-ML](https://gitlab.com/php-ai/php-ml) - Machine Learning library for PHP. Algorithms, Cross Validation, Neural Network, Preprocessing, Feature Extraction and much more in one library.\n* [PredictionBuilder](https://github.com/denissimon/prediction-builder) - A library for machine learning that builds predictions using a linear regression.\n* [Rubix ML](https://github.com/RubixML) - A high-level machine learning (ML) library that lets you build programs that learn from data using the PHP language.\n* [19 Questions](https://github.com/fulldecent/19-questions) - A machine learning / bayesian inference assigning attributes to objects.\n\n<a name=\"python\"></a>\n## Python\n\n<a name=\"python-computer-vision\"></a>\n#### Computer Vision\n\n* [LightlyTrain](https://github.com/lightly-ai/lightly-train) - Pretrain computer vision models on unlabeled data for industrial applications\n* [Scikit-Image](https://github.com/scikit-image/scikit-image) - A collection of algorithms for image processing in Python.\n* [Scikit-Opt](https://github.com/guofei9987/scikit-opt) - Swarm Intelligence in Python (Genetic Algorithm, Particle Swarm Optimization, Simulated Annealing, Ant Colony Algorithm, Immune Algorithm, Artificial Fish Swarm Algorithm in Python)\n* [SimpleCV](http://simplecv.org/) - An open source computer vision framework that gives access to several high-powered computer vision libraries, such as OpenCV. Written on Python and runs on Mac, Windows, and Ubuntu Linux.\n* [Vigranumpy](https://github.com/ukoethe/vigra) - Python bindings for the VIGRA C++ computer vision library.\n* [OpenFace](https://cmusatyalab.github.io/openface/) - Free and open source face recognition with deep neural networks.\n* [PCV](https://github.com/jesolem/PCV) - Open source Python module for computer vision. **[Deprecated]**\n* [face_recognition](https://github.com/ageitgey/face_recognition) - Face recognition library that recognizes and manipulates faces from Python or from the command line.\n* [deepface](https://github.com/serengil/deepface) - A lightweight face recognition and facial attribute analysis (age, gender, emotion and race) framework for Python covering cutting-edge models such as VGG-Face, FaceNet, OpenFace, DeepFace, DeepID, Dlib and ArcFace.\n* [retinaface](https://github.com/serengil/retinaface) - deep learning based cutting-edge facial detector for Python coming with facial landmarks\n* [dockerface](https://github.com/natanielruiz/dockerface) - Easy to install and use deep learning Faster R-CNN face detection for images and video in a docker container. **[Deprecated]**\n* [Detectron](https://github.com/facebookresearch/Detectron) - FAIR's software system that implements state-of-the-art object detection algorithms, including Mask R-CNN. It is written in Python and powered by the Caffe2 deep learning framework. **[Deprecated]**\n* [detectron2](https://github.com/facebookresearch/detectron2) - FAIR's next-generation research platform for object detection and segmentation. It is a ground-up rewrite of the previous version, Detectron, and is powered by the PyTorch deep learning framework.\n* [albumentations](https://github.com/albu/albumentations) - \u0410 fast and framework agnostic image augmentation library that implements a diverse set of augmentation techniques. Supports classification, segmentation, detection out of the box. Was used to win a number of Deep Learning competitions at Kaggle, Topcoder and those that were a part of the CVPR workshops.\n* [pytessarct](https://github.com/madmaze/pytesseract) - Python-tesseract is an optical character recognition (OCR) tool for python. That is, it will recognize and \"read\" the text embedded in images. Python-tesseract is a wrapper for [Google's Tesseract-OCR Engine](https://github.com/tesseract-ocr/tesseract).\n* [imutils](https://github.com/jrosebr1/imutils) - A library containing Convenience functions to make basic image processing operations such as translation, rotation, resizing, skeletonization, and displaying Matplotlib images easier with OpenCV and Python.\n* [PyTorchCV](https://github.com/donnyyou/PyTorchCV) - A PyTorch-Based Framework for Deep Learning in Computer Vision.\n* [joliGEN](https://github.com/jolibrain/joliGEN) - Generative AI Image Toolset with GANs and Diffusion for Real-World Applications.\n* [Self-supervised learning](https://pytorch-lightning-bolts.readthedocs.io/en/latest/self_supervised_models.html)\n* [neural-style-pt](https://github.com/ProGamerGov/neural-style-pt) - A PyTorch implementation of Justin Johnson's neural-style (neural style transfer).\n* [Detecto](https://github.com/alankbi/detecto) - Train and run a computer vision model with 5-10 lines of code.\n* [neural-dream](https://github.com/ProGamerGov/neural-dream) - A PyTorch implementation of DeepDream.\n* [Openpose](https://github.com/CMU-Perceptual-Computing-Lab/openpose) - A real-time multi-person keypoint detection library for body, face, hands, and foot estimation\n* [Deep High-Resolution-Net](https://github.com/leoxiaobin/deep-high-resolution-net.pytorch) - A PyTorch implementation of CVPR2019 paper \"Deep High-Resolution Representation Learning for Human Pose Estimation\"\n* [TF-GAN](https://github.com/tensorflow/gan) - TF-GAN is a lightweight library for training and evaluating Generative Adversarial Networks (GANs).\n* [dream-creator](https://github.com/ProGamerGov/dream-creator) - A PyTorch implementation of DeepDream. Allows individuals to quickly and easily train their own custom GoogleNet models with custom datasets for DeepDream.\n* [Lucent](https://github.com/greentfrapp/lucent) - Tensorflow and OpenAI Clarity's Lucid adapted for PyTorch.\n* [lightly](https://github.com/lightly-ai/lightly) - Lightly is a computer vision framework for self-supervised learning.\n* [Learnergy](https://github.com/gugarosa/learnergy) - Energy-based machine learning models built upon PyTorch.\n* [OpenVisionAPI](https://github.com/openvisionapi) - Open source computer vision API based on open source models.\n* [IoT Owl](https://github.com/Ret2Me/IoT-Owl) - Light face detection and recognition system with huge possibilities, based on Microsoft Face API and TensorFlow made for small IoT devices like raspberry pi.\n* [Exadel CompreFace](https://github.com/exadel-inc/CompreFace) - face recognition system that can be easily integrated into any system without prior machine learning skills. CompreFace provides REST API for face recognition, face verification, face detection, face mask detection, landmark detection, age, and gender recognition and is easily deployed with docker.\n* [computer-vision-in-action](https://github.com/Charmve/computer-vision-in-action) - as known as ``L0CV``, is a new generation of computer vision open source online learning media, a cross-platform interactive learning framework integrating graphics, source code and HTML. the L0CV ecosystem \u2014 Notebook, Datasets, Source Code, and from Diving-in to Advanced \u2014 as well as the L0CV Hub.\n* [timm](https://github.com/rwightman/pytorch-image-models) - PyTorch image models, scripts, pretrained weights -- ResNet, ResNeXT, EfficientNet, EfficientNetV2, NFNet, Vision Transformer, MixNet, MobileNet-V3/V2, RegNet, DPN, CSPNet, and more.\n* [segmentation_models.pytorch](https://github.com/qubvel/segmentation_models.pytorch) - A PyTorch-based toolkit that offers pre-trained segmentation models for computer vision tasks. It simplifies the development of image segmentation applications by providing a collection of popular architecture implementations, such as UNet and PSPNet, along with pre-trained weights, making it easier for researchers and developers to achieve high-quality pixel-level object segmentation in images.\n* [segmentation_models](https://github.com/qubvel/segmentation_models) - A TensorFlow Keras-based toolkit that offers pre-trained segmentation models for computer vision tasks. It simplifies the development of image segmentation applications by providing a collection of popular architecture implementations, such as UNet and PSPNet, along with pre-trained weights, making it easier for researchers and developers to achieve high-quality pixel-level object segmentation in images.\n* [MLX](https://github.com/ml-explore/mlx)- MLX is an array framework for machine learning on Apple silicon, developed by Apple machine learning research.\n\n<a name=\"python-natural-language-processing\"></a>\n#### Natural Language Processing\n\n* [pkuseg-python](https://github.com/lancopku/pkuseg-python) - A better version of Jieba, developed by Peking University.\n* [NLTK](https://www.nltk.org/) - A leading platform for building Python programs to work with human language data.\n* [Pattern](https://github.com/clips/pattern) - A web mining module for the Python programming language. It has tools for natural language processing, machine learning, among others.\n* [Quepy](https://github.com/machinalis/quepy) - A python framework to transform natural language questions to queries in a database query language.\n* [TextBlob](http://textblob.readthedocs.io/en/dev/) - Providing a consistent API for diving into common natural language processing (NLP) tasks. Stands on the giant shoulders of NLTK and Pattern, and plays nicely with both.\n* [YAlign](https://github.com/machinalis/yalign) - A sentence aligner, a friendly tool for extracting parallel sentences from comparable corpora. **[Deprecated]**\n* [jieba](https://github.com/fxsjy/jieba#jieba-1) - Chinese Words Segmentation Utilities.\n* [SnowNLP](https://github.com/isnowfy/snownlp) - A library for processing Chinese text.\n* [spammy](https://github.com/tasdikrahman/spammy) - A library for email Spam filtering built on top of NLTK\n* [loso](https://github.com/fangpenlin/loso) - Another Chinese segmentation library. **[Deprecated]**\n* [genius](https://github.com/duanhongyi/genius) - A Chinese segment based on Conditional Random Field.\n* [KoNLPy](http://konlpy.org) - A Python package for Korean natural language processing.\n* [nut](https://github.com/pprett/nut) - Natural language Understanding Toolkit. **[Deprecated]**\n* [Rosetta](https://github.com/columbia-applied-data-science/rosetta) - Text processing tools and wrappers (e.g. Vowpal Wabbit)\n* [BLLIP Parser](https://pypi.org/project/bllipparser/) - Python bindings for the BLLIP Natural Language Parser (also known as the Charniak-Johnson parser). **[Deprecated]**\n* [PyNLPl](https://github.com/proycon/pynlpl) - Python Natural Language Processing Library. General purpose NLP library for Python. Also contains some specific modules for parsing common NLP formats, most notably for [FoLiA](https://proycon.github.io/folia/), but also ARPA language models, Moses phrasetables, GIZA++ alignments.\n* [PySS3](https://github.com/sergioburdisso/pyss3) - Python package that implements a novel white-box machine learning model for text classification, called SS3. Since SS3 has the ability to visually explain its rationale, this package also comes with easy-to-use interactive visualizations tools ([online demos](http://tworld.io/ss3/)).\n* [python-ucto](https://github.com/proycon/python-ucto) - Python binding to ucto (a unicode-aware rule-based tokenizer for various languages).\n* [python-frog](https://github.com/proycon/python-frog) - Python binding to Frog, an NLP suite for Dutch. (pos tagging, lemmatisation, dependency parsing, NER)\n* [python-zpar](https://github.com/EducationalTestingService/python-zpar) - Python bindings for [ZPar](https://github.com/frcchang/zpar), a statistical part-of-speech-tagger, constituency parser, and dependency parser for English.\n* [colibri-core](https://github.com/proycon/colibri-core) - Python binding to C++ library for extracting and working with basic linguistic constructions such as n-grams and skipgrams in a quick and memory-efficient way.\n* [spaCy](https://github.com/explosion/spaCy) - Industrial strength NLP with Python and Cython.\n* [PyStanfordDependencies](https://github.com/dmcc/PyStanfordDependencies) - Python interface for converting Penn Treebank trees to Stanford Dependencies.\n* [Distance](https://github.com/doukremt/distance) - Levenshtein and Hamming distance computation. **[Deprecated]**\n* [Fuzzy Wuzzy](https://github.com/seatgeek/fuzzywuzzy) - Fuzzy String Matching in Python.\n* [Neofuzz](https://github.com/x-tabdeveloping/neofuzz) - Blazing fast, lightweight and customizable fuzzy and semantic text search in Python with fuzzywuzzy/thefuzz compatible API.\n* [jellyfish](https://github.com/jamesturk/jellyfish) - a python library for doing approximate and phonetic matching of strings.\n* [editdistance](https://pypi.org/project/editdistance/) - fast implementation of edit distance.\n* [textacy](https://github.com/chartbeat-labs/textacy) - higher-level NLP built on Spacy.\n* [stanford-corenlp-python](https://github.com/dasmith/stanford-corenlp-python) - Python wrapper for [Stanford CoreNLP](https://github.com/stanfordnlp/CoreNLP) **[Deprecated]**\n* [CLTK](https://github.com/cltk/cltk) - The Classical Language Toolkit.\n* [Rasa](https://github.com/RasaHQ/rasa) - A \"machine learning framework to automate text-and voice-based conversations.\"\n* [yase](https://github.com/PPACI/yase) - Transcode sentence (or other sequence) to list of word vector.\n* [Polyglot](https://github.com/aboSamoor/polyglot) - Multilingual text (NLP) processing toolkit.\n* [DrQA](https://github.com/facebookresearch/DrQA) - Reading Wikipedia to answer open-domain questions.\n* [Dedupe](https://github.com/dedupeio/dedupe) - A python library for accurate and scalable fuzzy matching, record deduplication and entity-resolution.\n* [Snips NLU](https://github.com/snipsco/snips-nlu) - Natural Language Understanding library for intent classification and entity extraction\n* [NeuroNER](https://github.com/Franck-Dernoncourt/NeuroNER) - Named-entity recognition using neural networks providing state-of-the-art-results\n* [DeepPavlov](https://github.com/deepmipt/DeepPavlov/) - conversational AI library with many pre-trained Russian NLP models.\n* [BigARTM](https://github.com/bigartm/bigartm) - topic modelling platform.\n* [NALP](https://github.com/gugarosa/nalp) - A Natural Adversarial Language Processing framework built over Tensorflow.\n* [DL Translate](https://github.com/xhlulu/dl-translate) - A deep learning-based translation library between 50 languages, built with `transformers`.\n* [Haystack](https://github.com/deepset-ai/haystack) - A framework for building industrial-strength applications with Transformer models and LLMs.\n* [CometLLM](https://github.com/comet-ml/comet-llm) - Track, log, visualize and evaluate your LLM prompts and prompt chains.\n* [Transformers](https://github.com/huggingface/transformers) - A deep learning library containing thousands of pre-trained models on different tasks. The goto place for anything related to Large Language Models.\n* [TextCL](https://github.com/alinapetukhova/textcl) - Text preprocessing package for use in NLP tasks.\n\n<a name=\"python-general-purpose-machine-learning\"></a>\n#### General-Purpose Machine Learning\n\n * [XAD](https://pypi.org/project/xad/) -> Fast and easy-to-use backpropagation tool.\n * [Aim](https://github.com/aimhubio/aim) -> An easy-to-use & supercharged open-source AI metadata tracker.\n * [RexMex](https://github.com/AstraZeneca/rexmex) -> A general purpose recommender metrics library for fair evaluation.\n * [ChemicalX](https://github.com/AstraZeneca/chemicalx) -> A PyTorch based deep learning library for drug pair scoring\n * [Microsoft ML for Apache Spark](https://github.com/Azure/mmlspark) -> A distributed machine learning framework Apache Spark\n * [Shapley](https://github.com/benedekrozemberczki/shapley) -> A data-driven framework to quantify the value of classifiers in a machine learning ensemble.\n * [igel](https://github.com/nidhaloff/igel) -> A delightful machine learning tool that allows you to train/fit, test and use models **without writing code**\n * [ML Model building](https://github.com/Shanky-21/Machine_learning) -> A Repository Containing Classification, Clustering, Regression, Recommender Notebooks with illustration to make them.\n * [ML/DL project template](https://github.com/PyTorchLightning/deep-learning-project-template)\n * [PyTorch Frame](https://github.com/pyg-team/pytorch-frame) -> A Modular Framework for Multi-Modal Tabular Learning.\n * [PyTorch Geometric](https://github.com/pyg-team/pytorch_geometric) -> Graph Neural Network Library for PyTorch.\n * [PyTorch Geometric Temporal](https://github.com/benedekrozemberczki/pytorch_geometric_temporal) -> A temporal extension of PyTorch Geometric for dynamic graph representation learning.\n * [Little Ball of Fur](https://github.com/benedekrozemberczki/littleballoffur) -> A graph sampling extension library for NetworkX with a Scikit-Learn like API.\n * [Karate Club](https://github.com/benedekrozemberczki/karateclub) -> An unsupervised machine learning extension library for NetworkX with a Scikit-Learn like API.\n* [Auto_ViML](https://github.com/AutoViML/Auto_ViML) -> Automatically Build Variant Interpretable ML models fast! Auto_ViML is pronounced \"auto vimal\", is a comprehensive and scalable Python AutoML toolkit with imbalanced handling, ensembling, stacking and built-in feature selection. Featured in <a href=\"https://towardsdatascience.com/why-automl-is-an-essential-new-tool-for-data-scientists-2d9ab4e25e46?source=friends_link&sk=d03a0cc55c23deb497d546d6b9be0653\">Medium article</a>.\n* [PyOD](https://github.com/yzhao062/pyod) -> Python Outlier Detection, comprehensive and scalable Python toolkit for detecting outlying objects in multivariate data. Featured for Advanced models, including Neural Networks/Deep Learning and Outlier Ensembles.\n* [steppy](https://github.com/neptune-ml/steppy) -> Lightweight, Python library for fast and reproducible machine learning experimentation. Introduces a very simple interface that enables clean machine learning pipeline design.\n* [steppy-toolkit](https://github.com/neptune-ml/steppy-toolkit) -> Curated collection of the neural networks, transformers and models that make your machine learning work faster and more effective.\n* [CNTK](https://github.com/Microsoft/CNTK) - Microsoft Cognitive Toolkit (CNTK), an open source deep-learning toolkit. Documentation can be found [here](https://docs.microsoft.com/cognitive-toolkit/).\n* [Couler](https://github.com/couler-proj/couler) - Unified interface for constructing and managing machine learning workflows on different workflow engines, such as Argo Workflows, Tekton Pipelines, and Apache Airflow.\n* [auto_ml](https://github.com/ClimbsRocks/auto_ml) - Automated machine learning for production and analytics. Lets you focus on the fun parts of ML, while outputting production-ready code, and detailed analytics of your dataset and results. Includes support for NLP, XGBoost, CatBoost, LightGBM, and soon, deep learning.\n* [dtaidistance](https://github.com/wannesm/dtaidistance) - High performance library for time series distances (DTW) and time series clustering.\n* [einops](https://github.com/arogozhnikov/einops) - Deep learning operations reinvented (for pytorch, tensorflow, jax and others).\n* [machine learning](https://github.com/jeff1evesque/machine-learning) - automated build consisting of a [web-interface](https://github.com/jeff1evesque/machine-learning#web-interface), and set of [programmatic-interface](https://github.com/jeff1evesque/machine-learning#programmatic-interface) API, for support vector machines. Corresponding dataset(s) are stored into a SQL database, then generated model(s) used for prediction(s), are stored into a NoSQL datastore.\n* [XGBoost](https://github.com/dmlc/xgboost) - Python bindings for eXtreme Gradient Boosting (Tree) Library.\n* [ChefBoost](https://github.com/serengil/chefboost) - a lightweight decision tree framework for Python with categorical feature support covering regular decision tree algorithms such as ID3, C4.5, CART, CHAID and regression tree; also some advanced bagging and boosting techniques such as gradient boosting, random forest and adaboost.\n* [Apache SINGA](https://singa.apache.org) - An Apache Incubating project for developing an open source machine learning library.\n* [Bayesian Methods for Hackers](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers) - Book/iPython notebooks on Probabilistic Programming in Python.\n* [Featureforge](https://github.com/machinalis/featureforge) A set of tools for creating and testing machine learning features, with a scikit-learn compatible API.\n* [MLlib in Apache Spark](http://spark.apache.org/docs/latest/mllib-guide.html) - Distributed machine learning library in Spark\n* [Hydrosphere Mist](https://github.com/Hydrospheredata/mist) - A service for deployment Apache Spark MLLib machine learning models as realtime, batch or reactive web services.\n* [Towhee](https://towhee.io) - A Python module that encode unstructured data into embeddings.\n* [scikit-learn](https://scikit-learn.org/) - A Python module for machine learning built on top of SciPy.\n* [metric-learn](https://github.com/metric-learn/metric-learn) - A Python module for metric learning.\n* [OpenMetricLearning](https://github.com/OML-Team/open-metric-learning) - A PyTorch-based framework to train and validate the models producing high-quality embeddings.\n* [Intel(R) Extension for Scikit-learn](https://github.com/intel/scikit-learn-intelex) - A seamless way to speed up your Scikit-learn applications with no accuracy loss and code changes.\n* [SimpleAI](https://github.com/simpleai-team/simpleai) Python implementation of many of the artificial intelligence algorithms described in the book \"Artificial Intelligence, a Modern Approach\". It focuses on providing an easy to use, well documented and tested library.\n* [astroML](https://www.astroml.org/) - Machine Learning and Data Mining for Astronomy.\n* [graphlab-create](https://turi.com/products/create/docs/) - A library with various machine learning models (regression, clustering, recommender systems, graph analytics, etc.) implemented on top of a disk-backed DataFrame.\n* [BigML](https://bigml.com) - A library that contacts external servers.\n* [pattern](https://github.com/clips/pattern) - Web mining module for Python.\n* [NuPIC](https://github.com/numenta/nupic) - Numenta Platform for Intelligent Computing.\n* [Pylearn2](https://github.com/lisa-lab/pylearn2) - A Machine Learning library based on [Theano](https://github.com/Theano/Theano). **[Deprecated]**\n* [keras](https://github.com/keras-team/keras) - High-level neural networks frontend for [TensorFlow](https://github.com/tensorflow/tensorflow), [CNTK](https://github.com/Microsoft/CNTK) and [Theano](https://github.com/Theano/Theano).\n* [Lasagne](https://github.com/Lasagne/Lasagne) - Lightweight library to build and train neural networks in Theano.\n* [hebel](https://github.com/hannes-brt/hebel) - GPU-Accelerated Deep Learning Library in Python. **[Deprecated]**\n* [Chainer](https://github.com/chainer/chainer) - Flexible neural network framework.\n* [prophet](https://facebook.github.io/prophet/) - Fast and automated time series forecasting framework by Facebook.\n* [skforecast](https://github.com/skforecast/skforecast) - Python library for time series forecasting using machine learning models. It works with any regressor compatible with the scikit-learn API, including popular options like LightGBM, XGBoost, CatBoost, Keras, and many others.\n* [Feature-engine](https://github.com/feature-engine/feature_engine) - Open source library with an exhaustive battery of feature engineering and selection methods based on pandas and scikit-learn.\n* [gensim](https://github.com/RaRe-Technologies/gensim) - Topic Modelling for Humans.\n* [tweetopic](https://centre-for-humanities-computing.github.io/tweetopic/) - Blazing fast short-text-topic-modelling for Python.\n* [topicwizard](https://github.com/x-tabdeveloping/topic-wizard) - Interactive topic model visualization/interpretation framework.\n* [topik](https://github.com/ContinuumIO/topik) - Topic modelling toolkit. **[Deprecated]**\n* [PyBrain](https://github.com/pybrain/pybrain) - Another Python Machine Learning Library.\n* [Brainstorm](https://github.com/IDSIA/brainstorm) - Fast, flexible and fun neural networks. This is the successor of PyBrain.\n* [Surprise](https://surpriselib.com) - A scikit for building and analyzing recommender systems.\n* [implicit](https://implicit.readthedocs.io/en/latest/quickstart.html) - Fast Python Collaborative Filtering for Implicit Datasets.\n* [LightFM](https://making.lyst.com/lightfm/docs/home.html) -  A Python implementation of a number of popular recommendation algorithms for both implicit and explicit feedback.\n* [Crab](https://github.com/muricoca/crab) - A flexible, fast recommender engine. **[Deprecated]**\n* [python-recsys](https://github.com/ocelma/python-recsys) - A Python library for implementing a Recommender System.\n* [thinking bayes](https://github.com/AllenDowney/ThinkBayes) - Book on Bayesian Analysis.\n* [Image-to-Image Translation with Conditional Adversarial Networks](https://github.com/williamFalcon/pix2pix-keras) - Implementation of image to image (pix2pix) translation from the paper by [isola et al](https://arxiv.org/pdf/1611.07004.pdf).[DEEP LEARNING]\n* [Restricted Boltzmann Machines](https://github.com/echen/restricted-boltzmann-machines) -Restricted Boltzmann Machines in Python. [DEEP LEARNING]\n* [Bolt](https://github.com/pprett/bolt) - Bolt Online Learning Toolbox. **[Deprecated]**\n* [CoverTree](https://github.com/patvarilly/CoverTree) - Python implementation of cover trees, near-drop-in replacement for scipy.spatial.kdtree **[Deprecated]**\n* [nilearn](https://github.com/nilearn/nilearn) - Machine learning for NeuroImaging in Python.\n* [neuropredict](https://github.com/raamana/neuropredict) - Aimed at novice machine learners and non-expert programmers, this package offers easy (no coding needed) and comprehensive machine learning (evaluation and full report of predictive performance WITHOUT requiring you to code) in Python for NeuroImaging and any other type of features. This is aimed at absorbing much of the ML workflow, unlike other packages like nilearn and pymvpa, which require you to learn their API and code to produce anything useful.\n* [imbalanced-learn](https://imbalanced-learn.org/stable/) - Python module to perform under sampling and oversampling with various techniques.\n* [imbalanced-ensemble](https://github.com/ZhiningLiu1998/imbalanced-ensemble) - Python toolbox for quick implementation, modification, evaluation, and visualization of ensemble learning algorithms for class-imbalanced data. Supports out-of-the-box multi-class imbalanced (long-tailed) classification.\n* [Shogun](https://github.com/shogun-toolbox/shogun) - The Shogun Machine Learning Toolbox.\n* [Pyevolve](https://github.com/perone/Pyevolve) - Genetic algorithm framework. **[Deprecated]**\n* [Caffe](https://github.com/BVLC/caffe) - A deep learning framework developed with cleanliness, readability, and speed in mind.\n* [breze](https://github.com/breze-no-salt/breze) - Theano based library for deep and recurrent neural networks.\n* [Cortex](https://github.com/cortexlabs/cortex) - Open source platform for deploying machine learning models in production.\n* [pyhsmm](https://github.com/mattjj/pyhsmm) - library for approximate unsupervised inference in Bayesian Hidden Markov Models (HMMs) and explicit-duration Hidden semi-Markov Models (HSMMs), focusing on the Bayesian Nonparametric extensions, the HDP-HMM and HDP-HSMM, mostly with weak-limit approximations.\n* [SKLL](https://github.com/EducationalTestingService/skll) - A wrapper around scikit-learn that makes it simpler to conduct experiments.\n* [neurolab](https://github.com/zueve/neurolab)\n* [Spearmint](https://github.com/HIPS/Spearmint) - Spearmint is a package to perform Bayesian optimization according to the algorithms outlined in the paper: Practical Bayesian Optimization of Machine Learning Algorithms. Jasper Snoek, Hugo Larochelle and Ryan P. Adams. Advances in Neural Information Processing Systems, 2012. **[Deprecated]**\n* [Pebl](https://github.com/abhik/pebl/) - Python Environment for Bayesian Learning. **[Deprecated]**\n* [Theano](https://github.com/Theano/Theano/) - Optimizing GPU-meta-programming code generating array oriented optimizing math compiler in Python.\n* [TensorFlow](https://github.com/tensorflow/tensorflow/) - Open source software library for numerical computation using data flow graphs.\n* [pomegranate](https://github.com/jmschrei/pomegranate) - Hidden Markov Models for Python, implemented in Cython for speed and efficiency.\n* [python-timbl](https://github.com/proycon/python-timbl) - A Python extension module wrapping the full TiMBL C++ programming interface. Timbl is an elaborate k-Nearest Neighbours machine learning toolkit.\n* [deap](https://github.com/deap/deap) - Evolutionary algorithm framework.\n* [pydeep](https://github.com/andersbll/deeppy) - Deep Learning In Python. **[Deprecated]**\n* [mlxtend](https://github.com/rasbt/mlxtend) - A library consisting of useful tools for data science and machine learning tasks.\n* [neon](https://github.com/NervanaSystems/neon) - Nervana's [high-performance](https://github.com/soumith/convnet-benchmarks) Python-based Deep Learning framework [DEEP LEARNING]. **[Deprecated]**\n* [Optunity](https://optunity.readthedocs.io/en/latest/) - A library dedicated to automated hyperparameter optimization with a simple, lightweight API to facilitate drop-in replacement of grid search.\n* [Neural Networks and Deep Learning](https://github.com/mnielsen/neural-networks-and-deep-learning) - Code samples for my book \"Neural Networks and Deep Learning\" [DEEP LEARNING].\n* [Annoy](https://github.com/spotify/annoy) - Approximate nearest neighbours implementation.\n* [TPOT](https://github.com/EpistasisLab/tpot) - Tool that automatically creates and optimizes machine learning pipelines using genetic programming. Consider it your personal data science assistant, automating a tedious part of machine learning.\n* [pgmpy](https://github.com/pgmpy/pgmpy) A python library for working with Probabilistic Graphical Models.\n* [DIGITS](https://github.com/NVIDIA/DIGITS) - The Deep Learning GPU Training System (DIGITS) is a web application for training deep learning models.\n* [Orange](https://orange.biolab.si/) - Open source data visualization and data analysis for novices and experts.\n* [MXNet](https://github.com/apache/incubator-mxnet) - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, JavaScript and more.\n* [milk](https://github.com/luispedro/milk) - Machine learning toolkit focused on supervised classification. **[Deprecated]**\n* [TFLearn](https://github.com/tflearn/tflearn) - Deep learning library featuring a higher-level API for TensorFlow.\n* [REP](https://github.com/yandex/rep) - an IPython-based environment for conducting data-driven research in a consistent and reproducible way. REP is not trying to substitute scikit-learn, but extends it and provides better user experience. **[Deprecated]**\n* [rgf_python](https://github.com/RGF-team/rgf) - Python bindings for Regularized Greedy Forest (Tree) Library.\n* [skbayes](https://github.com/AmazaspShumik/sklearn-bayes) - Python package for Bayesian Machine Learning with scikit-learn API.\n* [fuku-ml](https://github.com/fukuball/fuku-ml) - Simple machine learning library, including Perceptron, Regression, Support Vector Machine, Decision Tree and more, it's easy to use and easy to learn for beginners.\n* [Xcessiv](https://github.com/reiinakano/xcessiv) - A web-based application for quick, scalable, and automated hyperparameter tuning and stacked ensembling.\n* [PyTorch](https://github.com/pytorch/pytorch) - Tensors and Dynamic neural networks in Python with strong GPU acceleration\n* [PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning) - The lightweight PyTorch wrapper for high-performance AI research.\n* [PyTorch Lightning Bolts](https://github.com/PyTorchLightning/pytorch-lightning-bolts) - Toolbox of models, callbacks, and datasets for AI/ML researchers.\n* [skorch](https://github.com/skorch-dev/skorch) - A scikit-learn compatible neural network library that wraps PyTorch.\n* [ML-From-Scratch](https://github.com/eriklindernoren/ML-From-Scratch) - Implementations of Machine Learning models from scratch in Python with a focus on transparency. Aims to showcase the nuts and bolts of ML in an accessible way.\n* [Edward](http://edwardlib.org/) - A library for probabilistic modelling, inference, and criticism. Built on top of TensorFlow.\n* [xRBM](https://github.com/omimo/xRBM) - A library for Restricted Boltzmann Machine (RBM) and its conditional variants in Tensorflow.\n* [CatBoost](https://github.com/catboost/catboost) - General purpose gradient boosting on decision trees library with categorical features support out of the box. It is easy to install, well documented and supports CPU and GPU (even multi-GPU) computation.\n* [stacked_generalization](https://github.com/fukatani/stacked_generalization) - Implementation of machine learning stacking technique as a handy library in Python.\n* [modAL](https://github.com/modAL-python/modAL) - A modular active learning framework for Python, built on top of scikit-learn.\n* [Cogitare](https://github.com/cogitare-ai/cogitare): A Modern, Fast, and Modular Deep Learning and Machine Learning framework for Python.\n* [Parris](https://github.com/jgreenemi/Parris) - Parris, the automated infrastructure setup tool for machine learning algorithms.\n* [neonrvm](https://github.com/siavashserver/neonrvm) - neonrvm is an open source machine learning library based on RVM technique. It's written in C programming language and comes with Python programming language bindings.\n* [Turi Create](https://github.com/apple/turicreate) - Machine learning from Apple. Turi Create simplifies the development of custom machine learning models. You don't have to be a machine learning expert to add recommendations, object detection, image classification, image similarity or activity classification to your app.\n* [xLearn](https://github.com/aksnzhy/xlearn) - A high performance, easy-to-use, and scalable machine learning package, which can be used to solve large-scale machine learning problems. xLearn is especially useful for solving machine learning problems on large-scale sparse data, which is very common in Internet services such as online advertisement and recommender systems.\n* [mlens](https://github.com/flennerhag/mlens) - A high performance, memory efficient, maximally parallelized ensemble learning, integrated with scikit-learn.\n* [Thampi](https://github.com/scoremedia/thampi) - Machine Learning Prediction System on AWS Lambda\n* [MindsDB](https://github.com/mindsdb/mindsdb) - Open Source framework to streamline use of neural networks.\n* [Microsoft Recommenders](https://github.com/Microsoft/Recommenders): Examples and best practices for building recommendation systems, provided as Jupyter notebooks. The repo contains some of the latest state of the art algorithms from Microsoft Research as well as from other companies and institutions.\n* [StellarGraph](https://github.com/stellargraph/stellargraph): Machine Learning on Graphs, a Python library for machine learning on graph-structured (network-structured) data.\n* [BentoML](https://github.com/bentoml/bentoml): Toolkit for package and deploy machine learning models for serving in production\n* [MiraiML](https://github.com/arthurpaulino/miraiml): An asynchronous engine for continuous & autonomous machine learning, built for real-time usage.\n* [numpy-ML](https://github.com/ddbourgin/numpy-ml): Reference implementations of ML models written in numpy\n* [Neuraxle](https://github.com/Neuraxio/Neuraxle): A framework providing the right abstractions to ease research, development, and deployment of your ML pipelines.\n* [Cornac](https://github.com/PreferredAI/cornac) - A comparative framework for multimodal recommender systems with a focus on models leveraging auxiliary data.\n* [JAX](https://github.com/google/jax) - JAX is Autograd and XLA, brought together for high-performance machine learning research.\n* [Catalyst](https://github.com/catalyst-team/catalyst) - High-level utils for PyTorch DL & RL research. It was developed with a focus on reproducibility, fast experimentation and code/ideas reusing. Being able to research/develop something new, rather than write another regular train loop.\n* [Fastai](https://github.com/fastai/fastai) - High-level wrapper built on the top of Pytorch which supports vision, text, tabular data and collaborative filtering.\n* [scikit-multiflow](https://github.com/scikit-multiflow/scikit-multiflow) - A machine learning framework for multi-output/multi-label and stream data.\n* [Lightwood](https://github.com/mindsdb/lightwood) - A Pytorch based framework that breaks down machine learning problems into smaller blocks that can be glued together seamlessly with objective to build predictive models with one line of code.\n* [bayeso](https://github.com/jungtaekkim/bayeso) - A simple, but essential Bayesian optimization package, written in Python.\n* [mljar-supervised](https://github.com/mljar/mljar-supervised) - An Automated Machine Learning (AutoML) python package for tabular data. It can handle: Binary Classification, MultiClass Classification and Regression. It provides explanations and markdown reports.\n* [evostra](https://github.com/alirezamika/evostra) - A fast Evolution Strategy implementation in Python.\n* [Determined](https://github.com/determined-ai/determined) - Scalable deep learning training platform, including integrated support for distributed training, hyperparameter tuning, experiment tracking, and model management.\n* [PySyft](https://github.com/OpenMined/PySyft) - A Python library for secure and private Deep Learning built on PyTorch and TensorFlow.\n* [PyGrid](https://github.com/OpenMined/PyGrid/) - Peer-to-peer network of data owners and data scientists who can collectively train AI models using PySyft\n* [sktime](https://github.com/alan-turing-institute/sktime) - A unified framework for machine learning with time series\n* [OPFython](https://github.com/gugarosa/opfython) - A Python-inspired implementation of the Optimum-Path Forest classifier.\n* [Opytimizer](https://github.com/gugarosa/opytimizer) - Python-based meta-heuristic optimization techniques.\n* [Gradio](https://github.com/gradio-app/gradio) - A Python library for quickly creating and sharing demos of models. Debug models interactively in your browser, get feedback from collaborators, and generate public links without deploying anything.\n* [Hub](https://github.com/activeloopai/Hub) - Fastest unstructured dataset management for TensorFlow/PyTorch. Stream & version-control data. Store even petabyte-scale data in a single numpy-like array on the cloud accessible on any machine. Visit [activeloop.ai](https://activeloop.ai) for more info.\n* [Synthia](https://github.com/dmey/synthia) - Multidimensional synthetic data generation in Python.\n* [ByteHub](https://github.com/bytehub-ai/bytehub) - An easy-to-use, Python-based feature store. Optimized for time-series data.\n* [Backprop](https://github.com/backprop-ai/backprop) - Backprop makes it simple to use, finetune, and deploy state-of-the-art ML models.\n* [River](https://github.com/online-ml/river): A framework for general purpose online machine learning.\n* [FEDOT](https://github.com/nccr-itmo/FEDOT): An AutoML framework for the automated design of composite modelling pipelines. It can handle classification, regression, and time series forecasting tasks on different types of data (including multi-modal datasets).\n* [Sklearn-genetic-opt](https://github.com/rodrigo-arenas/Sklearn-genetic-opt): An AutoML package for hyperparameters tuning using evolutionary algorithms, with built-in callbacks, plotting, remote logging and more.\n* [Evidently](https://github.com/evidentlyai/evidently): Interactive reports to analyze machine learning models during validation or production monitoring.\n* [Streamlit](https://github.com/streamlit/streamlit): Streamlit is an framework to create beautiful data apps in hours, not weeks.\n* [Optuna](https://github.com/optuna/optuna): Optuna is an automatic hyperparameter optimization software framework, particularly designed for machine learning.\n* [Deepchecks](https://github.com/deepchecks/deepchecks): Validation & testing of machine learning models and data during model development, deployment, and production. This includes checks and suites related to various types of issues, such as model performance, data integrity, distribution mismatches, and more.\n* [Shapash](https://github.com/MAIF/shapash) : Shapash is a Python library that provides several types of visualization that display explicit labels that everyone can understand.\n* [Eurybia](https://github.com/MAIF/eurybia): Eurybia monitors data and model drift over time and securizes model deployment with data validation.\n* [Colossal-AI](https://github.com/hpcaitech/ColossalAI): An open-source deep learning system for large-scale model training and inference with high efficiency and low cost.\n* [skrub](https://github.com/skrub-data/skrub) - Skrub is a Python library that eases preprocessing and feature engineering for machine learning on dataframes.\n* [Upgini](https://github.com/upgini/upgini): Free automated data & feature enrichment library for machine learning - automatically searches through thousands of ready-to-use features from public and community shared data sources and enriches your training dataset with only the accuracy improving features.\n* [AutoML-Implementation-for-Static-and-Dynamic-Data-Analytics](https://github.com/Western-OC2-Lab/AutoML-Implementation-for-Static-and-Dynamic-Data-Analytics): A tutorial to help machine learning researchers to automatically obtain optimized machine learning models with the optimal learning performance on any specific task.\n* [SKBEL](https://github.com/robinthibaut/skbel): A Python library for Bayesian Evidential Learning (BEL) in order to estimate the uncertainty of a prediction.\n* [NannyML](https://bit.ly/nannyml-github-machinelearning): Python library capable of fully capturing the impact of data drift on performance. Allows estimation of post-deployment model performance without access to targets.\n* [cleanlab](https://github.com/cleanlab/cleanlab): The standard data-centric AI package for data quality and machine learning with messy, real-world data and labels.\n* [AutoGluon](https://github.com/awslabs/autogluon): AutoML for Image, Text, Tabular, Time-Series, and MultiModal Data.\n* [PyBroker](https://github.com/edtechre/pybroker) - Algorithmic Trading with Machine Learning.\n* [Frouros](https://github.com/IFCA/frouros): Frouros is an open source Python library for drift detection in machine learning systems.\n* [CometML](https://github.com/comet-ml/comet-examples): The best-in-class MLOps platform with experiment tracking, model production monitoring, a model registry, and data lineage from training straight through to production.\n* [Okrolearn](https://github.com/Okerew/okrolearn): A python machine learning library created to combine powefull data analasys features with tensors and machine learning components, while maintaining support for other libraries.\n* [Opik](https://github.com/comet-ml/opik): Evaluate, trace, test, and ship LLM applications across your dev and production lifecycles.\n* [pyclugen](https://github.com/clugen/pyclugen) - Multidimensional cluster generation in Python.\n\n<a name=\"python-data-analysis--data-visualization\"></a>\n#### Data Analysis / Data Visualization\n* [DataComPy](https://github.com/capitalone/datacompy) - A library to compare Pandas, Polars, and Spark data frames. It provides stats and lets users adjust for match accuracy.\n* [DataVisualization](https://github.com/Shanky-21/Data_visualization) - A GitHub Repository Where you can Learn Datavisualizatoin Basics to Intermediate level.\n* [Cartopy](https://scitools.org.uk/cartopy/docs/latest/) - Cartopy is a Python package designed for geospatial data processing in order to produce maps and other geospatial data analyses.\n* [SciPy](https://www.scipy.org/) - A Python-based ecosystem of open-source software for mathematics, science, and engineering.\n* [NumPy](https://www.numpy.org/) - A fundamental package for scientific computing with Python.\n* [AutoViz](https://github.com/AutoViML/AutoViz) AutoViz performs automatic visualization of any dataset with a single line of Python code. Give it any input file (CSV, txt or JSON) of any size and AutoViz will visualize it. See <a href=\"https://towardsdatascience.com/autoviz-a-new-tool-for-automated-visualization-ec9c1744a6ad?source=friends_link&sk=c9e9503ec424b191c6096d7e3f515d10\">Medium article</a>.\n* [Numba](https://numba.pydata.org/) - Python JIT (just in time) compiler to LLVM aimed at scientific Python by the developers of Cython and NumPy.\n* [Mars](https://github.com/mars-project/mars) - A tensor-based framework for large-scale data computation which is often regarded as a parallel and distributed version of NumPy.\n* [NetworkX](https://networkx.github.io/) - A high-productivity software for complex networks.\n* [igraph](https://igraph.org/python/) - binding to igraph library - General purpose graph library.\n* [Pandas](https://pandas.pydata.org/) - A library providing high-performance, easy-to-use data structures and data analysis tools.\n* [ParaMonte](https://github.com/cdslaborg/paramonte) - A general-purpose Python library for Bayesian data analysis and visualization via serial/parallel Monte Carlo and MCMC simulations. Documentation can be found [here](https://www.cdslab.org/paramonte/).\n* [Vaex](https://github.com/vaexio/vaex) - A high performance Python library for lazy Out-of-Core DataFrames (similar to Pandas), to visualize and explore big tabular datasets. Documentation can be found [here](https://vaex.io/docs/index.html).\n* [Open Mining](https://github.com/mining/mining) - Business Intelligence (BI) in Python (Pandas web interface) **[Deprecated]**\n* [PyMC](https://github.com/pymc-devs/pymc) - Markov Chain Monte Carlo sampling toolkit.\n* [zipline](https://github.com/quantopian/zipline) - A Pythonic algorithmic trading library.\n* [PyDy](https://www.pydy.org/) - Short for Python Dynamics, used to assist with workflow in the modelling of dynamic motion based around NumPy, SciPy, IPython, and matplotlib.\n* [SymPy](https://github.com/sympy/sympy) - A Python library for symbolic mathematics.\n* [statsmodels](https://github.com/statsmodels/statsmodels) - Statistical modelling and econometrics in Python.\n* [astropy](https://www.astropy.org/) - A community Python library for Astronomy.\n* [matplotlib](https://matplotlib.org/) - A Python 2D plotting library.\n* [bokeh](https://github.com/bokeh/bokeh) - Interactive Web Plotting for Python.\n* [plotly](https://plot.ly/python/) - Collaborative web plotting for Python and matplotlib.\n* [altair](https://github.com/altair-viz/altair) - A Python to Vega translator.\n* [d3py](https://github.com/mikedewar/d3py) - A plotting library for Python, based on [D3.js](https://d3js.org/).\n* [PyDexter](https://github.com/D3xterjs/pydexter) - Simple plotting for Python. Wrapper for D3xterjs; easily render charts in-browser.\n* [ggplot](https://github.com/yhat/ggpy) - Same API as ggplot2 for R. **[Deprecated]**\n* [ggfortify](https://github.com/sinhrks/ggfortify) - Unified interface to ggplot2 popular R packages.\n* [Kartograph.py](https://github.com/kartograph/kartograph.py) - Rendering beautiful SVG maps in Python.\n* [pygal](http://pygal.org/en/stable/) - A Python SVG Charts Creator.\n* [PyQtGraph](https://github.com/pyqtgraph/pyqtgraph) - A pure-python graphics and GUI library built on PyQt4 / PySide and NumPy.\n* [pycascading](https://github.com/twitter/pycascading) **[Deprecated]**\n* [Petrel](https://github.com/AirSage/Petrel) - Tools for writing, submitting, debugging, and monitoring Storm topologies in pure Python.\n* [Blaze](https://github.com/blaze/blaze) - NumPy and Pandas interface to Big Data.\n* [emcee](https://github.com/dfm/emcee) - The Python ensemble sampling toolkit for affine-invariant MCMC.\n* [windML](https://github.com/cigroup-ol/windml) - A Python Framework for Wind Energy Analysis and Prediction.\n* [vispy](https://github.com/vispy/vispy) - GPU-based high-performance interactive OpenGL 2D/3D data visualization library.\n* [cerebro2](https://github.com/numenta/nupic.cerebro2) A web-based visualization and debugging platform for NuPIC. **[Deprecated]**\n* [NuPIC Studio](https://github.com/htm-community/nupic.studio) An all-in-one NuPIC Hierarchical Temporal Memory visualization and debugging super-tool! **[Deprecated]**\n* [SparklingPandas](https://github.com/sparklingpandas/sparklingpandas) Pandas on PySpark (POPS).\n* [Seaborn](https://seaborn.pydata.org/) - A python visualization library based on matplotlib.\n* [ipychart](https://github.com/nicohlr/ipychart) - The power of Chart.js in Jupyter Notebook.\n* [bqplot](https://github.com/bloomberg/bqplot) - An API for plotting in Jupyter (IPython).\n* [pastalog](https://github.com/rewonc/pastalog) - Simple, realtime visualization of neural network training performance.\n* [Superset](https://github.com/apache/incubator-superset) - A data exploration platform designed to be visual, intuitive, and interactive.\n* [Dora](https://github.com/nathanepstein/dora) - Tools for exploratory data analysis in Python.\n* [Ruffus](http://www.ruffus.org.uk) - Computation Pipeline library for python.\n* [SOMPY](https://github.com/sevamoo/SOMPY) - Self Organizing Map written in Python (Uses neural networks for data analysis).\n* [somoclu](https://github.com/peterwittek/somoclu) Massively parallel self-organizing maps: accelerate training on multicore CPUs, GPUs, and clusters, has python API.\n* [HDBScan](https://github.com/lmcinnes/hdbscan) - implementation of the hdbscan algorithm in Python - used for clustering\n* [visualize_ML](https://github.com/ayush1997/visualize_ML) - A python package for data exploration and data analysis. **[Deprecated]**\n* [scikit-plot](https://github.com/reiinakano/scikit-plot) - A visualization library for quick and easy generation of common plots in data analysis and machine learning.\n* [Bowtie](https://github.com/jwkvam/bowtie) - A dashboard library for interactive visualizations using flask socketio and react.\n* [lime](https://github.com/marcotcr/lime) - Lime is about explaining what machine learning classifiers (or models) are doing. It is able to explain any black box classifier, with two or more classes.\n* [PyCM](https://github.com/sepandhaghighi/pycm) - PyCM is a multi-class confusion matrix library written in Python that supports both input data vectors and direct matrix, and a proper tool for post-classification model evaluation that supports most classes and overall statistics parameters\n* [Dash](https://github.com/plotly/dash) - A framework for creating analytical web applications built on top of Plotly.js, React, and Flask\n* [Lambdo](https://github.com/asavinov/lambdo) - A workflow engine for solving machine learning problems by combining in one analysis pipeline (i) feature engineering and machine learning (ii) model training and prediction (iii) table population and column evaluation via user-defined (Python) functions.\n* [TensorWatch](https://github.com/microsoft/tensorwatch) - Debugging and visualization tool for machine learning and data science. It extensively leverages Jupyter Notebook to show real-time visualizations of data in running processes such as machine learning training.\n* [dowel](https://github.com/rlworkgroup/dowel) - A little logger for machine learning research. Output any object to the terminal, CSV, TensorBoard, text logs on disk, and more with just one call to `logger.log()`.\n* [Flama](https://github.com/vortico/flama) - Ignite your models into blazing-fast machine learning APIs with a modern framework.\n\n<a name=\"python-misc-scripts--ipython-notebooks--codebases\"></a>\n#### Misc Scripts / iPython Notebooks / Codebases\n* [MiniGrad](https://github.com/kennysong/minigrad) \u2013 A minimal, educational, Pythonic implementation of autograd (~100 loc).\n* [Map/Reduce implementations of common ML algorithms](https://github.com/Yannael/BigDataAnalytics_INFOH515): Jupyter notebooks that cover how to implement from scratch different ML algorithms (ordinary least squares, gradient descent, k-means, alternating least squares), using Python NumPy, and how to then make these implementations scalable using Map/Reduce and Spark.\n* [BioPy](https://github.com/jaredthecoder/BioPy) - Biologically-Inspired and Machine Learning Algorithms in Python. **[Deprecated]**\n* [CAEs for Data Assimilation](https://github.com/julianmack/Data_Assimilation) - Convolutional autoencoders for 3D image/field compression applied to reduced order [Data Assimilation](https://en.wikipedia.org/wiki/Data_assimilation).\n* [handsonml](https://github.com/ageron/handson-ml) - Fundamentals of machine learning in python.\n* [SVM Explorer](https://github.com/plotly/dash-svm) - Interactive SVM Explorer, using Dash and scikit-learn\n* [pattern_classification](https://github.com/rasbt/pattern_classification)\n* [thinking stats 2](https://github.com/Wavelets/ThinkStats2)\n* [hyperopt](https://github.com/hyperopt/hyperopt-sklearn)\n* [numpic](https://github.com/numenta/nupic)\n* [2012-paper-diginorm](https://github.com/dib-lab/2012-paper-diginorm)\n* [A gallery of interesting IPython notebooks](https://github.com/jupyter/jupyter/wiki/A-gallery-of-interesting-Jupyter-Notebooks)\n* [ipython-notebooks](https://github.com/ogrisel/notebooks)\n* [data-science-ipython-notebooks](https://github.com/donnemartin/data-science-ipython-notebooks) - Continually updated Data Science Python Notebooks: Spark, Hadoop MapReduce, HDFS, AWS, Kaggle, scikit-learn, matplotlib, pandas, NumPy, SciPy, and various command lines.\n* [decision-weights](https://github.com/CamDavidsonPilon/decision-weights)\n* [Sarah Palin LDA](https://github.com/Wavelets/sarah-palin-lda) - Topic Modelling the Sarah Palin emails.\n* [Diffusion Segmentation](https://github.com/Wavelets/diffusion-segmentation) - A collection of image segmentation algorithms based on diffusion methods.\n* [Scipy Tutorials](https://github.com/Wavelets/scipy-tutorials) - SciPy tutorials. This is outdated, check out scipy-lecture-notes.\n* [Crab](https://github.com/marcelcaraciolo/crab) - A recommendation engine library for Python.\n* [BayesPy](https://github.com/maxsklar/BayesPy) - Bayesian Inference Tools in Python.\n* [scikit-learn tutorials](https://github.com/GaelVaroquaux/scikit-learn-tutorial) - Series of notebooks for learning scikit-learn.\n* [sentiment-analyzer](https://github.com/madhusudancs/sentiment-analyzer) - Tweets Sentiment Analyzer\n* [sentiment_classifier](https://github.com/kevincobain2000/sentiment_classifier) - Sentiment classifier using word sense disambiguation.\n* [group-lasso](https://github.com/fabianp/group_lasso) - Some experiments with the coordinate descent algorithm used in the (Sparse) Group Lasso model.\n* [jProcessing](https://github.com/kevincobain2000/jProcessing) - Kanji / Hiragana / Katakana to Romaji Converter. Edict Dictionary & parallel sentences Search. Sentence Similarity between two JP Sentences. Sentiment Analysis of Japanese Text. Run Cabocha(ISO--8859-1 configured) in Python.\n* [mne-python-notebooks](https://github.com/mne-tools/mne-python-notebooks) - IPython notebooks for EEG/MEG data processing using mne-python.\n* [Neon Course](https://github.com/NervanaSystems/neon_course) - IPython notebooks for a complete course around understanding Nervana's Neon.\n* [pandas cookbook](https://github.com/jvns/pandas-cookbook) - Recipes for using Python's pandas library.\n* [climin](https://github.com/BRML/climin) - Optimization library focused on machine learning, pythonic implementations of gradient descent, LBFGS, rmsprop, adadelta and others.\n* [Allen Downey\u2019s Data Science Course](https://github.com/AllenDowney/DataScience) - Code for Data Science at Olin College, Spring 2014.\n* [Allen Downey\u2019s Think Bayes Code](https://github.com/AllenDowney/ThinkBayes) - Code repository for Think Bayes.\n* [Allen Downey\u2019s Think Complexity Code](https://github.com/AllenDowney/ThinkComplexity) - Code for Allen Downey's book Think Complexity.\n* [Allen Downey\u2019s Think OS Code](https://github.com/AllenDowney/ThinkOS) - Text and supporting code for Think OS: A Brief Introduction to Operating Systems.\n* [Python Programming for the Humanities](https://www.karsdorp.io/python-course/) - Course for Python programming for the Humanities, assuming no prior knowledge. Heavy focus on text processing / NLP.\n* [GreatCircle](https://github.com/mwgg/GreatCircle) - Library for calculating great circle distance.\n* [Optunity examples](http://optunity.readthedocs.io/en/latest/notebooks/index.html) - Examples demonstrating how to use Optunity in synergy with machine learning libraries.\n* [Dive into Machine Learning  with Python Jupyter notebook and scikit-learn](https://github.com/hangtwenty/dive-into-machine-learning) - \"I learned Python by hacking first, and getting serious *later.* I wanted to do this with Machine Learning. If this is your style, join me in getting a bit ahead of yourself.\"\n* [TDB](https://github.com/ericjang/tdb) - TensorDebugger (TDB) is a visual debugger for deep learning. It features interactive, node-by-node debugging and visualization for TensorFlow.\n* [Suiron](https://github.com/kendricktan/suiron/) - Machine Learning for RC Cars.\n* [Introduction to machine learning with scikit-learn](https://github.com/justmarkham/scikit-learn-videos) - IPython notebooks from Data School's video tutorials on scikit-learn.\n* [Practical XGBoost in Python](https://parrotprediction.teachable.com/p/practical-xgboost-in-python) - comprehensive online course about using XGBoost in Python.\n* [Introduction to Machine Learning with Python](https://github.com/amueller/introduction_to_ml_with_python) - Notebooks and code for the book \"Introduction to Machine Learning with Python\"\n* [Pydata book](https://github.com/wesm/pydata-book) - Materials and IPython notebooks for \"Python for Data Analysis\" by Wes McKinney, published by O'Reilly Media\n* [Homemade Machine Learning](https://github.com/trekhleb/homemade-machine-learning) - Python examples of popular machine learning algorithms with interactive Jupyter demos and math being explained\n* [Prodmodel](https://github.com/prodmodel/prodmodel) - Build tool for data science pipelines.\n* [the-elements-of-statistical-learning](https://github.com/maitbayev/the-elements-of-statistical-learning) - This repository contains Jupyter notebooks implementing the algorithms found in the book and summary of the textbook.\n* [Hyperparameter-Optimization-of-Machine-Learning-Algorithms](https://github.com/LiYangHart/Hyperparameter-Optimization-of-Machine-Learning-Algorithms) - Code for hyperparameter tuning/optimization of machine learning and deep learning algorithms.\n* [Heart_Disease-Prediction](https://github.com/ShivamChoudhary17/Heart_Disease) - Given clinical parameters about a patient, can we predict whether or not they have heart disease?\n* [Flight Fare Prediction](https://github.com/ShivamChoudhary17/Flight_Fare_Prediction) - This basically to gauge the understanding of Machine Learning Workflow and Regression technique in specific.\n* [Keras Tuner](https://github.com/keras-team/keras-tuner) - An easy-to-use, scalable hyperparameter optimization framework that solves the pain points of hyperparameter search.\n\n\n\n<a name=\"python-neural-networks\"></a>\n#### Neural Networks\n\n* [Kinho](https://github.com/kinhosz/Neural) - Simple API for Neural Network. Better for image processing with CPU/GPU + Transfer Learning.\n* [nn_builder](https://github.com/p-christ/nn_builder) - nn_builder is a python package that lets you build neural networks in 1 line\n* [NeuralTalk](https://github.com/karpathy/neuraltalk) - NeuralTalk is a Python+numpy project for learning Multimodal Recurrent Neural Networks that describe images with sentences.\n* [NeuralTalk](https://github.com/karpathy/neuraltalk2) - NeuralTalk is a Python+numpy project for learning Multimodal Recurrent Neural Networks that describe images with sentences. **[Deprecated]**\n* [Neuron](https://github.com/molcik/python-neuron) - Neuron is simple class for time series predictions. It's utilize LNU (Linear Neural Unit), QNU (Quadratic Neural Unit), RBF (Radial Basis Function), MLP (Multi Layer Perceptron), MLP-ELM (Multi Layer Perceptron - Extreme Learning Machine) neural networks learned with Gradient descent or LeLevenberg\u2013Marquardt algorithm. **[Deprecated]**\n* [Data Driven Code](https://github.com/atmb4u/data-driven-code) - Very simple implementation of neural networks for dummies in python without using any libraries, with detailed comments.\n* [Machine Learning, Data Science and Deep Learning with Python](https://www.manning.com/livevideo/machine-learning-data-science-and-deep-learning-with-python) - LiveVideo course that covers machine learning, Tensorflow, artificial intelligence, and neural networks.\n* [TResNet: High Performance GPU-Dedicated Architecture](https://github.com/mrT23/TResNet) - TResNet models were designed and optimized to give the best speed-accuracy tradeoff out there on GPUs.\n* [TResNet: Simple and powerful neural network library for python](https://github.com/zueve/neurolab) - Variety of supported types of Artificial Neural Network and learning algorithms.\n* [Jina AI](https://jina.ai/) An easier way to build neural search in the cloud. Compatible with Jupyter Notebooks.\n* [sequitur](https://github.com/shobrook/sequitur) PyTorch library for creating and training sequence autoencoders in just two lines of code\n\n\n<a name=\"python-spiking-neural-networks\"></a>\n#### Spiking Neural Networks\n\n* [Rockpool](https://github.com/synsense/rockpool) - A machine learning library for spiking neural networks. Supports training with both torch and jax pipelines, and deployment to neuromorphic hardware.\n* [Sinabs](https://github.com/synsense/sinabs) - A deep learning library for spiking neural networks which is based on PyTorch, focuses on fast training and supports inference on neuromorphic hardware.\n* [Tonic](https://github.com/neuromorphs/tonic) - A library that makes downloading publicly available neuromorphic datasets a breeze and provides event-based data transformation/augmentation pipelines.\n\n<a name=\"python-survival-analysis\"></a>\n#### Python Survival Analysis\n* [lifelines](https://github.com/CamDavidsonPilon/lifelines) - lifelines is a complete survival analysis library, written in pure Python\n* [Scikit-Survival](https://github.com/sebp/scikit-survival) - scikit-survival is a Python module for survival analysis built on top of scikit-learn. It allows doing survival analysis while utilizing the power of scikit-learn, e.g., for pre-processing or doing cross-validation.\n\n<a name=\"python-federated-learning\"></a>\n#### Federated Learning\n* [Flower](https://flower.dev/) - A unified approach to federated learning, analytics, and evaluation. Federate any workload, any ML framework, and any programming language.\n* [PySyft](https://github.com/OpenMined/PySyft) - A Python library for secure and private Deep Learning.\n* [Tensorflow-Federated](https://www.tensorflow.org/federated) A federated learning framework for machine learning and other computations on decentralized data.\n\n<a name=\"python-kaggle-competition-source-code\"></a>\n#### Kaggle Competition Source Code\n* [open-solution-home-credit](https://github.com/neptune-ml/open-solution-home-credit) -> source code and [experiments results](https://app.neptune.ml/neptune-ml/Home-Credit-Default-Risk) for [Home Credit Default Risk](https://www.kaggle.com/c/home-credit-default-risk).\n* [open-solution-googleai-object-detection](https://github.com/neptune-ml/open-solution-googleai-object-detection) -> source code and [experiments results](https://app.neptune.ml/neptune-ml/Google-AI-Object-Detection-Challenge) for [Google AI Open Images - Object Detection Track](https://www.kaggle.com/c/google-ai-open-images-object-detection-track).\n* [open-solution-salt-identification](https://github.com/neptune-ml/open-solution-salt-identification) -> source code and [experiments results](https://app.neptune.ml/neptune-ml/Salt-Detection) for [TGS Salt Identification Challenge](https://www.kaggle.com/c/tgs-salt-identification-challenge).\n* [open-solution-ship-detection](https://github.com/neptune-ml/open-solution-ship-detection) -> source code and [experiments results](https://app.neptune.ml/neptune-ml/Ships) for [Airbus Ship Detection Challenge](https://www.kaggle.com/c/airbus-ship-detection).\n* [open-solution-data-science-bowl-2018](https://github.com/neptune-ml/open-solution-data-science-bowl-2018) -> source code and [experiments results](https://app.neptune.ml/neptune-ml/Data-Science-Bowl-2018) for [2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018).\n* [open-solution-value-prediction](https://github.com/neptune-ml/open-solution-value-prediction) -> source code and [experiments results](https://app.neptune.ml/neptune-ml/Santander-Value-Prediction-Challenge) for [Santander Value Prediction Challenge](https://www.kaggle.com/c/santander-value-prediction-challenge).\n* [open-solution-toxic-comments](https://github.com/neptune-ml/open-solution-toxic-comments) -> source code for [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge).\n* [wiki challenge](https://github.com/hammer/wikichallenge) - An implementation of Dell Zhang's solution to Wikipedia's Participation Challenge on Kaggle.\n* [kaggle insults](https://github.com/amueller/kaggle_insults) - Kaggle Submission for \"Detecting Insults in Social Commentary\".\n* [kaggle_acquire-valued-shoppers-challenge](https://github.com/MLWave/kaggle_acquire-valued-shoppers-challenge) - Code for the Kaggle acquire valued shoppers challenge.\n* [kaggle-cifar](https://github.com/zygmuntz/kaggle-cifar) - Code for the CIFAR-10 competition at Kaggle, uses cuda-convnet.\n* [kaggle-blackbox](https://github.com/zygmuntz/kaggle-blackbox) - Deep learning made easy.\n* [kaggle-accelerometer](https://github.com/zygmuntz/kaggle-accelerometer) - Code for Accelerometer Biometric Competition at Kaggle.\n* [kaggle-advertised-salaries](https://github.com/zygmuntz/kaggle-advertised-salaries) - Predicting job salaries from ads - a Kaggle competition.\n* [kaggle amazon](https://github.com/zygmuntz/kaggle-amazon) - Amazon access control challenge.\n* [kaggle-bestbuy_big](https://github.com/zygmuntz/kaggle-bestbuy_big) - Code for the Best Buy competition at Kaggle.\n* [kaggle-bestbuy_small](https://github.com/zygmuntz/kaggle-bestbuy_small)\n* [Kaggle Dogs vs. Cats](https://github.com/kastnerkyle/kaggle-dogs-vs-cats) - Code for Kaggle Dogs vs. Cats competition.\n* [Kaggle Galaxy Challenge](https://github.com/benanne/kaggle-galaxies) - Winning solution for the Galaxy Challenge on Kaggle.\n* [Kaggle Gender](https://github.com/zygmuntz/kaggle-gender) - A Kaggle competition: discriminate gender based on handwriting.\n* [Kaggle Merck](https://github.com/zygmuntz/kaggle-merck) - Merck challenge at Kaggle.\n* [Kaggle Stackoverflow](https://github.com/zygmuntz/kaggle-stackoverflow) - Predicting closed questions on Stack Overflow.\n* [kaggle_acquire-valued-shoppers-challenge](https://github.com/MLWave/kaggle_acquire-valued-shoppers-challenge) - Code for the Kaggle acquire valued shoppers challenge.\n* [wine-quality](https://github.com/zygmuntz/wine-quality) - Predicting wine quality.\n\n<a name=\"python-reinforcement-learning\"></a>\n#### Reinforcement Learning\n* [DeepMind Lab](https://github.com/deepmind/lab) - DeepMind Lab is a 3D learning environment based on id Software's Quake III Arena via ioquake3 and other open source software. Its primary purpose is to act as a testbed for research in artificial intelligence, especially deep reinforcement learning.\n* [Gymnasium](https://github.com/Farama-Foundation/Gymnasium) - A library for developing and comparing reinforcement learning algorithms (successor of [gym])(https://github.com/openai/gym).\n* [Serpent.AI](https://github.com/SerpentAI/SerpentAI) - Serpent.AI is a game agent framework that allows you to turn any video game you own into a sandbox to develop AI and machine learning experiments. For both researchers and hobbyists.\n* [ViZDoom](https://github.com/mwydmuch/ViZDoom) - ViZDoom allows developing AI bots that play Doom using only the visual information (the screen buffer). It is primarily intended for research in machine visual learning, and deep reinforcement learning, in particular.\n* [Roboschool](https://github.com/openai/roboschool) - Open-source software for robot simulation, integrated with OpenAI Gym.\n* [Retro](https://github.com/openai/retro) - Retro Games in Gym\n* [SLM Lab](https://github.com/kengz/SLM-Lab) - Modular Deep Reinforcement Learning framework in PyTorch.\n* [Coach](https://github.com/NervanaSystems/coach) - Reinforcement Learning Coach by Intel\u00ae AI Lab enables easy experimentation with state of the art Reinforcement Learning algorithms\n* [garage](https://github.com/rlworkgroup/garage) - A toolkit for reproducible reinforcement learning research\n* [metaworld](https://github.com/rlworkgroup/metaworld) - An open source robotics benchmark for meta- and multi-task reinforcement learning\n* [acme](https://deepmind.com/research/publications/Acme) - An Open Source Distributed Framework for Reinforcement Learning that makes build and train your agents easily.\n* [Spinning Up](https://spinningup.openai.com) - An educational resource designed to let anyone learn to become a skilled practitioner in deep reinforcement learning\n* [Maze](https://github.com/enlite-ai/maze) - Application-oriented deep reinforcement learning framework addressing real-world decision problems.\n* [RLlib](https://github.com/ray-project/ray) - RLlib is an industry level, highly scalable RL library for tf and torch, based on Ray. It's used by companies like Amazon and Microsoft to solve real-world decision making problems at scale.\n* [DI-engine](https://github.com/opendilab/DI-engine) - DI-engine is a generalized Decision Intelligence engine. It supports most basic deep reinforcement learning (DRL) algorithms, such as DQN, PPO, SAC, and domain-specific algorithms like QMIX in multi-agent RL, GAIL in inverse RL, and RND in exploration problems.\n* [Gym4ReaL](https://github.com/Daveonwave/gym4ReaL) - Gym4ReaL is a comprehensive suite of realistic environments designed to support the development and evaluation of RL algorithms that can operate in real-world scenarios. The suite includes a diverse set of tasks exposing RL algorithms to a variety of practical challenges.\n\n<a name=\"python-speech-recognition\"></a>\n#### Speech Recognition\n* [EspNet](https://github.com/espnet/espnet) - ESPnet is an end-to-end speech processing toolkit for tasks like speech recognition, translation, and enhancement, using PyTorch and Kaldi-style data processing.\n\n<a name=\"ruby\"></a>\n## Ruby\n\n<a name=\"ruby-natural-language-processing\"></a>\n#### Natural Language Processing\n\n* [Awesome NLP with Ruby](https://github.com/arbox/nlp-with-ruby) - Curated link list for practical natural language processing in Ruby.\n* [Treat](https://github.com/louismullie/treat) - Text Retrieval and Annotation Toolkit, definitely the most comprehensive toolkit I\u2019ve encountered so far for Ruby.\n* [Stemmer](https://github.com/aurelian/ruby-stemmer) - Expose libstemmer_c to Ruby. **[Deprecated]**\n* [Raspell](https://sourceforge.net/projects/raspell/) - raspell is an interface binding for ruby. **[Deprecated]**\n* [UEA Stemmer](https://github.com/ealdent/uea-stemmer) - Ruby port of UEALite Stemmer - a conservative stemmer for search and indexing.\n* [Twitter-text-rb](https://github.com/twitter/twitter-text/tree/master/rb) - A library that does auto linking and extraction of usernames, lists and hashtags in tweets.\n\n<a name=\"ruby-general-purpose-machine-learning\"></a>\n#### General-Purpose Machine Learning\n\n* [Awesome Machine Learning with Ruby](https://github.com/arbox/machine-learning-with-ruby) - Curated list of ML related resources for Ruby.\n* [Ruby Machine Learning](https://github.com/tsycho/ruby-machine-learning) - Some Machine Learning algorithms, implemented in Ruby. **[Deprecated]**\n* [Machine Learning Ruby](https://github.com/mizoR/machine-learning-ruby) **[Deprecated]**\n* [jRuby Mahout](https://github.com/vasinov/jruby_mahout) - JRuby Mahout is a gem that unleashes the power of Apache Mahout in the world of JRuby. **[Deprecated]**\n* [CardMagic-Classifier](https://github.com/cardmagic/classifier) - A general classifier module to allow Bayesian and other types of classifications.\n* [rb-libsvm](https://github.com/febeling/rb-libsvm) - Ruby language bindings for LIBSVM which is a Library for Support Vector Machines.\n* [Scoruby](https://github.com/asafschers/scoruby) - Creates Random Forest classifiers from PMML files.\n* [rumale](https://github.com/yoshoku/rumale) - Rumale is a machine learning library in Ruby\n\n<a name=\"ruby-data-analysis--data-visualization\"></a>\n#### Data Analysis / Data Visualization\n\n* [rsruby](https://github.com/alexgutteridge/rsruby) - Ruby - R bridge.\n* [data-visualization-ruby](https://github.com/chrislo/data_visualisation_ruby) - Source code and supporting content for my Ruby Manor presentation on Data Visualisation with Ruby. **[Deprecated]**\n* [ruby-plot](https://www.ruby-toolbox.com/projects/ruby-plot) - gnuplot wrapper for Ruby, especially for plotting ROC curves into SVG files. **[Deprecated]**\n* [plot-rb](https://github.com/zuhao/plotrb) - A plotting library in Ruby built on top of Vega and D3. **[Deprecated]**\n* [scruffy](https://github.com/delano/scruffy) - A beautiful graphing toolkit for Ruby.\n* [SciRuby](http://sciruby.com/)\n* [Glean](https://github.com/glean/glean) - A data management tool for humans. **[Deprecated]**\n* [Bioruby](https://github.com/bioruby/bioruby)\n* [Arel](https://github.com/nkallen/arel) **[Deprecated]**\n\n<a name=\"ruby-misc\"></a>\n#### Misc\n\n* [Big Data For Chimps](https://github.com/infochimps-labs/big_data_for_chimps)\n* [Listof](https://github.com/kevincobain2000/listof) - Community based data collection, packed in gem. Get list of pretty much anything (stop words, countries, non words) in txt, JSON or hash. [Demo/Search for a list](http://kevincobain2000.github.io/listof/)\n\n\n<a name=\"rust\"></a>\n## Rust\n\n<a name=\"rust-general-purpose-machine-learning\"></a>\n#### General-Purpose Machine Learning\n* [smartcore](https://github.com/smartcorelib/smartcore) - \"The Most Advanced Machine Learning Library In Rust.\"\n* [linfa](https://github.com/rust-ml/linfa) - a comprehensive toolkit to build Machine Learning applications with Rust\n* [deeplearn-rs](https://github.com/tedsta/deeplearn-rs) - deeplearn-rs provides simple networks that use matrix multiplication, addition, and ReLU under the MIT license.\n* [rustlearn](https://github.com/maciejkula/rustlearn) - a machine learning framework featuring logistic regression, support vector machines, decision trees and random forests.\n* [rusty-machine](https://github.com/AtheMathmo/rusty-machine) - a pure-rust machine learning library.\n* [leaf](https://github.com/autumnai/leaf) - open source framework for machine intelligence, sharing concepts from TensorFlow and Caffe. Available under the MIT license. [**[Deprecated]**](https://medium.com/@mjhirn/tensorflow-wins-89b78b29aafb#.s0a3uy4cc)\n* [RustNN](https://github.com/jackm321/RustNN) - RustNN is a feedforward neural network library. **[Deprecated]**\n* [RusticSOM](https://github.com/avinashshenoy97/RusticSOM) - A Rust library for Self Organising Maps (SOM).\n* [candle](https://github.com/huggingface/candle) - Candle is a minimalist ML framework for Rust with a focus on performance (including GPU support) and ease of use.\n* [linfa](https://github.com/rust-ml/linfa) - `linfa` aims to provide a comprehensive toolkit to build Machine Learning applications with Rust\n* [delta](https://github.com/delta-rs/delta) - An open source machine learning framework in Rust \u0394\n\n#### Deep Learning\n\n* [tch-rs](https://github.com/LaurentMazare/tch-rs) - Rust bindings for the C++ API of PyTorch\n* [dfdx](https://github.com/coreylowman/dfdx) - Deep learning in Rust, with shape checked tensors and neural networks\n* [burn](https://github.com/tracel-ai/burn) - Burn is a new comprehensive dynamic Deep Learning Framework built using Rust with extreme flexibility, compute efficiency and portability as its primary goals\n\n#### Natural Language Processing\n\n* [huggingface/tokenizers](https://github.com/huggingface/tokenizers) - Fast State-of-the-Art Tokenizers optimized for Research and Production\n* [rust-bert](https://github.com/guillaume-be/rust-bert) - Rust native ready-to-use NLP pipelines and transformer-based models (BERT, DistilBERT, GPT2,...)\n\n<a name=\"r\"></a>\n## R\n\n<a name=\"r-general-purpose-machine-learning\"></a>\n#### General-Purpose Machine Learning\n\n* [ahaz](https://cran.r-project.org/web/packages/ahaz/index.html) - ahaz: Regularization for semiparametric additive hazards regression. **[Deprecated]**\n* [arules](https://cran.r-project.org/web/packages/arules/index.html) - arules: Mining Association Rules and Frequent Itemsets\n* [biglasso](https://cran.r-project.org/web/packages/biglasso/index.html) - biglasso: Extending Lasso Model Fitting to Big Data in R.\n* [bmrm](https://cran.r-project.org/web/packages/bmrm/index.html) - bmrm: Bundle Methods for Regularized Risk Minimization Package.\n* [Boruta](https://cran.r-project.org/web/packages/Boruta/index.html) - Boruta: A wrapper algorithm for all-relevant feature selection.\n* [bst](https://cran.r-project.org/web/packages/bst/index.html) - bst: Gradient Boosting.\n* [C50](https://cran.r-project.org/web/packages/C50/index.html) - C50: C5.0 Decision Trees and Rule-Based Models.\n* [caret](https://topepo.github.io/caret/index.html) - Classification and Regression Training: Unified interface to ~150 ML algorithms in R.\n* [caretEnsemble](https://cran.r-project.org/web/packages/caretEnsemble/index.html) - caretEnsemble: Framework for fitting multiple caret models as well as creating ensembles of such models. **[Deprecated]**\n* [CatBoost](https://github.com/catboost/catboost) - General purpose gradient boosting on decision trees library with categorical features support out of the box for R.\n* [Clever Algorithms For Machine Learning](https://machinelearningmastery.com/)\n* [CORElearn](https://cran.r-project.org/web/packages/CORElearn/index.html) - CORElearn: Classification, regression, feature evaluation and ordinal evaluation.\n-* [CoxBoost](https://cran.r-project.org/web/packages/CoxBoost/index.html) - CoxBoost: Cox models by likelihood based boosting for a single survival endpoint or competing risks **[Deprecated]**\n* [Cubist](https://cran.r-project.org/web/packages/Cubist/index.html) - Cubist: Rule- and Instance-Based Regression Modelling.\n* [e1071](https://cran.r-project.org/web/packages/e1071/index.html) - e1071: Misc Functions of the Department of Statistics (e1071), TU Wien\n* [earth](https://cran.r-project.org/web/packages/earth/index.html) - earth: Multivariate Adaptive Regression Spline Models\n* [elasticnet](https://cran.r-project.org/web/packages/elasticnet/index.html) - elasticnet: Elastic-Net for Sparse Estimation and Sparse PCA.\n* [ElemStatLearn](https://cran.r-project.org/web/packages/ElemStatLearn/index.html) - ElemStatLearn: Data sets, functions and examples from the book: \"The Elements of Statistical Learning, Data Mining, Inference, and Prediction\" by Trevor Hastie, Robert Tibshirani and Jerome Friedman Prediction\" by Trevor Hastie, Robert Tibshirani and Jerome Friedman.\n* [evtree](https://cran.r-project.org/web/packages/evtree/index.html) - evtree: Evolutionary Learning of Globally Optimal Trees.\n* [forecast](https://cran.r-project.org/web/packages/forecast/index.html) - forecast: Timeseries forecasting using ARIMA, ETS, STLM, TBATS, and neural network models.\n* [forecastHybrid](https://cran.r-project.org/web/packages/forecastHybrid/index.html) - forecastHybrid: Automatic ensemble and cross validation of ARIMA, ETS, STLM, TBATS, and neural network models from the \"forecast\" package.\n* [fpc](https://cran.r-project.org/web/packages/fpc/index.html) - fpc: Flexible procedures for clustering.\n* [frbs](https://cran.r-project.org/web/packages/frbs/index.html) - frbs: Fuzzy Rule-based Systems for Classification and Regression Tasks. **[Deprecated]**\n* [GAMBoost](https://cran.r-project.org/web/packages/GAMBoost/index.html) - GAMBoost: Generalized linear and additive models by likelihood based boosting. **[Deprecated]**\n* [gamboostLSS](https://cran.r-project.org/web/packages/gamboostLSS/index.html) - gamboostLSS: Boosting Methods for GAMLSS.\n* [gbm](https://cran.r-project.org/web/packages/gbm/index.html) - gbm: Generalized Boosted Regression Models.\n* [glmnet](https://cran.r-project.org/web/packages/glmnet/index.html) - glmnet: Lasso and elastic-net regularized generalized linear models.\n* [glmpath](https://cran.r-project.org/web/packages/glmpath/index.html) - glmpath: L1 Regularization Path for Generalized Linear Models and Cox Proportional Hazards Model.\n* [GMMBoost](https://cran.r-project.org/web/packages/GMMBoost/index.html) - GMMBoost: Likelihood-based Boosting for Generalized mixed models. **[Deprecated]**\n* [grplasso](https://cran.r-project.org/web/packages/grplasso/index.html) - grplasso: Fitting user specified models with Group Lasso penalty.\n* [grpreg](https://cran.r-project.org/web/packages/grpreg/index.html) - grpreg: Regularization paths for regression models with grouped covariates.\n* [h2o](https://cran.r-project.org/web/packages/h2o/index.html) - A framework for fast, parallel, and distributed machine learning algorithms at scale -- Deeplearning, Random forests, GBM, KMeans, PCA, GLM.\n* [hda](https://cran.r-project.org/web/packages/hda/index.html) - hda: Heteroscedastic Discriminant Analysis. **[Deprecated]**\n* [Introduction to Statistical Learning](https://www-bcf.usc.edu/~gareth/ISL/)\n* [ipred](https://cran.r-project.org/web/packages/ipred/index.html) - ipred: Improved Predictors.\n* [kernlab](https://cran.r-project.org/web/packages/kernlab/index.html) - kernlab: Kernel-based Machine Learning Lab.\n* [klaR](https://cran.r-project.org/web/packages/klaR/index.html) - klaR: Classification and visualization.\n* [L0Learn](https://cran.r-project.org/web/packages/L0Learn/index.html) - L0Learn: Fast algorithms for best subset selection.\n* [lars](https://cran.r-project.org/web/packages/lars/index.html) - lars: Least Angle Regression, Lasso and Forward Stagewise. **[Deprecated]**\n* [lasso2](https://cran.r-project.org/web/packages/lasso2/index.html) - lasso2: L1 constrained estimation aka \u2018lasso\u2019.\n* [LiblineaR](https://cran.r-project.org/web/packages/LiblineaR/index.html) - LiblineaR: Linear Predictive Models Based On The Liblinear C/C++ Library.\n* [LogicReg](https://cran.r-project.org/web/packages/LogicReg/index.html) - LogicReg: Logic Regression.\n* [Machine Learning For Hackers](https://github.com/johnmyleswhite/ML_for_Hackers)\n* [maptree](https://cran.r-project.org/web/packages/maptree/index.html) - maptree: Mapping, pruning, and graphing tree models. **[Deprecated]**\n* [mboost](https://cran.r-project.org/web/packages/mboost/index.html) - mboost: Model-Based Boosting.\n* [medley](https://www.kaggle.com/general/3661) - medley: Blending regression models, using a greedy stepwise approach.\n* [mlr](https://cran.r-project.org/web/packages/mlr/index.html) - mlr: Machine Learning in R.\n* [ncvreg](https://cran.r-project.org/web/packages/ncvreg/index.html) - ncvreg: Regularization paths for SCAD- and MCP-penalized regression models.\n* [nnet](https://cran.r-project.org/web/packages/nnet/index.html) - nnet: Feed-forward Neural Networks and Multinomial Log-Linear Models. **[Deprecated]**\n* [pamr](https://cran.r-project.org/web/packages/pamr/index.html) - pamr: Pam: prediction analysis for microarrays. **[Deprecated]**\n* [party](https://cran.r-project.org/web/packages/party/index.html) - party: A Laboratory for Recursive Partitioning\n* [partykit](https://cran.r-project.org/web/packages/partykit/index.html) - partykit: A Toolkit for Recursive Partitioning.\n* [penalized](https://cran.r-project.org/web/packages/penalized/index.html) - penalized: L1 (lasso and fused lasso) and L2 (ridge) penalized estimation in GLMs and in the Cox model.\n* [penalizedLDA](https://cran.r-project.org/web/packages/penalizedLDA/index.html) - penalizedLDA: Penalized classification using Fisher's linear discriminant. **[Deprecated]**\n* [penalizedSVM](https://cran.r-project.org/web/packages/penalizedSVM/index.html) - penalizedSVM: Feature Selection SVM using penalty functions.\n* [quantregForest](https://cran.r-project.org/web/packages/quantregForest/index.html) - quantregForest: Quantile Regression Forests.\n* [randomForest](https://cran.r-project.org/web/packages/randomForest/index.html) - randomForest: Breiman and Cutler's random forests for classification and regression.\n* [randomForestSRC](https://cran.r-project.org/web/packages/randomForestSRC/index.html) - randomForestSRC: Random Forests for Survival, Regression and Classification (RF-SRC).\n* [rattle](https://cran.r-project.org/web/packages/rattle/index.html) - rattle: Graphical user interface for data mining in R.\n* [rda](https://cran.r-project.org/web/packages/rda/index.html) - rda: Shrunken Centroids Regularized Discriminant Analysis.\n* [rdetools](https://cran.r-project.org/web/packages/rdetools/index.html) - rdetools: Relevant Dimension Estimation (RDE) in Feature Spaces. **[Deprecated]**\n* [REEMtree](https://cran.r-project.org/web/packages/REEMtree/index.html) - REEMtree: Regression Trees with Random Effects for Longitudinal (Panel) Data. **[Deprecated]**\n* [relaxo](https://cran.r-project.org/web/packages/relaxo/index.html) - relaxo: Relaxed Lasso. **[Deprecated]**\n* [rgenoud](https://cran.r-project.org/web/packages/rgenoud/index.html) - rgenoud: R version of GENetic Optimization Using Derivatives\n* [Rmalschains](https://cran.r-project.org/web/packages/Rmalschains/index.html) - Rmalschains: Continuous Optimization using Memetic Algorithms with Local Search Chains (MA-LS-Chains) in R.\n* [rminer](https://cran.r-project.org/web/packages/rminer/index.html) - rminer: Simpler use of data mining methods (e.g. NN and SVM) in classification and regression. **[Deprecated]**\n* [ROCR](https://cran.r-project.org/web/packages/ROCR/index.html) - ROCR: Visualizing the performance of scoring classifiers. **[Deprecated]**\n* [RoughSets](https://cran.r-project.org/web/packages/RoughSets/index.html) - RoughSets: Data Analysis Using Rough Set and Fuzzy Rough Set Theories. **[Deprecated]**\n* [rpart](https://cran.r-project.org/web/packages/rpart/index.html) - rpart: Recursive Partitioning and Regression Trees.\n* [RPMM](https://cran.r-project.org/web/packages/RPMM/index.html) - RPMM: Recursively Partitioned Mixture Model.\n* [RSNNS](https://cran.r-project.org/web/packages/RSNNS/index.html) - RSNNS: Neural Networks in R using the Stuttgart Neural Network Simulator (SNNS).\n* [RWeka](https://cran.r-project.org/web/packages/RWeka/index.html) - RWeka: R/Weka interface.\n* [RXshrink](https://cran.r-project.org/web/packages/RXshrink/index.html) - RXshrink: Maximum Likelihood Shrinkage via Generalized Ridge or Least Angle Regression.\n* [sda](https://cran.r-project.org/web/packages/sda/index.html) - sda: Shrinkage Discriminant Analysis and CAT Score Variable Selection. **[Deprecated]**\n* [spectralGraphTopology](https://cran.r-project.org/web/packages/spectralGraphTopology/index.html) - spectralGraphTopology: Learning Graphs from Data via Spectral Constraints.\n* [SuperLearner](https://github.com/ecpolley/SuperLearner) - Multi-algorithm ensemble learning packages.\n* [svmpath](https://cran.r-project.org/web/packages/svmpath/index.html) - svmpath: svmpath: the SVM Path algorithm. **[Deprecated]**\n* [tgp](https://cran.r-project.org/web/packages/tgp/index.html) - tgp: Bayesian treed Gaussian process models. **[Deprecated]**\n* [tree](https://cran.r-project.org/web/packages/tree/index.html) - tree: Classification and regression trees.\n* [varSelRF](https://cran.r-project.org/web/packages/varSelRF/index.html) - varSelRF: Variable selection using random forests.\n* [XGBoost.R](https://github.com/tqchen/xgboost/tree/master/R-package) - R binding for eXtreme Gradient Boosting (Tree) Library.\n* [Optunity](https://optunity.readthedocs.io/en/latest/) - A library dedicated to automated hyperparameter optimization with a simple, lightweight API to facilitate drop-in replacement of grid search. Optunity is written in Python but interfaces seamlessly to R.\n* [igraph](https://igraph.org/r/) - binding to igraph library - General purpose graph library.\n* [MXNet](https://github.com/apache/incubator-mxnet) - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, JavaScript and more.\n* [TDSP-Utilities](https://github.com/Azure/Azure-TDSP-Utilities) - Two data science utilities in R from Microsoft: 1) Interactive Data Exploration, Analysis, and Reporting (IDEAR) ; 2) Automated Modelling and Reporting (AMR).\n* [clugenr](https://github.com/clugen/clugenr/) - Multidimensional cluster generation in R.\n\n<a name=\"r-data-analysis--data-visualization\"></a>\n#### Data Manipulation | Data Analysis | Data Visualization\n\n* [data.table](https://rdatatable.gitlab.io/data.table/) - `data.table` provides a high-performance version of base R\u2019s `data.frame` with syntax and feature enhancements for ease of use, convenience and programming speed.\n* [dplyr](https://www.rdocumentation.org/packages/dplyr/versions/0.7.8) - A data manipulation package that helps to solve the most common data manipulation problems.\n* [ggplot2](https://ggplot2.tidyverse.org/) - A data visualization package based on the grammar of graphics.\n* [tmap](https://cran.r-project.org/web/packages/tmap/vignettes/tmap-getstarted.html) for visualizing geospatial data with static maps and [leaflet](https://rstudio.github.io/leaflet/) for interactive maps\n* [tm](https://www.rdocumentation.org/packages/tm/) and [quanteda](https://quanteda.io/) are the main packages for managing,  analyzing, and visualizing textual data.\n* [shiny](https://shiny.rstudio.com/) is the basis for truly interactive displays and dashboards in R. However, some measure of interactivity can be achieved with [htmlwidgets](https://www.htmlwidgets.org/) bringing javascript libraries to R. These include, [plotly](https://plot.ly/r/), [dygraphs](http://rstudio.github.io/dygraphs), [highcharter](http://jkunst.com/highcharter/), and several others.\n\n<a name=\"sas\"></a>\n## SAS\n\n<a name=\"sas-general-purpose-machine-learning\"></a>\n#### General-Purpose Machine Learning\n\n* [Visual Data Mining and Machine Learning](https://www.sas.com/en_us/software/visual-data-mining-machine-learning.html) - Interactive, automated, and programmatic modelling with the latest machine learning algorithms in and end-to-end analytics environment, from data prep to deployment. Free trial available.\n* [Enterprise Miner](https://www.sas.com/en_us/software/enterprise-miner.html) - Data mining and machine learning that creates deployable models using a GUI or code.\n* [Factory Miner](https://www.sas.com/en_us/software/factory-miner.html) - Automatically creates deployable machine learning models across numerous market or customer segments using a GUI.\n\n<a name=\"sas-data-analysis--data-visualization\"></a>\n#### Data Analysis / Data Visualization\n\n* [SAS/STAT](https://www.sas.com/en_us/software/stat.html) - For conducting advanced statistical analysis.\n* [University Edition](https://www.sas.com/en_us/software/university-edition.html) - FREE! Includes all SAS packages necessary for data analysis and visualization, and includes online SAS courses.\n\n<a name=\"sas-natural-language-processing\"></a>\n#### Natural Language Processing\n\n* [Contextual Analysis](https://www.sas.com/en_us/software/contextual-analysis.html) - Add structure to unstructured text using a GUI.\n* [Sentiment Analysis](https://www.sas.com/en_us/software/sentiment-analysis.html) - Extract sentiment from text using a GUI.\n* [Text Miner](https://www.sas.com/en_us/software/text-miner.html) - Text mining using a GUI or code.\n\n<a name=\"sas-demos-and-scripts\"></a>\n#### Demos and Scripts\n\n* [ML_Tables](https://github.com/sassoftware/enlighten-apply/tree/master/ML_tables) - Concise cheat sheets containing machine learning best practices.\n* [enlighten-apply](https://github.com/sassoftware/enlighten-apply) - Example code and materials that illustrate applications of SAS machine learning techniques.\n* [enlighten-integration](https://github.com/sassoftware/enlighten-integration) - Example code and materials that illustrate techniques for integrating SAS with other analytics technologies in Java, PMML, Python and R.\n* [enlighten-deep](https://github.com/sassoftware/enlighten-deep) - Example code and materials that illustrate using neural networks with several hidden layers in SAS.\n* [dm-flow](https://github.com/sassoftware/dm-flow) - Library of SAS Enterprise Miner process flow diagrams to help you learn by example about specific data mining topics.\n\n\n<a name=\"scala\"></a>\n## Scala\n\n<a name=\"scala-natural-language-processing\"></a>\n#### Natural Language Processing\n\n* [ScalaNLP](http://www.scalanlp.org/) - ScalaNLP is a suite of machine learning and numerical computing libraries.\n* [Breeze](https://github.com/scalanlp/breeze) - Breeze is a numerical processing library for Scala.\n* [Chalk](https://github.com/scalanlp/chalk) - Chalk is a natural language processing library. **[Deprecated]**\n* [FACTORIE](https://github.com/factorie/factorie) - FACTORIE is a toolkit for deployable probabilistic modelling, implemented as a software library in Scala. It provides its users with a succinct language for creating relational factor graphs, estimating parameters and performing inference.\n* [Montague](https://github.com/Workday/upshot-montague) - Montague is a semantic parsing library for Scala with an easy-to-use DSL.\n* [Spark NLP](https://github.com/JohnSnowLabs/spark-nlp) - Natural language processing library built on top of Apache Spark ML to provide simple, performant, and accurate NLP annotations for machine learning pipelines, that scale easily in a distributed environment.\n\n<a name=\"scala-data-analysis--data-visualization\"></a>\n#### Data Analysis / Data Visualization\n\n* [NDScala](https://github.com/SciScala/NDScala) - N-dimensional arrays in Scala 3. Think NumPy ndarray, but with compile-time type-checking/inference over shapes, tensor/axis labels & numeric data types\n* [MLlib in Apache Spark](https://spark.apache.org/docs/latest/mllib-guide.html) - Distributed machine learning library in Spark\n* [Hydrosphere Mist](https://github.com/Hydrospheredata/mist) - a service for deployment Apache Spark MLLib machine learning models as realtime, batch or reactive web services.\n* [Scalding](https://github.com/twitter/scalding) - A Scala API for Cascading.\n* [Summing Bird](https://github.com/twitter/summingbird) - Streaming MapReduce with Scalding and Storm.\n* [Algebird](https://github.com/twitter/algebird) - Abstract Algebra for Scala.\n* [xerial](https://github.com/xerial/xerial) - Data management utilities for Scala. **[Deprecated]**\n* [PredictionIO](https://github.com/apache/predictionio) - PredictionIO, a machine learning server for software developers and data engineers.\n* [BIDMat](https://github.com/BIDData/BIDMat) - CPU and GPU-accelerated matrix library intended to support large-scale exploratory data analysis.\n* [Flink](https://flink.apache.org/) - Open source platform for distributed stream and batch data processing.\n* [Spark Notebook](http://spark-notebook.io) - Interactive and Reactive Data Science using Scala and Spark.\n\n<a name=\"scala-general-purpose-machine-learning\"></a>\n#### General-Purpose Machine Learning\n\n* [Microsoft ML for Apache Spark](https://github.com/Azure/mmlspark) -> A distributed machine learning framework Apache Spark\n* [ONNX-Scala](https://github.com/EmergentOrder/onnx-scala) - An ONNX (Open Neural Network eXchange) API and backend for typeful, functional deep learning in Scala (3).\n* [DeepLearning.scala](https://deeplearning.thoughtworks.school/) - Creating statically typed dynamic neural networks from object-oriented & functional programming constructs.\n* [Conjecture](https://github.com/etsy/Conjecture) - Scalable Machine Learning in Scalding.\n* [brushfire](https://github.com/stripe/brushfire) - Distributed decision tree ensemble learning in Scala.\n* [ganitha](https://github.com/tresata/ganitha) - Scalding powered machine learning. **[Deprecated]**\n* [adam](https://github.com/bigdatagenomics/adam) - A genomics processing engine and specialized file format built using Apache Avro, Apache Spark and Parquet. Apache 2 licensed.\n* [bioscala](https://github.com/bioscala/bioscala) - Bioinformatics for the Scala programming language\n* [BIDMach](https://github.com/BIDData/BIDMach) - CPU and GPU-accelerated Machine Learning Library.\n* [Figaro](https://github.com/p2t2/figaro) - a Scala library for constructing probabilistic models.\n* [H2O Sparkling Water](https://github.com/h2oai/sparkling-water) - H2O and Spark interoperability.\n* [FlinkML in Apache Flink](https://ci.apache.org/projects/flink/flink-docs-master/dev/libs/ml/index.html) - Distributed machine learning library in Flink.\n* [DynaML](https://github.com/transcendent-ai-labs/DynaML) - Scala Library/REPL for Machine Learning Research.\n* [Saul](https://github.com/CogComp/saul) - Flexible Declarative Learning-Based Programming.\n* [SwiftLearner](https://github.com/valdanylchuk/swiftlearner/) - Simply written algorithms to help study ML or write your own implementations.\n* [Smile](https://haifengl.github.io/) - Statistical Machine Intelligence and Learning Engine.\n* [doddle-model](https://github.com/picnicml/doddle-model) - An in-memory machine learning library built on top of Breeze. It provides immutable objects and exposes its functionality through a scikit-learn-like API.\n* [TensorFlow Scala](https://github.com/eaplatanios/tensorflow_scala) - Strongly-typed Scala API for TensorFlow.\n* [isolation-forest](https://github.com/linkedin/isolation-forest) - A distributed Spark/Scala implementation of the isolation forest algorithm for unsupervised outlier detection, featuring support for scalable training and ONNX export for easy cross-platform inference.\n\n<a name=\"scheme\"></a>\n## Scheme\n\n<a name=\"scheme-neural-networks\"></a>\n#### Neural Networks\n\n* [layer](https://github.com/cloudkj/layer) - Neural network inference from the command line, implemented in [CHICKEN Scheme](https://www.call-cc.org/).\n\n<a name=\"swift\"></a>\n## Swift\n\n<a name=\"swift-general-purpose-machine-learning\"></a>\n#### General-Purpose Machine Learning\n\n* [Bender](https://github.com/xmartlabs/Bender) - Fast Neural Networks framework built on top of Metal. Supports TensorFlow models.\n* [Swift AI](https://github.com/Swift-AI/Swift-AI) - Highly optimized artificial intelligence and machine learning library written in Swift.\n* [Swift for Tensorflow](https://github.com/tensorflow/swift) - a next-generation platform for machine learning, incorporating the latest research across machine learning, compilers, differentiable programming, systems design, and beyond.\n* [BrainCore](https://github.com/alejandro-isaza/BrainCore) - The iOS and OS X neural network framework.\n* [swix](https://github.com/stsievert/swix) - A bare bones library that includes a general matrix language and wraps some OpenCV for iOS development. **[Deprecated]**\n* [AIToolbox](https://github.com/KevinCoble/AIToolbox) - A toolbox framework of AI modules written in Swift: Graphs/Trees, Linear Regression, Support Vector Machines, Neural Networks, PCA, KMeans, Genetic Algorithms, MDP, Mixture of Gaussians.\n* [MLKit](https://github.com/Somnibyte/MLKit) - A simple Machine Learning Framework written in Swift. Currently features Simple Linear Regression, Polynomial Regression, and Ridge Regression.\n* [Swift Brain](https://github.com/vlall/Swift-Brain) - The first neural network / machine learning library written in Swift. This is a project for AI algorithms in Swift for iOS and OS X development. This project includes algorithms focused on Bayes theorem, neural networks, SVMs, Matrices, etc...\n* [Perfect TensorFlow](https://github.com/PerfectlySoft/Perfect-TensorFlow) - Swift Language Bindings of TensorFlow. Using native TensorFlow models on both macOS / Linux.\n* [PredictionBuilder](https://github.com/denissimon/prediction-builder-swift) - A library for machine learning that builds predictions using a linear regression.\n* [Awesome CoreML](https://github.com/SwiftBrain/awesome-CoreML-models) - A curated list of pretrained CoreML models.\n* [Awesome Core ML Models](https://github.com/likedan/Awesome-CoreML-Models) - A curated list of machine learning models in CoreML format.\n\n<a name=\"tensorflow\"></a>\n## TensorFlow\n\n<a name=\"tensorflow-general-purpose-machine-learning\"></a>\n#### General-Purpose Machine Learning\n* [Awesome Keras](https://github.com/markusschanta/awesome-keras) - A curated list of awesome Keras projects, libraries and resources.\n* [Awesome TensorFlow](https://github.com/jtoy/awesome-tensorflow) - A list of all things related to TensorFlow.\n* [Golden TensorFlow](https://golden.com/wiki/TensorFlow) - A page of content on TensorFlow, including academic papers and links to related topics.\n\n<a name=\"tools\"></a>\n## Tools\n\n<a name=\"tools-neural-networks\"></a>\n#### Neural Networks\n* [layer](https://github.com/cloudkj/layer) - Neural network inference from the command line\n\n<a name=\"tools-misc\"></a>\n#### Misc\n\n* [Wallaroo.AI](https://wallaroo.ai/) - Production AI plaftorm for deploying, managing, and observing any model at scale across any environment from cloud to edge. Let's go from python notebook to inferencing in minutes. \n* [Infinity](https://github.com/infiniflow/infinity) - The AI-native database built for LLM applications, providing incredibly fast vector and full-text search. Developed using C++20\n* [Synthical](https://synthical.com) - AI-powered collaborative research environment. You can use it to get recommendations of articles based on reading history, simplify papers, find out what articles are trending, search articles by meaning (not just keywords), create and share folders of articles, see lists of articles from specific companies and universities, and add highlights.\n* [Humanloop](https://humanloop.com) \u2013 Humanloop is a platform for prompt experimentation, finetuning models for better performance, cost optimization, and collecting model generated data and user feedback.\n* [Qdrant](https://qdrant.tech) \u2013 Qdrant is [open source](https://github.com/qdrant/qdrant) vector similarity search engine with extended filtering support, written in Rust.\n* [Localforge](https://localforge.dev/) \u2013 Is an [open source](https://github.com/rockbite/localforge) on-prem AI coding autonomous assistant that lives inside your repo, edits and tests files at SSD speed. Think Claude Code but with UI. plug in any LLM (OpenAI, Gemini, Ollama, etc.) and let it work for you.\n* [milvus](https://milvus.io) \u2013 Milvus is [open source](https://github.com/milvus-io/milvus) vector database for production AI, written in Go and C++, scalable and blazing fast for billions of embedding vectors.\n* [Weaviate](https://www.semi.technology/developers/weaviate/current/) \u2013 Weaviate is an [open source](https://github.com/semi-technologies/weaviate) vector search engine and vector database. Weaviate uses machine learning to vectorize and store data, and to find answers to natural language queries. With Weaviate you can also bring your custom ML models to production scale.\n* [txtai](https://github.com/neuml/txtai) - Build semantic search applications and workflows.\n* [MLReef](https://about.mlreef.com/) - MLReef is an end-to-end development platform using the power of git to give structure and deep collaboration possibilities to the ML development process.\n* [Chroma](https://www.trychroma.com/) - Chroma - the AI-native open-source embedding database\n* [Pinecone](https://www.pinecone.io/) - Vector database for applications that require real-time, scalable vector embedding and similarity search.\n* [CatalyzeX](https://chrome.google.com/webstore/detail/code-finder-for-research/aikkeehnlfpamidigaffhfmgbkdeheil) - Browser extension ([Chrome](https://chrome.google.com/webstore/detail/code-finder-for-research/aikkeehnlfpamidigaffhfmgbkdeheil) and [Firefox](https://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex/)) that automatically finds and shows code implementations for machine learning papers anywhere: Google, Twitter, Arxiv, Scholar, etc.\n* [ML Workspace](https://github.com/ml-tooling/ml-workspace) - All-in-one web-based IDE for machine learning and data science. The workspace is deployed as a docker container and is preloaded with a variety of popular data science libraries (e.g., Tensorflow, PyTorch) and dev tools (e.g., Jupyter, VS Code).\n* [Notebooks](https://github.com/rlan/notebooks) - A starter kit for Jupyter notebooks and machine learning. Companion docker images consist of all combinations of python versions, machine learning frameworks (Keras, PyTorch and Tensorflow) and CPU/CUDA versions.\n* [DVC](https://github.com/iterative/dvc) - Data Science Version Control is an open-source version control system for machine learning projects with pipelines support. It makes ML projects reproducible and shareable.\n* [DVClive](https://github.com/iterative/dvclive) - Python library for experiment metrics logging into simply formatted local files.\n* [VDP](https://github.com/instill-ai/vdp) - open source visual data ETL to streamline the end-to-end visual data processing pipeline: extract unstructured visual data from pre-built data sources, transform it into analysable structured insights by Vision AI models imported from various ML platforms, and load the insights into warehouses or applications.\n* [Kedro](https://github.com/quantumblacklabs/kedro/) - Kedro is a data and development workflow framework that implements best practices for data pipelines with an eye towards productionizing machine learning models.\n* [Hamilton](https://github.com/dagworks-inc/hamilton) - a lightweight library to define data transformations as a directed-acyclic graph (DAG). It helps author reliable feature engineering and machine learning pipelines, and more.\n* [guild.ai](https://guild.ai/) - Tool to log, analyze, compare and \"optimize\" experiments. It's cross-platform and framework independent, and provided integrated visualizers such as tensorboard.\n* [Sacred](https://github.com/IDSIA/sacred) - Python tool to help  you configure, organize, log and reproduce experiments. Like a notebook lab in the context of Chemistry/Biology. The community has built multiple add-ons leveraging the proposed standard.\n* [Comet](https://www.comet.com/) -  ML platform for tracking experiments, hyper-parameters, artifacts and more. It's deeply integrated with over 15+ deep learning frameworks and orchestration tools. Users can also use the platform to monitor their models in production.\n* [MLFlow](https://mlflow.org/) - platform to manage the ML lifecycle, including experimentation, reproducibility and deployment. Framework and language agnostic, take a look at all the built-in integrations.\n* [Weights & Biases](https://www.wandb.com/) - Machine learning experiment tracking, dataset versioning, hyperparameter search, visualization, and collaboration\n* More tools to improve the ML lifecycle: [Catalyst](https://github.com/catalyst-team/catalyst), [PachydermIO](https://www.pachyderm.io/). The following are GitHub-alike and targeting teams [Weights & Biases](https://www.wandb.com/), [Neptune.ai](https://neptune.ai/), [Comet.ml](https://www.comet.ml/), [Valohai.ai](https://valohai.com/), [DAGsHub](https://DAGsHub.com/).\n* [Arize AI](https://www.arize.com) - Model validation and performance monitoring, drift detection, explainability, visualization across structured and unstructured data\n* [MachineLearningWithTensorFlow2ed](https://www.manning.com/books/machine-learning-with-tensorflow-second-edition) - a book on general purpose machine learning techniques regression, classification, unsupervised clustering, reinforcement learning, auto encoders, convolutional neural networks, RNNs, LSTMs, using TensorFlow 1.14.1.\n* [m2cgen](https://github.com/BayesWitnesses/m2cgen) - A tool that allows the conversion of ML models into native code (Java, C, Python, Go, JavaScript, Visual Basic, C#, R, PowerShell, PHP, Dart) with zero dependencies.\n* [CML](https://github.com/iterative/cml) - A library for doing continuous integration with ML projects. Use GitHub Actions & GitLab CI to train and evaluate models in production like environments and automatically generate visual reports with metrics and graphs in pull/merge requests. Framework & language agnostic.\n* [Pythonizr](https://pythonizr.com) - An online tool to generate boilerplate machine learning code that uses scikit-learn.\n* [Flyte](https://flyte.org/) - Flyte makes it easy to create concurrent, scalable, and maintainable workflows for machine learning and data processing.\n* [Chaos Genius](https://github.com/chaos-genius/chaos_genius/) - ML powered analytics engine for outlier/anomaly detection and root cause analysis.\n* [MLEM](https://github.com/iterative/mlem) - Version and deploy your ML models following GitOps principles\n* [DockerDL](https://github.com/matifali/dockerdl) - Ready to use deeplearning docker images.\n* [Aqueduct](https://github.com/aqueducthq/aqueduct) - Aqueduct enables you to easily define, run, and manage AI & ML tasks on any cloud infrastructure.\n* [Ambrosia](https://github.com/reactorsh/ambrosia) - Ambrosia helps you clean up your LLM datasets using _other_ LLMs.\n* [Fiddler AI](https://www.fiddler.ai) - The all-in-one AI Observability and Security platform for responsible AI. It provides monitoring, analytics, and centralized controls to operationalize ML, GenAI, and LLM applications with trust. Fiddler helps enterprises scale LLM and ML deployments to deliver high performance AI, reduce costs, and be responsible in governance.\n* [Maxim AI](https://getmaxim.ai) - The agent simulation, evaluation, and observability platform helping product teams ship their AI applications with the quality and speed needed for real-world use.\n* [Agentic Radar](https://github.com/splx-ai/agentic-radar) -  Open-source CLI security scanner for agentic workflows. Scans your workflow\u2019s source code, detects vulnerabilities, and generates an interactive visualization along with a detailed security report. Supports LangGraph, CrewAI, n8n, OpenAI Agents, and more.\n\n<a name=\"books\"></a>\n## Books\n\n* [Distributed Machine Learning Patterns](https://github.com/terrytangyuan/distributed-ml-patterns)  - This book teaches you how to take machine learning models from your personal laptop to large distributed clusters. You\u2019ll explore key concepts and patterns behind successful distributed machine learning systems, and learn technologies like TensorFlow, Kubernetes, Kubeflow, and Argo Workflows directly from a key maintainer and contributor, with real-world scenarios and hands-on projects.\n* [Grokking Machine Learning](https://www.manning.com/books/grokking-machine-learning) - Grokking Machine Learning teaches you how to apply ML to your projects using only standard Python code and high school-level math.\n* [Machine Learning Bookcamp](https://www.manning.com/books/machine-learning-bookcamp) - Learn the essentials of machine learning by completing a carefully designed set of real-world projects.\n* [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1098125975) - Through a recent series of breakthroughs, deep learning has boosted the entire field of machine learning. Now, even programmers who know close to nothing about this technology can use simple, efficient tools to implement programs capable of learning from data. This bestselling book uses concrete examples, minimal theory, and production-ready Python frameworks (Scikit-Learn, Keras, and TensorFlow) to help you gain an intuitive understanding of the concepts and tools for building intelligent systems.\n* [Machine Learning Books for Beginners](https://www.appliedaicourse.com/blog/machine-learning-books/) - This blog provides a curated list of introductory books to help aspiring ML professionals to grasp foundational machine learning concepts and techniques.\n\n\n<a name=\"credits\"></a>\n* [Netron](https://netron.app/) - An opensource viewer for neural network, deep learning and machine learning models\n* [Teachable Machine](https://teachablemachine.withgoogle.com/) - Train Machine Learning models on the fly to recognize your own images, sounds, & poses.\n* [Pollinations.AI](https://pollinations.ai) - Free, no-signup APIs for text, image, and audio generation with no API keys required. Offers OpenAI-compatible interfaces and React hooks for easy integration.\n* [Model Zoo](https://modelzoo.co/) - Discover open source deep learning code and pretrained models.\n\n## Credits\n\n* Some of the python libraries were cut-and-pasted from [vinta](https://github.com/vinta/awesome-python)\n* References for Go were mostly cut-and-pasted from [gopherdata](https://github.com/gopherdata/resources/tree/master/tooling)\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 138839979,
    "name": "DeepLearning-500-questions",
    "full_name": "scutan90/DeepLearning-500-questions",
    "description": "\u6df1\u5ea6\u5b66\u4e60500\u95ee\uff0c\u4ee5\u95ee\u7b54\u5f62\u5f0f\u5bf9\u5e38\u7528\u7684\u6982\u7387\u77e5\u8bc6\u3001\u7ebf\u6027\u4ee3\u6570\u3001\u673a\u5668\u5b66\u4e60\u3001\u6df1\u5ea6\u5b66\u4e60\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u7b49\u70ed\u70b9\u95ee\u9898\u8fdb\u884c\u9610\u8ff0\uff0c\u4ee5\u5e2e\u52a9\u81ea\u5df1\u53ca\u6709\u9700\u8981\u7684\u8bfb\u8005\u3002 \u5168\u4e66\u5206\u4e3a18\u4e2a\u7ae0\u8282\uff0c50\u4f59\u4e07\u5b57\u3002\u7531\u4e8e\u6c34\u5e73\u6709\u9650\uff0c\u4e66\u4e2d\u4e0d\u59a5\u4e4b\u5904\u6073\u8bf7\u5e7f\u5927\u8bfb\u8005\u6279\u8bc4\u6307\u6b63\u3002   \u672a\u5b8c\u5f85\u7eed............ \u5982\u6709\u610f\u5408\u4f5c\uff0c\u8054\u7cfbscutjy2015@163.com                     \u7248\u6743\u6240\u6709\uff0c\u8fdd\u6743\u5fc5\u7a76       Tan 2018.06",
    "html_url": "https://github.com/scutan90/DeepLearning-500-questions",
    "clone_url": "https://github.com/scutan90/DeepLearning-500-questions.git",
    "owner_login": "scutan90",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/31844692?v=4",
    "stargazers_count": 56402,
    "watchers_count": 56402,
    "forks_count": 15980,
    "open_issues_count": 121,
    "size": 207074,
    "language": "JavaScript",
    "languages": {
      "JavaScript": 152358,
      "HTML": 2911
    },
    "topics": [],
    "license_name": "GNU General Public License v3.0",
    "created_at": "2018-06-27T06:36:45+00:00",
    "updated_at": "2025-08-05T15:33:50+00:00",
    "pushed_at": "2024-06-26T12:10:14+00:00",
    "contributors_count": 100,
    "readme_length": 6720,
    "readme_content": "\n# \u4eac\u4e1c\u552e\u5356\u94fe\u63a5\uff1ahttps://item.jd.com/12785031.html\n# \u4eac\u4e1c\u5546\u57ce100\u591a\u5bb6\u4e66\u5e97\u6709\u73b0\u8d27\u3002\n\n## \u6df1\u5ea6\u5b66\u4e60500\u95ee\u2014\u2014AI\u5de5\u7a0b\u5e08\u9762\u8bd5\u5b9d\u5178(\u535a\u6587\u89c6\u70b9\u51fa\u54c1)\uff0c\u8c08\u7ee7\u52c7 \u4e3b\u7f16\uff0c\u90ed\u5b50\u948a\uff0c\u674e\u5251\uff0c\u4f43\u677e\u5b9c \u526f\u4e3b\u7f16 \u8457\n\n# \u5185\u5bb9\u7b80\u4ecb\n\n\u672c\u4e66\u7cfb\u7edf\u5730\u63cf\u8ff0\u4e86\u6df1\u5ea6\u5b66\u4e60\u7684\u57fa\u672c\u7406\u8bba\u7b97\u6cd5\u53ca\u5e94\u7528\u3002\u5168\u4e66\u517114\u7ae0\uff0c\u7b2c1-3\u7ae0\u8bba\u8ff0\u4e86\u6570\u5b66\u57fa\u7840\u3001\u673a\u5668\u5b66\u4e60\u57fa\u7840\u548c\u6df1\u5ea6\u5b66\u4e60\u57fa\u7840\uff1b\u7b2c4-7\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u7ecf\u5178\u7f51\u7edc\u53ca\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u4e2d\u5e38\u7528\u7684CNN\u3001RNN\u3001GAN\u7b49\u7f51\u7edc\u7ed3\u6784\u6280\u672f\uff1b\u7b2c8-9\u7ae0\u4ecb\u7ecd\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7684\u76ee\u6807\u68c0\u6d4b\u53ca\u56fe\u50cf\u5206\u5272\u4e24\u5927\u5e94\u7528\uff1b\u7b2c10-14\u7ae0\u4ecb\u7ecd\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u4e3b\u8981\u7684\u4f18\u5316\u65b9\u6cd5\u53ca\u601d\u8def\u7b49\uff0c\u5305\u62ec\u8fc1\u79fb\u5b66\u4e60\u3001\u7f51\u7edc\u67b6\u6784\u53ca\u8bad\u7ec3\u3001\u7f51\u7edc\u4f18\u5316\u6280\u5de7\u3001\u8d85\u53c2\u6570\u8c03\u6574\u53ca\u6a21\u578b\u7684\u538b\u7f29\u548c\u52a0\u901f\u7b49\u3002\u672c\u4e66\u51dd\u805a\u4e86\u4f17\u591a\u4e00\u7ebf\u79d1\u7814\u4eba\u5458\u53ca\u5de5\u7a0b\u5e08\u7684\u7ecf\u9a8c\uff0c\u65e8\u5728\u57f9\u517b\u8bfb\u8005\u53d1\u73b0\u95ee\u9898\u3001\u89e3\u51b3\u95ee\u9898\u3001\u6269\u5c55\u95ee\u9898\u7684\u80fd\u529b\u3002\n\n\u672c\u4e66\u5185\u5bb9\u53d6\u6750\u4e8e\u7f16\u8005\u5728\u65e5\u5e38\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u603b\u7ed3\u7684\u77e5\u8bc6\u70b9\u53ca\u5404\u5927\u516c\u53f8\u5e38\u89c1\u7684\u7b14\u8bd5\u3001\u9762\u8bd5\u9898\u3002\u672c\u4e66\u53ef\u4e3a\u9ad8\u7b49\u9662\u6821\u8ba1\u7b97\u673a\u79d1\u5b66\u3001\u4fe1\u606f\u79d1\u5b66\u3001\u4eba\u5de5\u667a\u80fd\u3001\u63a7\u5236\u79d1\u5b66\u4e0e\u5de5\u7a0b\u3001\u7535\u5b50\u79d1\u5b66\u4e0e\u6280\u672f\u7b49\u9886\u57df\u7684\u7814\u7a76\u53ca\u6559\u5b66\u4eba\u5458\u63d0\u4f9b\u53c2\u8003\uff0c\u4e5f\u53ef\u4e3a\u76f8\u5173\u4e13\u4e1a\u672c\u79d1\u751f\u53ca\u7814\u7a76\u751f\u63d0\u4f9b\u601d\u8003\u65b9\u5411\uff0c\u8fd8\u53ef\u4e3a\u6df1\u5ea6\u5b66\u4e60\u53ca\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7684\u521d\u3001\u4e2d\u7ea7\u7814\u7a76\u4eba\u5458\u548c\u5de5\u7a0b\u6280\u672f\u4eba\u5458\u63d0\u4f9b\u53c2\u8003\uff0c\u5c24\u5176\u9002\u5408\u9700\u8981\u67e5\u6f0f\u8865\u7f3a\u7684\u5e94\u8058\u8005\u53ca\u63d0\u4f9b\u76f8\u5173\u5c97\u4f4d\u7684\u9762\u8bd5\u5b98\u9605\u8bfb\u3002\n\n# \u4f5c\u8005\u7b80\u4ecb\n\u8c08\u7ee7\u52c7 \u4e3b\u7f16\n\n\u5357\u65b9\u79d1\u6280\u5927\u5b66\u548c\u54c8\u5c14\u6ee8\u5de5\u4e1a\u5927\u5b66\u8054\u5408\u57f9\u517b\u535a\u58eb\uff08\u5728\u8bfb\uff09\uff0c\u73b0\u4efb\u701a\u7ef4\u667a\u80fd\u533b\u7597\u6280\u672f\u603b\u76d1\uff0c\u6df1\u5733\u5de5\u4fe1\u5c40\u4e13\u5bb6\u5e93\u4e13\u5bb6\uff0c\u517c\u4efb\u5357\u65b9\u79d1\u6280\u5927\u5b66\u3001\u56db\u5ddd\u5927\u5b66\u7814\u7a76\u751f\u4f01\u4e1a\u5bfc\u5e08\uff0c\u5357\u65b9\u79d1\u6280\u5927\u5b66\u548c\u701a\u7ef4\u667a\u80fd\u533b\u7597\u8054\u5408\u5b9e\u9a8c\u5ba4\u526f\u4e3b\u4efb\uff0c\u5317\u4eac\u63a2\u5de5\u6240\u7279\u8058\u6280\u672f\u4e13\u5bb6\uff0c\u66fe\u5148\u540e\u5728\u4e2d\u79d1\u9662\u4fe1\u5de5\u6240\u3001\u9999\u6e2f\u4e2d\u6587\u5927\u5b66\uff08\u6df1\u5733\uff09\u3001FOXCONN\u673a\u5668\u4eba\u4e0e\u4eba\u5de5\u667a\u80fd\u5b9e\u9a8c\u5ba4\u3001\u987a\u4e30\u79d1\u6280\u7b49\u5355\u4f4d\u4efb\u804c\u3002\u4e3b\u8981\u4e13\u6ce8\u4e8e\u667a\u80fd\u611f\u77e5\u4e0e\u63a7\u5236\u3001\u5b9e\u65f6\u667a\u80fd\u4e0e\u8ba1\u7b97\u673a\u89c6\u89c9\u65b9\u5411\u7684\u7814\u7a76\uff0c\u4e3b\u6301/\u4e3b\u7814\u56fd\u5bb6\u81ea\u7136\u79d1\u5b66\u57fa\u91d1\u3001\u7701\u91cd\u70b9\u7814\u53d1\u8ba1\u5212\u3001\u6df1\u5733\u6218\u7565\u6027\u65b0\u5174\u4ea7\u4e1a\u8ba1\u5212\u7b49\u9879\u76ee20\u4f59\u9879\uff0c\u53d1\u8868SCI/EI\u8bba\u658720\u4f59\u7bc7\uff0c\u7533\u8bf7\u53d1\u660e\u4e13\u522940\u4f59\u9879\uff0c\u83b7\u5168\u56fd\u53d1\u660e\u91d1\u5956\u3002\n\n\u90ed\u5b50\u948a \u526f\u4e3b\u7f16\n\n\u56db\u5ddd\u5927\u5b66\u8ba1\u7b97\u673a\u79d1\u5b66\u4e13\u4e1a\u535a\u58eb\uff0c\u7855\u58eb\u6bd5\u4e1a\u4e8e\u56db\u5ddd\u5927\u5b66\u81ea\u52a8\u5316\u7cfb\uff0c\u4e3b\u8981\u4ece\u4e8bAI\u82af\u7247\u3001\u6df1\u5ea6\u5b66\u4e60\u3001\u884c\u4e3a\u68c0\u6d4b\u8bc6\u522b\u3001\u4eba\u8138\u68c0\u6d4b\u8bc6\u522b\u7b49\u76f8\u5173\u7814\u7a76\u5de5\u4f5c\u3002\n\n\u674e\u5251 \u526f\u4e3b\u7f16\n\n\u540c\u6d4e\u5927\u5b66\u8ba1\u7b97\u673a\u79d1\u5b66\u4e13\u4e1a\u535a\u58eb\uff0c\u6d59\u6c5f\u519c\u6797\u5927\u5b66\u526f\u6559\u6388\u3001\u7855\u58eb\u751f\u5bfc\u5e08\uff0c\u4e3b\u8981\u4ece\u4e8b\u63a8\u8350\u7cfb\u7edf\u3001\u6392\u5e8f\u5b66\u4e60\u3001\u51f8\u4f18\u5316\u7b49\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u79d1\u7814\u548c\u6559\u5b66\u5de5\u4f5c\uff0c\u53d1\u8868SCI\u8bba\u658710\u4f59\u7bc7\uff0c\u66fe\u83b7\u6d59\u6c5f\u7701\u79d1\u6280\u8fdb\u6b65\u4e8c\u7b49\u5956\u7b49\u591a\u9879\u7701\u90e8\u7ea7\u5956\u9879\u3002\n\n\u4f43\u677e\u5b9c \u526f\u4e3b\u7f16\n\n\u65e5\u672c\u4e1c\u5317\u5927\u5b66\u535a\u58eb\uff0c\u56db\u5ddd\u5927\u5b66\u7535\u6c14\u5de5\u7a0b\u5b66\u9662\u6559\u6388\u3001\u81ea\u52a8\u5316\u7cfb\u7cfb\u4e3b\u4efb\uff0c\u56db\u5ddd\u7701\u4fe1\u606f\u4e0e\u81ea\u52a8\u5316\u6280\u672f\u91cd\u70b9\u5b9e\u9a8c\u5ba4\u4e3b\u4efb\u3002\u4e3b\u8981\u4e13\u6ce8\u4e8e\u5148\u8fdb\u63a7\u5236\u7406\u8bba\u4e0e\u4eba\u5de5\u667a\u80fd\u7b97\u6cd5\u7814\u7a76\u3001\u5d4c\u5165\u5f0f\u8ba1\u7b97\u4e0e\u5b9e\u65f6\u667a\u80fd\u7cfb\u7edf\u7684\u7814\u7a76\u4e0e\u5f00\u53d1\u3001\u673a\u5668\u4eba\u4e0e\u667a\u80fd\u88c5\u5907\u7684\u667a\u80fd\u611f\u77e5\u4e0e\u63a7\u5236\u6280\u672f\u7684\u7814\u7a76\u3001\u5de5\u4e1a\u6d4b\u63a7\u4e0e\u667a\u80fd\u7269\u8054\u7684\u7814\u7a76\u3002\u8fd15\u5e74\u6765\u4e3b\u6301\u5305\u62ec\u56fd\u5bb6\u91cd\u70b9\u7814\u53d1\u8ba1\u5212\u3001\u57fa\u91d1\u3001\u56fd\u7f51\u603b\u90e8\u9879\u76ee\u7b49\u5404\u7c7b\u79d1\u7814\u9879\u76ee\u8fd130\u9879\uff0c\u7d2f\u8ba1\u603b\u7ecf\u8d39\u8fd12200\u4e07\u5143\uff1b\u53d1\u8868\u8bba\u6587100\u591a\u7bc7\uff0c\u5176\u4e2dSCI/EI\u68c0\u7d22\u8fd140\u7bc7\uff0cESI\u9ad8\u5f15\u8bba\u65871\u7bc7\u3002\u53c2\u4e0e\u7f16\u64b0\u4e13\u84573\u90e8\uff08\u5176\u4e2d\u82f1\u6587\u4e13\u84571\u90e8\uff09\uff0c\u53c2\u7f16\u56fd\u5bb6\u4e5d\u4e94\u89c4\u5212\u6559\u67501\u90e8\u3002\n\n\u738b\u664b\u4e1c \u7279\u9080\u7f16\u59d4\n\n\u4e2d\u79d1\u9662\u8ba1\u7b97\u6240\u535a\u58eb\uff0c\u5fae\u8f6f\u4e9a\u6d32\u7814\u7a76\u9662\u673a\u5668\u5b66\u4e60\u7814\u7a76\u5458\uff0c\u4e3b\u8981\u4ece\u4e8b\u8fc1\u79fb\u5b66\u4e60\u548c\u673a\u5668\u5b66\u4e60\u65b9\u5411\u7684\u7814\u7a76\u5de5\u4f5c\uff0c\u5728IJCAI\u3001CVPR\u3001ICDM\u3001UbiComp\u3001ACMMM\u3001PERCOM\u3001IJCNN\u3001PRICAI\u3001IEEE TNNLS\u3001NEUNET\u3001PRL\u3001PMCJ\u3001IMWUT\u3001IJMLC\u3001ICME\u3001ACM TIST\u7b49\u56fd\u9645\u6743\u5a01\u671f\u520a\u548c\u4f1a\u8bae\u4e0a\u53d1\u8868\u8bba\u658720\u4f59\u7bc7\uff0c\u591a\u6b21\u83b7\u5f97\u201c\u6700\u4f73\u8bba\u6587\u201d\u5956\u3002\u4f5c\u54c1\u6709\u300a\u8fc1\u79fb\u5b66\u4e60\u7b80\u660e\u624b\u518c\u300b\u7b49\u3002\n\n\u738b\u8d85\u950b\n\n\u4e0a\u6d77\u5927\u5b66\u7855\u58eb\uff0c\u767e\u5ea6\u8ba1\u7b97\u673a\u89c6\u89c9\u7b97\u6cd5\u5de5\u7a0b\u5e08\uff0c\u4e3b\u7814\u56fe\u50cf\u5904\u7406\uff0c\u6df1\u5ea6\u5b66\u4e60\u7b49\u65b9\u5411\u3002\u66fe\u591a\u6b21\u5728\u56fd\u5185\u5916\u5404\u7c7b\u77e5\u540d\u8ba1\u7b97\u673a\u89c6\u89c9\u6311\u6218\u8d5b\u4e2d\u83b7\u5f97\u4f18\u5f02\u6210\u7ee9\u3002\n\n\u90ed\u6653\u950b\n\n\u4e2d\u56fd\u79d1\u5b66\u9662\u7855\u58eb\uff0c\u7231\u5947\u827a\u7b97\u6cd5\u5de5\u7a0b\u5e08\uff0c\u4e3b\u8981\u4ece\u4e8b\u56fe\u50cf\u5904\u7406\u3001\u6df1\u5ea6\u5b66\u4e60\u7b49\u65b9\u5411\u7684\u7814\u7a76\uff0c\u66fe\u83b7\u201c2017\u534e\u4e3a\u8f6f\u4ef6\u7cbe\u82f1\u6311\u6218\u8d5b\u201d\u590d\u8d5b\u7b2c6\u540d\u3002\n\n\u9ec4\u4f1f\n\n\u534e\u5357\u7406\u5de5\u5927\u5b66\u7855\u58eb\uff0c\u987a\u4e30\u79d1\u6280\u673a\u5668\u4eba\u7b97\u6cd5\u5de5\u7a0b\u5e08\uff0c\u4e3b\u8981\u4ece\u4e8b\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u65b9\u5411\u7684\u7814\u7a76\u3002\u66fe\u5728\u76f8\u5173\u9886\u57df\u56fd\u9645\u671f\u520a\u3001\u4f1a\u8bae\u4e0a\u53d1\u8868\u8bba\u6587\uff0c\u5e76\u5728\u76f8\u5173\u7ade\u8d5b\u4e2d\u83b7\u5f97\u4f18\u5f02\u6210\u7ee9\u3002\n\n\u9648\u65b9\u6770\uff08Amusi\uff09\n\n\u4e0a\u6d77\u5927\u5b66\u7855\u58eb\uff0cCVer\uff08\u8ba1\u7b97\u673a\u89c6\u89c9\u77e5\u8bc6\u5206\u4eab\u548c\u5b66\u4e60\u4ea4\u6d41\u5e73\u53f0\uff09\u521b\u59cb\u4eba\u3002\n\n\u674e\u5143\u4f1f\n\n\u56fd\u9632\u79d1\u6280\u5927\u5b66\u7855\u58eb\uff0c\u6df1\u5733\u701a\u7ef4\u667a\u80fd\u533b\u7597\u79d1\u6280\u516c\u53f8\u9ad8\u7ea7\u7b97\u6cd5\u5de5\u7a0b\u5e08\uff0c\u5357\u65b9\u79d1\u6280\u5927\u5b66\u548c\u701a\u7ef4\u667a\u80fd\u533b\u7597\u8054\u5408\u5b9e\u9a8c\u5ba4\u59d4\u5458\uff0c\u4e3b\u8981\u4ece\u4e8b\u673a\u5668\u89c6\u89c9\u3001\u56fe\u50cf\u5904\u7406\u53ca\u6df1\u5ea6\u5b66\u4e60\u65b9\u5411\u7684\u7814\u7a76\u5de5\u4f5c\uff0c\u53c2\u7f16\u666e\u901a\u9ad8\u7b49\u6559\u80b2\u5341\u4e09\u4e94\u89c4\u5212\u6559\u6750\u300a\u56fe\u50cf\u901a\u4fe1\u57fa\u7840\u300b\uff0c\u5728IEEE TCSVT\u3001COGN COMPUT\u7b49\u53d1\u8868\u5b66\u672f\u8bba\u6587\u591a\u7bc7\uff0c\u66fe\u83b7\u6e56\u5357\u7701/\u5168\u519b\u4f18\u79c0\u7855\u58eb\u8bba\u6587\u3002\n\n\u9648\u7433\n\n\u5317\u4eac\u822a\u7a7a\u822a\u5929\u5927\u5b66\u535a\u58eb\uff08\u5728\u8bfb\uff09\uff0c\u7814\u7a76\u65b9\u5411\u4e3a\u65e0\u4eba\u673a\u667a\u80fd\u63a7\u5236\u4e0e\u51b3\u7b56\u3001\u4eff\u751f\u667a\u80fd\u8ba1\u7b97\u3002\n\n\n# \u76ee\u5f55\n\u7b2c1\u7ae0 \u6570\u5b66\u57fa\u7840 1\n\n1.1 \u5411\u91cf\u548c\u77e9\u9635 1\n\n1.1.1 \u6807\u91cf\u3001\u5411\u91cf\u3001\u77e9\u9635\u548c\u5f20\u91cf 1\n\n1.1.2 \u5f20\u91cf\u4e0e\u77e9\u9635\u7684\u533a\u522b 2\n\n1.1.3 \u77e9\u9635\u548c\u5411\u91cf\u76f8\u4e58\u7684\u7ed3\u679c 2\n\n1.1.4 \u5411\u91cf\u548c\u77e9\u9635\u7684\u8303\u6570\u5f52\u7eb3 2\n\n1.1.5 \u5224\u65ad\u4e00\u4e2a\u77e9\u9635\u662f\u5426\u4e3a\u6b63\u5b9a\u77e9\u9635 4\n\n1.2 \u5bfc\u6570\u548c\u504f\u5bfc\u6570 5\n\n1.2.1 \u5bfc\u6570\u504f\u5bfc\u8ba1\u7b97 5\n\n1.2.2 \u5bfc\u6570\u548c\u504f\u5bfc\u6570\u7684\u533a\u522b 6\n\n1.3 \u7279\u5f81\u503c\u548c\u7279\u5f81\u5411\u91cf 6\n\n1.3.1 \u7279\u5f81\u503c\u5206\u89e3 6\n\n1.3.2 \u5947\u5f02\u503c\u548c\u7279\u5f81\u503c\u7684\u5173\u7cfb 6\n\n1.4 \u6982\u7387\u5206\u5e03\u4e0e\u968f\u673a\u53d8\u91cf 7\n\n1.4.1 \u673a\u5668\u5b66\u4e60\u4e3a\u4ec0\u4e48\u8981\u4f7f\u7528\u6982\u7387 7\n\n1.4.2 \u53d8\u91cf\u4e0e\u968f\u673a\u53d8\u91cf\u7684\u533a\u522b 7\n\n1.4.3 \u968f\u673a\u53d8\u91cf\u4e0e\u6982\u7387\u5206\u5e03\u7684\u8054\u7cfb 8\n\n1.4.4 \u79bb\u6563\u578b\u968f\u673a\u53d8\u91cf\u548c\u6982\u7387\u8d28\u91cf\u51fd\u6570 8\n\n1.4.5 \u8fde\u7eed\u578b\u968f\u673a\u53d8\u91cf\u548c\u6982\u7387\u5bc6\u5ea6\u51fd\u6570 8\n\n1.4.6 \u4e3e\u4f8b\u7406\u89e3\u6761\u4ef6\u6982\u7387 9\n\n1.4.7 \u8054\u5408\u6982\u7387\u4e0e\u8fb9\u7f18\u6982\u7387\u7684\u533a\u522b\u548c\u8054\u7cfb 9\n\n1.4.8 \u6761\u4ef6\u6982\u7387\u7684\u94fe\u5f0f\u6cd5\u5219 10\n\n1.4.9 \u72ec\u7acb\u6027\u548c\u6761\u4ef6\u72ec\u7acb\u6027 10\n\n1.5 \u5e38\u89c1\u6982\u7387\u5206\u5e03 11\n\n1.5.1 \u4f2f\u52aa\u5229\u5206\u5e03 11\n\n1.5.2 \u9ad8\u65af\u5206\u5e03 11\n\n1.5.3 \u4f55\u65f6\u91c7\u7528\u6b63\u6001\u5206\u5e03 12\n\n1.5.4 \u6307\u6570\u5206\u5e03 12\n\n1.5.5 Laplace\u5206\u5e03 13\n\n1.5.6 Dirac\u5206\u5e03\u548c\u7ecf\u9a8c\u5206\u5e03 13\n\n1.6 \u671f\u671b\u3001\u65b9\u5dee\u3001\u534f\u65b9\u5dee\u3001\u76f8\u5173\u7cfb\u6570 13\n\n1.6.1 \u671f\u671b 13\n\n1.6.2 \u65b9\u5dee 14\n\n1.6.3 \u534f\u65b9\u5dee 14\n\n1.6.4 \u76f8\u5173\u7cfb\u6570 15\n\n\u7b2c2\u7ae0 \u673a\u5668\u5b66\u4e60\u57fa\u7840 16\n\n2.1 \u57fa\u672c\u6982\u5ff5 16\n\n2.1.1 \u5927\u8bdd\u673a\u5668\u5b66\u4e60\u672c\u8d28 16\n\n2.1.2 \u4ec0\u4e48\u662f\u795e\u7ecf\u7f51\u7edc 16\n\n2.1.3 \u5404\u79cd\u5e38\u89c1\u7b97\u6cd5\u56fe\u793a 17\n\n2.1.4 \u8ba1\u7b97\u56fe\u7684\u5bfc\u6570\u8ba1\u7b97 17\n\n2.1.5 \u7406\u89e3\u5c40\u90e8\u6700\u4f18\u4e0e\u5168\u5c40\u6700\u4f18 18\n\n2.1.6 \u5927\u6570\u636e\u4e0e\u6df1\u5ea6\u5b66\u4e60\u4e4b\u95f4\u7684\u5173\u7cfb 19\n\n2.2 \u673a\u5668\u5b66\u4e60\u7684\u5b66\u4e60\u65b9\u5f0f 20\n\n2.2.1 \u76d1\u7763\u5b66\u4e60 20\n\n2.2.2 \u975e\u76d1\u7763\u5b66\u4e60 20\n\n2.2.3 \u534a\u76d1\u7763\u5b66\u4e60 20\n\n2.2.4 \u5f31\u76d1\u7763\u5b66\u4e60 20\n\n2.2.5 \u76d1\u7763\u5b66\u4e60\u6a21\u578b\u7684\u642d\u5efa\u6b65\u9aa4 21\n\n2.3 \u5206\u7c7b\u7b97\u6cd5 22\n\n2.3.1 \u5e38\u7528\u5206\u7c7b\u7b97\u6cd5\u7684\u4f18\u7f3a\u70b9 22\n\n2.3.2 \u5206\u7c7b\u7b97\u6cd5\u7684\u8bc4\u4f30\u65b9\u6cd5 23\n\n2.3.3 \u6b63\u786e\u7387\u80fd\u5426\u5f88\u597d\u5730\u8bc4\u4f30\u5206\u7c7b\u7b97\u6cd5 25\n\n2.3.4 \u4ec0\u4e48\u6837\u7684\u5206\u7c7b\u5668\u662f\u6700\u597d\u7684 26\n\n2.4 \u903b\u8f91\u56de\u5f52 26\n\n2.4.1 \u56de\u5f52\u7684\u79cd\u7c7b 26\n\n2.4.2 \u903b\u8f91\u56de\u5f52\u9002\u7528\u6027 27\n\n2.4.3 \u903b\u8f91\u56de\u5f52\u4e0e\u6734\u7d20\u8d1d\u53f6\u65af\u7684\u533a\u522b 27\n\n2.4.4 \u7ebf\u6027\u56de\u5f52\u4e0e\u903b\u8f91\u56de\u5f52\u7684\u533a\u522b 27\n\n2.5 \u4ee3\u4ef7\u51fd\u6570 28\n\n2.5.1 \u4e3a\u4ec0\u4e48\u9700\u8981\u4ee3\u4ef7\u51fd\u6570 28\n\n2.5.2 \u4ee3\u4ef7\u51fd\u6570\u4f5c\u7528\u539f\u7406 28\n\n2.5.3 \u5e38\u89c1\u4ee3\u4ef7\u51fd\u6570 30\n\n2.5.4 \u4e3a\u4ec0\u4e48\u4ee3\u4ef7\u51fd\u6570\u8981\u975e\u8d1f 31\n\n2.5.5 \u4e3a\u4ec0\u4e48\u7528\u4ea4\u53c9\u71b5\u4ee3\u66ff\u4e8c\u6b21\u4ee3\u4ef7\u51fd\u6570 31\n\n2.6 \u635f\u5931\u51fd\u6570 32\n\n2.6.1 \u4ec0\u4e48\u662f\u635f\u5931\u51fd\u6570 32\n\n2.6.2 \u5e38\u89c1\u7684\u635f\u5931\u51fd\u6570 32\n\n2.6.3 \u903b\u8f91\u56de\u5f52\u4e3a\u4ec0\u4e48\u4f7f\u7528\u5bf9\u6570\u635f\u5931\u51fd\u6570 34\n\n2.6.4 \u5bf9\u6570\u635f\u5931\u51fd\u6570\u5982\u4f55\u5ea6\u91cf\u635f\u5931 34\n\n2.7 \u68af\u5ea6\u4e0b\u964d\u6cd5 35\n\n2.7.1 \u68af\u5ea6\u4e0b\u964d\u6cd5\u7684\u4f5c\u7528 36\n\n2.7.2 \u68af\u5ea6\u4e0b\u964d\u6cd5\u7684\u76f4\u89c2\u7406\u89e3 36\n\n2.7.3 \u68af\u5ea6\u4e0b\u964d\u6cd5\u7b97\u6cd5\u63cf\u8ff0 37\n\n2.7.4 \u68af\u5ea6\u4e0b\u964d\u6cd5\u7684\u7f3a\u70b9 38\n\n2.7.5 \u5982\u4f55\u5bf9\u68af\u5ea6\u4e0b\u964d\u6cd5\u8fdb\u884c\u8c03\u4f18 38\n\n2.7.6 \u968f\u673a\u68af\u5ea6\u4e0b\u964d\u548c\u6279\u91cf\u68af\u5ea6\u4e0b\u964d\u7684\u533a\u522b 38\n\n2.7.7 \u5404\u79cd\u68af\u5ea6\u4e0b\u964d\u6cd5\u6027\u80fd\u6bd4\u8f83 40\n\n2.8 \u7ebf\u6027\u5224\u522b\u5206\u6790 40\n\n2.8.1 LDA\u601d\u60f3\u603b\u7ed3 40\n\n2.8.2 \u56fe\u89e3LDA\u6838\u5fc3\u601d\u60f3 41\n\n2.8.3 \u4e8c\u7c7bLDA\u7b97\u6cd5\u539f\u7406 41\n\n2.8.4 LDA\u7b97\u6cd5\u6d41\u7a0b\u603b\u7ed3 42\n\n2.8.5 LDA\u548cPCA\u7684\u5f02\u540c 43\n\n2.8.6 LDA\u7684\u4f18\u7f3a\u70b9 43\n\n2.9 \u4e3b\u6210\u5206\u5206\u6790 43\n\n2.9.1 \u56fe\u89e3PCA\u6838\u5fc3\u601d\u60f3 43\n\n2.9.2 PCA\u7b97\u6cd5\u63a8\u7406 44\n\n2.9.3 PCA\u7b97\u6cd5\u6d41\u7a0b\u603b\u7ed3 45\n\n2.9.4 PCA\u601d\u60f3\u603b\u7ed3 46\n\n2.9.5 PCA\u7b97\u6cd5\u7684\u4f18\u7f3a\u70b9 46\n\n2.9.6 \u964d\u7ef4\u7684\u5fc5\u8981\u6027\u53ca\u76ee\u7684 46\n\n2.9.7 KPCA\u4e0ePCA\u7684\u533a\u522b 47\n\n2.10 \u6a21\u578b\u8bc4\u4f30 47\n\n2.10.1 \u6a21\u578b\u8bc4\u4f30\u5e38\u7528\u65b9\u6cd5 48\n\n2.10.2 \u8bef\u5dee\u3001\u504f\u5dee\u548c\u65b9\u5dee\u7684\u533a\u522b\u548c\u8054\u7cfb 48\n\n2.10.3 \u4e3a\u4ec0\u4e48\u4f7f\u7528\u6807\u51c6\u5dee 49\n\n2.10.4 \u7ecf\u9a8c\u8bef\u5dee\u4e0e\u6cdb\u5316\u8bef\u5dee 50\n\n2.10.5 \u56fe\u89e3\u6b20\u62df\u5408\u4e0e\u8fc7\u62df\u5408 50\n\n2.10.6 \u5982\u4f55\u89e3\u51b3\u6b20\u62df\u5408\u4e0e\u8fc7\u62df\u5408 52\n\n2.10.7 \u4ea4\u53c9\u9a8c\u8bc1\u7684\u4e3b\u8981\u4f5c\u7528 52\n\n2.10.8 \u7406\u89e3K\u6298\u4ea4\u53c9\u9a8c\u8bc1 53\n\n2.10.9 \u7406\u89e3\u6df7\u6dc6\u77e9\u9635 53\n\n2.10.10 \u7406\u89e3\u67e5\u51c6\u7387\u4e0e\u67e5\u5168\u7387 53\n\n2.10.11 \u7406\u89e3ROC\u4e0eAUC 54\n\n2.10.12 \u5982\u4f55\u7ed8\u5236ROC\u66f2\u7ebf 55\n\n2.10.13 \u5982\u4f55\u8ba1\u7b97TPR\u548cFPR 56\n\n2.10.14 \u5982\u4f55\u8ba1\u7b97AUC 58\n\n2.10.15 \u76f4\u89c2\u7406\u89e3AUC 58\n\n2.10.16 ROC\u8bc4\u4f30\u5206\u7c7b\u5668 60\n\n2.10.17 \u4ee3\u4ef7\u654f\u611f\u9519\u8bef\u7387\u4e0e\u4ee3\u4ef7\u66f2\u7ebf 60\n\n2.10.18 \u6bd4\u8f83\u68c0\u9a8c\u65b9\u6cd5 61\n\n2.11 \u51b3\u7b56\u6811 61\n\n2.11.1 \u51b3\u7b56\u6811\u7684\u57fa\u672c\u539f\u7406 62\n\n2.11.2 \u51b3\u7b56\u6811\u7684\u751f\u6210\u8fc7\u7a0b 62\n\n2.11.3 \u51b3\u7b56\u6811\u5b66\u4e60\u57fa\u672c\u7b97\u6cd5\u6b65\u9aa4 62\n\n2.11.4 \u51b3\u7b56\u6811\u7b97\u6cd5\u7684\u4f18\u7f3a\u70b9 63\n\n2.11.5 \u51b3\u7b56\u6811\u548c\u71b5\u7684\u8054\u7cfb 63\n\n2.11.6 \u71b5\u7684\u6982\u5ff5\u53ca\u5b9a\u4e49 63\n\n2.11.7 \u7406\u89e3\u4fe1\u606f\u589e\u76ca 64\n\n2.11.8 \u51b3\u7b56\u6811\u4e2d\u71b5\u3001\u6761\u4ef6\u71b5\u548c\u4fe1\u606f\u589e\u76ca\u7684\u8054\u7cfb 64\n\n2.11.9 \u51b3\u7b56\u6811\u7b97\u6cd5\u4e2d\u526a\u679d\u7684\u4f5c\u7528\u53ca\u7b56\u7565 65\n\n2.12 \u652f\u6301\u5411\u91cf\u673a\uff08SVM\uff09 65\n\n2.12.1 \u4ec0\u4e48\u662fSVM 65\n\n2.12.2 SVM\u80fd\u89e3\u51b3\u7684\u95ee\u9898 66\n\n2.12.3 \u6838\u51fd\u6570\u7279\u70b9\u53ca\u5176\u4f5c\u7528 67\n\n2.12.4 SVM\u4e3a\u4ec0\u4e48\u5f15\u5165\u5bf9\u5076\u95ee\u9898 67\n\n2.12.5 \u5982\u4f55\u7406\u89e3SVM\u4e2d\u7684\u5bf9\u5076\u95ee\u9898 67\n\n2.12.6 \u5e38\u89c1\u7684\u6838\u51fd\u6570 69\n\n2.12.7 SVM\u7684\u4e3b\u8981\u7279\u70b9 69\n\n2.12.8 SVM\u7684\u4e3b\u8981\u7f3a\u70b9 70\n\n2.12.9 \u903b\u8f91\u56de\u5f52\u4e0eSVM\u7684\u5f02\u540c 70\n\n2.13 \u8d1d\u53f6\u65af\u5206\u7c7b\u5668 72\n\n2.13.1 \u8d1d\u53f6\u65af\u5206\u7c7b\u5668\u7684\u57fa\u672c\u539f\u7406 72\n\n2.13.2 \u6734\u7d20\u8d1d\u53f6\u65af\u5206\u7c7b\u5668 72\n\n2.13.3 \u4e3e\u4f8b\u7406\u89e3\u6734\u7d20\u8d1d\u53f6\u65af\u5206\u7c7b\u5668 73\n\n2.13.4 \u534a\u6734\u7d20\u8d1d\u53f6\u65af\u5206\u7c7b\u5668 75\n\n2.13.5 \u6781\u5927\u4f3c\u7136\u4f30\u8ba1\u548c\u8d1d\u53f6\u65af\u4f30\u8ba1\u7684\u8054\u7cfb\u4e0e\u533a\u522b 75\n\n2.13.6 \u6781\u5927\u4f3c\u7136\u4f30\u8ba1\u539f\u7406 76\n\n2.13.7 \u56fe\u89e3\u6781\u5927\u4f3c\u7136\u4f30\u8ba1 76\n\n2.14 EM\u7b97\u6cd5 77\n\n2.14.1 EM\u7b97\u6cd5\u7684\u57fa\u672c\u601d\u60f3 77\n\n2.14.2 EM\u7b97\u6cd5\u63a8\u5bfc 77\n\n2.14.3 \u56fe\u89e3EM\u7b97\u6cd5 78\n\n2.14.4 EM\u7b97\u6cd5\u6d41\u7a0b 79\n\n2.15 \u964d\u7ef4\u548c\u805a\u7c7b 79\n\n2.15.1 \u56fe\u89e3\u4e3a\u4ec0\u4e48\u4f1a\u4ea7\u751f\u7ef4\u6570\u707e\u96be 79\n\n2.15.2 \u600e\u6837\u907f\u514d\u7ef4\u6570\u707e\u96be 83\n\n2.15.3 \u805a\u7c7b\u548c\u964d\u7ef4 83\n\n2.15.4 \u805a\u7c7b\u7b97\u6cd5\u4f18\u52a3\u7684\u8861\u91cf\u6807\u51c6 84\n\n2.15.5 \u805a\u7c7b\u548c\u5206\u7c7b 85\n\n2.15.6 \u805a\u7c7b\u7b97\u6cd5\u7684\u6027\u80fd\u6bd4\u8f83 85\n\n2.15.7 4\u79cd\u5e38\u7528\u805a\u7c7b\u65b9\u6cd5\u6bd4\u8f83 85\n\n\u7b2c3\u7ae0 \u6df1\u5ea6\u5b66\u4e60\u57fa\u7840 89\n\n3.1 \u57fa\u672c\u6982\u5ff5 89\n\n3.1.1 \u795e\u7ecf\u7f51\u7edc\u7684\u7c7b\u578b 89\n\n3.1.2 \u795e\u7ecf\u7f51\u7edc\u7684\u5e38\u7528\u6a21\u578b\u7ed3\u6784 92\n\n3.1.3 \u6df1\u5ea6\u5b66\u4e60\u548c\u673a\u5668\u5b66\u4e60\u7684\u533a\u522b\u4e0e\u8054\u7cfb 93\n\n3.1.4 \u4e3a\u4ec0\u4e48\u4f7f\u7528\u6df1\u5c42\u8868\u793a 93\n\n3.1.5 \u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u5206\u7c7b 94\n\n3.1.6 \u5982\u4f55\u9009\u62e9\u6df1\u5ea6\u5b66\u4e60\u5f00\u53d1\u5e73\u53f0 94\n\n3.2 \u795e\u7ecf\u7f51\u7edc\u8ba1\u7b97 95\n\n3.2.1 \u524d\u5411\u4f20\u64ad\u548c\u53cd\u5411\u4f20\u64ad 95\n\n3.2.2 \u5982\u4f55\u8ba1\u7b97\u795e\u7ecf\u7f51\u7edc\u7684\u8f93\u51fa 96\n\n3.2.3 \u5982\u4f55\u8ba1\u7b97\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8f93\u51fa\u503c 97\n\n3.2.4 \u5982\u4f55\u8ba1\u7b97\u6c60\u5316\u5c42\u8f93\u51fa\u503c 100\n\n3.2.5 \u53cd\u5411\u4f20\u64ad\u5b9e\u4f8b 101\n\n3.2.6 \u795e\u7ecf\u7f51\u7edc\u66f4\u201c\u6df1\u201d\u7684\u610f\u4e49 104\n\n3.3 \u6fc0\u6d3b\u51fd\u6570 104\n\n3.3.1 \u4e3a\u4ec0\u4e48\u9700\u8981\u6fc0\u6d3b\u51fd\u6570 104\n\n3.3.2 \u4e3a\u4ec0\u4e48\u6fc0\u6d3b\u51fd\u6570\u9700\u8981\u975e\u7ebf\u6027\u51fd\u6570 105\n\n3.3.3 \u5e38\u89c1\u7684\u6fc0\u6d3b\u51fd\u6570\u53ca\u5176\u56fe\u50cf 105\n\n3.3.4 \u5e38\u89c1\u6fc0\u6d3b\u51fd\u6570\u7684\u5bfc\u6570\u8ba1\u7b97 107\n\n3.3.5 \u6fc0\u6d3b\u51fd\u6570\u6709\u54ea\u4e9b\u6027\u8d28 108\n\n3.3.6 \u5982\u4f55\u9009\u62e9\u6fc0\u6d3b\u51fd\u6570 108\n\n3.3.7 \u4e3a\u4ec0\u4e48tanh\u6536\u655b\u901f\u5ea6\u6bd4sigmoid\u5feb 109\n\n3.3.8 Relu\u6fc0\u6d3b\u51fd\u6570\u7684\u4f18\u70b9 109\n\n3.3.9 \u7406\u89e3Relu\u6fc0\u6d3b\u51fd\u6570\u7684\u7a00\u758f\u6fc0\u6d3b\u6027 109\n\n3.3.10 \u4ec0\u4e48\u65f6\u5019\u53ef\u4ee5\u7528\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570 109\n\n3.3.11 softmax\u51fd\u6570\u7684\u5b9a\u4e49\u53ca\u4f5c\u7528 110\n\n3.3.12 softmax\u51fd\u6570\u5982\u4f55\u5e94\u7528\u4e8e\u591a\u5206\u7c7b 110\n\n3.4 BATCH SIZE 112\n\n3.4.1 \u4e3a\u4ec0\u4e48\u9700\u8981Batch Size 112\n\n3.4.2 \u5982\u4f55\u9009\u62e9Batch Size\u503c 112\n\n3.4.3 \u8c03\u8282Batch Size\u5bf9\u8bad\u7ec3\u6548\u679c\u7684\u5f71\u54cd 113\n\n3.4.4 \u5728\u5408\u7406\u8303\u56f4\u5185\u589e\u5927Batch Size\u7684\u597d\u5904 113\n\n3.4.5 \u76f2\u76ee\u589e\u5927Batch Size\u7684\u574f\u5904 114\n\n3.5 \u5f52\u4e00\u5316 114\n\n3.5.1 \u7406\u89e3\u5f52\u4e00\u5316\u542b\u4e49 114\n\n3.5.2 \u5f52\u4e00\u5316\u548c\u6807\u51c6\u5316\u7684\u8054\u7cfb\u4e0e\u533a\u522b 114\n\n3.5.3 \u4e3a\u4ec0\u4e48\u8981\u5f52\u4e00\u5316\u6216\u6807\u51c6\u5316 115\n\n3.5.4 \u56fe\u89e3\u4e3a\u4ec0\u4e48\u8981\u5f52\u4e00\u5316 115\n\n3.5.5 \u4e3a\u4ec0\u4e48\u5f52\u4e00\u5316\u80fd\u63d0\u9ad8\u6c42\u6700\u4f18\u89e3\u901f\u5ea6 115\n\n3.5.6 \u5f52\u4e00\u5316\u6709\u54ea\u4e9b\u7c7b\u578b 116\n\n3.5.7 \u5c40\u90e8\u54cd\u5e94\u5f52\u4e00\u5316\u4f5c\u7528 116\n\n3.5.8 \u5c40\u90e8\u54cd\u5e94\u5f52\u4e00\u5316\u539f\u7406 117\n\n3.5.9 \u4ec0\u4e48\u662f\u6279\u5f52\u4e00\u5316 118\n\n3.5.10 \u6279\u5f52\u4e00\u5316\u7684\u4f18\u70b9 118\n\n3.5.11 \u6279\u5f52\u4e00\u5316\u7b97\u6cd5\u6d41\u7a0b 118\n\n3.5.12 \u6279\u5f52\u4e00\u5316\u548c\u7ec4\u5f52\u4e00\u5316\u6bd4\u8f83 119\n\n3.5.13 \u6743\u91cd\u5f52\u4e00\u5316\u548c\u6279\u5f52\u4e00\u5316\u6bd4\u8f83 119\n\n3.5.14 \u6279\u5f52\u4e00\u5316\u9002\u7528\u8303\u56f4 120\n\n3.5.15 BN\u3001LN\u3001IN\u548cGN\u7684\u5bf9\u6bd4 120\n\n3.6 \u53c2\u6570\u521d\u59cb\u5316 121\n\n3.6.1 \u53c2\u6570\u521d\u59cb\u5316\u5e94\u6ee1\u8db3\u7684\u6761\u4ef6 121\n\n3.6.2 \u5e38\u7528\u7684\u51e0\u79cd\u521d\u59cb\u5316\u65b9\u5f0f 121\n\n3.6.3 \u51680\u521d\u59cb\u5316\u5e26\u6765\u7684\u95ee\u9898 121\n\n3.6.4 \u5168\u90fd\u521d\u59cb\u5316\u4e3a\u540c\u6837\u7684\u503c 122\n\n3.6.5 \u521d\u59cb\u5316\u4e3a\u5c0f\u7684\u968f\u673a\u6570 123\n\n3.6.6 \u7528 \u6821\u51c6\u65b9\u5dee 123\n\n3.7 \u9884\u8bad\u7ec3\u4e0e\u5fae\u8c03 123\n\n3.7.1 \u4ec0\u4e48\u662f\u9884\u8bad\u7ec3\u548c\u5fae\u8c03 123\n\n3.7.2 \u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u7684\u4f5c\u7528 124\n\n3.7.3 \u9884\u8bad\u7ec3\u6a21\u578b\u7684\u590d\u7528 124\n\n3.7.4 \u9884\u8bad\u7ec3\u548c\u8fc1\u79fb\u5b66\u4e60 125\n\n3.7.5 \u5fae\u8c03\u65f6\u7f51\u7edc\u53c2\u6570\u662f\u5426\u66f4\u65b0 125\n\n3.7.6 \u5fae\u8c03\u6a21\u578b\u7684\u4e09\u79cd\u72b6\u6001 125\n\n3.7.7 \u4e3a\u4ec0\u4e48\u6df1\u5c42\u795e\u7ecf\u7f51\u7edc\u96be\u4ee5\u8bad\u7ec3 125\n\n3.8 \u8d85\u53c2\u6570 127\n\n3.8.1 \u8d85\u53c2\u6570\u6709\u54ea\u4e9b 127\n\n3.8.2 \u53c2\u6570\u548c\u6a21\u578b\u7684\u5173\u7cfb 127\n\n3.8.3 \u53c2\u6570\u548c\u8d85\u53c2\u6570\u7684\u533a\u522b 127\n\n3.8.4 \u5982\u4f55\u5bfb\u627e\u8d85\u53c2\u6570\u7684\u6700\u4f18\u503c 128\n\n3.8.5 \u8d85\u53c2\u6570\u641c\u7d22\u7684\u4e00\u822c\u8fc7\u7a0b 128\n\n3.9 \u5b66\u4e60\u7387 129\n\n3.9.1 \u5b66\u4e60\u7387\u7684\u4f5c\u7528 129\n\n3.9.2 \u5b66\u4e60\u7387\u8870\u51cf\u7684\u5e38\u7528\u53c2\u6570 129\n\n3.9.3 \u5e38\u7528\u7684\u5b66\u4e60\u7387\u8870\u51cf\u65b9\u6cd5 129\n\n3.10 \u6b63\u5219\u5316 133\n\n3.10.1 \u4e3a\u4ec0\u4e48\u8981\u6b63\u5219\u5316 133\n\n3.10.2 \u5e38\u89c1\u6b63\u5219\u5316\u65b9\u6cd5 133\n\n3.10.3 \u56fe\u89e3L1\u548cL2\u6b63\u5219\u5316 134\n\n3.10.4 Dropout\u5177\u4f53\u5de5\u4f5c\u6d41\u7a0b 135\n\n3.10.5 \u4e3a\u4ec0\u4e48Dropout\u53ef\u4ee5\u89e3\u51b3\u8fc7\u62df\u5408\u95ee\u9898 137\n\n3.10.6 Dropout\u7684\u7f3a\u70b9\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 1198539,
    "name": "netron",
    "full_name": "lutzroeder/netron",
    "description": "Visualizer for neural network, deep learning and machine learning models",
    "html_url": "https://github.com/lutzroeder/netron",
    "clone_url": "https://github.com/lutzroeder/netron.git",
    "owner_login": "lutzroeder",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/438516?v=4",
    "stargazers_count": 31115,
    "watchers_count": 31115,
    "forks_count": 2965,
    "open_issues_count": 23,
    "size": 63182,
    "language": "JavaScript",
    "languages": {
      "JavaScript": 6790095,
      "Python": 172677,
      "HTML": 52052,
      "Shell": 46313,
      "CSS": 6511
    },
    "topics": [
      "ai",
      "coreml",
      "deep-learning",
      "deeplearning",
      "keras",
      "machine-learning",
      "machinelearning",
      "ml",
      "neural-network",
      "numpy",
      "onnx",
      "pytorch",
      "safetensors",
      "tensorflow",
      "tensorflow-lite",
      "visualizer"
    ],
    "license_name": "MIT License",
    "created_at": "2010-12-26T12:53:43+00:00",
    "updated_at": "2025-08-06T01:46:31+00:00",
    "pushed_at": "2025-08-05T03:22:56+00:00",
    "contributors_count": 1,
    "readme_length": 3119,
    "readme_content": "<div align=\"center\">\n<img width=\"400px\" height=\"100px\" src=\"https://github.com/lutzroeder/netron/raw/main/.github/logo-light.svg#gh-light-mode-only\">\n<img width=\"400px\" height=\"100px\" src=\"https://github.com/lutzroeder/netron/raw/main/.github/logo-dark.svg#gh-dark-mode-only\">\n</div>\n\nNetron is a viewer for neural network, deep learning and machine learning models.\n\nNetron supports ONNX, TensorFlow Lite, Core ML, Keras, Caffe, Darknet, PyTorch, TensorFlow.js, Safetensors and NumPy.\n\nNetron has experimental support for TorchScript, torch.export, ExecuTorch, TensorFlow, OpenVINO, RKNN, ncnn, MNN, PaddlePaddle, GGUF and scikit-learn.\n\n<p align='center'><a href='https://www.lutzroeder.com/ai'><img src='.github/screenshot.png' width='800'></a></p>\n\n## Install\n\n**macOS**: [**Download**](https://github.com/lutzroeder/netron/releases/latest) the `.dmg` file or run `brew install --cask netron`\n\n**Linux**: [**Download**](https://github.com/lutzroeder/netron/releases/latest) the `.AppImage` file or run `snap install netron`\n\n**Windows**: [**Download**](https://github.com/lutzroeder/netron/releases/latest) the `.exe` installer or run `winget install -s winget netron`\n\n**Browser**: [**Start**](https://netron.app) the browser version.\n\n**Python**: `pip install netron`, then run `netron [FILE]` or `netron.start('[FILE]')`.\n\n## Models\n\nSample model files to download or open using the browser version:\n\n * **ONNX**: [squeezenet](https://github.com/onnx/models/raw/main/validated/vision/classification/squeezenet/model/squeezenet1.0-3.onnx) [[open](https://netron.app?url=https://github.com/onnx/models/raw/main/validated/vision/classification/squeezenet/model/squeezenet1.0-3.onnx)]\n * **TorchScript**: [traced_online_pred_layer](https://github.com/ApolloAuto/apollo/raw/master/modules/prediction/data/traced_online_pred_layer.pt) [[open](https://netron.app?url=https://github.com/ApolloAuto/apollo/raw/master/modules/prediction/data/traced_online_pred_layer.pt)]\n * **TensorFlow Lite**: [yamnet](https://huggingface.co/thelou1s/yamnet/resolve/main/lite-model_yamnet_tflite_1.tflite) [[open](https://netron.app?url=https://huggingface.co/thelou1s/yamnet/blob/main/lite-model_yamnet_tflite_1.tflite)]\n * **TensorFlow**: [chessbot](https://github.com/srom/chessbot/raw/master/model/chessbot.pb) [[open](https://netron.app?url=https://github.com/srom/chessbot/raw/master/model/chessbot.pb)]\n * **Keras**: [mobilenet](https://github.com/aio-libs/aiohttp-demos/raw/master/demos/imagetagger/tests/data/mobilenet.h5) [[open](https://netron.app?url=https://github.com/aio-libs/aiohttp-demos/raw/master/demos/imagetagger/tests/data/mobilenet.h5)]\n * **Core ML**: [exermote](https://github.com/Lausbert/Exermote/raw/master/ExermoteInference/ExermoteCoreML/ExermoteCoreML/Model/Exermote.mlmodel) [[open](https://netron.app?url=https://github.com/Lausbert/Exermote/raw/master/ExermoteInference/ExermoteCoreML/ExermoteCoreML/Model/Exermote.mlmodel)]\n * **Darknet**: [yolo](https://github.com/AlexeyAB/darknet/raw/master/cfg/yolo.cfg) [[open](https://netron.app?url=https://github.com/AlexeyAB/darknet/raw/master/cfg/yolo.cfg)]\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 18666758,
    "name": "darknet",
    "full_name": "pjreddie/darknet",
    "description": "Convolutional Neural Networks",
    "html_url": "https://github.com/pjreddie/darknet",
    "clone_url": "https://github.com/pjreddie/darknet.git",
    "owner_login": "pjreddie",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/302108?v=4",
    "stargazers_count": 26268,
    "watchers_count": 26268,
    "forks_count": 21286,
    "open_issues_count": 1983,
    "size": 6526,
    "language": "C",
    "languages": {
      "C": 843257,
      "Cuda": 72651,
      "Python": 8516,
      "C++": 3138,
      "Makefile": 3040,
      "Shell": 1947
    },
    "topics": [],
    "license_name": "Other",
    "created_at": "2014-04-11T07:59:16+00:00",
    "updated_at": "2025-08-05T18:59:56+00:00",
    "pushed_at": "2024-05-03T13:53:58+00:00",
    "contributors_count": 7,
    "readme_length": 5072,
    "readme_content": "![Darknet Logo](http://pjreddie.com/media/files/darknet-black-small.png)\n\n# Darknet #\nDarknet is an open source neural network framework written in C and CUDA. It is fast, easy to install, and supports CPU and GPU computation.\n\n**Discord** invite link for for communication and questions: https://discord.gg/zSq8rtW\n\n## YOLOv7: \n\n* **paper** - YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors: https://arxiv.org/abs/2207.02696\n\n* **source code - Pytorch (use to reproduce results):** https://github.com/WongKinYiu/yolov7\n\n----\n\nOfficial YOLOv7 is more accurate and faster than YOLOv5 by **120%** FPS, than YOLOX by **180%** FPS, than Dual-Swin-T by **1200%** FPS, than ConvNext by **550%** FPS, than SWIN-L by **500%** FPS.\n\nYOLOv7 surpasses all known object detectors in both speed and accuracy in the range from 5 FPS to 160 FPS and has the highest accuracy 56.8% AP among all known real-time object detectors with 30 FPS or higher on GPU V100, batch=1.\n\n* YOLOv7-e6 (55.9% AP, 56 FPS V100 b=1) by `+500%` FPS faster than SWIN-L Cascade-Mask R-CNN (53.9% AP, 9.2 FPS A100 b=1)\n* YOLOv7-e6 (55.9% AP, 56 FPS V100 b=1) by `+550%` FPS faster than ConvNeXt-XL C-M-RCNN (55.2% AP, 8.6 FPS A100 b=1)\n* YOLOv7-w6 (54.6% AP, 84 FPS V100 b=1) by `+120%` FPS faster than YOLOv5-X6-r6.1 (55.0% AP, 38 FPS V100 b=1)\n* YOLOv7-w6 (54.6% AP, 84 FPS V100 b=1) by `+1200%` FPS faster than Dual-Swin-T C-M-RCNN (53.6% AP, 6.5 FPS V100 b=1)\n* YOLOv7x (52.9% AP, 114 FPS V100 b=1) by `+150%` FPS faster than PPYOLOE-X (51.9% AP, 45 FPS V100 b=1)\n* YOLOv7 (51.2% AP, 161 FPS V100 b=1) by `+180%` FPS faster than YOLOX-X (51.1% AP, 58 FPS V100 b=1)\n\n----\n\n![more5](https://user-images.githubusercontent.com/4096485/179425274-f55a36d4-8450-4471-816b-8c105841effd.jpg)\n\n----\n\n![image](https://user-images.githubusercontent.com/4096485/177675030-a929ee00-0eba-4d93-95c2-225231d0fd61.png)\n\n\n----\n\n![yolov7_640_1280](https://user-images.githubusercontent.com/4096485/177688869-d75e0c36-63af-46ec-bdbd-81dbb281f257.png)\n\n----\n\n## Scaled-YOLOv4: \n\n* **paper (CVPR 2021)**: https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Scaled-YOLOv4_Scaling_Cross_Stage_Partial_Network_CVPR_2021_paper.html\n\n* **source code - Pytorch (use to reproduce results):** https://github.com/WongKinYiu/ScaledYOLOv4\n\n* **source code - Darknet:** https://github.com/AlexeyAB/darknet\n\n* **Medium:** https://alexeyab84.medium.com/scaled-yolo-v4-is-the-best-neural-network-for-object-detection-on-ms-coco-dataset-39dfa22fa982?source=friends_link&sk=c8553bfed861b1a7932f739d26f487c8\n\n## YOLOv4:\n\n* **paper:** https://arxiv.org/abs/2004.10934\n\n* **source code:** https://github.com/AlexeyAB/darknet\n\n* **Wiki:** https://github.com/AlexeyAB/darknet/wiki\n\n* **useful links:** https://medium.com/@alexeyab84/yolov4-the-most-accurate-real-time-neural-network-on-ms-coco-dataset-73adfd3602fe?source=friends_link&sk=6039748846bbcf1d960c3061542591d7\n\nFor more information see the [Darknet project website](http://pjreddie.com/darknet).\n\n\n<details><summary> <b>Expand</b> </summary>\n\n![yolo_progress](https://user-images.githubusercontent.com/4096485/146988929-1ed0cbec-1e01-4ad0-b42c-808dcef32994.png) https://paperswithcode.com/sota/object-detection-on-coco\n\n----\n\n![scaled_yolov4](https://user-images.githubusercontent.com/4096485/112776361-281d8380-9048-11eb-8083-8728b12dcd55.png) AP50:95 - FPS (Tesla V100) Paper: https://arxiv.org/abs/2011.08036\n\n----\n\n![YOLOv4Tiny](https://user-images.githubusercontent.com/4096485/101363015-e5c21200-38b1-11eb-986f-b3e516e05977.png)\n\n----\n\n![YOLOv4](https://user-images.githubusercontent.com/4096485/90338826-06114c80-dff5-11ea-9ba2-8eb63a7409b3.png)\n\n</details>\n\n----\n\n![OpenCV_TRT](https://user-images.githubusercontent.com/4096485/90338805-e5e18d80-dff4-11ea-8a68-5710956256ff.png)\n\n\n## Citation\n\n\n```\n@misc{https://doi.org/10.48550/arxiv.2207.02696,\n  doi = {10.48550/ARXIV.2207.02696},\n  url = {https://arxiv.org/abs/2207.02696},\n  author = {Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},\n  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  title = {YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors},\n  publisher = {arXiv},\n  year = {2022}, \n  copyright = {arXiv.org perpetual, non-exclusive license}\n}\n```\n\n```\n@misc{bochkovskiy2020yolov4,\n      title={YOLOv4: Optimal Speed and Accuracy of Object Detection}, \n      author={Alexey Bochkovskiy and Chien-Yao Wang and Hong-Yuan Mark Liao},\n      year={2020},\n      eprint={2004.10934},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\n```\n@InProceedings{Wang_2021_CVPR,\n    author    = {Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},\n    title     = {{Scaled-YOLOv4}: Scaling Cross Stage Partial Network},\n    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    month     = {June},\n    year      = {2021},\n    pages     = {13029-13038}\n}\n```\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 28723659,
    "name": "awesome-deep-learning",
    "full_name": "ChristosChristofidis/awesome-deep-learning",
    "description": "A curated list of awesome Deep Learning tutorials, projects and communities.",
    "html_url": "https://github.com/ChristosChristofidis/awesome-deep-learning",
    "clone_url": "https://github.com/ChristosChristofidis/awesome-deep-learning.git",
    "owner_login": "ChristosChristofidis",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/7767579?v=4",
    "stargazers_count": 25941,
    "watchers_count": 25941,
    "forks_count": 6161,
    "open_issues_count": 37,
    "size": 607,
    "language": null,
    "languages": {},
    "topics": [
      "awesome",
      "awesome-list",
      "deep-learning",
      "deep-learning-tutorial",
      "deep-networks",
      "face-images",
      "machine-learning",
      "neural-network",
      "recurrent-networks"
    ],
    "license_name": null,
    "created_at": "2015-01-02T19:28:35+00:00",
    "updated_at": "2025-08-06T00:45:33+00:00",
    "pushed_at": "2025-05-26T12:28:00+00:00",
    "contributors_count": 100,
    "readme_length": 72307,
    "readme_content": "\ufeff# Awesome Deep Learning [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)\n\n## Table of Contents\n\n* **[Books](#books)**\n\n* **[Courses](#courses)**  \n\n* **[Videos and Lectures](#videos-and-lectures)**  \n\n* **[Papers](#papers)**  \n\n* **[Tutorials](#tutorials)**  \n\n* **[Researchers](#researchers)**  \n\n* **[Websites](#websites)**  \n\n* **[Datasets](#datasets)**\n\n* **[Conferences](#Conferences)**\n\n* **[Frameworks](#frameworks)**  \n\n* **[Tools](#tools)**  \n\n* **[Miscellaneous](#miscellaneous)**  \n\n* **[Contributing](#contributing)**  \n\n\n### Books\n\n1.  [Deep Learning](http://www.deeplearningbook.org/) by Yoshua Bengio, Ian Goodfellow and Aaron Courville  (05/07/2015)\n2.  [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) by  Michael Nielsen (Dec 2014)\n3.  [Deep Learning](http://research.microsoft.com/pubs/209355/DeepLearning-NowPublishing-Vol7-SIG-039.pdf) by Microsoft Research (2013)\n4.  [Deep Learning Tutorial](http://deeplearning.net/tutorial/deeplearning.pdf) by LISA lab, University of Montreal (Jan 6 2015)\n5.  [neuraltalk](https://github.com/karpathy/neuraltalk) by Andrej Karpathy : numpy-based RNN/LSTM implementation\n6.  [An introduction to genetic algorithms](http://www.boente.eti.br/fuzzy/ebook-fuzzy-mitchell.pdf)\n7.  [Artificial Intelligence: A Modern Approach](http://aima.cs.berkeley.edu/)\n8.  [Deep Learning in Neural Networks: An Overview](http://arxiv.org/pdf/1404.7828v4.pdf)\n9.  [Artificial intelligence and machine learning: Topic wise explanation](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/)\n10. [Grokking Deep Learning for Computer Vision](https://www.manning.com/books/grokking-deep-learning-for-computer-vision)\n11. [Dive into Deep Learning](https://d2l.ai/) - numpy based interactive Deep Learning book\n12. [Practical Deep Learning for Cloud, Mobile, and Edge](https://www.oreilly.com/library/view/practical-deep-learning/9781492034858/) - A book for optimization techniques during production.\n13. [Math and Architectures of Deep Learning](https://www.manning.com/books/math-and-architectures-of-deep-learning) - by Krishnendu Chaudhury\n14. [TensorFlow 2.0 in Action](https://www.manning.com/books/tensorflow-in-action) - by Thushan Ganegedara\n15. [Deep Learning for Natural Language Processing](https://www.manning.com/books/deep-learning-for-natural-language-processing) - by Stephan Raaijmakers\n16. [Deep Learning Patterns and Practices](https://www.manning.com/books/deep-learning-patterns-and-practices) - by Andrew Ferlitsch\n17. [Inside Deep Learning](https://www.manning.com/books/inside-deep-learning) - by Edward Raff\n18. [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition) - by Fran\u00e7ois Chollet\n19. [Evolutionary Deep Learning](https://www.manning.com/books/evolutionary-deep-learning) - by Micheal Lanham\n20. [Engineering Deep Learning Platforms](https://www.manning.com/books/engineering-deep-learning-platforms) - by Chi Wang and Donald Szeto\n21. [Deep Learning with R, Second Edition](https://www.manning.com/books/deep-learning-with-r-second-edition) - by Fran\u00e7ois Chollet with Tomasz Kalinowski and J. J. Allaire\n22. [Regularization in Deep Learning](https://www.manning.com/books/regularization-in-deep-learning) - by Liu Peng\n23. [Jax in Action](https://www.manning.com/books/jax-in-action) - by Grigory Sapunov\n24. [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow](https://www.knowledgeisle.com/wp-content/uploads/2019/12/2-Aur%C3%A9lien-G%C3%A9ron-Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-Tensorflow_-Concepts-Tools-and-Techniques-to-Build-Intelligent-Systems-O%E2%80%99Reilly-Media-2019.pdf) by Aur\u00e9lien G\u00e9ron  | Oct 15, 2019\n\n### Courses\n\n1.  [Machine Learning - Stanford](https://class.coursera.org/ml-005) by Andrew Ng in Coursera (2010-2014)\n2.  [Machine Learning - Caltech](http://work.caltech.edu/lectures.html) by Yaser Abu-Mostafa (2012-2014)\n3.  [Machine Learning - Carnegie Mellon](http://www.cs.cmu.edu/~tom/10701_sp11/lectures.shtml) by Tom Mitchell (Spring 2011)\n2.  [Neural Networks for Machine Learning](https://class.coursera.org/neuralnets-2012-001) by Geoffrey Hinton in Coursera (2012)\n3.  [Neural networks class](https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH) by Hugo Larochelle from Universit\u00e9 de Sherbrooke (2013)\n4.  [Deep Learning Course](http://cilvr.cs.nyu.edu/doku.php?id=deeplearning:slides:start) by CILVR lab @ NYU (2014)\n5.  [A.I - Berkeley](https://courses.edx.org/courses/BerkeleyX/CS188x_1/1T2013/courseware/) by Dan Klein and Pieter Abbeel (2013)\n6.  [A.I - MIT](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/lecture-videos/) by Patrick Henry Winston (2010)\n7.  [Vision and learning - computers and brains](http://web.mit.edu/course/other/i2course/www/vision_and_learning_fall_2013.html) by Shimon Ullman, Tomaso Poggio, Ethan Meyers @ MIT (2013)\n9.  [Convolutional Neural Networks for Visual Recognition - Stanford](http://vision.stanford.edu/teaching/cs231n/syllabus.html) by Fei-Fei Li, Andrej Karpathy (2017)\n10.  [Deep Learning for Natural Language Processing - Stanford](http://cs224d.stanford.edu/)\n11.  [Neural Networks - usherbrooke](http://info.usherbrooke.ca/hlarochelle/neural_networks/content.html)\n12.  [Machine Learning - Oxford](https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/) (2014-2015)\n13.  [Deep Learning - Nvidia](https://developer.nvidia.com/deep-learning-courses) (2015)\n14.  [Graduate Summer School: Deep Learning, Feature Learning](https://www.youtube.com/playlist?list=PLHyI3Fbmv0SdzMHAy0aN59oYnLy5vyyTA) by Geoffrey Hinton, Yoshua Bengio, Yann LeCun, Andrew Ng, Nando de Freitas and several others @ IPAM, UCLA (2012)\n15.  [Deep Learning - Udacity/Google](https://www.udacity.com/course/deep-learning--ud730) by Vincent Vanhoucke and Arpan Chakraborty (2016)\n16.  [Deep Learning - UWaterloo](https://www.youtube.com/playlist?list=PLehuLRPyt1Hyi78UOkMPWCGRxGcA9NVOE) by Prof. Ali Ghodsi at University of Waterloo (2015)\n17.  [Statistical Machine Learning - CMU](https://www.youtube.com/watch?v=azaLcvuql_g&list=PLjbUi5mgii6BWEUZf7He6nowWvGne_Y8r) by Prof. Larry Wasserman\n18.  [Deep Learning Course](https://www.college-de-france.fr/site/en-yann-lecun/course-2015-2016.htm) by Yann LeCun (2016)\n19. [Designing, Visualizing and Understanding Deep Neural Networks-UC Berkeley](https://www.youtube.com/playlist?list=PLkFD6_40KJIxopmdJF_CLNqG3QuDFHQUm)\n20. [UVA Deep Learning Course](http://uvadlc.github.io) MSc in Artificial Intelligence for the University of Amsterdam.\n21. [MIT 6.S094: Deep Learning for Self-Driving Cars](http://selfdrivingcars.mit.edu/)\n22. [MIT 6.S191: Introduction to Deep Learning](http://introtodeeplearning.com/)\n23. [Berkeley CS 294: Deep Reinforcement Learning](http://rll.berkeley.edu/deeprlcourse/)\n24. [Keras in Motion video course](https://www.manning.com/livevideo/keras-in-motion)\n25. [Practical Deep Learning For Coders](http://course.fast.ai/) by Jeremy Howard - Fast.ai\n26. [Introduction to Deep Learning](http://deeplearning.cs.cmu.edu/) by Prof. Bhiksha Raj (2017)\n27. [AI for Everyone](https://www.deeplearning.ai/ai-for-everyone/) by Andrew Ng (2019)\n28. [MIT Intro to Deep Learning 7 day bootcamp](https://introtodeeplearning.com) - A seven day bootcamp designed in MIT to introduce deep learning methods and applications (2019)\n29. [Deep Blueberry: Deep Learning](https://mithi.github.io/deep-blueberry) - A free five-weekend plan to self-learners to learn the basics of deep-learning architectures like CNNs, LSTMs, RNNs, VAEs, GANs, DQN, A3C and more (2019)\n30. [Spinning Up in Deep Reinforcement Learning](https://spinningup.openai.com/) - A free deep reinforcement learning course by OpenAI (2019)\n31. [Deep Learning Specialization - Coursera](https://www.coursera.org/specializations/deep-learning) - Breaking into AI with the best course from Andrew NG.\n32. [Deep Learning - UC Berkeley | STAT-157](https://www.youtube.com/playlist?list=PLZSO_6-bSqHQHBCoGaObUljoXAyyqhpFW) by Alex Smola and Mu Li (2019)\n33. [Machine Learning for Mere Mortals video course](https://www.manning.com/livevideo/machine-learning-for-mere-mortals) by Nick Chase\n34. [Machine Learning Crash Course with TensorFlow APIs](https://developers.google.com/machine-learning/crash-course/) -Google AI\n35. [Deep Learning from the Foundations](https://course.fast.ai/part2) Jeremy Howard - Fast.ai\n36. [Deep Reinforcement Learning (nanodegree) - Udacity](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) a 3-6 month Udacity nanodegree, spanning multiple courses (2018)\n37. [Grokking Deep Learning in Motion](https://www.manning.com/livevideo/grokking-deep-learning-in-motion) by Beau Carnes (2018)\n38. [Face Detection with Computer Vision and Deep Learning](https://www.udemy.com/share/1000gAA0QdcV9aQng=/) by Hakan Cebeci\n39. [Deep Learning Online Course list at Classpert](https://classpert.com/deep-learning) List of Deep Learning online courses (some are free) from Classpert Online Course Search\n40. [AWS Machine Learning](https://aws.training/machinelearning) Machine Learning and Deep Learning Courses from Amazon's Machine Learning university\n41. [Intro to Deep Learning with PyTorch](https://www.udacity.com/course/deep-learning-pytorch--ud188) - A great introductory course on Deep Learning by Udacity and Facebook AI\n42. [Deep Learning by Kaggle](https://www.kaggle.com/learn/deep-learning) - Kaggle's  free course on Deep Learning\n43. [Yann LeCun\u2019s Deep Learning Course at CDS](https://cds.nyu.edu/deep-learning/) - DS-GA 1008 \u00b7 SPRING 2021 \n44. [Neural Networks and Deep Learning](https://webcms3.cse.unsw.edu.au/COMP9444/19T3/) - COMP9444 19T3\n45. [Deep Learning A.I.Shelf](http://aishelf.org/category/ia/deep-learning/)\n\n### Videos and Lectures\n\n1.  [How To Create A Mind](https://www.youtube.com/watch?v=RIkxVci-R4k) By Ray Kurzweil\n2.  [Deep Learning, Self-Taught Learning and Unsupervised Feature Learning](https://www.youtube.com/watch?v=n1ViNeWhC24) By Andrew Ng\n3.  [Recent Developments in Deep Learning](https://www.youtube.com/watch?v=vShMxxqtDDs&amp;index=3&amp;list=PL78U8qQHXgrhP9aZraxTT5-X1RccTcUYT) By Geoff Hinton\n4.  [The Unreasonable Effectiveness of Deep Learning](https://www.youtube.com/watch?v=sc-KbuZqGkI) by Yann LeCun\n5.  [Deep Learning of Representations](https://www.youtube.com/watch?v=4xsVFLnHC_0) by Yoshua bengio\n6.  [Principles of Hierarchical Temporal Memory](https://www.youtube.com/watch?v=6ufPpZDmPKA) by Jeff Hawkins\n7.  [Machine Learning Discussion Group - Deep Learning w/ Stanford AI Lab](https://www.youtube.com/watch?v=2QJi0ArLq7s&amp;list=PL78U8qQHXgrhP9aZraxTT5-X1RccTcUYT) by Adam Coates\n8.  [Making Sense of the World with Deep Learning](http://vimeo.com/80821560) By Adam Coates\n9.  [Demystifying Unsupervised Feature Learning ](https://www.youtube.com/watch?v=wZfVBwOO0-k) By Adam Coates\n10.  [Visual Perception with Deep Learning](https://www.youtube.com/watch?v=3boKlkPBckA) By Yann LeCun\n11.  [The Next Generation of Neural Networks](https://www.youtube.com/watch?v=AyzOUbkUf3M) By Geoffrey Hinton at GoogleTechTalks\n12.  [The wonderful and terrifying implications of computers that can learn](http://www.ted.com/talks/jeremy_howard_the_wonderful_and_terrifying_implications_of_computers_that_can_learn) By Jeremy Howard at TEDxBrussels\n13.  [Unsupervised Deep Learning - Stanford](http://web.stanford.edu/class/cs294a/handouts.html) by Andrew Ng in Stanford (2011)\n14.  [Natural Language Processing](http://web.stanford.edu/class/cs224n/handouts/) By Chris Manning in Stanford\n15.  [A beginners Guide to Deep Neural Networks](http://googleresearch.blogspot.com/2015/09/a-beginners-guide-to-deep-neural.html) By Natalie Hammel and Lorraine Yurshansky\n16.  [Deep Learning: Intelligence from Big Data](https://www.youtube.com/watch?v=czLI3oLDe8M) by Steve Jurvetson (and panel) at VLAB in Stanford.\n17. [Introduction to Artificial Neural Networks and Deep Learning](https://www.youtube.com/watch?v=FoO8qDB8gUU) by Leo Isikdogan at Motorola Mobility HQ\n18. [NIPS 2016 lecture and workshop videos](https://nips.cc/Conferences/2016/Schedule) - NIPS 2016\n19. [Deep Learning Crash Course](https://www.youtube.com/watch?v=oS5fz_mHVz0&list=PLWKotBjTDoLj3rXBL-nEIPRN9V3a9Cx07): a series of mini-lectures by Leo Isikdogan on YouTube (2018)\n20. [Deep Learning Crash Course](https://www.manning.com/livevideo/deep-learning-crash-course) By Oliver Zeigermann\n21. [Deep Learning with R in Motion](https://www.manning.com/livevideo/deep-learning-with-r-in-motion): a live video course that teaches how to apply deep learning to text and images using the powerful Keras library and its R language interface.\n22. [Medical Imaging with Deep Learning Tutorial](https://www.youtube.com/playlist?list=PLheiZMDg_8ufxEx9cNVcOYXsT3BppJP4b): This tutorial is styled as a graduate lecture about medical imaging with deep learning. This will cover the background of popular medical image domains (chest X-ray and histology) as well as methods to tackle multi-modality/view, segmentation, and counting tasks.\n23. [Deepmind x UCL Deeplearning](https://www.youtube.com/playlist?list=PLqYmG7hTraZCDxZ44o4p3N5Anz3lLRVZF): 2020 version \n24. [Deepmind x UCL Reinforcement Learning](https://www.youtube.com/playlist?list=PLqYmG7hTraZBKeNJ-JE_eyJHZ7XgBoAyb): Deep Reinforcement Learning\n25. [CMU 11-785 Intro to Deep learning Spring 2020](https://www.youtube.com/playlist?list=PLp-0K3kfddPzCnS4CqKphh-zT3aDwybDe) Course: 11-785, Intro to Deep Learning by Bhiksha Raj \n26. [Machine Learning CS 229](https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU) : End part focuses on deep learning By Andrew Ng\n27. [What is Neural Structured Learning by Andrew Ferlitsch](https://youtu.be/LXWSE_9gHd0)\n28. [Deep Learning Design Patterns by Andrew Ferlitsch](https://youtu.be/_DaviS6K0Vc)\n29. [Architecture of a Modern CNN: the design pattern approach by Andrew Ferlitsch](https://youtu.be/QCGSS3kyGo0)\n30. [Metaparameters in a CNN by Andrew Ferlitsch](https://youtu.be/K1PLeggQ33I)\n31. [Multi-task CNN: a real-world example by Andrew Ferlitsch](https://youtu.be/dH2nuI-1-qM)\n32. [A friendly introduction to deep reinforcement learning by Luis Serrano](https://youtu.be/1FyAh07jh0o)\n33. [What are GANs and how do they work? by Edward Raff](https://youtu.be/f6ivp84qFUc)\n34. [Coding a basic WGAN in PyTorch by Edward Raff](https://youtu.be/7VRdaqMDalQ)\n35. [Training a Reinforcement Learning Agent by Miguel Morales](https://youtu.be/8TMT-gHlj_Q)\n36. [Understand what is Deep Learning](https://www.scaler.com/topics/what-is-deep-learning/)\n\n### Papers\n*You can also find the most cited deep learning papers from [here](https://github.com/terryum/awesome-deep-learning-papers)*\n\n1.  [ImageNet Classification with Deep Convolutional Neural Networks](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)\n2.  [Using Very Deep Autoencoders for Content Based Image Retrieval](http://www.cs.toronto.edu/~hinton/absps/esann-deep-final.pdf)\n3.  [Learning Deep Architectures for AI](http://www.iro.umontreal.ca/~lisa/pointeurs/TR1312.pdf)\n4.  [CMU\u2019s list of papers](http://deeplearning.cs.cmu.edu/)\n5.  [Neural Networks for Named Entity Recognition](http://nlp.stanford.edu/~socherr/pa4_ner.pdf) [zip](http://nlp.stanford.edu/~socherr/pa4-ner.zip)\n6. [Training tricks by YB](http://www.iro.umontreal.ca/~bengioy/papers/YB-tricks.pdf)\n7. [Geoff Hinton's reading list (all papers)](http://www.cs.toronto.edu/~hinton/deeprefs.html)\n8. [Supervised Sequence Labelling with Recurrent Neural Networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n9.  [Statistical Language Models based on Neural Networks](http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf)\n10.  [Training Recurrent Neural Networks](http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf)\n11.  [Recursive Deep Learning for Natural Language Processing and Computer Vision](http://nlp.stanford.edu/~socherr/thesis.pdf)\n12.  [Bi-directional RNN](http://www.di.ufpe.br/~fnj/RNA/bibliografia/BRNN.pdf)\n13.  [LSTM](http://web.eecs.utk.edu/~itamar/courses/ECE-692/Bobby_paper1.pdf)\n14.  [GRU - Gated Recurrent Unit](http://arxiv.org/pdf/1406.1078v3.pdf)\n15.  [GFRNN](http://arxiv.org/pdf/1502.02367v3.pdf) [.](http://jmlr.org/proceedings/papers/v37/chung15.pdf) [.](http://jmlr.org/proceedings/papers/v37/chung15-supp.pdf)\n16.  [LSTM: A Search Space Odyssey](http://arxiv.org/pdf/1503.04069v1.pdf)\n17.  [A Critical Review of Recurrent Neural Networks for Sequence Learning](http://arxiv.org/pdf/1506.00019v1.pdf)\n18.  [Visualizing and Understanding Recurrent Networks](http://arxiv.org/pdf/1506.02078v1.pdf)\n19.  [Wojciech Zaremba, Ilya Sutskever, An Empirical Exploration of Recurrent Network Architectures](http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n20.  [Recurrent Neural Network based Language Model](http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf)\n21.  [Extensions of Recurrent Neural Network Language Model](http://www.fit.vutbr.cz/research/groups/speech/publi/2011/mikolov_icassp2011_5528.pdf)\n22.  [Recurrent Neural Network based Language Modeling in Meeting Recognition](http://www.fit.vutbr.cz/~imikolov/rnnlm/ApplicationOfRNNinMeetingRecognition_IS2011.pdf)\n23.  [Deep Neural Networks for Acoustic Modeling in Speech Recognition](http://cs224d.stanford.edu/papers/maas_paper.pdf)\n24.  [Speech Recognition with Deep Recurrent Neural Networks](http://www.cs.toronto.edu/~fritz/absps/RNN13.pdf)\n25.  [Reinforcement Learning Neural Turing Machines](http://arxiv.org/pdf/1505.00521v1)\n26.  [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](http://arxiv.org/pdf/1406.1078v3.pdf)\n27. [Google - Sequence to Sequence  Learning with Neural Networks](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)\n28. [Memory Networks](http://arxiv.org/pdf/1410.3916v10)\n29. [Policy Learning with Continuous Memory States for Partially Observed Robotic Control](http://arxiv.org/pdf/1507.01273v1)\n30. [Microsoft - Jointly Modeling Embedding and Translation to Bridge Video and Language](http://arxiv.org/pdf/1505.01861v1.pdf)\n31. [Neural Turing Machines](http://arxiv.org/pdf/1410.5401v2.pdf)\n32. [Ask Me Anything: Dynamic Memory Networks for Natural Language Processing](http://arxiv.org/pdf/1506.07285v1.pdf)\n33. [Mastering the Game of Go with Deep Neural Networks and Tree Search](http://www.nature.com/nature/journal/v529/n7587/pdf/nature16961.pdf)\n34. [Batch Normalization](https://arxiv.org/abs/1502.03167)\n35. [Residual Learning](https://arxiv.org/pdf/1512.03385v1.pdf)\n36. [Image-to-Image Translation with Conditional Adversarial Networks](https://arxiv.org/pdf/1611.07004v1.pdf)\n37. [Berkeley AI Research (BAIR) Laboratory](https://arxiv.org/pdf/1611.07004v1.pdf)\n38. [MobileNets by Google](https://arxiv.org/abs/1704.04861)\n39. [Cross Audio-Visual Recognition in the Wild Using Deep Learning](https://arxiv.org/abs/1706.05739)\n40. [Dynamic Routing Between Capsules](https://arxiv.org/abs/1710.09829)\n41. [Matrix Capsules With Em Routing](https://openreview.net/pdf?id=HJWLfGWRb)\n42. [Efficient BackProp](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)\n43. [Generative Adversarial Nets](https://arxiv.org/pdf/1406.2661v1.pdf)\n44. [Fast R-CNN](https://arxiv.org/pdf/1504.08083.pdf)\n45. [FaceNet: A Unified Embedding for Face Recognition and Clustering](https://arxiv.org/pdf/1503.03832.pdf)\n46. [Siamese Neural Networks for One-shot Image Recognition](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf)\n47. [Unsupervised Translation of Programming Languages](https://arxiv.org/pdf/2006.03511.pdf)\n48. [Matching Networks for One Shot Learning](http://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning.pdf)\n49. [VOLO: Vision Outlooker for Visual Recognition](https://arxiv.org/pdf/2106.13112.pdf)\n50. [ViT: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/pdf/2010.11929.pdf)\n51. [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://proceedings.mlr.press/v37/ioffe15.pdf)\n52. [DeepFaceDrawing: Deep Generation of Face Images from Sketches](http://geometrylearning.com/paper/DeepFaceDrawing.pdf?fbclid=IwAR0colWFHPGBCB1APZq9JVsWeWtmeZd9oCTNQvR52T5PRUJP_dLOwB8pt0I)\n\n### Tutorials\n\n1.  [UFLDL Tutorial 1](http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial)\n2.  [UFLDL Tutorial 2](http://ufldl.stanford.edu/tutorial/supervised/LinearRegression/)\n3.  [Deep Learning for NLP (without Magic)](http://www.socher.org/index.php/DeepLearningTutorial/DeepLearningTutorial)\n4.  [A Deep Learning Tutorial: From Perceptrons to Deep Networks](http://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-perceptrons-to-deep-networks)\n5.  [Deep Learning from the Bottom up](http://www.metacademy.org/roadmaps/rgrosse/deep_learning)\n6.  [Theano Tutorial](http://deeplearning.net/tutorial/deeplearning.pdf)\n7.  [Neural Networks for Matlab](http://uk.mathworks.com/help/pdf_doc/nnet/nnet_ug.pdf)\n8.  [Using convolutional neural nets to detect facial keypoints tutorial](http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/)\n9.  [Torch7 Tutorials](https://github.com/clementfarabet/ipam-tutorials/tree/master/th_tutorials)\n10.  [The Best Machine Learning Tutorials On The Web](https://github.com/josephmisiti/machine-learning-module)\n11. [VGG Convolutional Neural Networks Practical](http://www.robots.ox.ac.uk/~vgg/practicals/cnn/index.html)\n12. [TensorFlow tutorials](https://github.com/nlintz/TensorFlow-Tutorials)\n13. [More TensorFlow tutorials](https://github.com/pkmital/tensorflow_tutorials)\n13. [TensorFlow Python Notebooks](https://github.com/aymericdamien/TensorFlow-Examples)\n14. [Keras and Lasagne Deep Learning Tutorials](https://github.com/Vict0rSch/deep_learning)\n15. [Classification on raw time series in TensorFlow with a LSTM RNN](https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition)\n16. [Using convolutional neural nets to detect facial keypoints tutorial](http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/)\n17. [TensorFlow-World](https://github.com/astorfi/TensorFlow-World)\n18. [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python)\n19. [Grokking Deep Learning](https://www.manning.com/books/grokking-deep-learning)\n20. [Deep Learning for Search](https://www.manning.com/books/deep-learning-for-search)\n21. [Keras Tutorial: Content Based Image Retrieval Using a Convolutional Denoising Autoencoder](https://medium.com/sicara/keras-tutorial-content-based-image-retrieval-convolutional-denoising-autoencoder-dc91450cc511)\n22. [Pytorch Tutorial by Yunjey Choi](https://github.com/yunjey/pytorch-tutorial)\n23. [Understanding deep Convolutional Neural Networks with a practical use-case in Tensorflow and Keras](https://ahmedbesbes.com/understanding-deep-convolutional-neural-networks-with-a-practical-use-case-in-tensorflow-and-keras.html)\n24. [Overview and benchmark of traditional and deep learning models in text classification](https://ahmedbesbes.com/overview-and-benchmark-of-traditional-and-deep-learning-models-in-text-classification.html)\n25. [Hardware for AI: Understanding computer hardware & build your own computer](https://github.com/MelAbgrall/HardwareforAI)\n26. [Programming Community Curated Resources](https://hackr.io/tutorials/learn-artificial-intelligence-ai)\n27. [The Illustrated Self-Supervised Learning](https://amitness.com/2020/02/illustrated-self-supervised-learning/)\n28. [Visual Paper Summary: ALBERT (A Lite BERT)](https://amitness.com/2020/02/albert-visual-summary/)\n28. [Semi-Supervised Deep Learning with GANs for Melanoma Detection](https://www.manning.com/liveproject/semi-supervised-deep-learning-with-gans-for-melanoma-detection/)\n29. [Named Entity Recognition using Reformers](https://github.com/SauravMaheshkar/Trax-Examples/blob/main/NLP/NER%20using%20Reformer.ipynb)\n30. [Deep N-Gram Models on Shakespeare\u2019s works](https://github.com/SauravMaheshkar/Trax-Examples/blob/main/NLP/Deep%20N-Gram.ipynb)\n31. [Wide Residual Networks](https://github.com/SauravMaheshkar/Trax-Examples/blob/main/vision/illustrated-wideresnet.ipynb)\n32. [Fashion MNIST using Flax](https://github.com/SauravMaheshkar/Flax-Examples)\n33. [Fake News Classification (with streamlit deployment)](https://github.com/SauravMaheshkar/Fake-News-Classification)\n34. [Regression Analysis for Primary Biliary Cirrhosis](https://github.com/SauravMaheshkar/CoxPH-Model-for-Primary-Biliary-Cirrhosis)\n35. [Cross Matching Methods for Astronomical Catalogs](https://github.com/SauravMaheshkar/Cross-Matching-Methods-for-Astronomical-Catalogs)\n36. [Named Entity Recognition using BiDirectional LSTMs](https://github.com/SauravMaheshkar/Named-Entity-Recognition-)\n37. [Image Recognition App using Tflite and Flutter](https://github.com/SauravMaheshkar/Flutter_Image-Recognition)\n\n## Researchers\n\n1. [Aaron Courville](http://aaroncourville.wordpress.com)\n2. [Abdel-rahman Mohamed](http://www.cs.toronto.edu/~asamir/)\n3. [Adam Coates](http://cs.stanford.edu/~acoates/)\n4. [Alex Acero](http://research.microsoft.com/en-us/people/alexac/)\n5. [ Alex Krizhevsky ](http://www.cs.utoronto.ca/~kriz/index.html)\n6. [ Alexander Ilin ](http://users.ics.aalto.fi/alexilin/)\n7. [ Amos Storkey ](http://homepages.inf.ed.ac.uk/amos/)\n8. [ Andrej Karpathy ](https://karpathy.ai/)\n9. [ Andrew M. Saxe ](http://www.stanford.edu/~asaxe/)\n10. [ Andrew Ng ](http://www.cs.stanford.edu/people/ang/)\n11. [ Andrew W. Senior ](http://research.google.com/pubs/author37792.html)\n12. [ Andriy Mnih ](http://www.gatsby.ucl.ac.uk/~amnih/)\n13. [ Ayse Naz Erkan ](http://www.cs.nyu.edu/~naz/)\n14. [ Benjamin Schrauwen ](http://reslab.elis.ugent.be/benjamin)\n15. [ Bernardete Ribeiro ](https://www.cisuc.uc.pt/people/show/2020)\n16. [ Bo David Chen ](http://vision.caltech.edu/~bchen3/Site/Bo_David_Chen.html)\n17. [ Boureau Y-Lan ](http://cs.nyu.edu/~ylan/)\n18. [ Brian Kingsbury ](http://researcher.watson.ibm.com/researcher/view.php?person=us-bedk)\n19. [ Christopher Manning ](http://nlp.stanford.edu/~manning/)\n20. [ Clement Farabet ](http://www.clement.farabet.net/)\n21. [ Dan Claudiu Cire\u0219an ](http://www.idsia.ch/~ciresan/)\n22. [ David Reichert ](http://serre-lab.clps.brown.edu/person/david-reichert/)\n23. [ Derek Rose ](http://mil.engr.utk.edu/nmil/member/5.html)\n24. [ Dong Yu ](http://research.microsoft.com/en-us/people/dongyu/default.aspx)\n25. [ Drausin Wulsin ](http://www.seas.upenn.edu/~wulsin/)\n26. [ Erik M. Schmidt ](http://music.ece.drexel.edu/people/eschmidt)\n27. [ Eugenio Culurciello ](https://engineering.purdue.edu/BME/People/viewPersonById?resource_id=71333)\n28. [ Frank Seide ](http://research.microsoft.com/en-us/people/fseide/)\n29. [ Galen Andrew ](http://homes.cs.washington.edu/~galen/)\n30. [ Geoffrey Hinton ](http://www.cs.toronto.edu/~hinton/)\n31. [ George Dahl ](http://www.cs.toronto.edu/~gdahl/)\n32. [ Graham Taylor ](http://www.uoguelph.ca/~gwtaylor/)\n33. [ Gr\u00e9goire Montavon ](http://gregoire.montavon.name/)\n34. [ Guido Francisco Mont\u00fafar ](http://personal-homepages.mis.mpg.de/montufar/)\n35. [ Guillaume Desjardins ](http://brainlogging.wordpress.com/)\n36. [ Hannes Schulz ](http://www.ais.uni-bonn.de/~schulz/)\n37. [ H\u00e9l\u00e8ne Paugam-Moisy ](http://www.lri.fr/~hpaugam/)\n38. [ Honglak Lee ](http://web.eecs.umich.edu/~honglak/)\n39. [ Hugo Larochelle ](http://www.dmi.usherb.ca/~larocheh/index_en.html)\n40. [ Ilya Sutskever ](http://www.cs.toronto.edu/~ilya/)\n41. [ Itamar Arel ](http://mil.engr.utk.edu/nmil/member/2.html)\n42. [ James Martens ](http://www.cs.toronto.edu/~jmartens/)\n43. [ Jason Morton ](http://www.jasonmorton.com/)\n44. [ Jason Weston ](http://www.thespermwhale.com/jaseweston/)\n45. [ Jeff Dean ](http://research.google.com/pubs/jeff.html)\n46. [ Jiquan Mgiam ](http://cs.stanford.edu/~jngiam/)\n47. [ Joseph Turian ](http://www-etud.iro.umontreal.ca/~turian/)\n48. [ Joshua Matthew Susskind ](http://aclab.ca/users/josh/index.html)\n49. [ J\u00fcrgen Schmidhuber ](http://www.idsia.ch/~juergen/)\n50. [ Justin A. Blanco ](https://sites.google.com/site/blancousna/)\n51. [ Koray Kavukcuoglu ](http://koray.kavukcuoglu.org/)\n52. [ KyungHyun Cho ](http://users.ics.aalto.fi/kcho/)\n53. [ Li Deng ](http://research.microsoft.com/en-us/people/deng/)\n54. [ Lucas Theis ](http://www.kyb.tuebingen.mpg.de/nc/employee/details/lucas.html)\n55. [ Ludovic Arnold ](http://ludovicarnold.altervista.org/home/)\n56. [ Marc'Aurelio Ranzato ](http://www.cs.nyu.edu/~ranzato/)\n57. [ Martin L\u00e4ngkvist ](http://aass.oru.se/~mlt/)\n58. [ Misha Denil ](http://mdenil.com/)\n59. [ Mohammad Norouzi ](http://www.cs.toronto.edu/~norouzi/)\n60. [ Nando de Freitas ](http://www.cs.ubc.ca/~nando/)\n61. [ Navdeep Jaitly ](http://www.cs.utoronto.ca/~ndjaitly/)\n62. [ Nicolas Le Roux ](http://nicolas.le-roux.name/)\n63. [ Nitish Srivastava ](http://www.cs.toronto.edu/~nitish/)\n64. [ Noel Lopes ](https://www.cisuc.uc.pt/people/show/2028)\n65. [ Oriol Vinyals ](http://www.cs.berkeley.edu/~vinyals/)\n66. [ Pascal Vincent ](http://www.iro.umontreal.ca/~vincentp)\n67. [ Patrick Nguyen ](https://sites.google.com/site/drpngx/)\n68. [ Pedro Domingos ](http://homes.cs.washington.edu/~pedrod/)\n69. [ Peggy Series ](http://homepages.inf.ed.ac.uk/pseries/)\n70. [ Pierre Sermanet ](http://cs.nyu.edu/~sermanet)\n71. [ Piotr Mirowski ](http://www.cs.nyu.edu/~mirowski/)\n72. [ Quoc V. Le ](http://ai.stanford.edu/~quocle/)\n73. [ Reinhold Scherer ](http://bci.tugraz.at/scherer/)\n74. [ Richard Socher ](http://www.socher.org/)\n75. [ Rob Fergus ](http://cs.nyu.edu/~fergus/pmwiki/pmwiki.php)\n76. [ Robert Coop ](http://mil.engr.utk.edu/nmil/member/19.html)\n77. [ Robert Gens ](http://homes.cs.washington.edu/~rcg/)\n78. [ Roger Grosse ](http://people.csail.mit.edu/rgrosse/)\n79. [ Ronan Collobert ](http://ronan.collobert.com/)\n80. [ Ruslan Salakhutdinov ](http://www.utstat.toronto.edu/~rsalakhu/)\n81. [ Sebastian Gerwinn ](http://www.kyb.tuebingen.mpg.de/nc/employee/details/sgerwinn.html)\n82. [ St\u00e9phane Mallat ](http://www.cmap.polytechnique.fr/~mallat/)\n83. [ Sven Behnke ](http://www.ais.uni-bonn.de/behnke/)\n84. [ Tapani Raiko ](http://users.ics.aalto.fi/praiko/)\n85. [ Tara Sainath ](https://sites.google.com/site/tsainath/)\n86. [ Tijmen Tieleman ](http://www.cs.toronto.edu/~tijmen/)\n87. [ Tom Karnowski ](http://mil.engr.utk.edu/nmil/member/36.html)\n88. [ Tom\u00e1\u0161 Mikolov ](https://research.facebook.com/tomas-mikolov)\n89. [ Ueli Meier ](http://www.idsia.ch/~meier/)\n90. [ Vincent Vanhoucke ](http://vincent.vanhoucke.com)\n91. [ Volodymyr Mnih ](http://www.cs.toronto.edu/~vmnih/)\n92. [ Yann LeCun ](http://yann.lecun.com/)\n93. [ Yichuan Tang ](http://www.cs.toronto.edu/~tang/)\n94. [ Yoshua Bengio ](http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html)\n95. [ Yotaro Kubo ](http://yota.ro/)\n96. [ Youzhi (Will) Zou ](http://ai.stanford.edu/~wzou)\n97. [ Fei-Fei Li ](http://vision.stanford.edu/feifeili)\n98. [ Ian Goodfellow ](https://research.google.com/pubs/105214.html)\n99. [ Robert Lagani\u00e8re ](http://www.site.uottawa.ca/~laganier/)\n100. [Merve Ayy\u00fcce K\u0131zrak](http://www.ayyucekizrak.com/)\n\n\n### Websites\n\n1.  [deeplearning.net](http://deeplearning.net/)\n2.  [deeplearning.stanford.edu](http://deeplearning.stanford.edu/)\n3.  [nlp.stanford.edu](http://nlp.stanford.edu/)\n4.  [ai-junkie.com](http://www.ai-junkie.com/ann/evolved/nnt1.html)\n5.  [cs.brown.edu/research/ai](http://cs.brown.edu/research/ai/)\n6.  [eecs.umich.edu/ai](http://www.eecs.umich.edu/ai/)\n7.  [cs.utexas.edu/users/ai-lab](http://www.cs.utexas.edu/users/ai-lab/)\n8.  [cs.washington.edu/research/ai](http://www.cs.washington.edu/research/ai/)\n9.  [aiai.ed.ac.uk](http://www.aiai.ed.ac.uk/)\n10.  [www-aig.jpl.nasa.gov](http://www-aig.jpl.nasa.gov/)\n11.  [csail.mit.edu](http://www.csail.mit.edu/)\n12.  [cgi.cse.unsw.edu.au/~aishare](http://cgi.cse.unsw.edu.au/~aishare/)\n13.  [cs.rochester.edu/research/ai](http://www.cs.rochester.edu/research/ai/)\n14.  [ai.sri.com](http://www.ai.sri.com/)\n15.  [isi.edu/AI/isd.htm](http://www.isi.edu/AI/isd.htm)\n16.  [nrl.navy.mil/itd/aic](http://www.nrl.navy.mil/itd/aic/)\n17.  [hips.seas.harvard.edu](http://hips.seas.harvard.edu/)\n18.  [AI Weekly](http://aiweekly.co)\n19.  [stat.ucla.edu](http://statistics.ucla.edu/)\n20.  [deeplearning.cs.toronto.edu](http://deeplearning.cs.toronto.edu/i2t)\n21.  [jeffdonahue.com/lrcn/](http://jeffdonahue.com/lrcn/)\n22.  [visualqa.org](http://www.visualqa.org/)\n23.  [www.mpi-inf.mpg.de/departments/computer-vision...](https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/)\n24.  [Deep Learning News](http://news.startup.ml/)\n25.  [Machine Learning is Fun! Adam Geitgey's Blog](https://medium.com/@ageitgey/)\n26.  [Guide to Machine Learning](http://yerevann.com/a-guide-to-deep-learning/)\n27.  [Deep Learning for Beginners](https://spandan-madan.github.io/DeepLearningProject/)\n28.  [Machine Learning Mastery blog](https://machinelearningmastery.com/blog/)\n29.  [ML Compiled](https://ml-compiled.readthedocs.io/en/latest/)\n30.  [Programming Community Curated Resources](https://hackr.io/tutorials/learn-artificial-intelligence-ai)\n31.  [A Beginner's Guide To Understanding Convolutional Neural Networks](https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/)\n32.  [ahmedbesbes.com](http://ahmedbesbes.com)\n33.  [amitness.com](https://amitness.com/)\n34.  [AI Summer](https://theaisummer.com/)\n35.  [AI Hub - supported by AAAI, NeurIPS](https://aihub.org/)\n36.  [CatalyzeX: Machine Learning Hub for Builders and Makers](https://www.catalyzeX.com)\n37.  [The Epic Code](https://theepiccode.com/)\n38.  [all AI news](https://allainews.com/)\n\n### Datasets\n\n1.  [MNIST](http://yann.lecun.com/exdb/mnist/) Handwritten digits\n2.  [Google House Numbers](http://ufldl.stanford.edu/housenumbers/) from street view\n3.  [CIFAR-10 and CIFAR-100](http://www.cs.toronto.edu/~kriz/cifar.html)\n4.  [IMAGENET](http://www.image-net.org/)\n5.  [Tiny Images](http://groups.csail.mit.edu/vision/TinyImages/) 80 Million tiny images6.  \n6.  [Flickr Data](https://yahooresearch.tumblr.com/post/89783581601/one-hundred-million-creative-commons-flickr-images) 100 Million Yahoo dataset\n7.  [Berkeley Segmentation Dataset 500](http://www.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/)\n8.  [UC Irvine Machine Learning Repository](http://archive.ics.uci.edu/ml/)\n9.  [Flickr 8k](http://nlp.cs.illinois.edu/HockenmaierGroup/Framing_Image_Description/KCCA.html)\n10. [Flickr 30k](http://shannon.cs.illinois.edu/DenotationGraph/)\n11. [Microsoft COCO](http://mscoco.org/home/)\n12. [VQA](http://www.visualqa.org/)\n13. [Image QA](http://www.cs.toronto.edu/~mren/imageqa/data/cocoqa/)\n14. [AT&T Laboratories Cambridge face database](http://www.uk.research.att.com/facedatabase.html)\n15. [AVHRR Pathfinder](http://xtreme.gsfc.nasa.gov)\n16. [Air Freight](http://www.anc.ed.ac.uk/~amos/afreightdata.html) - The Air Freight data set is a ray-traced image sequence along with ground truth segmentation based on textural characteristics. (455 images + GT, each 160x120 pixels). (Formats: PNG)  \n17. [Amsterdam Library of Object Images](http://www.science.uva.nl/~aloi/) - ALOI is a color image collection of one-thousand small objects, recorded for scientific purposes. In order to capture the sensory variation in object recordings, we systematically varied viewing angle, illumination angle, and illumination color for each object, and additionally captured wide-baseline stereo images. We recorded over a hundred images of each object, yielding a total of 110,250 images for the collection. (Formats: png)\n18. [Annotated face, hand, cardiac & meat images](http://www.imm.dtu.dk/~aam/) - Most images & annotations are supplemented by various ASM/AAM analyses using the AAM-API. (Formats: bmp,asf)\n19. [Image Analysis and Computer Graphics](http://www.imm.dtu.dk/image/)  \n21. [Brown University Stimuli](http://www.cog.brown.edu/~tarr/stimuli.html) - A variety of datasets including geons, objects, and \"greebles\". Good for testing recognition algorithms. (Formats: pict)\n22. [CAVIAR video sequences of mall and public space behavior](http://homepages.inf.ed.ac.uk/rbf/CAVIARDATA1/) - 90K video frames in 90 sequences of various human activities, with XML ground truth of detection and behavior classification (Formats: MPEG2 & JPEG)\n23. [Machine Vision Unit](http://www.ipab.inf.ed.ac.uk/mvu/)\n25. [CCITT Fax standard images](http://www.cs.waikato.ac.nz/~singlis/ccitt.html) - 8 images (Formats: gif)\n26. [CMU CIL's Stereo Data with Ground Truth](cil-ster.html) - 3 sets of 11 images, including color tiff images with spectroradiometry (Formats: gif, tiff)\n27. [CMU PIE Database](http://www.ri.cmu.edu/projects/project_418.html) - A database of 41,368 face images of 68 people captured under 13 poses, 43 illuminations conditions, and with 4 different expressions.\n28. [CMU VASC Image Database](http://www.ius.cs.cmu.edu/idb/) - Images, sequences, stereo pairs (thousands of images) (Formats: Sun Rasterimage)\n29. [Caltech Image Database](http://www.vision.caltech.edu/html-files/archive.html) - about 20 images - mostly top-down views of small objects and toys. (Formats: GIF)\n30. [Columbia-Utrecht Reflectance and Texture Database](http://www.cs.columbia.edu/CAVE/curet/) - Texture and reflectance measurements for over 60 samples of 3D texture, observed with over 200 different combinations of viewing and illumination directions. (Formats: bmp)\n31. [Computational Colour Constancy Data](http://www.cs.sfu.ca/~colour/data/index.html) - A dataset oriented towards computational color constancy, but useful for computer vision in general. It includes synthetic data, camera sensor data, and over 700 images. (Formats: tiff)\n32. [Computational Vision Lab](http://www.cs.sfu.ca/~colour/)\n34. [Content-based image retrieval database](http://www.cs.washington.edu/research/imagedatabase/groundtruth/) - 11 sets of color images for testing algorithms for content-based retrieval. Most sets have a description file with names of objects in each image. (Formats: jpg)\n35. [Efficient Content-based Retrieval Group](http://www.cs.washington.edu/research/imagedatabase/)\n37. [Densely Sampled View Spheres](http://ls7-www.cs.uni-dortmund.de/~peters/pages/research/modeladaptsys/modeladaptsys_vba_rov.html) - Densely sampled view spheres - upper half of the view sphere of two toy objects with 2500 images each. (Formats: tiff)\n38. [Computer Science VII (Graphical Systems)](http://ls7-www.cs.uni-dortmund.de/)\n40. [Digital Embryos](https://web-beta.archive.org/web/20011216051535/vision.psych.umn.edu/www/kersten-lab/demos/digitalembryo.html) - Digital embryos are novel objects which may be used to develop and test object recognition systems. They have an organic appearance. (Formats: various formats are available on request)\n41. [Univerity of Minnesota Vision Lab](http://vision.psych.umn.edu/users/kersten//kersten-lab/kersten-lab.html) \n42. [El Salvador Atlas of Gastrointestinal VideoEndoscopy](http://www.gastrointestinalatlas.com) - Images and Videos of his-res of studies taken from Gastrointestinal Video endoscopy. (Formats: jpg, mpg, gif)\n43. [FG-NET Facial Aging Database](http://sting.cycollege.ac.cy/~alanitis/fgnetaging/index.htm) - Database contains 1002 face images showing subjects at different ages. (Formats: jpg)\n44. [FVC2000 Fingerprint Databases](http://bias.csr.unibo.it/fvc2000/) - FVC2000 is the First International Competition for Fingerprint Verification Algorithms. Four fingerprint databases constitute the FVC2000 benchmark (3520 fingerprints in all).\n45. [Biometric Systems Lab](http://biolab.csr.unibo.it/home.asp) - University of Bologna\n46. [Face and Gesture images and image sequences](http://www.fg-net.org) - Several image datasets of faces and gestures that are ground truth annotated for benchmarking\n47. [German Fingerspelling Database](http://www-i6.informatik.rwth-aachen.de/~dreuw/database.html) - The database contains 35 gestures and consists of 1400 image sequences that contain gestures of 20 different persons recorded under non-uniform daylight lighting conditions. (Formats: mpg,jpg)  \n48. [Language Processing and Pattern Recognition](http://www-i6.informatik.rwth-aachen.de/)\n50. [Groningen Natural Image Database](http://hlab.phys.rug.nl/archive.html) - 4000+ 1536x1024 (16 bit) calibrated outdoor images (Formats: homebrew)\n51. [ICG Testhouse sequence](http://www.icg.tu-graz.ac.at/~schindler/Data) -  2 turntable sequences from different viewing heights, 36 images each, resolution 1000x750, color (Formats: PPM)\n52. [Institute of Computer Graphics and Vision](http://www.icg.tu-graz.ac.at)\n54. [IEN Image Library](http://www.ien.it/is/vislib/) - 1000+ images, mostly outdoor sequences (Formats: raw, ppm)  \n55. [INRIA's Syntim images database](http://www-rocq.inria.fr/~tarel/syntim/images.html) - 15 color image of simple objects (Formats: gif)\n56. [INRIA](http://www.inria.fr/)\n57. [INRIA's Syntim stereo databases](http://www-rocq.inria.fr/~tarel/syntim/paires.html) - 34 calibrated color stereo pairs (Formats: gif)\n58. [Image Analysis Laboratory](http://www.ece.ncsu.edu/imaging/Archives/ImageDataBase/index.html) - Images obtained from a variety of imaging modalities -- raw CFA images, range images and a host of \"medical images\". (Formats: homebrew)\n59. [Image Analysis Laboratory](http://www.ece.ncsu.edu/imaging)\n61. [Image Database](http://www.prip.tuwien.ac.at/prip/image.html) - An image database including some textures  \n62. [JAFFE Facial Expression Image Database](http://www.mis.atr.co.jp/~mlyons/jaffe.html) - The JAFFE database consists of 213 images of Japanese female subjects posing 6 basic facial expressions as well as a neutral pose. Ratings on emotion adjectives are also available, free of charge, for research purposes. (Formats: TIFF Grayscale images.)\n63. [ATR Research, Kyoto, Japan](http://www.mic.atr.co.jp/)\n64. [JISCT Stereo Evaluation](ftp://ftp.vislist.com/IMAGERY/JISCT/) - 44 image pairs. These data have been used in an evaluation of stereo analysis, as described in the April 1993 ARPA Image Understanding Workshop paper ``The JISCT Stereo Evaluation'' by R.C.Bolles, H.H.Baker, and M.J.Hannah, 263--274 (Formats: SSI)\n65. [MIT Vision Texture](https://vismod.media.mit.edu/vismod/imagery/VisionTexture/vistex.html) - Image archive (100+ images) (Formats: ppm)\n66. [MIT face images and more](ftp://whitechapel.media.mit.edu/pub/images) - hundreds of images (Formats: homebrew)\n67. [Machine Vision](http://vision.cse.psu.edu/book/testbed/images/) - Images from the textbook by Jain, Kasturi, Schunck (20+ images) (Formats: GIF TIFF)\n68. [Mammography Image Databases](http://marathon.csee.usf.edu/Mammography/Database.html) - 100 or more images of mammograms with ground truth. Additional images available by request, and links to several other mammography databases are provided. (Formats: homebrew)\n69. [ftp://ftp.cps.msu.edu/pub/prip](ftp://ftp.cps.msu.edu/pub/prip) - many images (Formats: unknown)\n70. [Middlebury Stereo Data Sets with Ground Truth](http://www.middlebury.edu/stereo/data.html) - Six multi-frame stereo data sets of scenes containing planar regions. Each data set contains 9 color images and subpixel-accuracy ground-truth data. (Formats: ppm)\n71. [Middlebury Stereo Vision Research Page](http://www.middlebury.edu/stereo) - Middlebury College\n72. [Modis Airborne simulator, Gallery and data set](http://ltpwww.gsfc.nasa.gov/MODIS/MAS/) - High Altitude Imagery from around the world for environmental modeling in support of NASA EOS program (Formats: JPG and HDF)\n73. [NIST Fingerprint and handwriting](ftp://sequoyah.ncsl.nist.gov/pub/databases/data) - datasets - thousands of images (Formats: unknown)\n74. [NIST Fingerprint data](ftp://ftp.cs.columbia.edu/jpeg/other/uuencoded) - compressed multipart uuencoded tar file\n75. [NLM HyperDoc Visible Human Project](http://www.nlm.nih.gov/research/visible/visible_human.html) - Color, CAT and MRI image samples - over 30 images (Formats: jpeg)\n76. [National Design Repository](http://www.designrepository.org) - Over 55,000 3D CAD and solid models of (mostly) mechanical/machined engineering designs. (Formats: gif,vrml,wrl,stp,sat) \n77. [Geometric & Intelligent Computing Laboratory](http://gicl.mcs.drexel.edu)\n79. [OSU (MSU) 3D Object Model Database](http://eewww.eng.ohio-state.edu/~flynn/3DDB/Models/) - several sets of 3D object models collected over several years to use in object recognition research (Formats: homebrew, vrml)\n80. [OSU (MSU/WSU) Range Image Database](http://eewww.eng.ohio-state.edu/~flynn/3DDB/RID/) - Hundreds of real and synthetic images (Formats: gif, homebrew)\n81. [OSU/SAMPL Database: Range Images, 3D Models, Stills, Motion Sequences](http://sampl.eng.ohio-state.edu/~sampl/database.htm) - Over 1000 range images, 3D object models, still images and motion sequences (Formats: gif, ppm, vrml, homebrew)\n82. [Signal Analysis and Machine Perception Laboratory](http://sampl.eng.ohio-state.edu)\n84. [Otago Optical Flow Evaluation Sequences](http://www.cs.otago.ac.nz/research/vision/Research/OpticalFlow/opticalflow.html) - Synthetic and real sequences with machine-readable ground truth optical flow fields, plus tools to generate ground truth for new sequences. (Formats: ppm,tif,homebrew)\n85. [Vision Research Group](http://www.cs.otago.ac.nz/research/vision/index.html)\n87. [ftp://ftp.limsi.fr/pub/quenot/opflow/testdata/piv/](ftp://ftp.limsi.fr/pub/quenot/opflow/testdata/piv/) - Real and synthetic image sequences used for testing a Particle Image Velocimetry application. These images may be used for the test of optical flow and image matching algorithms. (Formats: pgm (raw))\n88. [LIMSI-CNRS/CHM/IMM/vision](http://www.limsi.fr/Recherche/IMM/PageIMM.html)\n89. [LIMSI-CNRS](http://www.limsi.fr/)\n90. [Photometric 3D Surface Texture Database](http://www.taurusstudio.net/research/pmtexdb/index.htm) - This is the first 3D texture database which provides both full real surface rotations and registered photometric stereo data (30 textures, 1680 images). (Formats: TIFF)\n91. [SEQUENCES FOR OPTICAL FLOW ANALYSIS (SOFA)](http://www.cee.hw.ac.uk/~mtc/sofa) - 9 synthetic sequences designed for testing motion analysis applications, including full ground truth of motion and camera parameters. (Formats: gif)\n92. [Computer Vision Group](http://www.cee.hw.ac.uk/~mtc/research.html)\n94. [Sequences for Flow Based Reconstruction](http://www.nada.kth.se/~zucch/CAMERA/PUB/seq.html) - synthetic sequence for testing structure from motion algorithms (Formats: pgm)\n95. [Stereo Images with Ground Truth Disparity and Occlusion](http://www-dbv.cs.uni-bonn.de/stereo_data/) - a small set of synthetic images of a hallway with varying amounts of noise added. Use these images to benchmark your stereo algorithm. (Formats: raw, viff (khoros), or tiff)\n96. [Stuttgart Range Image Database](http://range.informatik.uni-stuttgart.de) - A collection of synthetic range images taken from high-resolution polygonal models available on the web (Formats: homebrew)\n97. [Department Image Understanding](http://www.informatik.uni-stuttgart.de/ipvr/bv/bv_home_engl.html)\n99. [The AR Face Database](http://www2.ece.ohio-state.edu/~aleix/ARdatabase.html) - Contains over 4,000 color images corresponding to 126 people's faces (70 men and 56 women). Frontal views with variations in facial expressions, illumination, and occlusions. (Formats: RAW (RGB 24-bit))\n100. [Purdue Robot Vision Lab](http://rvl.www.ecn.purdue.edu/RVL/)\n101. [The MIT-CSAIL Database of Objects and Scenes](http://web.mit.edu/torralba/www/database.html) - Database for testing multiclass object detection and scene recognition algorithms. Over 72,000 images with 2873 annotated frames. More than 50 annotated object classes. (Formats: jpg)\n102. [The RVL SPEC-DB (SPECularity DataBase)](http://rvl1.ecn.purdue.edu/RVL/specularity_database/) - A collection of over 300 real images of 100 objects taken under three different illuminaiton conditions (Diffuse/Ambient/Directed). -- Use these images to test algorithms for detecting and compensating specular highlights in color images. (Formats: TIFF )\n103. [Robot Vision Laboratory](http://rvl1.ecn.purdue.edu/RVL/)\n105. [The Xm2vts database](http://xm2vtsdb.ee.surrey.ac.uk) - The XM2VTSDB contains four digital recordings of 295 people taken over a period of four months. This database contains both image and video data of faces.\n106. [Centre for Vision, Speech and Signal Processing](http://www.ee.surrey.ac.uk/Research/CVSSP)\n107. [Traffic Image Sequences and 'Marbled Block' Sequence](http://i21www.ira.uka.de/image_sequences) - thousands of frames of digitized traffic image sequences as well as the 'Marbled Block' sequence (grayscale images) (Formats: GIF)\n108. [IAKS/KOGS](http://i21www.ira.uka.de)\n110. [U Bern Face images](ftp://ftp.iam.unibe.ch/pub/Images/FaceImages) - hundreds of images (Formats: Sun rasterfile)\n111. [U Michigan textures](ftp://freebie.engin.umich.edu/pub/misc/textures) (Formats: compressed raw)\n112. [U Oulu wood and knots database](http://www.ee.oulu.fi/~olli/Projects/Lumber.Grading.html) - Includes classifications - 1000+ color images (Formats: ppm)\n113. [UCID - an Uncompressed Colour Image Database](http://vision.doc.ntu.ac.uk/datasets/UCID/ucid.html) - a benchmark database for image retrieval with predefined ground truth. (Formats: tiff)\n115. [UMass Vision Image Archive](http://vis-www.cs.umass.edu/~vislib/) - Large image database with aerial, space, stereo, medical images and more. (Formats: homebrew)\n116. [UNC's 3D image database](ftp://sunsite.unc.edu/pub/academic/computer-science/virtual-reality/3d) - many images (Formats: GIF)\n117. [USF Range Image Data with Segmentation Ground Truth](http://marathon.csee.usf.edu/range/seg-comp/SegComp.html) - 80 image sets (Formats: Sun rasterimage)\n118. [University of Oulu Physics-based Face Database](http://www.ee.oulu.fi/research/imag/color/pbfd.html) - contains color images of faces under different illuminants and camera calibration conditions as well as skin spectral reflectance measurements of each person.\n119. [Machine Vision and Media Processing Unit](http://www.ee.oulu.fi/mvmp/)\n121. [University of Oulu Texture Database](http://www.outex.oulu.fi) - Database of 320 surface textures, each captured under three illuminants, six spatial resolutions and nine rotation angles. A set of test suites is also provided so that texture segmentation, classification, and retrieval algorithms can be tested in a standard manner. (Formats: bmp, ras, xv)\n122. [Machine Vision Group](http://www.ee.oulu.fi/mvg)\n124. [Usenix face database](ftp://ftp.uu.net/published/usenix/faces) - Thousands of face images from many different sites (circa 994)\n125. [View Sphere Database](http://www-prima.inrialpes.fr/Prima/hall/view_sphere.html) - Images of 8 objects seen from many different view points. The view sphere is sampled using a geodesic with 172 images/sphere. Two sets for training and testing are available. (Formats: ppm)\n126. [PRIMA, GRAVIR](http://www-prima.inrialpes.fr/Prima/)\n127. [Vision-list Imagery Archive](ftp://ftp.vislist.com/IMAGERY/) - Many images, many formats\n128. [Wiry Object Recognition Database](http://www.cs.cmu.edu/~owenc/word.htm) - Thousands of images of a cart, ladder, stool, bicycle, chairs, and cluttered scenes with ground truth labelings of edges and regions. (Formats: jpg)\n129. [3D Vision Group](http://www.cs.cmu.edu/0.000000E+003dvision/)\n131. [Yale Face Database](http://cvc.yale.edu/projects/yalefaces/yalefaces.html) -  165 images (15 individuals) with different lighting, expression, and occlusion configurations.\n132. [Yale Face Database B](http://cvc.yale.edu/projects/yalefacesB/yalefacesB.html) - 5760 single light source images of 10 subjects each seen under 576 viewing conditions (9 poses x 64 illumination conditions). (Formats: PGM)\n133. [Center for Computational Vision and Control](http://cvc.yale.edu/)\n134. [DeepMind QA Corpus](https://github.com/deepmind/rc-data) - Textual QA corpus from CNN and DailyMail. More than 300K documents in total. [Paper](http://arxiv.org/abs/1506.03340) for reference.\n135. [YouTube-8M Dataset](https://research.google.com/youtube8m/) - YouTube-8M is a large-scale labeled video dataset that consists of 8 million YouTube video IDs and associated labels from a diverse vocabulary of 4800 visual entities.\n136. [Open Images dataset](https://github.com/openimages/dataset) - Open Images is a dataset of ~9 million URLs to images that have been annotated with labels spanning over 6000 categories.\n137. [Visual Object Classes Challenge 2012 (VOC2012)](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html#devkit) - VOC2012 dataset containing 12k images with 20 annotated classes for object detection and segmentation.\n138. [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) - MNIST like fashion product dataset consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes.\n139. [Large-scale Fashion (DeepFashion) Database](http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) - Contains over 800,000 diverse fashion images.  Each image in this dataset is labeled with 50 categories, 1,000 descriptive attributes, bounding box and clothing landmarks\n140. [FakeNewsCorpus](https://github.com/several27/FakeNewsCorpus) - Contains about 10 million news articles classified using [opensources.co](http://opensources.co) types\n141. [LLVIP](https://github.com/bupt-ai-cz/LLVIP) - 15488 visible-infrared paired images (30976 images) for low-light vision research, [Project_Page](https://bupt-ai-cz.github.io/LLVIP/)\n142. [MSDA](https://github.com/bupt-ai-cz/Meta-SelfLearning) - Over over 5 million images from 5 different domains for multi-source ocr/text recognition DA research, [Project_Page](https://bupt-ai-cz.github.io/Meta-SelfLearning/)\n143. [SANAD: Single-Label Arabic News Articles Dataset for Automatic Text Categorization](https://data.mendeley.com/datasets/57zpx667y9/2) - SANAD Dataset is a large collection of Arabic news articles that can be used in different Arabic NLP tasks such as Text Classification and Word Embedding. The articles were collected using Python scripts written specifically for three popular news websites: AlKhaleej, AlArabiya and Akhbarona. \n144. [Referit3D](https://referit3d.github.io) - Two large-scale and complementary visio-linguistic datasets (aka Nr3D and Sr3D) for identifying fine-grained 3D objects in ScanNet scenes. Nr3D contains 41.5K natural, free-form utterances, and Sr3d contains 83.5K template-based utterances.\n145. [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) - Stanford released ~100,000 English QA pairs and ~50,000 unanswerable questions\n146. [FQuAD](https://fquad.illuin.tech/) - ~25,000 French QA pairs released by Illuin Technology\n147. [GermanQuAD and GermanDPR](https://www.deepset.ai/germanquad) - deepset released ~14,000 German QA pairs\n148. [SberQuAD](https://github.com/annnyway/QA-for-Russian) - Sberbank released ~90,000 Russian QA pairs\n149. [ArtEmis](http://artemisdataset.org/) - Contains 450K affective annotations of emotional responses and linguistic explanations for 80,000 artworks of WikiArt.\n\n### Conferences\n\n1. [CVPR - IEEE Conference on Computer Vision and Pattern Recognition](http://cvpr2018.thecvf.com)\n2. [AAMAS - International Joint Conference on Autonomous Agents and Multiagent Systems](http://celweb.vuse.vanderbilt.edu/aamas18/)\n3. [IJCAI - \tInternational Joint Conference on Artificial Intelligence](https://www.ijcai-18.org/)\n4. [ICML - \tInternational Conference on Machine Learning](https://icml.cc)\n5. [ECML - European Conference on Machine Learning](http://www.ecmlpkdd2018.org)\n6. [KDD - Knowledge Discovery and Data Mining](http://www.kdd.org/kdd2018/)\n7. [NIPS - Neural Information Processing Systems](https://nips.cc/Conferences/2018)\n8. [O'Reilly AI Conference - \tO'Reilly Artificial Intelligence Conference](https://conferences.oreilly.com/artificial-intelligence/ai-ny)\n9. [ICDM - International Conference on Data Mining](https://www.waset.org/conference/2018/07/istanbul/ICDM)\n10. [ICCV - International Conference on Computer Vision](http://iccv2017.thecvf.com)\n11. [AAAI - Association for the Advancement of Artificial Intelligence](https://www.aaai.org)\n12. [MAIS - Montreal AI Symposium](https://montrealaisymposium.wordpress.com/)\n\n### Frameworks\n\n1.  [Caffe](http://caffe.berkeleyvision.org/)  \n2.  [Torch7](http://torch.ch/)\n3.  [Theano](http://deeplearning.net/software/theano/)\n4.  [cuda-convnet](https://code.google.com/p/cuda-convnet2/)\n5.  [convetjs](https://github.com/karpathy/convnetjs)\n5.  [Ccv](http://libccv.org/doc/doc-convnet/)\n6.  [NuPIC](http://numenta.org/nupic.html)\n7.  [DeepLearning4J](http://deeplearning4j.org/)\n8.  [Brain](https://github.com/harthur/brain)\n9.  [DeepLearnToolbox](https://github.com/rasmusbergpalm/DeepLearnToolbox)\n10.  [Deepnet](https://github.com/nitishsrivastava/deepnet)\n11.  [Deeppy](https://github.com/andersbll/deeppy)\n12.  [JavaNN](https://github.com/ivan-vasilev/neuralnetworks)\n13.  [hebel](https://github.com/hannes-brt/hebel)\n14.  [Mocha.jl](https://github.com/pluskid/Mocha.jl)\n15.  [OpenDL](https://github.com/guoding83128/OpenDL)\n16.  [cuDNN](https://developer.nvidia.com/cuDNN)\n17.  [MGL](http://melisgl.github.io/mgl-pax-world/mgl-manual.html)\n18.  [Knet.jl](https://github.com/denizyuret/Knet.jl)\n19.  [Nvidia DIGITS - a web app based on Caffe](https://github.com/NVIDIA/DIGITS)\n20.  [Neon - Python based Deep Learning Framework](https://github.com/NervanaSystems/neon)\n21.  [Keras - Theano based Deep Learning Library](http://keras.io)\n22.  [Chainer - A flexible framework of neural networks for deep learning](http://chainer.org/)\n23.  [RNNLM Toolkit](http://rnnlm.org/)\n24.  [RNNLIB - A recurrent neural network library](http://sourceforge.net/p/rnnl/wiki/Home/)\n25.  [char-rnn](https://github.com/karpathy/char-rnn)\n26.  [MatConvNet: CNNs for MATLAB](https://github.com/vlfeat/matconvnet)\n27.  [Minerva - a fast and flexible tool for deep learning on multi-GPU](https://github.com/dmlc/minerva)\n28.  [Brainstorm - Fast, flexible and fun neural networks.](https://github.com/IDSIA/brainstorm)\n29.  [Tensorflow - Open source software library for numerical computation using data flow graphs](https://github.com/tensorflow/tensorflow)\n30.  [DMTK - Microsoft Distributed Machine Learning Tookit](https://github.com/Microsoft/DMTK)\n31.  [Scikit Flow - Simplified interface for TensorFlow (mimicking Scikit Learn)](https://github.com/google/skflow)\n32.  [MXnet - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning framework](https://github.com/apache/incubator-mxnet)\n33.  [Veles - Samsung Distributed machine learning platform](https://github.com/Samsung/veles)\n34.  [Marvin - A Minimalist GPU-only N-Dimensional ConvNets Framework](https://github.com/PrincetonVision/marvin)\n35.  [Apache SINGA - A General Distributed Deep Learning Platform](http://singa.incubator.apache.org/)\n36.  [DSSTNE - Amazon's library for building Deep Learning models](https://github.com/amznlabs/amazon-dsstne)\n37.  [SyntaxNet - Google's syntactic parser - A TensorFlow dependency library](https://github.com/tensorflow/models/tree/master/syntaxnet)\n38.  [mlpack - A scalable Machine Learning library](http://mlpack.org/)\n39.  [Torchnet - Torch based Deep Learning Library](https://github.com/torchnet/torchnet)\n40.  [Paddle - PArallel Distributed Deep LEarning by Baidu](https://github.com/baidu/paddle)\n41.  [NeuPy - Theano based Python library for ANN and Deep Learning](http://neupy.com)\n42.  [Lasagne - a lightweight library to build and train neural networks in Theano](https://github.com/Lasagne/Lasagne)\n43.  [nolearn - wrappers and abstractions around existing neural network libraries, most notably Lasagne](https://github.com/dnouri/nolearn)\n44.  [Sonnet - a library for constructing neural networks by Google's DeepMind](https://github.com/deepmind/sonnet)\n45.  [PyTorch - Tensors and Dynamic neural networks in Python with strong GPU acceleration](https://github.com/pytorch/pytorch)\n46.  [CNTK - Microsoft Cognitive Toolkit](https://github.com/Microsoft/CNTK)\n47.  [Serpent.AI - Game agent framework: Use any video game as a deep learning sandbox](https://github.com/SerpentAI/SerpentAI)\n48.  [Caffe2 - A New Lightweight, Modular, and Scalable Deep Learning Framework](https://github.com/caffe2/caffe2)\n49.  [deeplearn.js - Hardware-accelerated deep learning and linear algebra (NumPy) library for the web](https://github.com/PAIR-code/deeplearnjs)\n50.  [TVM - End to End Deep Learning Compiler Stack for CPUs, GPUs and specialized accelerators](https://tvm.ai/)\n51.  [Coach - Reinforcement Learning Coach by Intel\u00ae AI Lab](https://github.com/NervanaSystems/coach)\n52.  [albumentations - A fast and framework agnostic image augmentation library](https://github.com/albu/albumentations)\n53.  [Neuraxle - A general-purpose ML pipelining framework](https://github.com/Neuraxio/Neuraxle)\n54.  [Catalyst: High-level utils for PyTorch DL & RL research. It was developed with a focus on reproducibility, fast experimentation and code/ideas reusing](https://github.com/catalyst-team/catalyst)\n55.  [garage - A toolkit for reproducible reinforcement learning research](https://github.com/rlworkgroup/garage)\n56.  [Detecto - Train and run object detection models with 5-10 lines of code](https://github.com/alankbi/detecto)\n57.  [Karate Club - An unsupervised machine learning library for graph structured data](https://github.com/benedekrozemberczki/karateclub)\n58.  [Synapses - A lightweight library for neural networks that runs anywhere](https://github.com/mrdimosthenis/Synapses)\n59.  [TensorForce - A TensorFlow library for applied reinforcement learning](https://github.com/reinforceio/tensorforce)\n60.  [Hopsworks - A Feature Store for ML and Data-Intensive AI](https://github.com/logicalclocks/hopsworks)\n61.  [Feast - A Feature Store for ML for GCP by Gojek/Google](https://github.com/gojek/feast)\n62.  [PyTorch Geometric Temporal - Representation learning on dynamic graphs](https://github.com/gojek/feast)\n63.  [lightly - A computer vision framework for self-supervised learning](https://github.com/lightly-ai/lightly)\n64.  [Trax \u2014 Deep Learning with Clear Code and Speed](https://github.com/google/trax)\n65.  [Flax - a neural network ecosystem for JAX that is designed for flexibility](https://github.com/google/flax)\n66.  [QuickVision](https://github.com/Quick-AI/quickvision)\n67.  [Colossal-AI - An Integrated Large-scale Model Training System with Efficient Parallelization Techniques](https://github.com/hpcaitech/ColossalAI)\n68.  [haystack: an open-source neural search framework](https://haystack.deepset.ai/docs/intromd)\n69.  [Maze](https://github.com/enlite-ai/maze) - Application-oriented deep reinforcement learning framework addressing real-world decision problems.\n70.  [InsNet - A neural network library for building instance-dependent NLP models with padding-free dynamic batching](https://github.com/chncwang/InsNet)\n\n### Tools\n\n1.  [Nebullvm](https://github.com/nebuly-ai/nebullvm) - Easy-to-use library to boost deep learning inference leveraging multiple deep learning compilers.\n2.  [Netron](https://github.com/lutzroeder/netron) - Visualizer for deep learning and machine learning models\n2.  [Jupyter Notebook](http://jupyter.org) - Web-based notebook environment for interactive computing\n3.  [TensorBoard](https://github.com/tensorflow/tensorboard) - TensorFlow's Visualization Toolkit\n4.  [Visual Studio Tools for AI](https://www.microsoft.com/en-us/research/project/visual-studio-code-tools-ai/) - Develop, debug and deploy deep learning and AI solutions\n5.  [TensorWatch](https://github.com/microsoft/tensorwatch) - Debugging and visualization for deep learning\n6. [ML Workspace](https://github.com/ml-tooling/ml-workspace) - All-in-one web-based IDE for machine learning and data science.\n7.  [dowel](https://github.com/rlworkgroup/dowel) - A little logger for machine learning research. Log any object to the console, CSVs, TensorBoard, text log files, and more with just one call to `logger.log()`\n8.  [Neptune](https://neptune.ai/) - Lightweight tool for experiment tracking and results visualization. \n9.  [CatalyzeX](https://chrome.google.com/webstore/detail/code-finder-for-research/aikkeehnlfpamidigaffhfmgbkdeheil) - Browser extension ([Chrome](https://chrome.google.com/webstore/detail/code-finder-for-research/aikkeehnlfpamidigaffhfmgbkdeheil) and [Firefox](https://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex/)) that automatically finds and links to code implementations for ML papers anywhere online: Google, Twitter, Arxiv, Scholar, etc.\n10. [Determined](https://github.com/determined-ai/determined) - Deep learning training platform with integrated support for distributed training, hyperparameter tuning, smart GPU scheduling, experiment tracking, and a model registry.\n11. [DAGsHub](https://dagshub.com/) - Community platform for Open Source ML \u2013 Manage experiments, data & models and create collaborative ML projects easily.\n12. [hub](https://github.com/activeloopai/Hub) - Fastest unstructured dataset management for TensorFlow/PyTorch by activeloop.ai. Stream & version-control data. Converts large data into single     numpy-like array on the cloud, accessible on any machine.\n13. [DVC](https://dvc.org/) - DVC is built to make ML models shareable and reproducible. It is designed to handle large files, data sets, machine learning models, and metrics as well as code.\n14. [CML](https://cml.dev/) - CML helps you bring your favorite DevOps tools to machine learning.\n15. [MLEM](https://mlem.ai/) - MLEM is a tool to easily package, deploy and serve Machine Learning models. It seamlessly supports a variety of scenarios like real-time serving and batch processing.\n16. [Maxim AI](https://getmaxim.ai) - Tool for AI Agent Simulation, Evaluation & Observability.\n\n\n### Miscellaneous\n\n1.  [Caffe Webinar](http://on-demand-gtc.gputechconf.com/gtcnew/on-demand-gtc.php?searchByKeyword=shelhamer&amp;searchItems=&amp;sessionTopic=&amp;sessionEvent=4&amp;sessionYear=2014&amp;sessionFormat=&amp;submit=&amp;select=+)\n2.  [100 Best Github Resources in Github for DL](http://meta-guide.com/software-meta-guide/100-best-github-deep-learning/)\n3.  [Word2Vec](https://code.google.com/p/word2vec/)\n4.  [Caffe DockerFile](https://github.com/tleyden/docker/tree/master/caffe)\n5.  [TorontoDeepLEarning convnet](https://github.com/TorontoDeepLearning/convnet)\n6.  [gfx.js](https://github.com/clementfarabet/gfx.js)\n7.  [Torch7 Cheat sheet](https://github.com/torch/torch7/wiki/Cheatsheet)\n8. [Misc from MIT's 'Advanced Natural Language Processing' course](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-864-advanced-natural-language-processing-fall-2005/)\n9. [Misc from MIT's 'Machine Learning' course](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-867-machine-learning-fall-2006/lecture-notes/)\n10. [Misc from MIT's 'Networks for Learning: Regression and Classification' course](http://ocw.mit.edu/courses/brain-and-cognitive-sciences/9-520-a-networks-for-learning-regression-and-classification-spring-2001/)\n11. [Misc from MIT's 'Neural Coding and Perception of Sound' course](http://ocw.mit.edu/courses/health-sciences-and-technology/hst-723j-neural-coding-and-perception-of-sound-spring-2005/index.htm)\n12. [Implementing a Distributed Deep Learning Network over Spark](http://www.datasciencecentral.com/profiles/blogs/implementing-a-distributed-deep-learning-network-over-spark)\n13. [A chess AI that learns to play chess using deep learning.](https://github.com/erikbern/deep-pink)\n14. [Reproducing the results of \"Playing Atari with Deep Reinforcement Learning\" by DeepMind](https://github.com/kristjankorjus/Replicating-DeepMind)\n15. [Wiki2Vec. Getting Word2vec vectors for entities and word from Wikipedia Dumps](https://github.com/idio/wiki2vec)\n16. [The original code from the DeepMind article + tweaks](https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner)\n17. [Google deepdream - Neural Network art](https://github.com/google/deepdream)\n18. [An efficient, batched LSTM.](https://gist.github.com/karpathy/587454dc0146a6ae21fc)\n19. [A recurrent neural network designed to generate classical music.](https://github.com/hexahedria/biaxial-rnn-music-composition)\n20. [Memory Networks Implementations - Facebook](https://github.com/facebook/MemNN)\n21. [Face recognition with Google's FaceNet deep neural network.](https://github.com/cmusatyalab/openface)\n22. [Basic digit recognition neural network](https://github.com/joeledenberg/DigitRecognition)\n23. [Emotion Recognition API Demo - Microsoft](https://www.projectoxford.ai/demo/emotion#detection)\n24. [Proof of concept for loading Caffe models in TensorFlow](https://github.com/ethereon/caffe-tensorflow)\n25. [YOLO: Real-Time Object Detection](http://pjreddie.com/darknet/yolo/#webcam)\n26. [YOLO: Practical Implementation using Python](https://www.analyticsvidhya.com/blog/2018/12/practical-guide-object-detection-yolo-framewor-python/)\n27. [AlphaGo - A replication of DeepMind's 2016 Nature publication, \"Mastering the game of Go with deep neural networks and tree search\"](https://github.com/Rochester-NRT/AlphaGo)\n28. [Machine Learning for Software Engineers](https://github.com/ZuzooVn/machine-learning-for-software-engineers)\n29. [Machine Learning is Fun!](https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471#.oa4rzez3g)\n30. [Siraj Raval's Deep Learning tutorials](https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A)\n31. [Dockerface](https://github.com/natanielruiz/dockerface) - Easy to install and use deep learning Faster R-CNN face detection for images and video in a docker container.\n32. [Awesome Deep Learning Music](https://github.com/ybayle/awesome-deep-learning-music) - Curated list of articles related to deep learning scientific research applied to music\n33. [Awesome Graph Embedding](https://github.com/benedekrozemberczki/awesome-graph-embedding) - Curated list of articles related to deep learning scientific research on graph structured data at the graph level.\n34. [Awesome Network Embedding](https://github.com/chihming/awesome-network-embedding) - Curated list of articles related to deep learning scientific research on graph structured data at the node level.\n35. [Microsoft Recommenders](https://github.com/Microsoft/Recommenders) contains examples, utilities and best practices for building recommendation systems. Implementations of several state-of-the-art algorithms are provided for self-study and customization in your own applications.\n36. [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) - Andrej Karpathy blog post about using RNN for generating text.\n37. [Ladder Network](https://github.com/divamgupta/ladder_network_keras) - Keras Implementation of Ladder Network for Semi-Supervised Learning \n38. [toolbox: Curated list of ML libraries](https://github.com/amitness/toolbox)\n39. [CNN Explainer](https://poloclub.github.io/cnn-explainer/)\n40. [AI Expert Roadmap](https://github.com/AMAI-GmbH/AI-Expert-Roadmap) - Roadmap to becoming an Artificial Intelligence Expert\n41. [Awesome Drug Interactions, Synergy, and Polypharmacy Prediction](https://github.com/AstraZeneca/awesome-polipharmacy-side-effect-prediction/)\n\n-----\n### Contributing\nHave anything in mind that you think is awesome and would fit in this list? Feel free to send a [pull request](https://github.com/ashara12/awesome-deeplearning/pulls).\n\n-----\n## License\n\n[![CC0](http://i.creativecommons.org/p/zero/1.0/88x31.png)](http://creativecommons.org/publicdomain/zero/1.0/)\n\nTo the extent possible under law, [Christos Christofidis](https://linkedin.com/in/Christofidis) has waived all copyright and related or neighboring rights to this work.\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 51863547,
    "name": "handson-ml",
    "full_name": "ageron/handson-ml",
    "description": "\u26d4\ufe0f DEPRECATED \u2013 See https://github.com/ageron/handson-ml3 instead.",
    "html_url": "https://github.com/ageron/handson-ml",
    "clone_url": "https://github.com/ageron/handson-ml.git",
    "owner_login": "ageron",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/76661?v=4",
    "stargazers_count": 25538,
    "watchers_count": 25538,
    "forks_count": 12883,
    "open_issues_count": 143,
    "size": 87255,
    "language": "Jupyter Notebook",
    "languages": {
      "Jupyter Notebook": 21812614,
      "Python": 67948,
      "Dockerfile": 3280,
      "Shell": 748,
      "Makefile": 260
    },
    "topics": [
      "deep-learning",
      "deprecated",
      "distributed",
      "jupyter-notebook",
      "machine-learning",
      "ml",
      "neural-network",
      "python",
      "scikit-learn",
      "tensorflow"
    ],
    "license_name": "Apache License 2.0",
    "created_at": "2016-02-16T19:48:39+00:00",
    "updated_at": "2025-08-05T18:25:28+00:00",
    "pushed_at": "2023-10-03T23:44:04+00:00",
    "contributors_count": 36,
    "readme_length": 5713,
    "readme_content": "Machine Learning Notebooks\n==========================\n\n# \u26a0 THE <a href=\"https://github.com/ageron/handson-ml3\">THIRD EDITION OF MY BOOK</a> IS NOW AVAILABLE.\n\nThis project is for the first edition, which is now outdated.\n\n<details>\n\nThis project aims at teaching you the fundamentals of Machine Learning in\npython. It contains the example code and solutions to the exercises in my O'Reilly book [Hands-on Machine Learning with Scikit-Learn and TensorFlow](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781491962282/):\n\n[![book](http://akamaicovers.oreilly.com/images/9781491962282/cat.gif)](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781491962282/)\n\n\n## Quick Start\n\n### Want to play with these notebooks online without having to install anything?\nUse any of the following services.\n\n**WARNING**: Please be aware that these services provide temporary environments: anything you do will be deleted after a while, so make sure you download any data you care about.\n\n* **Recommended**: open this repository in [Colaboratory](https://colab.research.google.com/github/ageron/handson-ml/blob/master/):\n<a href=\"https://colab.research.google.com/github/ageron/handson-ml/blob/master/\"><img src=\"https://colab.research.google.com/img/colab_favicon.ico\" width=\"90\" /></a>\n\n* Or open it in [Binder](https://mybinder.org/v2/gh/ageron/handson-ml/master):\n<a href=\"https://mybinder.org/v2/gh/ageron/handson-ml/master\"><img src=\"https://matthiasbussonnier.com/posts/img/binder_logo_128x128.png\" width=\"90\" /></a>\n\n  * _Note_: Most of the time, Binder starts up quickly and works great, but when handson-ml is updated, Binder creates a new environment from scratch, and this can take quite some time.\n\n* Or open it in [Deepnote](https://beta.deepnote.com/launch?template=data-science&url=https%3A//github.com/ageron/handson-ml/blob/master/index.ipynb):\n<a href=\"https://beta.deepnote.com/launch?template=data-science&url=https%3A//github.com/ageron/handson-ml/blob/master/index.ipynb\"><img src=\"https://www.deepnote.com/static/illustration.png\" width=\"150\" /></a>\n\n### Just want to quickly look at some notebooks, without executing any code?\n\nBrowse this repository using [jupyter.org's notebook viewer](https://nbviewer.jupyter.org/github/ageron/handson-ml/blob/master/index.ipynb):\n<a href=\"https://nbviewer.jupyter.org/github/ageron/handson-ml/blob/master/index.ipynb\"><img src=\"https://jupyter.org/assets/logos/rectanglelogo-greytext-orangebody-greymoons.svg\" width=\"150\" /></a>\n\n_Note_: [github.com's notebook viewer](index.ipynb) also works but it is slower and the math equations are not always displayed correctly.\n\n### Want to run this project using a Docker image?\nRead the [Docker instructions](https://github.com/ageron/handson-ml/tree/master/docker).\n\n### Want to install this project on your own machine?\n\nStart by installing [Anaconda](https://www.anaconda.com/distribution/) (or [Miniconda](https://docs.conda.io/en/latest/miniconda.html)), [git](https://git-scm.com/downloads), and if you have a TensorFlow-compatible GPU, install the [GPU driver](https://www.nvidia.com/Download/index.aspx), as well as the appropriate version of CUDA and cuDNN (see TensorFlow's documentation for more details).\n\nNext, clone this project by opening a terminal and typing the following commands (do not type the first `$` signs on each line, they just indicate that these are terminal commands):\n\n    $ git clone https://github.com/ageron/handson-ml.git\n    $ cd handson-ml\n\nNext, run the following commands:\n\n    $ conda env create -f environment.yml\n    $ conda activate tf1\n    $ python -m ipykernel install --user --name=python3\n\nFinally, start Jupyter:\n\n    $ jupyter notebook\n\nIf you need further instructions, read the [detailed installation instructions](INSTALL.md).\n\n# FAQ\n\n**Which Python version should I use?**\n\nI recommend Python 3.7. If you follow the installation instructions above, that's the version you will get. Most code will work with other versions of Python 3, but some libraries do not support Python 3.8 or 3.9 yet, which is why I recommend Python 3.7.\n\n**I'm getting an error when I call `load_housing_data()`**\n\nMake sure you call `fetch_housing_data()` *before* you call `load_housing_data()`. If you're getting an HTTP error, make sure you're running the exact same code as in the notebook (copy/paste it if needed). If the problem persists, please check your network configuration.\n\n**I'm getting an SSL error on MacOSX**\n\nYou probably need to install the SSL certificates (see this [StackOverflow question](https://stackoverflow.com/questions/27835619/urllib-and-ssl-certificate-verify-failed-error)). If you downloaded Python from the official website, then run `/Applications/Python\\ 3.7/Install\\ Certificates.command` in a terminal (change `3.7` to whatever version you installed). If you installed Python using MacPorts, run `sudo port install curl-ca-bundle` in a terminal.\n\n**I've installed this project locally. How do I update it to the latest version?**\n\nSee [INSTALL.md](INSTALL.md)\n\n**How do I update my Python libraries to the latest versions, when using Anaconda?**\n\nSee [INSTALL.md](INSTALL.md)\n\n## Contributors\nI would like to thank everyone [who contributed to this project](https://github.com/ageron/handson-ml/graphs/contributors), either by providing useful feedback, filing issues or submitting Pull Requests. Special thanks go to Haesun Park and Ian Beauregard who reviewed every notebook and submitted many PRs, including help on some of the exercise solutions. Thanks as well to Steven Bunkley and Ziembla who created the `docker` directory, and to github user SuperYorio who helped on some exercise solutions.\n\n</details>\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 268163609,
    "name": "qdrant",
    "full_name": "qdrant/qdrant",
    "description": "Qdrant - High-performance, massive-scale Vector Database and Vector Search Engine for the next generation of AI. Also available in the cloud https://cloud.qdrant.io/",
    "html_url": "https://github.com/qdrant/qdrant",
    "clone_url": "https://github.com/qdrant/qdrant.git",
    "owner_login": "qdrant",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/73504361?v=4",
    "stargazers_count": 25125,
    "watchers_count": 25125,
    "forks_count": 1738,
    "open_issues_count": 393,
    "size": 36172,
    "language": "Rust",
    "languages": {
      "Rust": 9060177,
      "Python": 1096343,
      "Shell": 96139,
      "C": 41650,
      "Dockerfile": 8184,
      "Nix": 6894,
      "JavaScript": 1502,
      "Mermaid": 970
    },
    "topics": [
      "ai-search",
      "ai-search-engine",
      "embeddings-similarity",
      "hnsw",
      "image-search",
      "knn-algorithm",
      "machine-learning",
      "mlops",
      "nearest-neighbor-search",
      "neural-network",
      "neural-search",
      "recommender-system",
      "search",
      "search-engine",
      "search-engines",
      "similarity-search",
      "vector-database",
      "vector-search",
      "vector-search-engine"
    ],
    "license_name": "Apache License 2.0",
    "created_at": "2020-05-30T21:37:01+00:00",
    "updated_at": "2025-08-06T02:05:26+00:00",
    "pushed_at": "2025-08-06T00:18:59+00:00",
    "contributors_count": 100,
    "readme_length": 11579,
    "readme_content": "<p align=\"center\">\n  <picture>\n      <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://github.com/qdrant/qdrant/raw/master/docs/logo-dark.svg\">\n      <source media=\"(prefers-color-scheme: light)\" srcset=\"https://github.com/qdrant/qdrant/raw/master/docs/logo-light.svg\">\n      <img height=\"100\" alt=\"Qdrant\" src=\"https://github.com/qdrant/qdrant/raw/master/docs/logo.svg\">\n  </picture>\n</p>\n\n<p align=\"center\">\n    <b>Vector Search Engine for the next generation of AI applications</b>\n</p>\n\n<p align=center>\n    <a href=\"https://github.com/qdrant/qdrant/actions/workflows/rust.yml\"><img src=\"https://img.shields.io/github/actions/workflow/status/qdrant/qdrant/rust.yml?style=flat-square\" alt=\"Tests status\"></a>\n    <a href=\"https://api.qdrant.tech/\"><img src=\"https://img.shields.io/badge/Docs-OpenAPI%203.0-success?style=flat-square\" alt=\"OpenAPI Docs\"></a>\n    <a href=\"https://github.com/qdrant/qdrant/blob/master/LICENSE\"><img src=\"https://img.shields.io/github/license/qdrant/qdrant?style=flat-square\" alt=\"Apache 2.0 License\"></a>\n    <a href=\"https://qdrant.to/discord\"><img src=\"https://img.shields.io/discord/907569970500743200?logo=Discord&style=flat-square&color=7289da\" alt=\"Discord\"></a>\n    <a href=\"https://qdrant.to/roadmap\"><img src=\"https://img.shields.io/badge/Roadmap-2025-bc1439.svg?style=flat-square\" alt=\"Roadmap 2025\"></a>\n    <a href=\"https://cloud.qdrant.io/\"><img src=\"https://img.shields.io/badge/Qdrant-Cloud-24386C.svg?logo=cloud&style=flat-square\" alt=\"Qdrant Cloud\"></a>\n</p>\n\n**Qdrant** (read: _quadrant_) is a vector similarity search engine and vector database.\nIt provides a production-ready service with a convenient API to store, search, and manage points\u2014vectors with an additional payload\nQdrant is tailored to extended filtering support. It makes it useful for all sorts of neural-network or semantic-based matching, faceted search, and other applications.\n\nQdrant is written in Rust \ud83e\udd80, which makes it fast and reliable even under high load. See [benchmarks](https://qdrant.tech/benchmarks/).\n\nWith Qdrant, embeddings or neural network encoders can be turned into full-fledged applications for matching, searching, recommending, and much more!\n\nQdrant is also available as a fully managed **[Qdrant Cloud](https://cloud.qdrant.io/)** \u26c5 including a **free tier**.\n\n<p align=\"center\">\n<strong><a href=\"docs/QUICK_START.md\">Quick Start</a> \u2022 <a href=\"#clients\">Client Libraries</a> \u2022 <a href=\"#demo-projects\">Demo Projects</a> \u2022 <a href=\"#integrations\">Integrations</a> \u2022 <a href=\"#contacts\">Contact</a>\n\n</strong>\n</p>\n\n## Getting Started\n\n### Python\n\n```\npip install qdrant-client\n```\n\nThe python client offers a convenient way to start with Qdrant locally:\n\n```python\nfrom qdrant_client import QdrantClient\nqdrant = QdrantClient(\":memory:\") # Create in-memory Qdrant instance, for testing, CI/CD\n# OR\nclient = QdrantClient(path=\"path/to/db\")  # Persists changes to disk, fast prototyping\n```\n\n### Client-Server\n\nTo experience the full power of Qdrant locally, run the container with this command:\n\n```bash\ndocker run -p 6333:6333 qdrant/qdrant\n```\n\nNow you can connect to this with any client, including Python:\n\n```python\nqdrant = QdrantClient(\"http://localhost:6333\") # Connect to existing Qdrant instance\n```\n\nBefore deploying Qdrant to production, be sure to read our [installation](https://qdrant.tech/documentation/guides/installation/) and [security](https://qdrant.tech/documentation/guides/security/) guides.\n\n### Clients\n\nQdrant offers the following client libraries to help you integrate it into your application stack with ease:\n\n- Official:\n  - [Go client](https://github.com/qdrant/go-client)\n  - [Rust client](https://github.com/qdrant/rust-client)\n  - [JavaScript/TypeScript client](https://github.com/qdrant/qdrant-js)\n  - [Python client](https://github.com/qdrant/qdrant-client)\n  - [.NET/C# client](https://github.com/qdrant/qdrant-dotnet)\n  - [Java client](https://github.com/qdrant/java-client)\n- Community:\n  - [Elixir](https://hexdocs.pm/qdrant/readme.html)\n  - [PHP](https://github.com/hkulekci/qdrant-php)\n  - [Ruby](https://github.com/andreibondarev/qdrant-ruby)\n  - [Java](https://github.com/metaloom/qdrant-java-client)\n\n### Where do I go from here?\n\n- [Quick Start Guide](docs/QUICK_START.md)\n- End to End [Colab Notebook](https://colab.research.google.com/drive/1Bz8RSVHwnNDaNtDwotfPj0w7AYzsdXZ-?usp=sharing) demo with SentenceBERT and Qdrant\n- Detailed [Documentation](https://qdrant.tech/documentation/) are great starting points\n- [Step-by-Step Tutorial](https://qdrant.to/qdrant-tutorial) to create your first neural network project with Qdrant\n\n## Demo Projects  <a href=\"https://replit.com/@qdrant\"><img align=\"right\" src=\"https://replit.com/badge/github/qdrant/qdrant\" alt=\"Run on Repl.it\"></a>\n\n### Discover Semantic Text Search \ud83d\udd0d\n\nUnlock the power of semantic embeddings with Qdrant, transcending keyword-based search to find meaningful connections in short texts. Deploy a neural search in minutes using a pre-trained neural network, and experience the future of text search. [Try it online!](https://qdrant.to/semantic-search-demo)\n\n### Explore Similar Image Search - Food Discovery \ud83c\udf55\n\nThere's more to discovery than text search, especially when it comes to food. People often choose meals based on appearance rather than descriptions and ingredients. Let Qdrant help your users find their next delicious meal using visual search, even if they don't know the dish's name. [Check it out!](https://qdrant.to/food-discovery)\n\n### Master Extreme Classification - E-commerce Product Categorization \ud83d\udcfa\n\nEnter the cutting-edge realm of extreme classification, an emerging machine learning field tackling multi-class and multi-label problems with millions of labels. Harness the potential of similarity learning models, and see how a pre-trained transformer model and Qdrant can revolutionize e-commerce product categorization. [Play with it online!](https://qdrant.to/extreme-classification-demo)\n\n<details>\n<summary> More solutions </summary>\n\n<table>\n    <tr>\n        <td width=\"30%\">\n            <img src=\"https://qdrant.tech/content/images/text_search.png\">\n        </td>\n        <td width=\"30%\">\n            <img src=\"https://qdrant.tech/content/images/image_search.png\">\n        </td>\n        <td width=\"30%\">\n            <img src=\"https://qdrant.tech/content/images/recommendations.png\">\n        </td>\n    </tr>\n    <tr>\n        <td>\n            Semantic Text Search\n        </td>\n        <td>\n            Similar Image Search\n        </td>\n        <td>\n            Recommendations\n        </td>\n    </tr>\n</table>\n\n<table>\n    <tr>\n        <td>\n            <img width=\"300px\" src=\"https://qdrant.tech/content/images/chat_bots.png\">\n        </td>\n        <td>\n            <img width=\"300px\" src=\"https://qdrant.tech/content/images/matching_engines.png\">\n        </td>\n        <td>\n            <img width=\"300px\" src=\"https://qdrant.tech/content/images/anomalies_detection.png\">\n        </td>\n    </tr>\n    <tr>\n        <td>\n            Chat Bots\n        </td>\n        <td>\n            Matching Engines\n        </td>\n        <td>\n            Anomaly Detection\n        </td>\n    </tr>\n</table>\n\n</details>\n\n## API\n\n### REST\n\nOnline OpenAPI 3.0 documentation is available [here](https://api.qdrant.tech/).\nOpenAPI makes it easy to generate a client for virtually any framework or programming language.\n\nYou can also download raw OpenAPI [definitions](https://github.com/qdrant/qdrant/blob/master/docs/redoc/master/openapi.json).\n\n### gRPC\n\nFor faster production-tier searches, Qdrant also provides a gRPC interface. You can find gRPC documentation [here](https://qdrant.tech/documentation/interfaces/#grpc-interface).\n\n## Features\n\n### Filtering and Payload\n\nQdrant can attach any JSON payloads to vectors, allowing for both the storage and filtering of data based on the values in these payloads.\nPayload supports a wide range of data types and query conditions, including keyword matching, full-text filtering, numerical ranges, geo-locations, and more.\n\nFiltering conditions can be combined in various ways, including `should`, `must`, and `must_not` clauses,\nensuring that you can implement any desired business logic on top of similarity matching.\n\n\n### Hybrid Search with Sparse Vectors\n\nTo address the limitations of vector embeddings when searching for specific keywords, Qdrant introduces support for sparse vectors in addition to the regular dense ones.\n\nSparse vectors can be viewed as an generalization of BM25 or TF-IDF ranking. They enable you to harness the capabilities of transformer-based neural networks to weigh individual tokens effectively.\n\n\n### Vector Quantization and On-Disk Storage\n\nQdrant provides multiple options to make vector search cheaper and more resource-efficient.\nBuilt-in vector quantization reduces RAM usage by up to 97% and dynamically manages the trade-off between search speed and precision.\n\n\n### Distributed Deployment\n\nQdrant offers comprehensive horizontal scaling support through two key mechanisms:\n1. Size expansion via sharding and throughput enhancement via replication\n2. Zero-downtime rolling updates and seamless dynamic scaling of the collections\n\n\n### Highlighted Features\n\n* **Query Planning and Payload Indexes** - leverages stored payload information to optimize query execution strategy.\n* **SIMD Hardware Acceleration** - utilizes modern CPU x86-x64 and Neon architectures to deliver better performance.\n* **Async I/O** - uses `io_uring` to maximize disk throughput utilization even on a network-attached storage.\n* **Write-Ahead Logging** - ensures data persistence with update confirmation, even during power outages.\n\n\n# Integrations\n\nExamples and/or documentation of Qdrant integrations:\n\n- [Cohere](https://docs.cohere.com/docs/qdrant-and-cohere) ([blogpost on building a QA app with Cohere and Qdrant](https://qdrant.tech/articles/qa-with-cohere-and-qdrant/)) - Use Cohere embeddings with Qdrant\n- [DocArray](https://docs.docarray.org/user_guide/storing/index_qdrant/) - Use Qdrant as a document store in DocArray\n- [Haystack](https://haystack.deepset.ai/integrations/qdrant-document-store) - Use Qdrant as a document store with Haystack ([blogpost](https://haystack.deepset.ai/blog/qdrant-integration)).\n- [LangChain](https://python.langchain.com/docs/integrations/providers/qdrant/) ([blogpost](https://qdrant.tech/articles/langchain-integration/)) - Use Qdrant as a memory backend for LangChain.\n- [LlamaIndex](https://gpt-index.readthedocs.io/en/latest/examples/vector_stores/QdrantIndexDemo.html) - Use Qdrant as a Vector Store with LlamaIndex.\n- [OpenAI - ChatGPT retrieval plugin](https://github.com/openai/chatgpt-retrieval-plugin/blob/main/docs/providers/qdrant/setup.md) - Use Qdrant as a memory backend for ChatGPT\n- [Microsoft Semantic Kernel](https://devblogs.microsoft.com/semantic-kernel/the-power-of-persistent-memory-with-semantic-kernel-and-qdrant-vector-database/) - Use Qdrant as persistent memory with Semantic Kernel\n\n## Contacts\n\n- Have questions? Join our [Discord channel](https://qdrant.to/discord) or mention [@qdrant_engine on Twitter](https://qdrant.to/twitter)\n- Want to stay in touch with latest releases? Subscribe to our [Newsletters](https://qdrant.tech/subscribe/)\n- Looking for a managed cloud? Check [pricing](https://qdrant.tech/pricing/), need something personalised? We're at [info@qdrant.tech](mailto:info@qdrant.tech)\n\n## License\n\nQdrant is licensed under the Apache License, Version 2.0. View a copy of the [License file](https://github.com/qdrant/qdrant/blob/master/LICENSE).\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 142187665,
    "name": "PlotNeuralNet",
    "full_name": "HarisIqbal88/PlotNeuralNet",
    "description": "Latex code for making neural networks diagrams",
    "html_url": "https://github.com/HarisIqbal88/PlotNeuralNet",
    "clone_url": "https://github.com/HarisIqbal88/PlotNeuralNet.git",
    "owner_login": "HarisIqbal88",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/17570785?v=4",
    "stargazers_count": 23759,
    "watchers_count": 23759,
    "forks_count": 2993,
    "open_issues_count": 86,
    "size": 2269,
    "language": "TeX",
    "languages": {
      "TeX": 12494,
      "Python": 12463,
      "Shell": 164
    },
    "topics": [
      "deep-neural-networks",
      "latex"
    ],
    "license_name": "MIT License",
    "created_at": "2018-07-24T16:51:34+00:00",
    "updated_at": "2025-08-06T01:54:55+00:00",
    "pushed_at": "2023-08-21T17:47:04+00:00",
    "contributors_count": 10,
    "readme_length": 3397,
    "readme_content": "# PlotNeuralNet\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.2526396.svg)](https://doi.org/10.5281/zenodo.2526396)\n\nLatex code for drawing neural networks for reports and presentation. Have a look into examples to see how they are made. Additionally, lets consolidate any improvements that you make and fix any bugs to help more people with this code.\n\n## Examples\n\nFollowing are some network representations:\n\n<p align=\"center\"><img  src=\"https://user-images.githubusercontent.com/17570785/50308846-c2231880-049c-11e9-8763-3daa1024de78.png\" width=\"85%\" height=\"85%\"></p>\n<h6 align=\"center\">FCN-8 (<a href=\"https://www.overleaf.com/read/kkqntfxnvbsk\">view on Overleaf</a>)</h6>\n\n\n<p align=\"center\"><img  src=\"https://user-images.githubusercontent.com/17570785/50308873-e2eb6e00-049c-11e9-9587-9da6bdec011b.png\" width=\"85%\" height=\"85%\"></p>\n<h6 align=\"center\">FCN-32 (<a href=\"https://www.overleaf.com/read/wsxpmkqvjnbs\">view on Overleaf</a>)</h6>\n\n\n<p align=\"center\"><img  src=\"https://user-images.githubusercontent.com/17570785/50308911-03b3c380-049d-11e9-92d9-ce15669017ad.png\" width=\"85%\" height=\"85%\"></p>\n<h6 align=\"center\">Holistically-Nested Edge Detection (<a href=\"https://www.overleaf.com/read/jxhnkcnwhfxp\">view on Overleaf</a>)</h6>\n\n## Getting Started\n1. Install the following packages on Ubuntu.\n    * Ubuntu 16.04\n        ```\n        sudo apt-get install texlive-latex-extra\n        ```\n\n    * Ubuntu 18.04.2\nBase on this [website](https://gist.github.com/rain1024/98dd5e2c6c8c28f9ea9d), please install the following packages.\n        ```\n        sudo apt-get install texlive-latex-base\n        sudo apt-get install texlive-fonts-recommended\n        sudo apt-get install texlive-fonts-extra\n        sudo apt-get install texlive-latex-extra\n        ```\n\n    * Windows\n    1. Download and install [MikTeX](https://miktex.org/download).\n    2. Download and install bash runner on Windows, recommends [Git bash](https://git-scm.com/download/win) or Cygwin(https://www.cygwin.com/)\n\n2. Execute the example as followed.\n    ```\n    cd pyexamples/\n    bash ../tikzmake.sh test_simple\n    ```\n\n## TODO\n\n- [X] Python interface\n- [ ] Add easy legend functionality\n- [ ] Add more layer shapes like TruncatedPyramid, 2DSheet etc\n- [ ] Add examples for RNN and likes.\n\n## Latex usage\n\nSee [`examples`](examples) directory for usage.\n\n## Python usage\n\nFirst, create a new directory and a new Python file:\n\n    $ mkdir my_project\n    $ cd my_project\n    vim my_arch.py\n\nAdd the following code to your new file:\n\n```python\nimport sys\nsys.path.append('../')\nfrom pycore.tikzeng import *\n\n# defined your arch\narch = [\n    to_head( '..' ),\n    to_cor(),\n    to_begin(),\n    to_Conv(\"conv1\", 512, 64, offset=\"(0,0,0)\", to=\"(0,0,0)\", height=64, depth=64, width=2 ),\n    to_Pool(\"pool1\", offset=\"(0,0,0)\", to=\"(conv1-east)\"),\n    to_Conv(\"conv2\", 128, 64, offset=\"(1,0,0)\", to=\"(pool1-east)\", height=32, depth=32, width=2 ),\n    to_connection( \"pool1\", \"conv2\"),\n    to_Pool(\"pool2\", offset=\"(0,0,0)\", to=\"(conv2-east)\", height=28, depth=28, width=1),\n    to_SoftMax(\"soft1\", 10 ,\"(3,0,0)\", \"(pool1-east)\", caption=\"SOFT\"  ),\n    to_connection(\"pool2\", \"soft1\"),\n    to_end()\n    ]\n\ndef main():\n    namefile = str(sys.argv[0]).split('.')[0]\n    to_generate(arch, namefile + '.tex' )\n\nif __name__ == '__main__':\n    main()\n```\n\nNow, run the program as follows:\n\n    bash ../tikzmake.sh my_arch\n\n\n\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 65711522,
    "name": "Paddle",
    "full_name": "PaddlePaddle/Paddle",
    "description": "PArallel Distributed Deep LEarning: Machine Learning Framework from Industrial Practice \uff08\u300e\u98de\u6868\u300f\u6838\u5fc3\u6846\u67b6\uff0c\u6df1\u5ea6\u5b66\u4e60&\u673a\u5668\u5b66\u4e60\u9ad8\u6027\u80fd\u5355\u673a\u3001\u5206\u5e03\u5f0f\u8bad\u7ec3\u548c\u8de8\u5e73\u53f0\u90e8\u7f72\uff09",
    "html_url": "https://github.com/PaddlePaddle/Paddle",
    "clone_url": "https://github.com/PaddlePaddle/Paddle.git",
    "owner_login": "PaddlePaddle",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/23534030?v=4",
    "stargazers_count": 23108,
    "watchers_count": 23108,
    "forks_count": 5776,
    "open_issues_count": 1788,
    "size": 508616,
    "language": "C++",
    "languages": {
      "C++": 55634385,
      "Python": 51538892,
      "Cuda": 8517740,
      "CMake": 1158059,
      "Shell": 909864,
      "C": 297711,
      "Batchfile": 108149,
      "Jinja": 93034,
      "Go": 40954,
      "Java": 16293,
      "Dockerfile": 16006,
      "R": 1331
    },
    "topics": [
      "deep-learning",
      "distributed-training",
      "efficiency",
      "machine-learning",
      "neural-network",
      "paddlepaddle",
      "python",
      "scalability"
    ],
    "license_name": "Apache License 2.0",
    "created_at": "2016-08-15T06:59:08+00:00",
    "updated_at": "2025-08-06T02:08:58+00:00",
    "pushed_at": "2025-08-06T02:08:53+00:00",
    "contributors_count": 100,
    "readme_length": 5630,
    "readme_content": "<p align=\"center\">\n<img align=\"center\" src=\"doc/imgs/logo.png\", width=1600>\n<p>\n\n--------------------------------------------------------------------------------\n\nEnglish | [\u7b80\u4f53\u4e2d\u6587](./README_cn.md) | [\u65e5\u672c\u8a9e](./README_ja.md)\n\n[![Documentation Status](https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat)](https://paddlepaddle.org.cn/documentation/docs/en/guides/index_en.html)\n[![Documentation Status](https://img.shields.io/badge/\u4e2d\u6587\u6587\u6863-\u6700\u65b0-brightgreen.svg)](https://paddlepaddle.org.cn/documentation/docs/zh/guides/index_cn.html)\n[![Release](https://img.shields.io/github/release/PaddlePaddle/Paddle.svg)](https://github.com/PaddlePaddle/Paddle/releases)\n[![License](https://img.shields.io/badge/license-Apache%202-blue.svg)](LICENSE)\n![X (formerly Twitter) URL](https://img.shields.io/twitter/url?url=https%3A%2F%2Fx.com%2FPaddlePaddle)\n\nWelcome to the PaddlePaddle GitHub.\n\nPaddlePaddle, as the first independent R&D deep learning platform in China, has been officially open-sourced to professional communities since 2016. It is an industrial platform with advanced technologies and rich features that cover core deep learning frameworks, basic model libraries, end-to-end development kits, tools & components as well as service platforms.\nPaddlePaddle originates from industrial practices with dedication and commitments to industrialization. It has been widely adopted by a wide range of sectors including manufacturing, agriculture, enterprise service, and so on while serving more than 21.85 million developers, 670,000 companies and generating 1,100,000 models. With such advantages, PaddlePaddle has helped an increasing number of partners commercialize AI.\n\n## Installation\n\n### Latest PaddlePaddle Release: [3.1](https://github.com/PaddlePaddle/Paddle/tree/release/3.1)\n\nOur vision is to enable deep learning for everyone via PaddlePaddle.\nPlease refer to our [release announcement](https://github.com/PaddlePaddle/Paddle/releases) to track the latest features of PaddlePaddle.\n\n### Install Latest Stable Release\n\n``` sh\n# CPU\npip install paddlepaddle\n# GPU\npip install paddlepaddle-gpu\n```\n\nFor more information about installation, please view [Quick Install](https://www.paddlepaddle.org.cn/install/quick)\n\n## **PaddlePaddle New Generation Framework 3.1**\n\n* **Unified Dynamic/Static Graphs and Automatic Parallelism**\n\n    By requiring only minimal tensor partitioning annotations based on a single-card configuration, PaddlePaddle automatically discovers the most efficient distributed parallel strategy. This significantly reduces the costs of industrial development and training, enabling developers to focus more intently on model and algorithm innovation.\n\n* **Integrated Training and Inference for Large Models**\n\n    The same framework supports both training and inference, achieving code reuse and seamless integration between these stages. This provides a unified development experience and maximum training efficiency for the entire large model workflow, offering the industry a superior development experience.\n\n* **High-Order Differentiation for Scientific Computing**\n\n    Provides capabilities such as high-order automatic differentiation, complex number operations, Fourier transforms, compilation optimization, and distributed training support. It facilitates scientific exploration in fields including mathematics, mechanics, materials science, meteorology, and biology, substantially improving the speed of solving differential equations.\n\n* **Neural Network Compiler**\n\n    Adopting an integrated framework design, it supports efficient training and flexible inference for diverse models, including generative and scientific computing models. It achieves an effective balance between computational flexibility and high performance, significantly lowering performance optimization costs.\n\n* **Heterogeneous Multi-Chip Adaptation**\n    Features a mature and complete unified adaptation solution for multiple hardware types. Through standardized interfaces, it abstracts the variations in development interfaces across different chip software stacks, realizing a pluggable architecture.\n\n## Documentation\n\nWe provide [English](https://www.paddlepaddle.org.cn/documentation/docs/en/guides/index_en.html) and\n[Chinese](https://www.paddlepaddle.org.cn/documentation/docs/zh/guide/index_cn.html) documentation.\n\n* [Guides](https://www.paddlepaddle.org.cn/documentation/docs/en/guides/index_en.html)\n\n  You might want to start from how to implement deep learning basics with PaddlePaddle.\n\n* [Practice](https://www.paddlepaddle.org.cn/documentation/docs/zh/tutorial/index_cn.html)\n\n  So far you have already been familiar with Fluid. And the next step should be building a more efficient model or inventing your original Operator.\n\n* [API Reference](https://www.paddlepaddle.org.cn/documentation/docs/en/api/index_en.html)\n\n   Our new API enables much shorter programs.\n\n* [How to Contribute](https://www.paddlepaddle.org.cn/documentation/docs/en/guides/08_contribution/index_en.html)\n\n   We appreciate your contributions!\n\n## Open Source Community\n\n* [Github Issues](https://github.com/PaddlePaddle/Paddle/issues): bug reports, feature requests, install issues, usage issues, etc.\n* Many of our contribution events offer varying levels of mentorship from experienced community members, please check the events in the pinned issues, and consider attending.\n* Community Blog: <https://pfcc.blog/>\n* See more details about PaddlePaddle community at [community](https://github.com/PaddlePaddle/community).\n\n## Copyright and License\n\nPaddlePaddle is provided under the [Apache-2.0 license](LICENSE).\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 106024057,
    "name": "pytorch_geometric",
    "full_name": "pyg-team/pytorch_geometric",
    "description": "Graph Neural Network Library for PyTorch",
    "html_url": "https://github.com/pyg-team/pytorch_geometric",
    "clone_url": "https://github.com/pyg-team/pytorch_geometric.git",
    "owner_login": "pyg-team",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/89995122?v=4",
    "stargazers_count": 22713,
    "watchers_count": 22713,
    "forks_count": 3860,
    "open_issues_count": 1210,
    "size": 24188,
    "language": "Python",
    "languages": {
      "Python": 5549744,
      "Jinja": 15850,
      "Shell": 10098,
      "Dockerfile": 788
    },
    "topics": [
      "deep-learning",
      "geometric-deep-learning",
      "graph-convolutional-networks",
      "graph-neural-networks",
      "pytorch"
    ],
    "license_name": "MIT License",
    "created_at": "2017-10-06T16:03:03+00:00",
    "updated_at": "2025-08-05T21:42:53+00:00",
    "pushed_at": "2025-08-05T12:10:35+00:00",
    "contributors_count": 100,
    "readme_length": 59909,
    "readme_content": "<p align=\"center\">\n  <img height=\"150\" src=\"https://raw.githubusercontent.com/pyg-team/pyg_sphinx_theme/master/pyg_sphinx_theme/static/img/pyg_logo_text.svg?sanitize=true\" />\n</p>\n\n______________________________________________________________________\n\n[![PyPI Version][pypi-image]][pypi-url]\n[![Testing Status][testing-image]][testing-url]\n[![Linting Status][linting-image]][linting-url]\n[![Docs Status][docs-image]][docs-url]\n[![Contributing][contributing-image]][contributing-url]\n[![Slack][slack-image]][slack-url]\n\n**[Documentation](https://pytorch-geometric.readthedocs.io)** | **[PyG 1.0 Paper](https://arxiv.org/abs/1903.02428)** | **[PyG 2.0 Paper](https://arxiv.org/abs/2507.16991)** | **[Colab Notebooks](https://pytorch-geometric.readthedocs.io/en/latest/get_started/colabs.html)** | **[External Resources](https://pytorch-geometric.readthedocs.io/en/latest/external/resources.html)** | **[OGB Examples](https://github.com/snap-stanford/ogb/tree/master/examples)**\n\n**PyG** *(PyTorch Geometric)* is a library built upon [PyTorch](https://pytorch.org/) to easily write and train Graph Neural Networks (GNNs) for a wide range of applications related to structured data.\n\nIt consists of various methods for deep learning on graphs and other irregular structures, also known as *[geometric deep learning](http://geometricdeeplearning.com/)*, from a variety of published papers.\nIn addition, it consists of easy-to-use mini-batch loaders for operating on many small and single giant graphs, [multi GPU-support](https://github.com/pyg-team/pytorch_geometric/tree/master/examples/multi_gpu), [`torch.compile`](https://pytorch-geometric.readthedocs.io/en/latest/advanced/compile.html) support, [`DataPipe`](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/datapipe.py) support, a large number of common benchmark datasets (based on simple interfaces to create your own), and helpful transforms, both for learning on arbitrary graphs as well as on 3D meshes or point clouds.\n\n**[Click here to join our Slack community!][slack-url]**\n\n<p align=\"center\">\n  <a href=\"https://medium.com/stanford-cs224w\"><img style=\"max-width: 941px\" src=\"https://data.pyg.org/img/cs224w_tutorials.png\" /></a>\n</p>\n\n______________________________________________________________________\n\n- [Library Highlights](#library-highlights)\n- [Quick Tour for New Users](#quick-tour-for-new-users)\n- [Architecture Overview](#architecture-overview)\n- [Implemented GNN Models](#implemented-gnn-models)\n- [Installation](#installation)\n\n## Library Highlights\n\nWhether you are a machine learning researcher or first-time user of machine learning toolkits, here are some reasons to try out PyG for machine learning on graph-structured data.\n\n- **Easy-to-use and unified API**:\n  All it takes is 10-20 lines of code to get started with training a GNN model (see the next section for a [quick tour](#quick-tour-for-new-users)).\n  PyG is *PyTorch-on-the-rocks*: It utilizes a tensor-centric API and keeps design principles close to vanilla PyTorch.\n  If you are already familiar with PyTorch, utilizing PyG is straightforward.\n- **Comprehensive and well-maintained GNN models**:\n  Most of the state-of-the-art Graph Neural Network architectures have been implemented by library developers or authors of research papers and are ready to be applied.\n- **Great flexibility**:\n  Existing PyG models can easily be extended for conducting your own research with GNNs.\n  Making modifications to existing models or creating new architectures is simple, thanks to its easy-to-use message passing API, and a variety of operators and utility functions.\n- **Large-scale real-world GNN models**:\n  We focus on the need of GNN applications in challenging real-world scenarios, and support learning on diverse types of graphs, including but not limited to: scalable GNNs for graphs with millions of nodes; dynamic GNNs for node predictions over time; heterogeneous GNNs with multiple node types and edge types.\n\n## Quick Tour for New Users\n\nIn this quick tour, we highlight the ease of creating and training a GNN model with only a few lines of code.\n\n### Train your own GNN model\n\nIn the first glimpse of PyG, we implement the training of a GNN for classifying papers in a citation graph.\nFor this, we load the [Cora](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.Planetoid.html) dataset, and create a simple 2-layer GCN model using the pre-defined [`GCNConv`](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCNConv.html):\n\n```python\nimport torch\nfrom torch import Tensor\nfrom torch_geometric.nn import GCNConv\nfrom torch_geometric.datasets import Planetoid\n\ndataset = Planetoid(root='.', name='Cora')\n\nclass GCN(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels):\n        super().__init__()\n        self.conv1 = GCNConv(in_channels, hidden_channels)\n        self.conv2 = GCNConv(hidden_channels, out_channels)\n\n    def forward(self, x: Tensor, edge_index: Tensor) -> Tensor:\n        # x: Node feature matrix of shape [num_nodes, in_channels]\n        # edge_index: Graph connectivity matrix of shape [2, num_edges]\n        x = self.conv1(x, edge_index).relu()\n        x = self.conv2(x, edge_index)\n        return x\n\nmodel = GCN(dataset.num_features, 16, dataset.num_classes)\n```\n\n<details>\n<summary>We can now optimize the model in a training loop, similar to the <a href=\"https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html#full-implementation\">standard PyTorch training procedure</a>.</summary>\n\n```python\nimport torch.nn.functional as F\n\ndata = dataset[0]\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nfor epoch in range(200):\n    pred = model(data.x, data.edge_index)\n    loss = F.cross_entropy(pred[data.train_mask], data.y[data.train_mask])\n\n    # Backpropagation\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n```\n\n</details>\n\nMore information about evaluating final model performance can be found in the corresponding [example](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/gcn.py).\n\n### Create your own GNN layer\n\nIn addition to the easy application of existing GNNs, PyG makes it simple to implement custom Graph Neural Networks (see [here](https://pytorch-geometric.readthedocs.io/en/latest/tutorial/create_gnn.html) for the accompanying tutorial).\nFor example, this is all it takes to implement the [edge convolutional layer](https://arxiv.org/abs/1801.07829) from Wang *et al.*:\n\n$$x_i^{\\\\prime} ~ = ~ \\\\max\\_{j \\\\in \\\\mathcal{N}(i)} ~ \\\\textrm{MLP}\\_{\\\\theta} \\\\left( [ ~ x_i, ~ x_j - x_i ~ ] \\\\right)$$\n\n```python\nimport torch\nfrom torch import Tensor\nfrom torch.nn import Sequential, Linear, ReLU\nfrom torch_geometric.nn import MessagePassing\n\nclass EdgeConv(MessagePassing):\n    def __init__(self, in_channels, out_channels):\n        super().__init__(aggr=\"max\")  # \"Max\" aggregation.\n        self.mlp = Sequential(\n            Linear(2 * in_channels, out_channels),\n            ReLU(),\n            Linear(out_channels, out_channels),\n        )\n\n    def forward(self, x: Tensor, edge_index: Tensor) -> Tensor:\n        # x: Node feature matrix of shape [num_nodes, in_channels]\n        # edge_index: Graph connectivity matrix of shape [2, num_edges]\n        return self.propagate(edge_index, x=x)  # shape [num_nodes, out_channels]\n\n    def message(self, x_j: Tensor, x_i: Tensor) -> Tensor:\n        # x_j: Source node features of shape [num_edges, in_channels]\n        # x_i: Target node features of shape [num_edges, in_channels]\n        edge_features = torch.cat([x_i, x_j - x_i], dim=-1)\n        return self.mlp(edge_features)  # shape [num_edges, out_channels]\n```\n\n## Architecture Overview\n\nPyG provides a multi-layer framework that enables users to build Graph Neural Network solutions on both low and high levels.\nIt comprises of the following components:\n\n- The PyG **engine** utilizes the powerful PyTorch deep learning framework with full [`torch.compile`](https://pytorch-geometric.readthedocs.io/en/latest/advanced/compile.html) and [TorchScript](https://pytorch-geometric.readthedocs.io/en/latest/advanced/jit.html) support, as well as additions of efficient CPU/CUDA libraries for operating on sparse data, *e.g.*, [`pyg-lib`](https://github.com/pyg-team/pyg-lib).\n- The PyG **storage** handles data processing, transformation and loading pipelines. It is capable of handling and processing large-scale graph datasets, and provides effective solutions for heterogeneous graphs. It further provides a variety of sampling solutions, which enable training of GNNs on large-scale graphs.\n- The PyG **operators** bundle essential functionalities for implementing Graph Neural Networks. PyG supports important GNN building blocks that can be combined and applied to various parts of a GNN model, ensuring rich flexibility of GNN design.\n- Finally, PyG provides an abundant set of GNN **models**, and examples that showcase GNN models on standard graph benchmarks. Thanks to its flexibility, users can easily build and modify custom GNN models to fit their specific needs.\n\n<p align=\"center\">\n  <img width=\"100%\" src=\"https://raw.githubusercontent.com/pyg-team/pytorch_geometric/master/docs/source/_figures/architecture.svg?sanitize=true\" />\n</p>\n\n## Implemented GNN Models\n\nWe list currently supported PyG models, layers and operators according to category:\n\n**GNN layers:**\nAll Graph Neural Network layers are implemented via the **[`nn.MessagePassing`](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.MessagePassing.html)** interface.\nA GNN layer specifies how to perform message passing, *i.e.* by designing different message, aggregation and update functions as defined [here](https://pytorch-geometric.readthedocs.io/en/latest/tutorial/create_gnn.html).\nThese GNN layers can be stacked together to create Graph Neural Network models.\n\n- **[GCNConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCNConv.html)** from Kipf and Welling: [Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907) (ICLR 2017) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/gcn.py)\\]\n- **[ChebConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.ChebConv.html)** from Defferrard *et al.*: [Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering](https://arxiv.org/abs/1606.09375) (NIPS 2016) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/gcn.py#L36-L37)\\]\n- **[GATConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GATConv.html)** from Veli\u010dkovi\u0107 *et al.*: [Graph Attention Networks](https://arxiv.org/abs/1710.10903) (ICLR 2018) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/gat.py)\\]\n\n<details>\n<summary><b>Expand to see all implemented GNN layers...</b></summary>\n\n- **[GCN2Conv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCN2Conv.html)** from Chen *et al.*: [Simple and Deep Graph Convolutional Networks](https://arxiv.org/abs/2007.02133) (ICML 2020) \\[[**Example1**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/gcn2_cora.py), [**Example2**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/gcn2_ppi.py)\\]\n- **[SplineConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.SplineConv.html)** from Fey *et al.*: [SplineCNN: Fast Geometric Deep Learning with Continuous B-Spline Kernels](https://arxiv.org/abs/1711.08920) (CVPR 2018) \\[[**Example1**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/cora.py), [**Example2**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/faust.py)\\]\n- **[NNConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.NNConv.html)** from Gilmer *et al.*: [Neural Message Passing for Quantum Chemistry](https://arxiv.org/abs/1704.01212) (ICML 2017) \\[[**Example1**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/qm9_nn_conv.py), [**Example2**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/mnist_nn_conv.py)\\]\n- **[CGConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.CGConv.html)** from Xie and Grossman: [Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.120.145301) (Physical Review Letters 120, 2018)\n- **[ECConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.ECConv.html)** from Simonovsky and Komodakis: [Edge-Conditioned Convolution on Graphs](https://arxiv.org/abs/1704.02901) (CVPR 2017)\n- **[EGConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.EGConv.html)** from Tailor *et al.*: [Adaptive Filters and Aggregator Fusion for Efficient Graph Convolutions](https://arxiv.org/abs/2104.01481) (GNNSys 2021) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/egc.py)\\]\n- **[GATv2Conv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GATv2Conv.html)** from Brody *et al.*: [How Attentive are Graph Attention Networks?](https://arxiv.org/abs/2105.14491) (ICLR 2022)\n- **[TransformerConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.TransformerConv.html)** from Shi *et al.*: [Masked Label Prediction: Unified Message Passing Model for Semi-Supervised Classification](https://arxiv.org/abs/2009.03509) (CoRR 2020) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/unimp_arxiv.py)\\]\n- **[SAGEConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.SAGEConv.html)** from Hamilton *et al.*: [Inductive Representation Learning on Large Graphs](https://arxiv.org/abs/1706.02216) (NIPS 2017) \\[[**Example1**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/reddit.py), [**Example2**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/ogbn_train.py), [**Example3**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/graph_sage_unsup.py), [**Example4**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/graph_sage_unsup_ppi.py)\\]\n- **[GraphConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GraphConv.html)** from, *e.g.*, Morris *et al.*: [Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks](https://arxiv.org/abs/1810.02244) (AAAI 2019)\n- **[GatedGraphConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GatedGraphConv.html)** from Li *et al.*: [Gated Graph Sequence Neural Networks](https://arxiv.org/abs/1511.05493) (ICLR 2016)\n- **[ResGatedGraphConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.ResGatedGraphConv.html)** from Bresson and Laurent: [Residual Gated Graph ConvNets](https://arxiv.org/abs/1711.07553) (CoRR 2017)\n- **[GINConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GINConv.html)** from Xu *et al.*: [How Powerful are Graph Neural Networks?](https://arxiv.org/abs/1810.00826) (ICLR 2019) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/mutag_gin.py)\\]\n- **[GINEConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GINEConv.html)** from Hu *et al.*: [Strategies for Pre-training Graph Neural Networks](https://arxiv.org/abs/1905.12265) (ICLR 2020)\n- **[ARMAConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.ARMAConv.html)** from Bianchi *et al.*: [Graph Neural Networks with Convolutional ARMA Filters](https://arxiv.org/abs/1901.01343) (CoRR 2019) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/arma.py)\\]\n- **[SGConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.SGConv.html)** from Wu *et al.*: [Simplifying Graph Convolutional Networks](https://arxiv.org/abs/1902.07153) (CoRR 2019) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/sgc.py)\\]\n- **[APPNP](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.APPNP.html)** from Klicpera *et al.*: [Predict then Propagate: Graph Neural Networks meet Personalized PageRank](https://arxiv.org/abs/1810.05997) (ICLR 2019) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/benchmark/citation/appnp.py)\\]\n- **[MFConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.MFConv.html)** from Duvenaud *et al.*: [Convolutional Networks on Graphs for Learning Molecular Fingerprints](https://arxiv.org/abs/1509.09292) (NIPS 2015)\n- **[AGNNConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.AGNNConv.html)** from Thekumparampil *et al.*: [Attention-based Graph Neural Network for Semi-Supervised Learning](https://arxiv.org/abs/1803.03735) (CoRR 2017) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/agnn.py)\\]\n- **[TAGConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.TAGConv.html)** from Du *et al.*: [Topology Adaptive Graph Convolutional Networks](https://arxiv.org/abs/1710.10370) (CoRR 2017) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/tagcn.py)\\]\n- **[PNAConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.PNAConv.html)** from Corso *et al.*: [Principal Neighbourhood Aggregation for Graph Nets](https://arxiv.org/abs/2004.05718) (CoRR 2020) \\[**[Example](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/pna.py)**\\]\n- **[FAConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.FAConv.html)** from Bo *et al.*: [Beyond Low-Frequency Information in Graph Convolutional Networks](https://arxiv.org/abs/2101.00797) (AAAI 2021)\n- **[PDNConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.nn.conv.PDNConv.html)** from Rozemberczki *et al.*: [Pathfinder Discovery Networks for Neural Message Passing](https://arxiv.org/abs/2010.12878) (WWW 2021)\n- **[RGCNConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.RGCNConv.html)** from Schlichtkrull *et al.*: [Modeling Relational Data with Graph Convolutional Networks](https://arxiv.org/abs/1703.06103) (ESWC 2018) \\[[**Example1**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/rgcn.py), [**Example2**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/rgcn_link_pred.py)\\]\n- **[RGATConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.RGATConv.html)** from Busbridge *et al.*: [Relational Graph Attention Networks](https://arxiv.org/abs/1904.05811) (CoRR 2019) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/rgat.py)\\]\n- **[FiLMConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.FiLMConv.html)** from Brockschmidt: [GNN-FiLM: Graph Neural Networks with Feature-wise Linear Modulation](https://arxiv.org/abs/1906.12192) (ICML 2020) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/film.py)\\]\n- **[SignedConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.SignedConv.html)** from Derr *et al.*: [Signed Graph Convolutional Network](https://arxiv.org/abs/1808.06354) (ICDM 2018) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/signed_gcn.py)\\]\n- **[DNAConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.DNAConv.html)** from Fey: [Just Jump: Dynamic Neighborhood Aggregation in Graph Neural Networks](https://arxiv.org/abs/1904.04849) (ICLR-W 2019) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/dna.py)\\]\n- **[PANConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.PANConv.html)** from Ma *et al.*: [Path Integral Based Convolution and Pooling for Graph Neural Networks](https://arxiv.org/abs/2006.16811) (NeurIPS 2020)\n- **[PointNetConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.PointNetConv.html)** (including **[Iterative Farthest Point Sampling](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.pool.fps.html)**, dynamic graph generation based on **[nearest neighbor](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.pool.knn_graph.html)** or **[maximum distance](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.pool.radius_graph.html)**, and **[k-NN interpolation](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.unpool.knn_interpolate.html)** for upsampling) from Qi *et al.*: [PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation](https://arxiv.org/abs/1612.00593) (CVPR 2017) and [PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space](https://arxiv.org/abs/1706.02413) (NIPS 2017) \\[[**Example1**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/pointnet2_classification.py), [**Example2**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/pointnet2_segmentation.py)\\]\n- **[EdgeConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.EdgeConv.html)** from Wang *et al.*: [Dynamic Graph CNN for Learning on Point Clouds](https://arxiv.org/abs/1801.07829) (CoRR, 2018) \\[[**Example1**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/dgcnn_classification.py), [**Example2**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/dgcnn_segmentation.py)\\]\n- **[XConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.XConv.html)** from Li *et al.*: [PointCNN: Convolution On X-Transformed Points](https://arxiv.org/abs/1801.07791) (NeurIPS 2018) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/benchmark/points/point_cnn.py)\\]\n- **[PPFConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.PPFConv.html)** from Deng *et al.*: [PPFNet: Global Context Aware Local Features for Robust 3D Point Matching](https://arxiv.org/abs/1802.02669) (CVPR 2018)\n- **[GMMConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GMMConv.html)** from Monti *et al.*: [Geometric Deep Learning on Graphs and Manifolds using Mixture Model CNNs](https://arxiv.org/abs/1611.08402) (CVPR 2017)\n- **[FeaStConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.FeaStConv.html)** from Verma *et al.*: [FeaStNet: Feature-Steered Graph Convolutions for 3D Shape Analysis](https://arxiv.org/abs/1706.05206) (CVPR 2018)\n- **[PointTransformerConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.PointTransformerConv.html)** from Zhao *et al.*: [Point Transformer](https://arxiv.org/abs/2012.09164) (2020)\n- **[HypergraphConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.HypergraphConv.html)** from Bai *et al.*: [Hypergraph Convolution and Hypergraph Attention](https://arxiv.org/abs/1901.08150) (CoRR 2019)\n- **[GravNetConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GravNetConv.html)** from Qasim *et al.*: [Learning Representations of Irregular Particle-detector Geometry with Distance-weighted Graph Networks](https://arxiv.org/abs/1902.07987) (European Physics Journal C, 2019)\n- **[SuperGAT](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.SuperGATConv.html)** from Kim and Oh: [How To Find Your Friendly Neighborhood: Graph Attention Design With Self-Supervision](https://openreview.net/forum?id=Wi5KUNlqWty) (ICLR 2021) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/super_gat.py)\\]\n- **[HGTConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.HGTConv.html)** from Hu *et al.*: [Heterogeneous Graph Transformer](https://arxiv.org/abs/2003.01332) (WWW 2020) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/hetero/hgt_dblp.py)\\]\n- **[HEATConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.HEATonv.html)** from Mo *et al.*: [Heterogeneous Edge-Enhanced Graph Attention Network For Multi-Agent Trajectory Prediction](https://arxiv.org/abs/2106.07161) (CoRR 2021)\n- **[SSGConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.SSGConv.html)** from Zhu *et al.*: [Simple Spectral Graph Convolution](https://openreview.net/forum?id=CYO5T-YjWZV) (ICLR 2021)\n- **[FusedGATConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.FusedGATConv.html)** from Zhang *et al.*: [Understanding GNN Computational Graph: A Coordinated Computation, IO, and Memory Perspective](https://proceedings.mlsys.org/paper/2022/file/9a1158154dfa42caddbd0694a4e9bdc8-Paper.pdf) (MLSys 2022)\n- **[GPSConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GPSConv.html)** from Ramp\u00e1\u0161ek *et al.*: [Recipe for a General, Powerful, Scalable Graph Transformer](https://arxiv.org/abs/2205.12454) (NeurIPS 2022) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/graph_gps.py)\\]\n\n</details>\n\n**Pooling layers:**\nGraph pooling layers combine the vectorial representations of a set of nodes in a graph (or a subgraph) into a single vector representation that summarizes its properties of nodes.\nIt is commonly applied to graph-level tasks, which require combining node features into a single graph representation.\n\n- **[Top-K Pooling](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.pool.TopKPooling.html)** from Gao and Ji: [Graph U-Nets](https://arxiv.org/abs/1905.05178) (ICML 2019), Cangea *et al.*: [Towards Sparse Hierarchical Graph Classifiers](https://arxiv.org/abs/1811.01287) (NeurIPS-W 2018) and Knyazev *et al.*: [Understanding Attention and Generalization in Graph Neural Networks](https://arxiv.org/abs/1905.02850) (ICLR-W 2019) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/proteins_topk_pool.py)\\]\n- **[DiffPool](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.dense.dense_diff_pool.html)** from Ying *et al.*: [Hierarchical Graph Representation Learning with Differentiable Pooling](https://arxiv.org/abs/1806.08804) (NeurIPS 2018) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/proteins_diff_pool.py)\\]\n\n<details>\n<summary><b>Expand to see all implemented pooling layers...</b></summary>\n\n- **[Attentional Aggregation](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.aggr.AttentionalAggregation.html)** from Li *et al.*: [Graph Matching Networks for Learning the Similarity of Graph Structured Objects](https://arxiv.org/abs/1904.12787) (ICML 2019) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/benchmark/kernel/global_attention.py)\\]\n- **[Set2Set](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.aggr.Set2Set.html)** from Vinyals *et al.*: [Order Matters: Sequence to Sequence for Sets](https://arxiv.org/abs/1511.06391) (ICLR 2016) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/benchmark/kernel/set2set.py)\\]\n- **[Sort Aggregation](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.aggr.SortAggregation.html)** from Zhang *et al.*: [An End-to-End Deep Learning Architecture for Graph Classification](https://www.cse.wustl.edu/~muhan/papers/AAAI_2018_DGCNN.pdf) (AAAI 2018) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/benchmark/kernel/sort_pool.py)\\]\n- **[MinCut Pooling](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.dense.dense_mincut_pool.html)** from Bianchi *et al.*: [Spectral Clustering with Graph Neural Networks for Graph Pooling](https://arxiv.org/abs/1907.00481) (ICML 2020) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/proteins_mincut_pool.py)\\]\n- **[DMoN Pooling](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.dense.DMoNPooling.html)** from Tsitsulin *et al.*: [Graph Clustering with Graph Neural Networks](https://arxiv.org/abs/2006.16904) (CoRR 2020) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/proteins_dmon_pool.py)\\]\n- **[Graclus Pooling](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.pool.graclus.html)** from Dhillon *et al.*: [Weighted Graph Cuts without Eigenvectors: A Multilevel Approach](http://www.cs.utexas.edu/users/inderjit/public_papers/multilevel_pami.pdf) (PAMI 2007) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/mnist_graclus.py)\\]\n- **[Voxel Grid Pooling](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.pool.voxel_grid.html)** from, *e.g.*, Simonovsky and Komodakis: [Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs](https://arxiv.org/abs/1704.02901) (CVPR 2017) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/mnist_voxel_grid.py)\\]\n- **[SAG Pooling](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.pool.SAGPooling.html)** from Lee *et al.*: [Self-Attention Graph Pooling](https://arxiv.org/abs/1904.08082) (ICML 2019) and Knyazev *et al.*: [Understanding Attention and Generalization in Graph Neural Networks](https://arxiv.org/abs/1905.02850) (ICLR-W 2019) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/benchmark/kernel/sag_pool.py)\\]\n- **[Edge Pooling](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.pool.EdgePooling.html)** from Diehl *et al.*: [Towards Graph Pooling by Edge Contraction](https://graphreason.github.io/papers/17.pdf) (ICML-W 2019) and Diehl: [Edge Contraction Pooling for Graph Neural Networks](https://arxiv.org/abs/1905.10990) (CoRR 2019) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/benchmark/kernel/edge_pool.py)\\]\n- **[ASAPooling](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.pool.ASAPooling.html)** from Ranjan *et al.*: [ASAP: Adaptive Structure Aware Pooling for Learning Hierarchical Graph Representations](https://arxiv.org/abs/1911.07979) (AAAI 2020) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/benchmark/kernel/asap.py)\\]\n- **[PANPooling](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.pool.PANPooling.html)** from Ma *et al.*: [Path Integral Based Convolution and Pooling for Graph Neural Networks](https://arxiv.org/abs/2006.16811) (NeurIPS 2020)\n- **[MemPooling](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.pool.MemPooling.html)** from Khasahmadi *et al.*: [Memory-Based Graph Networks](https://arxiv.org/abs/2002.09518) (ICLR 2020) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/mem_pool.py)\\]\n- **[Graph Multiset Transformer](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.aggr.GraphMultisetTransformer.html)** from Baek *et al.*: [Accurate Learning of Graph Representations with Graph Multiset Pooling](https://arxiv.org/abs/2102.11533) (ICLR 2021) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/proteins_gmt.py)\\]\n- **[Equilibrium Aggregation](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.aggr.EquilibriumAggregation.html)** from Bartunov *et al.*: [](https://arxiv.org/abs/2202.12795) (UAI 2022) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/equilibrium_median.py)\\]\n\n</details>\n\n**GNN models:**\nOur supported GNN models incorporate multiple message passing layers, and users can directly use these pre-defined models to make predictions on graphs.\nUnlike simple stacking of GNN layers, these models could involve pre-processing, additional learnable parameters, skip connections, graph coarsening, etc.\n\n- **[SchNet](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.SchNet.html)** from Sch\u00fctt *et al.*: [SchNet: A Continuous-filter Convolutional Neural Network for Modeling Quantum Interactions](https://arxiv.org/abs/1706.08566) (NIPS 2017) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/qm9_pretrained_schnet.py)\\]\n- **[DimeNet](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.DimeNet.html)** and **[DimeNetPlusPlus](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.DimeNetPlusPlus.html)** from Klicpera *et al.*: [Directional Message Passing for Molecular Graphs](https://arxiv.org/abs/2003.03123) (ICLR 2020) and [Fast and Uncertainty-Aware Directional Message Passing for Non-Equilibrium Molecules](https://arxiv.org/abs/2011.14115) (NeurIPS-W 2020) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/qm9_pretrained_dimenet.py)\\]\n- **[Node2Vec](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.Node2Vec.html)** from Grover and Leskovec: [node2vec: Scalable Feature Learning for Networks](https://arxiv.org/abs/1607.00653) (KDD 2016) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/node2vec.py)\\]\n- **[Deep Graph Infomax](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.DeepGraphInfomax.html)** from Veli\u010dkovi\u0107 *et al.*: [Deep Graph Infomax](https://arxiv.org/abs/1809.10341) (ICLR 2019) \\[[**Example1**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/infomax_transductive.py), [**Example2**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/infomax_inductive.py)\\]\n- **Deep Multiplex Graph Infomax** from Park *et al.*: [Unsupervised Attributed Multiplex Network Embedding](https://arxiv.org/abs/1911.06750) (AAAI 2020) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/hetero/dmgi_unsup.py)\\]\n- **[Masked Label Prediction](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.MaskLabel.html)** from Shi *et al.*: [Masked Label Prediction: Unified Message Passing Model for Semi-Supervised Classification](https://arxiv.org/abs/2009.03509) (CoRR 2020) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/unimp_arxiv.py)\\]\n- **[PMLP](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.PMLP.html)** from Yang *et al.*: [Graph Neural Networks are Inherently Good Generalizers: Insights by Bridging GNNs and MLPs](https://arxiv.org/abs/2212.09034) (ICLR 2023)\n\n<details>\n<summary><b>Expand to see all implemented GNN models...</b></summary>\n\n- **[Jumping Knowledge](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.JumpingKnowledge.html)** from Xu *et al.*: [Representation Learning on Graphs with Jumping Knowledge Networks](https://arxiv.org/abs/1806.03536) (ICML 2018) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/benchmark/kernel/gin.py#L54-L106)\\]\n- A **[MetaLayer](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.MetaLayer.html)** for building any kind of graph network similar to the [TensorFlow Graph Nets library](https://github.com/deepmind/graph_nets) from Battaglia *et al.*: [Relational Inductive Biases, Deep Learning, and Graph Networks](https://arxiv.org/abs/1806.01261) (CoRR 2018)\n- **[MetaPath2Vec](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.MetaPath2Vec.html)** from Dong *et al.*: [metapath2vec: Scalable Representation Learning for Heterogeneous Networks](https://ericdongyx.github.io/papers/KDD17-dong-chawla-swami-metapath2vec.pdf) (KDD 2017) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/hetero/metapath2vec.py)\\]\n- All variants of **[Graph Autoencoders](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.GAE.html)** and **[Variational Autoencoders](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.VGAE.html)** from:\n  - [Variational Graph Auto-Encoders](https://arxiv.org/abs/1611.07308) from Kipf and Welling (NIPS-W 2016) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/autoencoder.py)\\]\n  - [Adversarially Regularized Graph Autoencoder for Graph Embedding](https://arxiv.org/abs/1802.04407) from Pan *et al.* (IJCAI 2018) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/argva_node_clustering.py)\\]\n  - [Simple and Effective Graph Autoencoders with One-Hop Linear Models](https://arxiv.org/abs/2001.07614) from Salha *et al.* (ECML 2020) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/autoencoder.py)\\]\n- **[SEAL](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/seal_link_pred.py)** from Zhang and Chen: [Link Prediction Based on Graph Neural Networks](https://arxiv.org/pdf/1802.09691.pdf) (NeurIPS 2018) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/seal_link_pred.py)\\]\n- **[RENet](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.RENet.html)** from Jin *et al.*: [Recurrent Event Network for Reasoning over Temporal Knowledge Graphs](https://arxiv.org/abs/1904.05530) (ICLR-W 2019) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/renet.py)\\]\n- **[GraphUNet](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.GraphUNet.html)** from Gao and Ji: [Graph U-Nets](https://arxiv.org/abs/1905.05178) (ICML 2019) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/graph_unet.py)\\]\n- **[AttentiveFP](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.AttentiveFP.html)** from Xiong *et al.*: [Pushing the Boundaries of Molecular Representation for Drug Discovery with the Graph Attention Mechanism](https://pubs.acs.org/doi/10.1021/acs.jmedchem.9b00959) (J. Med. Chem. 2020) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/attentive_fp.py)\\]\n- **[DeepGCN](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.DeepGCNLayer.html)** and the **[GENConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GENConv.html)** from Li *et al.*: [DeepGCNs: Can GCNs Go as Deep as CNNs?](https://arxiv.org/abs/1904.03751) (ICCV 2019) and [DeeperGCN: All You Need to Train Deeper GCNs](https://arxiv.org/abs/2006.07739) (CoRR 2020) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/ogbn_proteins_deepgcn.py)\\]\n- **[RECT](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.RECT_L.html)** from Wang *et al.*: [Network Embedding with Completely-imbalanced Labels](https://ieeexplore.ieee.org/document/8979355) (TKDE 2020) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/rect.py)\\]\n- **[GNNExplainer](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.explain.algorithm.GNNExplainer.html)** from Ying *et al.*: [GNNExplainer: Generating Explanations for Graph Neural Networks](https://arxiv.org/abs/1903.03894) (NeurIPS 2019) \\[[**Example1**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/explain/gnn_explainer.py), [**Example2**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/explain/gnn_explainer_ba_shapes.py), [**Example3**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/explain/gnn_explainer_link_pred.py)\\]\n- **Graph-less Neural Networks** from Zhang *et al.*: [Graph-less Neural Networks: Teaching Old MLPs New Tricks via Distillation](https://arxiv.org/abs/2110.08727) (CoRR 2021) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/glnn.py)\\]\n- **[LINKX](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.LINKX.html)** from Lim *et al.*: [Large Scale Learning on Non-Homophilous Graphs:\n  New Benchmarks and Strong Simple Methods](https://arxiv.org/abs/2110.14446) (NeurIPS 2021) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/linkx.py)\\]\n- **[RevGNN](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.GroupAddRev.html)** from Li *et al.*: [Training Graph Neural with 1000 Layers](https://arxiv.org/abs/2106.07476) (ICML 2021) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/rev_gnn.py)\\]\n- **[TransE](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.kge.TransE.html)** from Bordes *et al.*: [Translating Embeddings for Modeling Multi-Relational Data](https://proceedings.neurips.cc/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf) (NIPS 2013) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/kge_fb15k_237.py)\\]\n- **[ComplEx](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.kge.ComplEx.html)** from Trouillon *et al.*: [Complex Embeddings for Simple Link Prediction](https://arxiv.org/abs/1606.06357) (ICML 2016) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/kge_fb15k_237.py)\\]\n- **[DistMult](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.kge.DistMult.html)** from Yang *et al.*: [Embedding Entities and Relations for Learning and Inference in Knowledge Bases](https://arxiv.org/abs/1412.6575) (ICLR 2015) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/kge_fb15k_237.py)\\]\n- **[RotatE](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.kge.RotatE.html)** from Sun *et al.*: [RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space](https://arxiv.org/abs/1902.10197) (ICLR 2019) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/kge_fb15k_237.py)\\]\n\n</details>\n\n**GNN operators and utilities:**\nPyG comes with a rich set of neural network operators that are commonly used in many GNN models.\nThey follow an extensible design: It is easy to apply these operators and graph utilities to existing GNN layers and models to further enhance model performance.\n\n- **[DropEdge](https://pytorch-geometric.readthedocs.io/en/latest/modules/utils.html#torch_geometric.utils.dropout_edge)** from Rong *et al.*: [DropEdge: Towards Deep Graph Convolutional Networks on Node Classification](https://openreview.net/forum?id=Hkx1qkrKPr) (ICLR 2020)\n- **[DropNode](https://pytorch-geometric.readthedocs.io/en/latest/modules/utils.html#torch_geometric.utils.dropout_node)**, **[MaskFeature](https://pytorch-geometric.readthedocs.io/en/latest/modules/utils.html#torch_geometric.utils.mask_feature)** and **[AddRandomEdge](https://pytorch-geometric.readthedocs.io/en/latest/modules/utils.html#torch_geometric.utils.add_random_edge)** from You *et al.*: [Graph Contrastive Learning with Augmentations](https://arxiv.org/abs/2010.13902) (NeurIPS 2020)\n- **[DropPath](https://pytorch-geometric.readthedocs.io/en/latest/modules/utils.html#torch_geometric.utils.dropout_path)** from Li *et al.*: [MaskGAE: Masked Graph Modeling Meets Graph Autoencoders](https://arxiv.org/abs/2205.10053) (arXiv 2022)\n- **[ShuffleNode](https://pytorch-geometric.readthedocs.io/en/latest/modules/utils.html#torch_geometric.utils.shuffle_node)** from Veli\u010dkovi\u0107 *et al.*: [Deep Graph Infomax](https://arxiv.org/abs/1809.10341) (ICLR 2019)\n- **[GraphNorm](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.norm.GraphNorm.html)** from Cai *et al.*: [GraphNorm: A Principled Approach to Accelerating Graph Neural Network Training](https://proceedings.mlr.press/v139/cai21e.html) (ICML 2021)\n- **[GDC](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.transforms.GDC.html)** from Klicpera *et al.*: [Diffusion Improves Graph Learning](https://arxiv.org/abs/1911.05485) (NeurIPS 2019) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/gcn.py)\\]\n\n<details>\n<summary><b>Expand to see all implemented GNN operators and utilities...</b></summary>\n\n- **[GraphSizeNorm](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.norm.GraphSizeNorm.html)** from Dwivedi *et al.*: [Benchmarking Graph Neural Networks](https://arxiv.org/abs/2003.00982) (CoRR 2020)\n- **[PairNorm](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.norm.PairNorm.html)** from Zhao and Akoglu: [PairNorm: Tackling Oversmoothing in GNNs](https://arxiv.org/abs/1909.12223) (ICLR 2020)\n- **[MeanSubtractionNorm](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.norm.MeanSubtractionNorm.html)** from Yang *et al.*: [Revisiting \"Over-smoothing\" in Deep GCNs](https://arxiv.org/abs/2003.13663) (CoRR 2020)\n- **[DiffGroupNorm](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.norm.DiffGroupNorm.html)** from Zhou *et al.*: [Towards Deeper Graph Neural Networks with Differentiable Group Normalization](https://arxiv.org/abs/2006.06972) (NeurIPS 2020)\n- **[Tree Decomposition](https://pytorch-geometric.readthedocs.io/en/latest/modules/utils.html#torch_geometric.utils.tree_decomposition)** from Jin *et al.*: [Junction Tree Variational Autoencoder for Molecular Graph Generation](https://arxiv.org/abs/1802.04364) (ICML 2018)\n- **[TGN](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.TGNMemory.html)** from Rossi *et al.*: [Temporal Graph Networks for Deep Learning on Dynamic Graphs](https://arxiv.org/abs/2006.10637) (GRL+ 2020) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/tgn.py)\\]\n- **[Weisfeiler Lehman Operator](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.WLConv.html)** from Weisfeiler and Lehman: [A Reduction of a Graph to a Canonical Form and an Algebra Arising During this Reduction](https://www.iti.zcu.cz/wl2018/pdf/wl_paper_translation.pdf) (Nauchno-Technicheskaya Informatsia 1968) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/wl_kernel.py)\\]\n- **[Continuous Weisfeiler Lehman Operator](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.WLConvContinuous.html)** from Togninalli *et al.*: [Wasserstein Weisfeiler-Lehman Graph Kernels](https://arxiv.org/abs/1906.01277) (NeurIPS 2019)\n- **[Label Propagation](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.LabelPropagation.html)** from Zhu and Ghahramani: [Learning from Labeled and Unlabeled Data with Label Propagation](http://mlg.eng.cam.ac.uk/zoubin/papers/CMU-CALD-02-107.pdf) (CMU-CALD 2002) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/label_prop.py)\\]\n- **[Local Degree Profile](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.transforms.LocalDegreeProfile)** from Cai and Wang: [A Simple yet Effective Baseline for Non-attribute Graph Classification](https://arxiv.org/abs/1811.03508) (CoRR 2018)\n- **[CorrectAndSmooth](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.CorrectAndSmooth.html)** from Huang *et al.*: [Combining Label Propagation And Simple Models Out-performs Graph Neural Networks](https://arxiv.org/abs/2010.13993) (CoRR 2020) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/correct_and_smooth.py)\\]\n- **[Gini](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.functional.gini.html)** and **[BRO](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.functional.bro.html)** regularization from Henderson *et al.*: [Improving Molecular Graph Neural Network Explainability with Orthonormalization and Induced Sparsity](https://arxiv.org/abs/2105.04854) (ICML 2021)\n- **[RootedEgoNets](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.transforms.RootedEgoNets)** and **[RootedRWSubgraph](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.transforms.RootedRWSubgraph)** from Zhao *et al.*: [From Stars to Subgraphs: Uplifting Any GNN with Local Structure Awareness](https://arxiv.org/abs/2110.03753) (ICLR 2022)\n- **[FeaturePropagation](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.transforms.FeaturePropagation)** from Rossi *et al.*: [On the Unreasonable Effectiveness of Feature Propagation in Learning on Graphs with Missing Node Features](https://arxiv.org/abs/2111.12128) (CoRR 2021)\n\n</details>\n\n**Scalable GNNs:**\nPyG supports the implementation of Graph Neural Networks that can scale to large-scale graphs.\nSuch application is challenging since the entire graph, its associated features and the GNN parameters cannot fit into GPU memory.\nMany state-of-the-art scalability approaches tackle this challenge by sampling neighborhoods for mini-batch training, graph clustering and partitioning, or by using simplified GNN models.\nThese approaches have been implemented in PyG, and can benefit from the above GNN layers, operators and models.\n\n- **[NeighborLoader](https://pytorch-geometric.readthedocs.io/en/latest/modules/loader.html#torch_geometric.loader.NeighborLoader)** from Hamilton *et al.*: [Inductive Representation Learning on Large Graphs](https://arxiv.org/abs/1706.02216) (NIPS 2017) \\[[**Example1**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/reddit.py), [**Example2**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/ogbn_train.py), [**Example3**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/hetero/to_hetero_mag.py)\\]\n- **[ClusterGCN](https://pytorch-geometric.readthedocs.io/en/latest/modules/loader.html#torch_geometric.loader.ClusterLoader)** from Chiang *et al.*: [Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks](https://arxiv.org/abs/1905.07953) (KDD 2019) \\[[**Example1**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/cluster_gcn_reddit.py), [**Example2**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/cluster_gcn_ppi.py)\\]\n- **[GraphSAINT](https://pytorch-geometric.readthedocs.io/en/latest/modules/loader.html#torch_geometric.loader.GraphSAINTSampler)** from Zeng *et al.*: [GraphSAINT: Graph Sampling Based Inductive Learning Method](https://arxiv.org/abs/1907.04931) (ICLR 2020) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/graph_saint.py)\\]\n\n<details>\n<summary><b>Expand to see all implemented scalable GNNs...</b></summary>\n\n- **[ShaDow](https://pytorch-geometric.readthedocs.io/en/latest/modules/loader.html#torch_geometric.loader.ShaDowKHopSampler)** from Zeng *et al.*: [Decoupling the Depth and Scope of Graph Neural Networks](https://arxiv.org/abs/2201.07858) (NeurIPS 2021) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/shadow.py)\\]\n- **[SIGN](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.transforms.SIGN.html)** from Rossi *et al.*: [SIGN: Scalable Inception Graph Neural Networks](https://arxiv.org/abs/2004.11198) (CoRR 2020) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/sign.py)\\]\n- **[HGTLoader](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.loader.HGTLoader.html)** from Hu *et al.*: [Heterogeneous Graph Transformer](https://arxiv.org/abs/2003.01332) (WWW 2020) \\[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/hetero/to_hetero_mag.py)\\]\n\n</details>\n\n## Installation\n\nPyG is available for Python 3.9 to Python 3.13.\n\nFrom **PyG 2.3** onwards, you can install and use PyG **without any external library** required except for PyTorch.\nFor this, simply run\n\n```\npip install torch_geometric\n```\n\n### Additional Libraries\n\nIf you want to utilize the full set of features from PyG, there exists several additional libraries you may want to install:\n\n- **[`pyg-lib`](https://github.com/pyg-team/pyg-lib)**: Heterogeneous GNN operators and graph sampling routines\n- **[`torch-scatter`](https://github.com/rusty1s/pytorch_scatter)**: Accelerated and efficient sparse reductions\n- **[`torch-sparse`](https://github.com/rusty1s/pytorch_sparse)**: [`SparseTensor`](https://pytorch-geometric.readthedocs.io/en/latest/advanced/sparse_tensor.html) support\n- **[`torch-cluster`](https://github.com/rusty1s/pytorch_cluster)**: Graph clustering routines\n- **[`torch-spline-conv`](https://github.com/rusty1s/pytorch_spline_conv)**: [`SplineConv`](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.SplineConv.html) support\n\nThese packages come with their own CPU and GPU kernel implementations based on the [PyTorch C++/CUDA/hip(ROCm) extension interface](https://github.com/pytorch/extension-cpp).\nFor a basic usage of PyG, these dependencies are **fully optional**.\nWe recommend to start with a minimal installation, and install additional dependencies once you start to actually need them.\n\nFor ease of installation of these extensions, we provide `pip` wheels for all major OS/PyTorch/CUDA combinations, see [here](https://data.pyg.org/whl).\n\n#### PyTorch 2.7\n\nTo install the binaries for PyTorch 2.7.0, simply run\n\n```\npip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.7.0+${CUDA}.html\n```\n\nwhere `${CUDA}` should be replaced by either `cpu`, `cu118`, `cu126`, or `cu128` depending on your PyTorch installation.\n\n|             | `cpu` | `cu118` | `cu126` | `cu128` |\n| ----------- | ----- | ------- | ------- | ------- |\n| **Linux**   | \u2705    | \u2705      | \u2705      | \u2705      |\n| **Windows** | \u2705    | \u2705      | \u2705      | \u2705      |\n| **macOS**   | \u2705    |         |         |         |\n\n#### PyTorch 2.6\n\nTo install the binaries for PyTorch 2.6.0, simply run\n\n```\npip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.6.0+${CUDA}.html\n```\n\nwhere `${CUDA}` should be replaced by either `cpu`, `cu118`, `cu124`, or `cu126` depending on your PyTorch installation.\n\n|             | `cpu` | `cu118` | `cu124` | `cu126` |\n| ----------- | ----- | ------- | ------- | ------- |\n| **Linux**   | \u2705    | \u2705      | \u2705      | \u2705      |\n| **Windows** | \u2705    | \u2705      | \u2705      | \u2705      |\n| **macOS**   | \u2705    |         |         |         |\n\n**Note:** Binaries of older versions are also provided for PyTorch 1.4.0, PyTorch 1.5.0, PyTorch 1.6.0, PyTorch 1.7.0/1.7.1, PyTorch 1.8.0/1.8.1, PyTorch 1.9.0, PyTorch 1.10.0/1.10.1/1.10.2, PyTorch 1.11.0, PyTorch 1.12.0/1.12.1, PyTorch 1.13.0/1.13.1, PyTorch 2.0.0/2.0.1, PyTorch 2.1.0/2.1.1/2.1.2, PyTorch 2.2.0/2.2.1/2.2.2, PyTorch 2.3.0/2.3.1, PyTorch 2.4.0/2.4.1, and PyTorch 2.5.0/2.5.1 (following the same procedure).\n**For older versions, you might need to explicitly specify the latest supported version number** or install via `pip install --no-index` in order to prevent a manual installation from source.\nYou can look up the latest supported version number [here](https://data.pyg.org/whl).\n\n### NVIDIA PyG Container\n\nNVIDIA provides a PyG docker container for effortlessly training and deploying GPU accelerated GNNs with PyG, see [here](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pyg).\n\n### Nightly and Master\n\nIn case you want to experiment with the latest PyG features which are not fully released yet, either install the **nightly version** of PyG via\n\n```\npip install pyg-nightly\n```\n\nor install PyG **from master** via\n\n```\npip install git+https://github.com/pyg-team/pytorch_geometric.git\n```\n\n### ROCm Wheels\n\nThe external [`pyg-rocm-build` repository](https://github.com/Looong01/pyg-rocm-build) provides wheels and detailed instructions on how to install PyG for ROCm.\nIf you have any questions about it, please open an issue [here](https://github.com/Looong01/pyg-rocm-build/issues).\n\n## Cite\n\nPlease cite our [PyG 1.0](https://arxiv.org/abs/1903.02428) and [PyG 2.0](https://www.arxiv.org/abs/2507.16991) papers if you use this code in your own work:\n\n```\n@inproceedings{Fey/Lenssen/2019,\n  title={Fast Graph Representation Learning with {PyTorch Geometric}},\n  author={Fey, Matthias and Lenssen, Jan E.},\n  booktitle={ICLR Workshop on Representation Learning on Graphs and Manifolds},\n  year={2019},\n}\n\n@inproceedings{Fey/etal/2025,\n  title={{PyG} 2.0: Scalable Learning on Real World Graphs},\n  author={Fey, Matthias and Sunil, Jinu and Nitta, Akihiro and Puri, Rishi and Shah, Manan, and Stojanovi{\\v{c}, Bla{\\v{z} and Bendias, Ramona and Alexandria, Barghi and Kocijan, Vid and Zhang, Zecheng and He, Xinwei and Lenssen, Jan E. and Leskovec, Jure},\n  booktitle={Temporal Graph Learning Workshop @\u00a0KDD},\n  year={2025},\n}\n```\n\nFeel free to [email us](mailto:matthias.fey@tu-dortmund.de) if you wish your work to be listed in the [external resources](https://pytorch-geometric.readthedocs.io/en/latest/external/resources.html).\nIf you notice anything unexpected, please open an [issue](https://github.com/pyg-team/pytorch_geometric/issues) and let us know.\nIf you have any questions or are missing a specific feature, feel free [to discuss them with us](https://github.com/pyg-team/pytorch_geometric/discussions).\nWe are motivated to constantly make PyG even better.\n\n[contributing-image]: https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat\n[contributing-url]: https://github.com/pyg-team/pytorch_geometric/blob/master/.github/CONTRIBUTING.md\n[docs-image]: https://readthedocs.org/projects/pytorch-geometric/badge/?version=latest\n[docs-url]: https://pytorch-geometric.readthedocs.io/en/latest\n[linting-image]: https://github.com/pyg-team/pytorch_geometric/actions/workflows/linting.yml/badge.svg\n[linting-url]: https://github.com/pyg-team/pytorch_geometric/actions/workflows/linting.yml\n[pypi-image]: https://badge.fury.io/py/torch-geometric.svg\n[pypi-url]: https://pypi.python.org/pypi/torch-geometric\n[slack-image]: https://img.shields.io/badge/slack-pyg-brightgreen\n[slack-url]: https://data.pyg.org/slack.html\n[testing-image]: https://github.com/pyg-team/pytorch_geometric/actions/workflows/testing.yml/badge.svg\n[testing-url]: https://github.com/pyg-team/pytorch_geometric/actions/workflows/testing.yml\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 95879426,
    "name": "ncnn",
    "full_name": "Tencent/ncnn",
    "description": "ncnn is a high-performance neural network inference framework optimized for the mobile platform",
    "html_url": "https://github.com/Tencent/ncnn",
    "clone_url": "https://github.com/Tencent/ncnn.git",
    "owner_login": "Tencent",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/18461506?v=4",
    "stargazers_count": 21868,
    "watchers_count": 21868,
    "forks_count": 4302,
    "open_issues_count": 1128,
    "size": 31053,
    "language": "C++",
    "languages": {
      "C++": 15939965,
      "C": 10971636,
      "GLSL": 1505392,
      "Python": 1448359,
      "CMake": 305686,
      "Shell": 12242,
      "Batchfile": 2325,
      "SWIG": 1787
    },
    "topics": [
      "android",
      "arm-neon",
      "artificial-intelligence",
      "caffe",
      "darknet",
      "deep-learning",
      "high-preformance",
      "inference",
      "ios",
      "keras",
      "mlir",
      "mxnet",
      "ncnn",
      "neural-network",
      "onnx",
      "pytorch",
      "riscv",
      "simd",
      "tensorflow",
      "vulkan"
    ],
    "license_name": "Other",
    "created_at": "2017-06-30T10:55:37+00:00",
    "updated_at": "2025-08-06T00:15:31+00:00",
    "pushed_at": "2025-08-06T00:15:23+00:00",
    "contributors_count": 100,
    "readme_length": 25276,
    "readme_content": "![ncnn](https://raw.githubusercontent.com/Tencent/ncnn/master/images/256-ncnn.png)\n\n# ncnn\n\n[![License](https://img.shields.io/badge/license-BSD_3_Clause-blue.svg?style=for-the-badge)](LICENSE.txt)\n[![Download Total Count](https://img.shields.io/github/downloads/Tencent/ncnn/total.svg?style=for-the-badge)](https://github.com/Tencent/ncnn/releases)\n[![codecov](https://img.shields.io/codecov/c/github/Tencent/ncnn/master?style=for-the-badge)](https://codecov.io/gh/Tencent/ncnn)\n\nncnn is a high-performance neural network inference computing framework optimized for mobile platforms.\nncnn is deeply considerate about deployment and uses on mobile phones from the beginning of design.\nncnn does not have third-party dependencies.\nIt is cross-platform and runs faster than all known open-source frameworks on mobile phone cpu.\nDevelopers can easily deploy deep learning algorithm models to the mobile platform by using efficient ncnn implementation, creating intelligent APPs, and bringing artificial intelligence to your fingertips.\nncnn is currently being used in many Tencent applications, such as QQ, Qzone, WeChat, Pitu, and so on.\n\nncnn \u662f\u4e00\u4e2a\u4e3a\u624b\u673a\u7aef\u6781\u81f4\u4f18\u5316\u7684\u9ad8\u6027\u80fd\u795e\u7ecf\u7f51\u7edc\u524d\u5411\u8ba1\u7b97\u6846\u67b6\u3002\nncnn \u4ece\u8bbe\u8ba1\u4e4b\u521d\u6df1\u523b\u8003\u8651\u624b\u673a\u7aef\u7684\u90e8\u7f72\u548c\u4f7f\u7528\u3002\n\u65e0\u7b2c\u4e09\u65b9\u4f9d\u8d56\uff0c\u8de8\u5e73\u53f0\uff0c\u624b\u673a\u7aef cpu \u7684\u901f\u5ea6\u5feb\u4e8e\u76ee\u524d\u6240\u6709\u5df2\u77e5\u7684\u5f00\u6e90\u6846\u67b6\u3002\n\u57fa\u4e8e ncnn\uff0c\u5f00\u53d1\u8005\u80fd\u591f\u5c06\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u8f7b\u677e\u79fb\u690d\u5230\u624b\u673a\u7aef\u9ad8\u6548\u6267\u884c\uff0c\n\u5f00\u53d1\u51fa\u4eba\u5de5\u667a\u80fd APP\uff0c\u5c06 AI \u5e26\u5230\u4f60\u7684\u6307\u5c16\u3002\nncnn \u76ee\u524d\u5df2\u5728\u817e\u8baf\u591a\u6b3e\u5e94\u7528\u4e2d\u4f7f\u7528\uff0c\u5982\uff1aQQ\uff0cQzone\uff0c\u5fae\u4fe1\uff0c\u5929\u5929 P \u56fe\u7b49\u3002\n\n---\n\n<table>\n<tr>\n<td>\n<b>\u6280\u672f\u4ea4\u6d41 QQ \u7fa4</b><br />\n637093648 (\u8d85\u591a\u5927\u4f6c)<br />\n\u7b54\u6848\uff1a\u5377\u5377\u5377\u5377\u5377\uff08\u5df2\u6ee1\uff09\n</td>\n<td rowspan=3>\n<b>Telegram Group</b>\n\n<https://t.me/ncnnyes>\n</td>\n<td rowspan=3>\n<b>Discord Channel</b>\n\n<https://discord.gg/YRsxgmF>\n</td>\n</tr>\n<tr>\n<td>\n<b>Pocky QQ \u7fa4\uff08MLIR YES!\uff09</b><br />\n677104663 (\u8d85\u591a\u5927\u4f6c)<br />\n\u7b54\u6848\uff1amulti-level intermediate representation\n</td>\n</tr>\n<tr>\n<td>\n<b>\u4ed6\u4eec\u90fd\u4e0d\u77e5\u9053 pnnx \u6709\u591a\u597d\u7528\u7fa4</b><br />\n818998520 (\u65b0\u7fa4\uff01)\n</td>\n</tr>\n</table>\n\n---\n\n## Download & Build status\n\nhttps://github.com/Tencent/ncnn/releases/latest\n\n\n<table>\n<tr>\n<td rowspan=2>\n  <img src=\"https://user-images.githubusercontent.com/25181517/192108372-f71d70ac-7ae6-4c0d-8395-51d8870c2ef0.png\" width=\"120\" height=\"auto\">\n</td>\n<td colspan=3>\n\n  **[how to build ncnn library](https://github.com/Tencent/ncnn/wiki/how-to-build) on Linux / Windows / macOS / Raspberry Pi3, Pi4 / POWER / Android / NVIDIA Jetson / iOS / WebAssembly / AllWinner D1 / Loongson 2K1000**\n\n</td>\n</tr>\n<tr>\n<td>Source</td>\n<td colspan=2>\n\n  [<img src=\"https://img.shields.io/badge/download-blue?style=for-the-badge\">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-full-source.zip)\n\n</td>\n</tr>\n\n<tr>\n<td rowspan=3>\n  <img src=\"https://user-images.githubusercontent.com/25181517/117269608-b7dcfb80-ae58-11eb-8e66-6cc8753553f0.png\" width=\"120\" height=\"auto\">\n</td>\n<td colspan=3>\n\n- [Build for Android](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-android)\n- [Build for Termux on Android](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-termux-on-android)\n\n</td>\n</tr>\n<tr>\n<td>Android</td>\n<td>\n\n  [<img src=\"https://img.shields.io/badge/download-blue?style=for-the-badge\">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-android-vulkan.zip)\n  [<img src=\"https://img.shields.io/badge/+cpuonly-blue?style=for-the-badge\">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-android.zip)\n\n</td>\n<td rowspan=2>\n\n  [<img src=\"https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/android.yml?branch=master&style=for-the-badge&label=build\">](https://github.com/Tencent/ncnn/actions?query=workflow%3Aandroid)\n\n</td>\n</tr>\n<tr>\n<td>Android shared</td>\n<td>\n\n  [<img src=\"https://img.shields.io/badge/download-blue?style=for-the-badge\">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-android-vulkan-shared.zip)\n  [<img src=\"https://img.shields.io/badge/+cpuonly-blue?style=for-the-badge\">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-android-shared.zip)\n\n</td>\n</tr>\n\n<tr>\n<td rowspan=3>\n  <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/37/HMOS_Logo_Icon.svg/240px-HMOS_Logo_Icon.svg.png\" width=\"120\" height=\"auto\">\n</td>\n<td colspan=3>\n\n- [Build for HarmonyOS with cross-compiling](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-harmonyos-with-cross-compiling)\n\n</td>\n</tr>\n<tr>\n<td>HarmonyOS</td>\n<td>\n\n</td>\n<td rowspan=2>\n\n  [<img src=\"https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/harmonyos.yml?branch=master&style=for-the-badge&label=build\">](https://github.com/Tencent/ncnn/actions?query=workflow%3Aharmonyos)\n\n</td>\n</tr>\n<tr>\n<td>HarmonyOS shared</td>\n<td>\n\n</td>\n</tr>\n\n<tr>\n<td rowspan=3>\n  <img src=\"https://user-images.githubusercontent.com/25181517/121406611-a8246b80-c95e-11eb-9b11-b771486377f6.png\" width=\"120\" height=\"auto\">\n</td>\n<td colspan=3>\n\n- [Build for iOS on macOS with xcode](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-ios-on-macos-with-xcode)\n\n</td>\n</tr>\n<tr>\n<td>iOS</td>\n<td>\n\n  [<img src=\"https://img.shields.io/badge/download-blue?style=for-the-badge\">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-ios-vulkan.zip)\n  [<img src=\"https://img.shields.io/badge/+cpuonly-blue?style=for-the-badge\">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-ios.zip)\n\n</td>\n<td rowspan=2>\n\n  [<img src=\"https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/ios.yml?branch=master&style=for-the-badge&label=build\">](https://github.com/Tencent/ncnn/actions?query=workflow%3Aios)\n\n</td>\n</tr>\n<tr>\n<td>iOS-Simulator</td>\n<td>\n\n  [<img src=\"https://img.shields.io/badge/download-blue?style=for-the-badge\">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-ios-simulator-vulkan.zip)\n  [<img src=\"https://img.shields.io/badge/+cpuonly-blue?style=for-the-badge\">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-ios-simulator.zip)\n\n</td>\n</tr>\n\n<tr>\n<td rowspan=10>\n  <img src=\"https://user-images.githubusercontent.com/25181517/186884152-ae609cca-8cf1-4175-8d60-1ce1fa078ca2.png\" width=\"120\" height=\"auto\">\n</td>\n<td colspan=3>\n\n- [Build for macOS](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-macos)\n\n</td>\n</tr>\n<tr>\n<td>macOS</td>\n<td>\n\n  [<img src=\"https://img.shields.io/badge/download-blue?style=for-the-badge\">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-macos-vulkan.zip)\n  [<img src=\"https://img.shields.io/badge/+cpuonly-blue?style=for-the-badge\">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-macos.zip)\n\n</td>\n<td rowspan=1>\n\n  [<img src=\"https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/macos.yml?branch=master&style=for-the-badge&label=build\">](https://github.com/Tencent/ncnn/actions?query=workflow%3Amacos)\n\n</td>\n</tr>\n<tr>\n<td>Mac-Catalyst</td>\n<td>\n\n  [<img src=\"https://img.shields.io/badge/download-blue?style=for-the-badge\">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-mac-catalyst-vulkan.zip)\n  [<img src=\"https://img.shields.io/badge/+cpuonly-blue?style=for-the-badge\">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-mac-catalyst.zip)\n\n</td>\n<td rowspan=1>\n\n  [<img src=\"https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/mac-catalyst.yml?branch=master&style=for-the-badge&label=build\">](https://github.com/Tencent/ncnn/actions?query=workflow%3Amac-catalyst)\n\n</td>\n</tr>\n<tr>\n<td>watchOS</td>\n<td>\n\n  [<img src=\"https://img.shields.io/badge/download-blue?style=for-the-badge\">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-watchos.zip)\n\n</td>\n<td rowspan=2>\n\n  [<img src=\"https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/watchos.yml?branch=master&style=for-the-badge&label=build\">](https://github.com/Tencent/ncnn/actions?query=workflow%3Awatchos)\n\n</td>\n</tr>\n<tr>\n<td>watchOS-Simulator</td>\n<td>\n\n  [<img src=\"https://img.shields.io/badge/download-blue?style=for-the-badge\">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-watchos-simulator.zip)\n\n</td>\n</tr>\n<tr>\n<td>tvOS</td>\n<td>\n\n  [<img src=\"https://img.shields.io/badge/download-blue?style=for-the-badge\">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-tvos-vulkan.zip)\n  [<img src=\"https://img.shields.io/badge/+cpuonly-blue?style=for-the-badge\">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-tvos.zip)\n\n</td>\n<td rowspan=2>\n\n  [<img src=\"https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/tvos.yml?branch=master&style=for-the-badge&label=build\">](https://github.com/Tencent/ncnn/actions?query=workflow%3Atvos)\n\n</td>\n</tr>\n<tr>\n<td>tvOS-Simulator</td>\n<td>\n\n  [<img src=\"https://img.shields.io/badge/download-blue?style=for-the-badge\">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-tvos-simulator-vulkan.zip)\n  [<img src=\"https://img.shields.io/badge/+cpuonly-blue?style=for-the-badge\">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-tvos-simulator.zip)\n\n</td>\n</tr>\n<tr>\n<td>visionOS</td>\n<td>\n\n  [<img src=\"https://img.shields.io/badge/download-blue?style=for-the-badge\">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-visionos-vulkan.zip)\n  [<img src=\"https://img.shields.io/badge/+cpuonly-blue?style=for-the-badge\">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-visionos.zip)\n\n</td>\n<td rowspan=2>\n\n  [<img src=\"https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/visionos.yml?branch=master&style=for-the-badge&label=build\">](https://github.com/Tencent/ncnn/actions?query=workflow%3Avisionos)\n\n</td>\n</tr>\n<tr>\n<td>visionOS-Simulator</td>\n<td>\n\n  [<img src=\"https://img.shields.io/badge/download-blue?style=for-the-badge\">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-visionos-simulator-vulkan.zip)\n  [<img src=\"https://img.shields.io/badge/+cpuonly-blue?style=for-the-badge\">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-visionos-simulator.zip)\n\n</td>\n</tr>\n<tr>\n<td>Apple xcframework</td>\n<td>\n\n  [<img src=\"https://img.shields.io/badge/download-blue?style=for-the-badge\">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-apple-vulkan.zip)\n  [<img src=\"https://img.shields.io/badge/+cpuonly-blue?style=for-the-badge\">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-apple.zip)\n\n</td>\n<td rowspan=1>\n\n</td>\n</tr>\n\n<tr>\n<td rowspan=3>\n  <img src=\"https://user-images.githubusercontent.com/25181517/186884153-99edc188-e4aa-4c84-91b0-e2df260ebc33.png\" width=\"120\" height=\"auto\">\n</td>\n<td colspan=3>\n\n- [Build for Linux / NVIDIA Jetson / Raspberry Pi3, Pi4 / POWER](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-linux)\n\n</td>\n</tr>\n<tr>\n<td>Ubuntu 22.04</td>\n<td>\n\n  [<img src=\"https://img.shields.io/badge/download-blue?style=for-the-badge\">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-ubuntu-2204.zip)\n  [<img src=\"https://img.shields.io/badge/+shared-blue?style=for-the-badge\">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-ubuntu-2204-shared.zip)\n\n</td>\n<td rowspan=2>\n\n  [<img src=\"https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/linux-x64-gpu-gcc.yml?branch=master&style=for-the-badge&label=build\">](https://github.com/Tencent/ncnn/actions?query=workflow%3Alinux-x64-gpu-gcc)\n\n</td>\n</tr>\n<tr>\n<td>Ubuntu 24.04</td>\n<td>\n\n  [<img src=\"https://img.shields.io/badge/download-blue?style=for-the-badge\">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-ubuntu-2404.zip)\n  [<img src=\"https://img.shields.io/badge/+shared-blue?style=for-the-badge\">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-ubuntu-2404-shared.zip)\n\n</td>\n</tr>\n\n<tr>\n<td rowspan=5>\n  <img alt=\"windows\" src=\"https://user-images.githubusercontent.com/25181517/186884150-05e9ff6d-340e-4802-9533-2c3f02363ee3.png\" width=\"120\" height=\"auto\">\n</td>\n<td colspan=3>\n\n- [Build for Windows x64 using VS2017](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-windows-x64-using-visual-studio-community-2017)\n- [Build for Windows x64 using MinGW-w64](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-windows-x64-using-mingw-w64)\n\n</td>\n</tr>\n<tr>\n<td>VS2015</td>\n<td>\n\n  [<img src=\"https://img.shields.io/badge/download-blue?style=for-the-badge\">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-windows-vs2015.zip)\n  [<img src=\"https://img.shields.io/badge/+shared-blue?style=for-the-badge\">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-windows-vs2015-shared.zip)\n\n</td>\n<td rowspan=4>\n\n  [<img src=\"https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/windows.yml?branch=master&style=for-the-badge&label=build\">](https://github.com/Tencent/ncnn/actions?query=workflow%3Awindows)\n\n</td>\n</tr>\n<tr>\n<td>VS2017</td>\n<td>\n\n  [<img src=\"https://img.shields.io/badge/download-blue?style=for-the-badge\">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-windows-vs2017.zip)\n  [<img src=\"https://img.shields.io/badge/+shared-blue?style=for-the-badge\">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-windows-vs2017-shared.zip)\n\n</td>\n</tr>\n<tr>\n<td>VS2019</td>\n<td>\n\n  [<img src=\"https://img.shields.io/badge/download-blue?style=for-the-badge\">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-windows-vs2019.zip)\n  [<img src=\"https://img.shields.io/badge/+shared-blue?style=for-the-badge\">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-windows-vs2019-shared.zip)\n\n</td>\n</tr>\n<tr>\n<td>VS2022</td>\n<td>\n\n  [<img src=\"https://img.shields.io/badge/download-blue?style=for-the-badge\">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-windows-vs2022.zip)\n  [<img src=\"https://img.shields.io/badge/+shared-blue?style=for-the-badge\">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-windows-vs2022-shared.zip)\n\n</td>\n</tr>\n\n<tr>\n<td rowspan=2>\n  <img src=\"https://user-images.githubusercontent.com/25181517/188324036-d704ac9a-6e61-4722-b978-254b25b61bed.png\" width=\"120\" height=\"auto\">\n</td>\n<td colspan=3>\n\n- [Build for WebAssembly](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-webassembly)\n\n</td>\n</tr>\n<tr>\n<td>WebAssembly</td>\n<td>\n\n  [<img src=\"https://img.shields.io/badge/download-blue?style=for-the-badge\">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-webassembly.zip)\n\n</td>\n<td>\n\n  [<img src=\"https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/web-assembly.yml?branch=master&style=for-the-badge&label=build\">](https://github.com/Tencent/ncnn/actions?query=workflow%3Aweb-assembly)\n\n</td>\n</tr>\n\n<tr>\n<td rowspan=8>\n  <img src=\"https://github.com/marwin1991/profile-technology-icons/assets/76662862/2481dc48-be6b-4ebb-9e8c-3b957efe69fa\" width=\"120\" height=\"auto\">\n</td>\n<td colspan=3>\n\n- [Build for ARM Cortex-A family with cross-compiling](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-arm-cortex-a-family-with-cross-compiling)\n- [Build for Hisilicon platform with cross-compiling](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-hisilicon-platform-with-cross-compiling)\n- [Build for AllWinner D1](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-allwinner-d1)\n- [Build for Loongson 2K1000](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-loongson-2k1000)\n- [Build for QNX](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-qnx)\n\n</td>\n</tr>\n<tr>\n<td>Linux (arm)</td>\n<td></td>\n<td>\n\n  [<img src=\"https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/linux-arm.yml?branch=master&style=for-the-badge&label=build\">](https://github.com/Tencent/ncnn/actions?query=workflow%3Alinux-arm)\n\n</td>\n</tr>\n<tr>\n<td>Linux (aarch64)</td>\n<td></td>\n<td>\n\n  [<img src=\"https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/linux-aarch64.yml?branch=master&style=for-the-badge&label=build\">](https://github.com/Tencent/ncnn/actions?query=workflow%3Alinux-aarch64)\n\n</td>\n</tr>\n<tr>\n<td>Linux (mips)</td>\n<td></td>\n<td>\n\n  [<img src=\"https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/linux-mips.yml?branch=master&style=for-the-badge&label=build\">](https://github.com/Tencent/ncnn/actions?query=workflow%3Alinux-mips)\n\n</td>\n</tr>\n<tr>\n<td>Linux (mips64)</td>\n<td></td>\n<td>\n\n  [<img src=\"https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/linux-mips64.yml?branch=master&style=for-the-badge&label=build\">](https://github.com/Tencent/ncnn/actions?query=workflow%3Alinux-mips64)\n\n</td>\n</tr>\n<tr>\n<td>Linux (ppc64)</td>\n<td></td>\n<td>\n\n  [<img src=\"https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/linux-ppc64.yml?branch=master&style=for-the-badge&label=build\">](https://github.com/Tencent/ncnn/actions?query=workflow%3Alinux-ppc64)\n\n</td>\n</tr>\n<tr>\n<td>Linux (riscv64)</td>\n<td></td>\n<td>\n\n  [<img src=\"https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/linux-riscv64.yml?branch=master&style=for-the-badge&label=build\">](https://github.com/Tencent/ncnn/actions?query=workflow%3Alinux-riscv64)\n\n</td>\n</tr>\n<tr>\n<td>Linux (loongarch64)</td>\n<td></td>\n<td>\n\n  [<img src=\"https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/linux-loongarch64.yml?branch=master&style=for-the-badge&label=build\">](https://github.com/Tencent/ncnn/actions?query=workflow%3Alinux-loongarch64)\n\n</td>\n</tr>\n\n</table>\n\n\n---\n\n## Support most commonly used CNN network\n\n## \u652f\u6301\u5927\u90e8\u5206\u5e38\u7528\u7684 CNN \u7f51\u7edc\n\n- Classical CNN:\n  [VGG](https://github.com/BVLC/caffe/wiki/Model-Zoo#models-used-by-the-vgg-team-in-ilsvrc-2014)\n  [AlexNet](https://github.com/BVLC/caffe/tree/9b891540183ddc834a02b2bd81b31afae71b2153/models/bvlc_alexnet)\n  [GoogleNet](https://github.com/BVLC/caffe/tree/9b891540183ddc834a02b2bd81b31afae71b2153/models/bvlc_googlenet)\n  Inception\n  ...\n- Practical CNN:\n  [ResNet](https://github.com/tornadomeet/ResNet)\n  [DenseNet](https://github.com/liuzhuang13/DenseNet)\n  [SENet](https://github.com/hujie-frank/SENet)\n  [FPN](https://github.com/unsky/FPN)\n  ...\n- Light-weight CNN:\n  [SqueezeNet](https://github.com/forresti/SqueezeNet)\n  [MobileNetV1](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md)\n  [MobileNetV2/V3](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md)\n  [ShuffleNetV1](https://github.com/farmingyard/ShuffleNet)\n  [ShuffleNetV2](https://github.com/opconty/keras-shufflenetV2)\n  [MNasNet](https://github.com/tensorflow/models/tree/master/research/slim/nets/nasnet)\n  ...\n- Face Detection:\n  [MTCNN](https://github.com/ipazc/mtcnn)\n  [RetinaFace](https://github.com/biubug6/Pytorch_Retinaface)\n  [scrfd](https://github.com/nihui/ncnn-android-scrfd)\n  ...\n- Detection:\n  [VGG-SSD](https://github.com/lzx1413/CAFFE_SSD)\n  [MobileNet-SSD](https://github.com/chuanqi305/MobileNet-SSD)\n  [SqueezeNet-SSD](https://github.com/chuanqi305/SqueezeNet-SSD)\n  [MobileNetV2-SSDLite](https://github.com/chuanqi305/MobileNetv2-SSDLite)\n  [MobileNetV3-SSDLite](https://github.com/XiaoyuHuang96/MobilenetV3SSDLite-tfkeras)\n  ...\n- Detection:\n  [Faster-RCNN](https://github.com/rbgirshick/py-faster-rcnn)\n  [R-FCN](https://github.com/daijifeng001/R-FCN)\n  ...\n- Detection:\n  [YOLOv2](https://github.com/longcw/yolo2-pytorch)\n  [YOLOv3](https://github.com/ultralytics/yolov3)\n  [MobileNet-YOLOv3](https://github.com/eric612/MobileNet-YOLO)\n  [YOLOv4](https://github.com/Tianxiaomo/pytorch-YOLOv4)\n  [YOLOv5](https://github.com/ultralytics/yolov5)\n  [YOLOv7](https://github.com/WongKinYiu/yolov7)\n  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)\n  [YOLOv8](https://github.com/nihui/ncnn-android-yolov8)\n  ...\n- Detection:\n  [NanoDet](https://github.com/RangiLyu/nanodet)\n- Segmentation:\n  [FCN](https://github.com/unsky/FPN)\n  [PSPNet](https://github.com/hszhao/PSPNet)\n  [UNet](https://github.com/zhixuhao/unet)\n  [YOLACT](https://github.com/dbolya/yolact)\n  ...\n- Pose Estimation:\n  [SimplePose](https://github.com/dog-qiuqiu/Ultralight-SimplePose)\n  ...\n\n---\n\n## HowTo\n\n**[use ncnn with alexnet](https://github.com/Tencent/ncnn/wiki/use-ncnn-with-alexnet) with detailed steps, recommended for beginners :)**\n\n**[ncnn \u7ec4\u4ef6\u4f7f\u7528\u6307\u5317 alexnet](https://github.com/Tencent/ncnn/wiki/use-ncnn-with-alexnet.zh) \u9644\u5e26\u8be6\u7ec6\u6b65\u9aa4\uff0c\u65b0\u4eba\u5f3a\u70c8\u63a8\u8350 :)**\n\n**[use netron for ncnn model visualization](https://netron.app)**\n\n**[use ncnn with pytorch or onnx](https://github.com/Tencent/ncnn/wiki/use-ncnn-with-pytorch-or-onnx)**\n\n[ncnn low-level operation api](https://github.com/Tencent/ncnn/wiki/low-level-operation-api)\n\n[ncnn param and model file spec](https://github.com/Tencent/ncnn/wiki/param-and-model-file-structure)\n\n[ncnn operation param weight table](https://github.com/Tencent/ncnn/wiki/operation-param-weight-table)\n\n[how to implement custom layer step by step](https://github.com/Tencent/ncnn/wiki/how-to-implement-custom-layer-step-by-step)\n\n---\n\n## FAQ\n\n**[ncnn deepwiki](https://deepwiki.com/Tencent/ncnn) LLM Answering Questions ;)** \n\n**[ncnn throw error](https://github.com/Tencent/ncnn/wiki/FAQ-ncnn-throw-error)**\n\n**[ncnn produce wrong result](https://github.com/Tencent/ncnn/wiki/FAQ-ncnn-produce-wrong-result)**\n\n**[ncnn vulkan](https://github.com/Tencent/ncnn/wiki/FAQ-ncnn-vulkan)**\n\n---\n\n## Features\n\n- Supports convolutional neural networks, supports multiple input and multi-branch structure, can calculate part of the branch\n- No third-party library dependencies, does not rely on BLAS / NNPACK or any other computing framework\n- Pure C++ implementation, cross-platform, supports Android, iOS and so on\n- ARM NEON assembly level of careful optimization, calculation speed is extremely high\n- Sophisticated memory management and data structure design, very low memory footprint\n- Supports multi-core parallel computing acceleration, ARM big.LITTLE CPU scheduling optimization\n- Supports GPU acceleration via the next-generation low-overhead Vulkan API\n- Extensible model design, supports 8bit quantization and half-precision floating point storage, can import caffe/pytorch/mxnet/onnx/darknet/keras/tensorflow(mlir) models\n- Support direct memory zero copy reference load network model\n- Can be registered with custom layer implementation and extended\n- Well, it is strong, not afraid of being stuffed with \u5377 QvQ\n\n## \u529f\u80fd\u6982\u8ff0\n\n- \u652f\u6301\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u652f\u6301\u591a\u8f93\u5165\u548c\u591a\u5206\u652f\u7ed3\u6784\uff0c\u53ef\u8ba1\u7b97\u90e8\u5206\u5206\u652f\n- \u65e0\u4efb\u4f55\u7b2c\u4e09\u65b9\u5e93\u4f9d\u8d56\uff0c\u4e0d\u4f9d\u8d56 BLAS/NNPACK \u7b49\u8ba1\u7b97\u6846\u67b6\n- \u7eaf C++ \u5b9e\u73b0\uff0c\u8de8\u5e73\u53f0\uff0c\u652f\u6301 Android / iOS \u7b49\n- ARM Neon \u6c47\u7f16\u7ea7\u826f\u5fc3\u4f18\u5316\uff0c\u8ba1\u7b97\u901f\u5ea6\u6781\u5feb\n- \u7cbe\u7ec6\u7684\u5185\u5b58\u7ba1\u7406\u548c\u6570\u636e\u7ed3\u6784\u8bbe\u8ba1\uff0c\u5185\u5b58\u5360\u7528\u6781\u4f4e\n- \u652f\u6301\u591a\u6838\u5e76\u884c\u8ba1\u7b97\u52a0\u901f\uff0cARM big.LITTLE CPU \u8c03\u5ea6\u4f18\u5316\n- \u652f\u6301\u57fa\u4e8e\u5168\u65b0\u4f4e\u6d88\u8017\u7684 Vulkan API GPU \u52a0\u901f\n- \u53ef\u6269\u5c55\u7684\u6a21\u578b\u8bbe\u8ba1\uff0c\u652f\u6301 8bit [\u91cf\u5316](tools/quantize) \u548c\u534a\u7cbe\u5ea6\u6d6e\u70b9\u5b58\u50a8\uff0c\u53ef\u5bfc\u5165 caffe/pytorch/mxnet/onnx/darknet/keras/tensorflow(mlir) \u6a21\u578b\n- \u652f\u6301\u76f4\u63a5\u5185\u5b58\u96f6\u62f7\u8d1d\u5f15\u7528\u52a0\u8f7d\u7f51\u7edc\u6a21\u578b\n- \u53ef\u6ce8\u518c\u81ea\u5b9a\u4e49\u5c42\u5b9e\u73b0\u5e76\u6269\u5c55\n- \u6069\uff0c\u5f88\u5f3a\u5c31\u662f\u4e86\uff0c\u4e0d\u6015\u88ab\u585e\u5377 QvQ\n\n---\n\n## supported platform matrix\n\n- \u2705 = known work and runs fast with good optimization\n- \u2714\ufe0f = known work, but speed may not be fast enough\n- \u2754 = shall work, not confirmed\n- / = not applied\n\n|            | Windows | Linux | Android | macOS | iOS |\n| ---------- | ------- | ----- | ------- | ----- | --- |\n| intel-cpu  | \u2714\ufe0f      | \u2714\ufe0f    | \u2754      | \u2714\ufe0f    | /   |\n| intel-gpu  | \u2714\ufe0f      | \u2714\ufe0f    | \u2754      | \u2754    | /   |\n| amd-cpu    | \u2714\ufe0f      | \u2714\ufe0f    | \u2754      | \u2714\ufe0f    | /   |\n| amd-gpu    | \u2714\ufe0f      | \u2714\ufe0f    | \u2754      | \u2754    | /   |\n| nvidia-gpu | \u2714\ufe0f      | \u2714\ufe0f    | \u2754      | \u2754    | /   |\n| qcom-cpu   | \u2754      | \u2714\ufe0f    | \u2705      | /     | /   |\n| qcom-gpu   | \u2754      | \u2714\ufe0f    | \u2714\ufe0f      | /     | /   |\n| arm-cpu    | \u2754      | \u2754    | \u2705      | /     | /   |\n| arm-gpu    | \u2754      | \u2754    | \u2714\ufe0f      | /     | /   |\n| apple-cpu  | /       | /     | /       | \u2714\ufe0f    | \u2705  |\n| apple-gpu  | /       | /     | /       | \u2714\ufe0f    | \u2714\ufe0f  |\n| ibm-cpu    | /       | \u2714\ufe0f     | /       | /    | /  |\n\n---\n\n## Project examples\n\n- <https://github.com/nihui/ncnn-android-squeezenet>\n- <https://github.com/nihui/ncnn-android-styletransfer>\n- <https://github.com/nihui/ncnn-android-mobilenetssd>\n- <https://github.com/moli232777144/mtcnn_ncnn>\n- <https://github.com/nihui/ncnn-android-yolov5>\n- <https://github.com/xiang-wuu/ncnn-android-yolov7>\n- <https://github.com/nihui/ncnn-android-scrfd> \ud83e\udd29\n- <https://github.com/shaoshengsong/qt_android_ncnn_lib_encrypt_example>\n\n<img src=\"https://github.com/nihui/ncnn-assets/raw/master/20181217/ncnn-2.jpg\" height =\"230\"/><img src=\"https://github.com/nihui/ncnn-assets/raw/master/20181217/4.jpg\" height =\"230\"/><img src=\"https://github.com/nihui/ncnn-assets/raw/master/20181217/ncnn-33.jpg\" height =\"230\"/><img src=\"https://github.com/nihui/ncnn-assets/raw/master/20181217/ncnn-m.png\" height =\"230\"/><img src=\"https://github.com/nihui/ncnn-android-yolov5/raw/master/screenshot.jpg\" height =\"230\"/><img src=\"https://github.com/nihui/ncnn-android-scrfd/raw/master/screenshot.jpg\" height =\"230\"/><br>\n\n- <https://github.com/magicse/ncnn-colorization-siggraph17><br>\n<img src=\"https://user-images.githubusercontent.com/13585785/189326958-f5a8d6f8-caef-49bf-88da-ae494371195d.jpg\" width =\"700\"/>\n\n- <https://github.com/mizu-bai/ncnn-fortran> Call ncnn from Fortran\n\n- <https://github.com/k2-fsa/sherpa> Use ncnn for real-time speech\n  recognition (i.e., speech-to-text); also support embedded devices and provide\n  mobile Apps (e.g., Android App)\n\n---\n\n## License\n\n[BSD 3 Clause](LICENSE.txt)\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 281229404,
    "name": "ultimatevocalremovergui",
    "full_name": "Anjok07/ultimatevocalremovergui",
    "description": " GUI for a Vocal Remover that uses Deep Neural Networks.",
    "html_url": "https://github.com/Anjok07/ultimatevocalremovergui",
    "clone_url": "https://github.com/Anjok07/ultimatevocalremovergui.git",
    "owner_login": "Anjok07",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/68268275?v=4",
    "stargazers_count": 21433,
    "watchers_count": 21433,
    "forks_count": 1585,
    "open_issues_count": 1183,
    "size": 561099,
    "language": "Python",
    "languages": {
      "Python": 902296,
      "Tcl": 432549,
      "Shell": 87
    },
    "topics": [
      "audio",
      "instrumental",
      "karaoke",
      "kareokee",
      "music",
      "pytorch",
      "separation",
      "source",
      "spectrogram",
      "vocal",
      "vocal-remover",
      "vocals"
    ],
    "license_name": "MIT License",
    "created_at": "2020-07-20T21:29:01+00:00",
    "updated_at": "2025-08-05T21:30:54+00:00",
    "pushed_at": "2025-03-13T21:44:03+00:00",
    "contributors_count": 8,
    "readme_length": 13717,
    "readme_content": "# Ultimate Vocal Remover GUI v5.6\r\n<img src=\"https://raw.githubusercontent.com/Anjok07/ultimatevocalremovergui/master/gui_data/img/UVR_v5.6.png?raw=true\" />\r\n\r\n[![Release](https://img.shields.io/github/release/anjok07/ultimatevocalremovergui.svg)](https://github.com/anjok07/ultimatevocalremovergui/releases/latest)\r\n[![Downloads](https://img.shields.io/github/downloads/anjok07/ultimatevocalremovergui/total.svg)](https://github.com/anjok07/ultimatevocalremovergui/releases)\r\n\r\n## About\r\n\r\nThis application uses state-of-the-art source separation models to remove vocals from audio files. UVR's core developers trained all of the models provided in this package (except for the Demucs v3 and v4 4-stem models).\r\n\r\n- **Core Developers**\r\n    - [Anjok07](https://github.com/anjok07)\r\n    - [aufr33](https://github.com/aufr33)\r\n\r\n- **Support the Project**\r\n    - [Donate](https://www.buymeacoffee.com/uvr5)\r\n\r\n## Installation\r\n\r\nThese bundles contain the UVR interface, Python, PyTorch, and other dependencies needed to run the application effectively. No prerequisites are required.\r\n\r\n### Windows Installation\r\n\r\n- Please Note:\r\n    - This installer is intended for those running Windows 10 or higher. \r\n    - Application functionality for systems running Windows 7 or lower is not guaranteed.\r\n    - Application functionality for Intel Pentium & Celeron CPUs systems is not guaranteed.\r\n    - You must install UVR to the main C:\\ drive. Installing UVR to a secondary drive will cause instability.\r\n\r\n- Download the UVR installer for Windows via the link below:\r\n    - [Main Download Link](https://github.com/Anjok07/ultimatevocalremovergui/releases/download/v5.6/UVR_v5.6.0_setup.exe)\r\n    - [Main Download Link mirror](https://www.mediafire.com/file_premium/jiatpgp0ljou52p/UVR_v5.6.0_setup.exe/file)\r\n- If you use an **AMD Radeon or Intel Arc graphics card**, you can try the DirectML version:\r\n    - [DirectML Version - Main Download Link](https://github.com/Anjok07/ultimatevocalremovergui/releases/download/v5.6/UVR_1_15_25_22_30_BETA_full.exe)\r\n- Update Package instructions for those who have UVR already installed:\r\n    - If you already have UVR installed you can install this package over it or download it straight from the application or [click here for the patch](https://github.com/Anjok07/ultimatevocalremovergui/releases/download/v5.6/UVR_Patch_10_6_23_4_27.exe).\r\n\r\n<details id=\"WindowsManual\">\r\n  <summary>Windows Manual Installation</summary>\r\n\r\n### Manual Windows Installation\r\n\r\n- Download and extract the repository [here](https://github.com/Anjok07/ultimatevocalremovergui/archive/refs/heads/master.zip)\r\n- Download and install Python [here](https://www.python.org/ftp/python/3.9.8/python-3.9.8-amd64.exe)\r\n   - Make sure to check \"Add python.exe to PATH\" during the install\r\n- Run the following commands from the extracted repo directory:\r\n\r\n```\r\npython.exe -m pip install -r requirements.txt\r\n```\r\n\r\nIf you have a compatible Nvidia GPU, run the following command:\r\n\r\n```\r\npython.exe -m pip install --upgrade torch --extra-index-url https://download.pytorch.org/whl/cu117\r\n```\r\n\r\nIf you do not have FFmpeg or Rubber Band installed and want to avoid going through the process of installing them the long way, follow the instructions below.\r\n\r\n**FFmpeg Installation**\r\n\r\n- Download the precompiled build [here](https://www.gyan.dev/ffmpeg/builds/ffmpeg-release-essentials.zip)\r\n- From the archive, extract the following file to the UVR application directory:\r\n   - ```ffmpeg-5.1.2-essentials_build/bin/ffmpeg.exe```\r\n\r\n**Rubber Band Installation**\r\n\r\nIn order to use the Time Stretch or Change Pitch tool, you'll need Rubber Band.\r\n\r\n- Download the precompiled build [here](https://breakfastquay.com/files/releases/rubberband-3.1.2-gpl-executable-windows.zip)\r\n- From the archive, extract the following files to the UVR application directory:\r\n   - ```rubberband-3.1.2-gpl-executable-windows/rubberband.exe```\r\n   - ```rubberband-3.1.2-gpl-executable-windows/sndfile.dll```\r\n\r\n</details>\r\n\r\n### MacOS Installation\r\n- Please Note:\r\n    - The MacOS Sonoma mouse clicking issue has been fixed.\r\n    - MPS (GPU) acceleration for Mac M1 has been expanded to work with Demucs v4 and all MDX-Net models.\r\n    - This bundle is intended for those running macOS Big Sur and above.\r\n    - Application functionality for systems running macOS Catalina or lower is not guaranteed.\r\n    - Application functionality for older or budget Mac systems is not guaranteed.\r\n    - Once everything is installed, the application may take up to 5-10 minutes to start for the first time (depending on your Macbook).\r\n\r\n- Download the UVR dmg for MacOS via one of the links below:\r\n    - Mac M1 (arm64) users:\r\n       - [Main Download Link](https://github.com/Anjok07/ultimatevocalremovergui/releases/download/v5.6/Ultimate_Vocal_Remover_v5_6_MacOS_arm64.dmg)\r\n       - [Main Download Link mirror](https://www.mediafire.com/file_premium/u3rk54wsqadpy93/Ultimate_Vocal_Remover_v5_6_MacOS_arm64.dmg/file)\r\n\r\n    - Mac Intel (x86_64) users:\r\n       - [Main Download Link](https://github.com/Anjok07/ultimatevocalremovergui/releases/download/v5.6/Ultimate_Vocal_Remover_v5_6_MacOS_x86_64.dmg)\r\n       - [Main Download Link mirror](https://www.mediafire.com/file_premium/2gf1werx5ly5ylz/Ultimate_Vocal_Remover_v5_6_MacOS_x86_64.dmg/file)\r\n\r\n<details id=\"CannotOpen\">\r\n  <summary>MacOS Users: Having Trouble Opening UVR?</summary>\r\n\r\n> Due to Apples strict application security, you may need to follow these steps to open UVR.\r\n>\r\n> First, run the following command via Terminal.app to allow applications to run from all sources (it's recommended that you re-enable this once UVR opens properly.)\r\n> \r\n> ```bash\r\n> sudo spctl --master-disable\r\n> ```\r\n> \r\n> Second, run the following command to bypass Notarization: \r\n> \r\n> ```bash\r\n> sudo xattr -rd com.apple.quarantine /Applications/Ultimate\\ Vocal\\ Remover.app\r\n> ```\r\n\r\n</details>\r\n\r\n<details id=\"MacInstall\">\r\n  <summary>Manual MacOS Installation</summary>\r\n\r\n### Manual MacOS Installation\r\n\r\n- Download and save this repository [here](https://github.com/Anjok07/ultimatevocalremovergui/archive/refs/heads/master.zip)\r\n- Download and install Python 3.10 [here](https://www.python.org/ftp/python/3.10.9/python-3.10.9-macos11.pkg)\r\n- From the saved directory run the following - \r\n\r\n```\r\npip3 install -r requirements.txt\r\n```\r\n\r\n- If your Mac is running with an M1, please run the following command next. If not, skip this step. - \r\n\r\n```\r\ncp /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/_soundfile_data/libsndfile_arm64.dylib /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/_soundfile_data/libsndfile.dylib\r\n```\r\n\r\n**FFmpeg Installation**\r\n\r\n- Once everything is done installing, download the correct FFmpeg binary for your system [here](http://www.osxexperts.net) and place it into the main application directory.\r\n\r\n**Rubber Band Installation**\r\n\r\nIn order to use the Time Stretch or Change Pitch tool, you'll need Rubber Band.\r\n\r\n- Download the precompiled build [here](https://breakfastquay.com/files/releases/rubberband-3.1.2-gpl-executable-windows.zip)\r\n- From the archive, extract the following files to the UVR/lib_v5 application directory:\r\n   - ```rubberband-3.1.2-gpl-executable-macos/rubberband```\r\n\r\nThis process has been tested on a MacBook Pro 2021 (using M1) and a MacBook Air 2017 and is confirmed to be working on both.\r\n\r\n</details>\r\n\r\n\r\n### Linux Installation (Updated Instructions)\r\n\r\n<details id=\"LinuxInstall\">\r\n  <summary>See Linux Installation Instructions</summary>\r\n\r\n<br />\r\n\r\n**These installation instructions are for Debian & Arch-based Linux systems.**\r\n\r\n---\r\n\r\n#### **Step 1: Download the Repository**\r\n- Download and save this repository from [GitHub](https://github.com/Anjok07/ultimatevocalremovergui/archive/refs/heads/master.zip).\r\n- Extract the downloaded file to a directory of your choice.\r\n\r\n---\r\n\r\n#### **Step 2: Install Dependencies**\r\nUse the following commands based on your system type:\r\n\r\n**For Debian-based systems (Ubuntu, Mint, etc.):**\r\n```bash\r\nsudo apt update && sudo apt upgrade\r\nsudo apt-get install -y ffmpeg python3-pip python3-tk\r\n```\r\n\r\n**For Arch-based systems (EndeavourOS):**\r\n```bash\r\nsudo pacman -Syu\r\nsudo pacman -S ffmpeg python-pip tk\r\n```\r\n\r\n---\r\n\r\n#### **Step 3: Set Up a Virtual Environment (Recommended)**\r\nSetting up a virtual environment (venv) ensures that the program's dependencies do not interfere with system-wide Python packages.\r\n\r\n1. **Navigate to the extracted repository directory:**\r\n   ```bash\r\n   cd /path/to/ultimatevocalremovergui\r\n   ```\r\n\r\n2. **Create a virtual environment:**\r\n   ```bash\r\n   python3 -m venv venv\r\n   ```\r\n\r\n3. **Activate the virtual environment:**\r\n   - For **Debian-based and Arch-based systems:**\r\n     ```bash\r\n     source venv/bin/activate\r\n     ```\r\n\r\n4. **Install dependencies in the virtual environment:**\r\n   ```bash\r\n   pip install -r requirements.txt\r\n   ```\r\n\r\n---\r\n\r\n#### **Step 4: Run the Application**\r\nWhile the virtual environment is activated, start the application:\r\n```bash\r\npython UVR.py\r\n```\r\n\r\n---\r\n\r\n#### **Important Notes**\r\n1. **Avoid Modifying System Files:**  \r\n   Previous instructions suggested deleting the `/usr/lib/python3.11/EXTERNALLY-MANAGED` file, which is dangerous and can break Python package management. Do **NOT** delete this file.\r\n\r\n2. **Why Use Virtual Environments?**  \r\n   Virtual environments isolate the program's dependencies, preventing conflicts with system Python packages. More information is available [here](https://stackoverflow.com/questions/75602063/pip-install-r-requirements-txt-is-failing-this-environment-is-externally-mana/75696359#75696359).\r\n\r\n3. **Known Issues and Discussions:**  \r\n   - [Issue #1578](https://github.com/Anjok07/ultimatevocalremovergui/issues/1578)  \r\n   - [Pull Request #1068](https://github.com/Anjok07/ultimatevocalremovergui/pull/1068)\r\n\r\n---\r\n\r\nIf you encounter issues, refer to the [GitHub Issues](https://github.com/Anjok07/ultimatevocalremovergui/issues) page for help. \r\n\r\n</details>\r\n\r\n### Other Application Notes\r\n- Nvidia GTX 1060 6GB is the minimum requirement for GPU conversions.\r\n- Nvidia GPUs with at least 8GBs of V-RAM are recommended.\r\n- AMD Radeon GPU supported is limited at this time.\r\n   - There is currently a working branch for AMD GPU users [here](https://github.com/Anjok07/ultimatevocalremovergui/tree/v5.6-amd-gpu)\r\n- This application is only compatible with 64-bit platforms. \r\n- This application relies on the Rubber Band library for the Time-Stretch and Pitch-Shift options.\r\n- This application relies on FFmpeg to process non-wav audio files.\r\n- The application will automatically remember your settings when closed.\r\n- Conversion times will significantly depend on your hardware. \r\n- These models are computationally intensive. \r\n\r\n### Performance:\r\n- Model load times are faster.\r\n- Importing/exporting audio files is faster.\r\n\r\n## Troubleshooting\r\n\r\n### Common Issues\r\n\r\n- If FFmpeg is not installed, the application will throw an error if the user attempts to convert a non-WAV file.\r\n- Memory allocation errors can usually be resolved by lowering the \"Segment\" or \"Window\" sizes.\r\n\r\n#### MacOS Sonoma Left-click Bug\r\nThere's a known issue on MacOS Sonoma where left-clicks aren't registering correctly within the app. This was impacting all applications built with Tkinter on Sonoma and has since been resolved. Please download the latest version via the following link if you are still experiencing issues - [link](https://github.com/Anjok07/ultimatevocalremovergui/releases/tag/v5.6)\r\n\r\nThis issue was being tracked [here](https://github.com/Anjok07/ultimatevocalremovergui/issues/840).\r\n\r\n### Issue Reporting\r\n\r\nPlease be as detailed as possible when posting a new issue. \r\n\r\nIf possible, click the \"Settings Button\" to the left of the \"Start Processing\" button and click the \"Error Log\" button for detailed error information that can be provided to us.\r\n\r\n## License\r\n\r\nThe **Ultimate Vocal Remover GUI** code is [MIT-licensed](LICENSE). \r\n\r\n- **Please Note:** For all third-party application developers who wish to use our models, please honor the MIT license by providing credit to UVR and its developers.\r\n\r\n## Credits\r\n- [ZFTurbo](https://github.com/ZFTurbo) - Created & trained the weights for the new MDX23C models. \r\n- [DilanBoskan](https://github.com/DilanBoskan) - Your contributions at the start of this project were essential to the success of UVR. Thank you!\r\n- [Bas Curtiz](https://www.youtube.com/user/bascurtiz) - Designed the official UVR logo, icon, banner, and splash screen.\r\n- [tsurumeso](https://github.com/tsurumeso) - Developed the original VR Architecture code. \r\n- [Kuielab & Woosung Choi](https://github.com/kuielab) - Developed the original MDX-Net AI code. \r\n- [Adefossez & Demucs](https://github.com/facebookresearch/demucs) - Developed the original Demucs AI code. \r\n- [KimberleyJSN](https://github.com/KimberleyJensen) - Advised and aided the implementation of the training scripts for MDX-Net and Demucs. Thank you!\r\n- [Hv](https://github.com/NaJeongMo/Colab-for-MDX_B) - Helped implement chunks into the MDX-Net AI code. Thank you!\r\n\r\n## Contributing\r\n\r\n- For anyone interested in the ongoing development of **Ultimate Vocal Remover GUI**, please send us a pull request, and we will review it. \r\n- This project is 100% open-source and free for anyone to use and modify as they wish. \r\n- We only maintain the development and support for the **Ultimate Vocal Remover GUI** and the models provided. \r\n\r\n## References\r\n- [1] Takahashi et al., \"Multi-scale Multi-band DenseNets for Audio Source Separation\", https://arxiv.org/pdf/1706.09588.pdf\r\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 160124067,
    "name": "pytorch-handbook",
    "full_name": "zergtant/pytorch-handbook",
    "description": "pytorch handbook\u662f\u4e00\u672c\u5f00\u6e90\u7684\u4e66\u7c4d\uff0c\u76ee\u6807\u662f\u5e2e\u52a9\u90a3\u4e9b\u5e0c\u671b\u548c\u4f7f\u7528PyTorch\u8fdb\u884c\u6df1\u5ea6\u5b66\u4e60\u5f00\u53d1\u548c\u7814\u7a76\u7684\u670b\u53cb\u5feb\u901f\u5165\u95e8\uff0c\u5176\u4e2d\u5305\u542b\u7684Pytorch\u6559\u7a0b\u5168\u90e8\u901a\u8fc7\u6d4b\u8bd5\u4fdd\u8bc1\u53ef\u4ee5\u6210\u529f\u8fd0\u884c",
    "html_url": "https://github.com/zergtant/pytorch-handbook",
    "clone_url": "https://github.com/zergtant/pytorch-handbook.git",
    "owner_login": "zergtant",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/6510464?v=4",
    "stargazers_count": 21120,
    "watchers_count": 21120,
    "forks_count": 5429,
    "open_issues_count": 56,
    "size": 149507,
    "language": "Jupyter Notebook",
    "languages": {
      "Jupyter Notebook": 4131729
    },
    "topics": [
      "deep-learning",
      "machine-learning",
      "neural-network",
      "pytorch",
      "pytorch-handbook",
      "pytorch-tutorials"
    ],
    "license_name": null,
    "created_at": "2018-12-03T02:57:04+00:00",
    "updated_at": "2025-08-04T10:57:04+00:00",
    "pushed_at": "2024-07-25T07:17:26+00:00",
    "contributors_count": 31,
    "readme_length": 3472,
    "readme_content": "# PyTorch \u4e2d\u6587\u624b\u518c\uff08pytorch handbook\uff09\r\n![pytorch](pytorch-logo-dark.png)\r\n\r\n## \u4e66\u7c4d\u4ecb\u7ecd\r\n\u8fd9\u662f\u4e00\u672c\u5f00\u6e90\u7684\u4e66\u7c4d\uff0c\u76ee\u6807\u662f\u5e2e\u52a9\u90a3\u4e9b\u5e0c\u671b\u548c\u4f7f\u7528PyTorch\u8fdb\u884c\u6df1\u5ea6\u5b66\u4e60\u5f00\u53d1\u548c\u7814\u7a76\u7684\u670b\u53cb\u5feb\u901f\u5165\u95e8\u3002\r\n\r\n\u7531\u4e8e\u672c\u4eba\u6c34\u5e73\u6709\u9650\uff0c\u5728\u5199\u6b64\u6559\u7a0b\u7684\u65f6\u5019\u53c2\u8003\u4e86\u4e00\u4e9b\u7f51\u4e0a\u7684\u8d44\u6599\uff0c\u5728\u8fd9\u91cc\u5bf9\u4ed6\u4eec\u8868\u793a\u656c\u610f\uff0c\u6211\u4f1a\u5728\u6bcf\u4e2a\u5f15\u7528\u4e2d\u9644\u4e0a\u539f\u6587\u5730\u5740\uff0c\u65b9\u4fbf\u5927\u5bb6\u53c2\u8003\u3002\r\n\r\n\u6df1\u5ea6\u5b66\u4e60\u7684\u6280\u672f\u5728\u98de\u901f\u7684\u53d1\u5c55\uff0c\u540c\u65f6PyTorch\u4e5f\u5728\u4e0d\u65ad\u66f4\u65b0\uff0c\u4e14\u672c\u4eba\u4f1a\u9010\u6b65\u5b8c\u5584\u76f8\u5173\u5185\u5bb9\u3002\r\n\r\n## \u7248\u672c\u8bf4\u660e\r\n\u7531\u4e8ePyTorch\u7248\u672c\u66f4\u8fed\uff0c\u6559\u7a0b\u7684\u7248\u672c\u4f1a\u4e0ePyTorch\u7248\u672c\uff0c\u4fdd\u6301\u4e00\u81f4\u3002\r\n\r\n[pytorch\u5927\u7248\u672c\u66f4\u65b0\u7684\u4e3b\u8981\u53d8\u52a8\u603b\u7ed3](pytorch-changelog.md)  \u5f53\u524d\u7248\u672c 1.11\r\n\r\n## \u5728\u7ebf\u7248\u672c\u548cPDF\r\n\r\n\u56fd\u5185\u7684\u955c\u50cf\uff0c\u901f\u5ea6\u5f88\u5feb\uff0c\u4e0d\u4f1a\u88ab\u5899\uff1ahttps://www.pytorch.wiki/\r\n\r\nPDF\u6587\u4ef6\u76ee\u524d\u8fd8\u6ca1\u6709\u627e\u5230\u597d\u7684\u751f\u6210\u65b9\u6cd5\uff0c\u6709\u719f\u6089\u8fd9\u65b9\u9762\u7684\u670b\u53cb\u53ef\u4ee5\u8054\u7cfb\u6211\uff0c\u611f\u6fc0\u4e0d\u5c3d\r\n\r\n## QQ 6\u7fa4 \r\n\r\n\u7fa4\u53f7\uff1a760443051\r\n\r\n\r\n![QR](PyTorch-Handbook-6.png) \r\n\r\n\u70b9\u51fb\u94fe\u63a5\u52a0\u5165\u7fa4\u804a\u3010PyTorch\u00a0Handbook\u00a0\u4ea4\u6d416\u7fa4\u3011\uff1ahttps://jq.qq.com/?_wv=1027&k=X4Ro6uWv\r\n\r\n\r\n1\u7fa4(985896536)\u5df2\u6ee1\uff0c2\u7fa4(681980831) 3\u7fa4(773681699)\u5df2\u6ee1  4\u7fa4(884017356)\u5df2\u6ee1  5\u7fa4(894059877)\u5df2\u6ee1\r\n\r\n\u4e0d\u8981\u518d\u52a0\u4e86\r\n\r\n## \u65b0\u798f\u5229\r\n\r\n\u516c\u4f17\u8d26\u53f7\u6bcf\u65e5\u5206\u4eab\u5e72\u8d27\u6587\u7ae0\r\n![weixin QR](deephub.jpg) \r\n\r\n\r\n\r\n## \u8bf4\u660e\r\n\r\n- \u4fee\u6539\u9519\u522b\u5b57\u8bf7\u76f4\u63a5\u63d0issue\u6216PR\r\n\r\n- PR\u65f6\u8bf7\u6ce8\u610f\u7248\u672c\r\n\r\n- \u6709\u95ee\u9898\u4e5f\u8bf7\u76f4\u63a5\u63d0issue\r\n\r\n\u611f\u8c22\r\n\r\n## \u76ee\u5f55\r\n\r\n### \u7b2c\u4e00\u7ae0\uff1aPyTorch \u5165\u95e8\r\n\r\n1. [PyTorch \u7b80\u4ecb](chapter1/1.1-pytorch-introduction.md)\r\n2. [PyTorch \u73af\u5883\u642d\u5efa](chapter1/1.2-pytorch-installation.md)\r\n3. [PyTorch \u6df1\u5ea6\u5b66\u4e60\uff1a60\u5206\u949f\u5feb\u901f\u5165\u95e8\uff08\u5b98\u65b9\uff09](chapter1/1.3-deep-learning-with-pytorch-60-minute-blitz.md)\r\n    - [\u5f20\u91cf](chapter1/1_tensor_tutorial.ipynb)\r\n    - [Autograd\uff1a\u81ea\u52a8\u6c42\u5bfc](chapter1/2_autograd_tutorial.ipynb) \r\n    - [\u795e\u7ecf\u7f51\u7edc](chapter1/3_neural_networks_tutorial.ipynb)\r\n    - [\u8bad\u7ec3\u4e00\u4e2a\u5206\u7c7b\u5668](chapter1/4_cifar10_tutorial.ipynb)\r\n    - [\u9009\u8bfb\uff1a\u6570\u636e\u5e76\u884c\u5904\u7406\uff08\u591aGPU\uff09](chapter1/5_data_parallel_tutorial.ipynb)\r\n4. [\u76f8\u5173\u8d44\u6e90\u4ecb\u7ecd](chapter1/1.4-pytorch-resource.md)\r\n\r\n### \u7b2c\u4e8c\u7ae0 \u57fa\u7840\r\n#### \u7b2c\u4e00\u8282 PyTorch \u57fa\u7840\r\n1. [\u5f20\u91cf](chapter2/2.1.1.pytorch-basics-tensor.ipynb)\r\n2. [\u81ea\u52a8\u6c42\u5bfc](chapter2/2.1.2-pytorch-basics-autograd.ipynb)\r\n3. [\u795e\u7ecf\u7f51\u7edc\u5305nn\u548c\u4f18\u5316\u5668optm](chapter2/2.1.3-pytorch-basics-nerual-network.ipynb)\r\n4. [\u6570\u636e\u7684\u52a0\u8f7d\u548c\u9884\u5904\u7406](chapter2/2.1.4-pytorch-basics-data-loader.ipynb)\r\n#### \u7b2c\u4e8c\u8282 \u6df1\u5ea6\u5b66\u4e60\u57fa\u7840\u53ca\u6570\u5b66\u539f\u7406\r\n\r\n[\u6df1\u5ea6\u5b66\u4e60\u57fa\u7840\u53ca\u6570\u5b66\u539f\u7406](chapter2/2.2-deep-learning-basic-mathematics.ipynb)\r\n\r\n#### \u7b2c\u4e09\u8282 \u795e\u7ecf\u7f51\u7edc\u7b80\u4ecb\r\n\r\n[\u795e\u7ecf\u7f51\u7edc\u7b80\u4ecb](chapter2/2.3-deep-learning-neural-network-introduction.ipynb)  \u6ce8\uff1a\u672c\u7ae0\u5728\u672c\u5730\u4f7f\u7528\u5fae\u8f6f\u7684Edge\u6253\u5f00\u4f1a\u5d29\u6e83\uff0c\u8bf7\u4f7fChrome Firefox\u6253\u5f00\u67e5\u770b\r\n\r\n#### \u7b2c\u56db\u8282 \u5377\u79ef\u795e\u7ecf\u7f51\u7edc\r\n\r\n[\u5377\u79ef\u795e\u7ecf\u7f51\u7edc](chapter2/2.4-cnn.ipynb)\r\n\r\n#### \u7b2c\u4e94\u8282 \u5faa\u73af\u795e\u7ecf\u7f51\u7edc\r\n\r\n[\u5faa\u73af\u795e\u7ecf\u7f51\u7edc](chapter2/2.5-rnn.ipynb)\r\n\r\n### \u7b2c\u4e09\u7ae0 \u5b9e\u8df5\r\n#### \u7b2c\u4e00\u8282 logistic\u56de\u5f52\u4e8c\u5143\u5206\u7c7b\r\n\r\n[logistic\u56de\u5f52\u4e8c\u5143\u5206\u7c7b](chapter3/3.1-logistic-regression.ipynb)\r\n\r\n\r\n#### \u7b2c\u4e8c\u8282 CNN:MNIST\u6570\u636e\u96c6\u624b\u5199\u6570\u5b57\u8bc6\u522b\r\n\r\n[CNN:MNIST\u6570\u636e\u96c6\u624b\u5199\u6570\u5b57\u8bc6\u522b](chapter3/3.2-mnist.ipynb)\r\n\r\n#### \u7b2c\u4e09\u8282 RNN\u5b9e\u4f8b\uff1a\u901a\u8fc7Sin\u9884\u6d4bCos\r\n\r\n[RNN\u5b9e\u4f8b\uff1a\u901a\u8fc7Sin\u9884\u6d4bCos](chapter3/3.3-rnn.ipynb)\r\n\r\n### \u7b2c\u56db\u7ae0 \u63d0\u9ad8\r\n#### \u7b2c\u4e00\u8282 Fine-tuning\r\n\r\n[Fine-tuning](chapter4/4.1-fine-tuning.ipynb)\r\n\r\n#### \u7b2c\u4e8c\u8282 \u53ef\u89c6\u5316\r\n\r\n[visdom](chapter4/4.2.1-visdom.ipynb)\r\n\r\n[tensorboardx](chapter4/4.2.2-tensorboardx.ipynb) \r\n\r\n[\u53ef\u89c6\u5316\u7406\u89e3\u5377\u79ef\u795e\u7ecf\u7f51\u7edc](chapter4/4.2.3-cnn-visualizing.ipynb)\r\n\r\n#### \u7b2c\u4e09\u8282 Fast.ai\r\n[Fast.ai](chapter4/4.3-fastai.ipynb)\r\n#### \u7b2c\u56db\u8282 \u8bad\u7ec3\u7684\u4e00\u4e9b\u6280\u5de7\r\n\r\n#### \u7b2c\u4e94\u8282 \u591aGPU\u5e76\u884c\u8bad\u7ec3\r\n[\u591aGPU\u5e76\u884c\u8ba1\u7b97](chapter4/4.5-multiply-gpu-parallel-training.ipynb)\r\n\r\n#### \u8865\u5145\u7ffb\u8bd1\u6587\u7ae0\uff1a\u5728PyTorch\u4e2d\u4f7f\u7528DistributedDataParallel\u8fdb\u884c\u591aGPU\u5206\u5e03\u5f0f\u6a21\u578b\u8bad\u7ec3\r\n[\u5728PyTorch\u4e2d\u4f7f\u7528DistributedDataParallel\u8fdb\u884c\u591aGPU\u5206\u5e03\u5f0f\u6a21\u578b\u8bad\u7ec3](chapter4/distributeddataparallel)\r\n\r\n\r\n### \u7b2c\u4e94\u7ae0 \u5e94\u7528\r\n#### \u7b2c\u4e00\u8282 Kaggle\u4ecb\u7ecd\r\n[Kaggle\u4ecb\u7ecd](chapter5/5.1-kaggle.md)\r\n#### \u7b2c\u4e8c\u8282 \u7ed3\u6784\u5316\u6570\u636e\r\n[Pytorch\u5904\u7406\u7ed3\u6784\u5316\u6570\u636e](chapter5/5.2-Structured-Data.ipynb)\r\n#### \u7b2c\u4e09\u8282 \u8ba1\u7b97\u673a\u89c6\u89c9\r\n[Fashion MNIST \u56fe\u50cf\u5206\u7c7b](chapter5/5.3-Fashion-MNIST.ipynb)\r\n#### \u7b2c\u56db\u8282 \u81ea\u7136\u8bed\u8a00\u5904\u7406\r\n#### \u7b2c\u4e94\u8282 \u534f\u540c\u8fc7\u6ee4\r\n\r\n### \u7b2c\u516d\u7ae0 \u8d44\u6e90\r\n\r\n[torchaudio](torchaudio/intro.ipynb)\r\n\r\n\r\n### \u7b2c\u4e03\u7ae0 \u9644\u5f55\r\n\r\n[\u6811\u8393\u6d3e\u7f16\u8bd1\u5b89\u88c5 pytorch 1.4](pi/)\r\n\r\ntransforms\u7684\u5e38\u7528\u64cd\u4f5c\u603b\u7ed3\r\n\r\npytorch\u7684\u635f\u5931\u51fd\u6570\u603b\u7ed3\r\n\r\npytorch\u7684\u4f18\u5316\u5668\u603b\u7ed3\r\n\r\n\r\n## Script\r\nscript\u76ee\u5f55\u662f\u6211\u5199\u7684\u5c06ipynb\u8f6c\u6362\u6210\u5728\u7ebf\u7684\u7248\u672c\u548cpdf\u6587\u4ef6\u7684\u811a\u672c\uff0c\u56e0\u4e3a\u8fd8\u5728\u6d4b\u8bd5\u9636\u6bb5\uff0c\u6240\u4ee5\u6709\u4ec0\u4e48\u95ee\u9898\u8bf7\u5927\u5bb6\u63d0\u51fa\r\n\r\n\r\n## License\r\n\r\n![](https://i.creativecommons.org/l/by-nc-sa/3.0/88x31.png)\r\n\r\n[\u672c\u4f5c\u54c1\u91c7\u7528\u77e5\u8bc6\u5171\u4eab\u7f72\u540d-\u975e\u5546\u4e1a\u6027\u4f7f\u7528-\u76f8\u540c\u65b9\u5f0f\u5171\u4eab 3.0  \u4e2d\u56fd\u5927\u9646\u8bb8\u53ef\u534f\u8bae\u8fdb\u884c\u8bb8\u53ef](http://creativecommons.org/licenses/by-nc-sa/3.0/cn)\r\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 102692863,
    "name": "onnx",
    "full_name": "onnx/onnx",
    "description": "Open standard for machine learning interoperability",
    "html_url": "https://github.com/onnx/onnx",
    "clone_url": "https://github.com/onnx/onnx.git",
    "owner_login": "onnx",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/31675368?v=4",
    "stargazers_count": 19378,
    "watchers_count": 19378,
    "forks_count": 3778,
    "open_issues_count": 290,
    "size": 56354,
    "language": "Python",
    "languages": {
      "Python": 3191593,
      "C++": 2848428,
      "PureBasic": 2310548,
      "CMake": 27602,
      "Shell": 2433,
      "C": 1905,
      "PowerShell": 1371,
      "Batchfile": 424
    },
    "topics": [
      "deep-learning",
      "deep-neural-networks",
      "dnn",
      "keras",
      "machine-learning",
      "ml",
      "neural-network",
      "onnx",
      "pytorch",
      "scikit-learn",
      "tensorflow"
    ],
    "license_name": "Apache License 2.0",
    "created_at": "2017-09-07T04:53:45+00:00",
    "updated_at": "2025-08-05T20:31:00+00:00",
    "pushed_at": "2025-08-05T20:30:50+00:00",
    "contributors_count": 100,
    "readme_length": 6626,
    "readme_content": "<!--\nCopyright (c) ONNX Project Contributors\n\nSPDX-License-Identifier: Apache-2.0\n-->\n\n<p align=\"center\"><img width=\"40%\" src=\"https://github.com/onnx/onnx/raw/main/docs/onnx-horizontal-color.png\" /></p>\n\n[![PyPI - Version](https://img.shields.io/pypi/v/onnx.svg)](https://pypi.org/project/onnx)\n[![CI](https://github.com/onnx/onnx/actions/workflows/main.yml/badge.svg)](https://github.com/onnx/onnx/actions/workflows/main.yml)\n[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/3313/badge)](https://bestpractices.coreinfrastructure.org/projects/3313)\n[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/onnx/onnx/badge)](https://api.securityscorecards.dev/projects/github.com/onnx/onnx)\n[![REUSE compliant](https://api.reuse.software/badge/github.com/onnx/onnx)](https://api.reuse.software/info/github.com/onnx/onnx)\n[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)\n[![Black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n\n[Open Neural Network Exchange (ONNX)](https://onnx.ai) is an open ecosystem that empowers AI developers\nto choose the right tools as their project evolves. ONNX provides an open source format for AI models, both deep learning and traditional ML. It defines an extensible computation graph model, as well as definitions of built-in operators and standard\ndata types. Currently we focus on the capabilities needed for inferencing (scoring).\n\nONNX is [widely supported](http://onnx.ai/supported-tools) and can be found in many frameworks, tools, and hardware. Enabling interoperability between different frameworks and streamlining the path from research to production helps increase the speed of innovation in the AI community. We invite the community to join us and further evolve ONNX.\n\n# Use ONNX\n\n* [Documentation of ONNX Python Package](https://onnx.ai/onnx/)\n* [Tutorials for creating ONNX models](https://github.com/onnx/tutorials)\n* [Pre-trained ONNX models](https://github.com/onnx/models)\n\n# Learn about the ONNX spec\n\n* [Overview](https://github.com/onnx/onnx/blob/main/docs/Overview.md)\n* [ONNX intermediate representation spec](https://github.com/onnx/onnx/blob/main/docs/IR.md)\n* [Versioning principles of the spec](https://github.com/onnx/onnx/blob/main/docs/Versioning.md)\n* [Operators documentation](https://github.com/onnx/onnx/blob/main/docs/Operators.md)\n* [Operators documentation](https://onnx.ai/onnx/operators/index.html) (latest release)\n* [Python API Overview](https://github.com/onnx/onnx/blob/main/docs/PythonAPIOverview.md)\n\n# Programming utilities for working with ONNX Graphs\n\n* [Shape and Type Inference](https://github.com/onnx/onnx/blob/main/docs/ShapeInference.md)\n* [Graph Optimization](https://github.com/onnx/optimizer)\n* [Opset Version Conversion](https://github.com/onnx/onnx/blob/main/docs/docsgen/source/api/version_converter.md)\n\n# Contribute\n\nONNX is a community project and the open governance model is described [here](https://github.com/onnx/onnx/blob/main/community/readme.md). We encourage you to join the effort and contribute feedback, ideas, and code. You can participate in the [Special Interest Groups](https://github.com/onnx/onnx/blob/main/community/sigs.md) and [Working Groups](https://github.com/onnx/onnx/blob/main/community/working-groups.md) to shape the future of ONNX.\n\nCheck out our [contribution guide](https://github.com/onnx/onnx/blob/main/CONTRIBUTING.md) to get started.\n\nIf you think some operator should be added to ONNX specification, please read\n[this document](https://github.com/onnx/onnx/blob/main/docs/AddNewOp.md).\n\n# Community meetings\n\nThe schedules of the regular meetings of the Steering Committee, the working groups and the SIGs can be found [here](https://onnx.ai/calendar)\n\nCommunity Meetups are held at least once a year. Content from previous community meetups are at:\n\n* 2020.04.09 <https://lf-aidata.atlassian.net/wiki/spaces/DL/pages/14091402/LF+AI+Day+-ONNX+Community+Virtual+Meetup+-+Silicon+Valley+-+2020+April+9>\n* 2020.10.14 <https://lf-aidata.atlassian.net/wiki/spaces/DL/pages/14092138/LF+AI+Day+-+ONNX+Community+Workshop+-+2020+October+14>\n* 2021.03.24 <https://lf-aidata.atlassian.net/wiki/spaces/DL/pages/14092424/Instructions+for+Event+Hosts+-+LF+AI+Data+Day+-+ONNX+Virtual+Community+Meetup+-+March+2021>\n* 2021.10.21 <https://lf-aidata.atlassian.net/wiki/spaces/DL/pages/14093194/LF+AI+Data+Day+ONNX+Community+Virtual+Meetup+-+October+2021>\n* 2022.06.24 <https://lf-aidata.atlassian.net/wiki/spaces/DL/pages/14093969/ONNX+Community+Day+-+2022+June+24>\n* 2023.06.28 <https://lf-aidata.atlassian.net/wiki/spaces/DL/pages/14094507/ONNX+Community+Day+2023+-+June+28>\n\n\n\n# Discuss\n\nWe encourage you to open [Issues](https://github.com/onnx/onnx/issues), or use [Slack](https://lfaifoundation.slack.com/) (If you have not joined yet, please use this [link](https://join.slack.com/t/lfaifoundation/shared_invite/zt-o65errpw-gMTbwNr7FnNbVXNVFkmyNA) to join the group) for more real-time discussion.\n\n# Follow Us\n\nStay up to date with the latest ONNX news. [[Facebook](https://www.facebook.com/onnxai/)] [[Twitter/X](https://twitter.com/onnxai)]\n\n# Roadmap\n\nA roadmap process takes place every year. More details can be found [here](https://github.com/onnx/steering-committee/tree/main/roadmap)\n\n# Installation\n\nONNX released packages are published in PyPi.\n\n```sh\npip install onnx # or pip install onnx[reference] for optional reference implementation dependencies\n```\n\n[ONNX weekly packages](https://pypi.org/project/onnx-weekly/) are published in PyPI to enable experimentation and early testing.\n\nDetailed install instructions, including Common Build Options and Common Errors can be found [here](https://github.com/onnx/onnx/blob/main/INSTALL.md)\n\n# Testing\n\nONNX uses [pytest](https://docs.pytest.org) as test driver. In order to run tests, you will first need to install `pytest`:\n\n```sh\npip install pytest\n```\n\nAfter installing pytest, use the following command to run tests.\n\n```sh\npytest\n```\n\n# Development\n\nCheck out the [contributor guide](https://github.com/onnx/onnx/blob/main/CONTRIBUTING.md) for instructions.\n\n# License\n\n[Apache License v2.0](LICENSE)\n\n# Trademark\nCheckout [https://trademarks.justia.com](https://trademarks.justia.com/877/25/onnx-87725026.html) for the trademark.\n\n[General rules of the Linux Foundation on Trademark usage](https://www.linuxfoundation.org/legal/trademark-usage)\n\n# Code of Conduct\n\n[ONNX Open Source Code of Conduct](https://onnx.ai/codeofconduct.html)\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 123870315,
    "name": "tfjs",
    "full_name": "tensorflow/tfjs",
    "description": "A WebGL accelerated JavaScript library for training and deploying ML models.",
    "html_url": "https://github.com/tensorflow/tfjs",
    "clone_url": "https://github.com/tensorflow/tfjs.git",
    "owner_login": "tensorflow",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/15658638?v=4",
    "stargazers_count": 18916,
    "watchers_count": 18916,
    "forks_count": 1985,
    "open_issues_count": 600,
    "size": 173670,
    "language": "TypeScript",
    "languages": {
      "TypeScript": 11621703,
      "JavaScript": 1235769,
      "C++": 654020,
      "Python": 543733,
      "Starlark": 193248,
      "HTML": 133719,
      "Shell": 72254,
      "CSS": 27140,
      "Objective-C": 5247,
      "Java": 4081,
      "Batchfile": 2165,
      "Ruby": 1981,
      "Dockerfile": 1819,
      "C": 1149
    },
    "topics": [
      "deep-learning",
      "deep-neural-network",
      "gpu-acceleration",
      "javascript",
      "machine-learning",
      "neural-network",
      "typescript",
      "wasm",
      "web-assembly",
      "webgl"
    ],
    "license_name": "Apache License 2.0",
    "created_at": "2018-03-05T05:41:02+00:00",
    "updated_at": "2025-08-05T16:13:09+00:00",
    "pushed_at": "2025-07-30T18:16:49+00:00",
    "contributors_count": 100,
    "readme_length": 7760,
    "readme_content": "# TensorFlow.js\n\nTensorFlow.js is an open-source hardware-accelerated JavaScript library for\ntraining and deploying machine learning models.\n\n\n**Develop ML in the Browser** <br/>\nUse flexible and intuitive APIs to build models from scratch using the low-level\nJavaScript linear algebra library or the high-level layers API.\n\n**Develop ML in Node.js** <br/>\nExecute native TensorFlow with the same TensorFlow.js API under the Node.js\nruntime.\n\n**Run Existing models** <br/>\nUse TensorFlow.js model converters to run pre-existing TensorFlow models right\nin the browser.\n\n**Retrain Existing models** <br/>\nRetrain pre-existing ML models using sensor data connected to the browser or\nother client-side data.\n\n## About this repo\n\nThis repository contains the logic and scripts that combine\nseveral packages.\n\nAPIs:\n- [TensorFlow.js Core](/tfjs-core),\n  a flexible low-level API for neural networks and numerical computation.\n- [TensorFlow.js Layers](/tfjs-layers),\n  a high-level API which implements functionality similar to\n  [Keras](https://keras.io/).\n- [TensorFlow.js Data](/tfjs-data),\n  a simple API to load and prepare data analogous to\n  [tf.data](https://www.tensorflow.org/guide/datasets).\n- [TensorFlow.js Converter](/tfjs-converter),\n  tools to import a TensorFlow SavedModel to TensorFlow.js\n- [TensorFlow.js Vis](/tfjs-vis),\n  in-browser visualization for TensorFlow.js models\n- [TensorFlow.js AutoML](/tfjs-automl),\n  Set of APIs to load and run models produced by\n  [AutoML Edge](https://cloud.google.com/vision/automl/docs/edge-quickstart).\n\n\nBackends/Platforms:\n- [TensorFlow.js CPU Backend](/tfjs-backend-cpu), pure-JS backend for Node.js and the browser.\n- [TensorFlow.js WebGL Backend](/tfjs-backend-webgl), WebGL backend for the browser.\n- [TensorFlow.js WASM Backend](/tfjs-backend-wasm), WebAssembly backend for the browser.\n- [TensorFlow.js WebGPU](/tfjs-backend-webgpu), WebGPU backend for the browser.\n- [TensorFlow.js Node](/tfjs-node), Node.js platform via TensorFlow C++ adapter.\n- [TensorFlow.js React Native](/tfjs-react-native), React Native platform via expo-gl adapter.\n\nIf you care about bundle size, you can import those packages individually.\n\nIf you are looking for Node.js support, check out the [TensorFlow.js Node directory](/tfjs-node).\n\n## Examples\n\nCheck out our\n[examples repository](https://github.com/tensorflow/tfjs-examples)\nand our [tutorials](https://js.tensorflow.org/tutorials/).\n\n## Gallery\n\nBe sure to check out [the gallery](GALLERY.md) of all projects related to TensorFlow.js.\n\n## Pre-trained models\n\nBe sure to also check out our [models repository](https://github.com/tensorflow/tfjs-models) where we host pre-trained models\non NPM.\n\n## Benchmarks\n\n* [Local benchmark tool](https://tfjs-benchmarks.web.app/). Use this webpage tool to collect the performance related metrics (speed, memory, etc) of TensorFlow.js models and kernels **on your local device** with CPU, WebGL or WASM backends. You can benchmark custom models by following this [guide](https://github.com/tensorflow/tfjs/blob/master/e2e/benchmarks/local-benchmark/README.md).\n* [Multi-device benchmark tool](https://github.com/tensorflow/tfjs/tree/master/e2e/benchmarks/browserstack-benchmark/README.md). Use this tool to collect the same performance related metrics **on a collection of remote devices**.\n\n## Getting started\n\nThere are two main ways to get TensorFlow.js in your JavaScript project:\nvia <a href=\"https://developer.mozilla.org/en-US/docs/Learn/HTML/Howto/Use_JavaScript_within_a_webpage\" target=\"_blank\">script tags</a> <strong>or</strong> by installing it from <a href=\"https://www.npmjs.com/\" target=\"_blank\">NPM</a>\nand using a build tool like <a href=\"https://parceljs.org/\" target=\"_blank\">Parcel</a>,\n<a href=\"https://webpack.js.org/\" target=\"_blank\">WebPack</a>, or <a href=\"https://rollupjs.org/guide/en\" target=\"_blank\">Rollup</a>.\n\n### via Script Tag\n\nAdd the following code to an HTML file:\n\n```html\n<html>\n  <head>\n    <!-- Load TensorFlow.js -->\n    <script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js\"> </script>\n\n\n    <!-- Place your code in the script tag below. You can also use an external .js file -->\n    <script>\n      // Notice there is no 'import' statement. 'tf' is available on the index-page\n      // because of the script tag above.\n\n      // Define a model for linear regression.\n      const model = tf.sequential();\n      model.add(tf.layers.dense({units: 1, inputShape: [1]}));\n\n      // Prepare the model for training: Specify the loss and the optimizer.\n      model.compile({loss: 'meanSquaredError', optimizer: 'sgd'});\n\n      // Generate some synthetic data for training.\n      const xs = tf.tensor2d([1, 2, 3, 4], [4, 1]);\n      const ys = tf.tensor2d([1, 3, 5, 7], [4, 1]);\n\n      // Train the model using the data.\n      model.fit(xs, ys).then(() => {\n        // Use the model to do inference on a data point the model hasn't seen before:\n        // Open the browser devtools to see the output\n        model.predict(tf.tensor2d([5], [1, 1])).print();\n      });\n    </script>\n  </head>\n\n  <body>\n  </body>\n</html>\n```\n\nOpen up that HTML file in your browser, and the code should run!\n\n### via NPM\n\nAdd TensorFlow.js to your project using <a href=\"https://yarnpkg.com/en/\" target=\"_blank\">yarn</a> <em>or</em> <a href=\"https://docs.npmjs.com/cli/npm\" target=\"_blank\">npm</a>. <b>Note:</b> Because\nwe use ES2017 syntax (such as `import`), this workflow assumes you are using a modern browser or a bundler/transpiler\nto convert your code to something older browsers understand. See our\n<a href='https://github.com/tensorflow/tfjs-examples' target=\"_blank\">examples</a>\nto see how we use <a href=\"https://parceljs.org/\" target=\"_blank\">Parcel</a> to build\nour code. However, you are free to use any build tool that you prefer.\n\n\n\n```js\nimport * as tf from '@tensorflow/tfjs';\n\n// Define a model for linear regression.\nconst model = tf.sequential();\nmodel.add(tf.layers.dense({units: 1, inputShape: [1]}));\n\n// Prepare the model for training: Specify the loss and the optimizer.\nmodel.compile({loss: 'meanSquaredError', optimizer: 'sgd'});\n\n// Generate some synthetic data for training.\nconst xs = tf.tensor2d([1, 2, 3, 4], [4, 1]);\nconst ys = tf.tensor2d([1, 3, 5, 7], [4, 1]);\n\n// Train the model using the data.\nmodel.fit(xs, ys).then(() => {\n  // Use the model to do inference on a data point the model hasn't seen before:\n  model.predict(tf.tensor2d([5], [1, 1])).print();\n});\n```\n\nSee our <a href=\"https://js.tensorflow.org/tutorials/\" target=\"_blank\">tutorials</a>, <a href=\"https://github.com/tensorflow/tfjs-examples\" target=\"_blank\">examples</a>\nand <a href=\"https://js.tensorflow.org/api/latest/\">documentation</a> for more details.\n\n## Importing pre-trained models\n\nWe support porting pre-trained models from:\n- [TensorFlow SavedModel](https://www.tensorflow.org/js/tutorials/conversion/import_saved_model)\n- [Keras](https://js.tensorflow.org/tutorials/import-keras.html)\n\n## Various ops supported in different backends\n\nPlease refer below :\n- [TFJS Ops Matrix](https://docs.google.com/spreadsheets/d/1D25XtWaBrmUEErbGQB0QmNhH-xtwHo9LDl59w0TbxrI/edit#gid=0)\n\n## Find out more\n\n[TensorFlow.js](https://js.tensorflow.org) is a part of the\n[TensorFlow](https://www.tensorflow.org) ecosystem. For more info:\n- For help from the community, use the `tfjs` tag on the [TensorFlow Forum](https://discuss.tensorflow.org/tag/tfjs).\n- [TensorFlow.js Website](https://js.tensorflow.org)\n- [Tutorials](https://js.tensorflow.org/tutorials)\n- [API reference](https://js.tensorflow.org/api/latest/)\n- [TensorFlow.js Blog](https://blog.tensorflow.org/search?label=TensorFlow.js)\n\nThanks, <a href=\"https://www.browserstack.com/\">BrowserStack</a>, for providing testing support.\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 67487554,
    "name": "nndl.github.io",
    "full_name": "nndl/nndl.github.io",
    "description": "\u300a\u795e\u7ecf\u7f51\u7edc\u4e0e\u6df1\u5ea6\u5b66\u4e60\u300b \u90b1\u9521\u9e4f\u8457 Neural Network and Deep Learning ",
    "html_url": "https://github.com/nndl/nndl.github.io",
    "clone_url": "https://github.com/nndl/nndl.github.io.git",
    "owner_login": "nndl",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/22023289?v=4",
    "stargazers_count": 18437,
    "watchers_count": 18437,
    "forks_count": 3665,
    "open_issues_count": 85,
    "size": 672598,
    "language": "HTML",
    "languages": {
      "HTML": 155876
    },
    "topics": [],
    "license_name": null,
    "created_at": "2016-09-06T08:09:06+00:00",
    "updated_at": "2025-08-05T06:55:38+00:00",
    "pushed_at": "2022-10-07T09:50:14+00:00",
    "contributors_count": 8,
    "readme_length": 963,
    "readme_content": "# nndl.github.io\n\u7f51\u7ad9\uff1a https://nndl.github.io\n\n2020-03-31\u66f4\u65b0\u8bf4\u660e\uff1a\n\n1. \u4fee\u6539\u4e86\u90e8\u5206\u63cf\u8ff0\uff1a\u7b2c 7\u7ae0  \u524d\u8a00\u30017.1.3 \u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u7684\u6539\u5584\u65b9\u6cd5 \u30015.1.1 \u5377\u79ef\u7684\u5b9a\u4e49\n2. \u8c03\u6574\u4e86\u90e8\u5206\u4e60\u9898\uff1a\u53bb\u6389\u539f3-3\uff1b\u589e\u52a05-1\u300114-1 \u300114-7\n3. \u8c03\u6574\u4e86\u5927\u90e8\u5206\u56fe\u8868\u7684\u5927\u5c0f\uff0c\u8282\u7701\u7a7a\u95f4\n\n2020-03-21\u66f4\u65b0\u8bf4\u660e\uff1a\n\n1. \u4fee\u6539\u5927\u91cf\u8bed\u6cd5\u95ee\u9898\n2. \u8c03\u6574\u7248\u5fc3\u5927\u5c0f\n\n2020-03-19\u66f4\u65b0\u8bf4\u660e\uff1a\n\n1. \u4fee\u6539\u90e8\u5206\u63cf\u8ff0 7.4  \u6570\u636e\u9884\u5904\u7406\u30017.5 \u9010\u5c42\u5f52\u4e00\u5316\n\n2020-02-11\u66f4\u65b0\u8bf4\u660e\uff1a\n\n1. \u5b8c\u5584\u4e86 7.1.1 \u7f51\u7edc\u4f18\u5316\u7684\u96be\u70b9\n2. \u91cd\u5199\u4e86 7.3 \u53c2\u6570\u521d\u59cb\u5316\n3. \u5b8c\u5584\u4e86 A.2.4  \u7279\u5f81\u503c\u4e0e\u7279\u5f81\u5411\u91cf\u3001A.2.5 \u77e9\u9635\u5206\u89e3\n\n2020-01-03\u66f4\u65b0\u8bf4\u660e\uff1a\n\n1. \u4fee\u6539typos\n\n2019-12-30\u66f4\u65b0\u8bf4\u660e\uff1a\n\n1. \u4fee\u6539typos\n2. \u4fee\u6539\u7b2c2\u7ae0 \u611f\u77e5\u5668\u4e00\u8282\n3. \u90e8\u5206\u7edf\u4e00\u4e86\u914d\u56fe\u7684\u989c\u8272\n\n2019-12-27\u66f4\u65b0\u8bf4\u660e\uff1a\n\n1. \u89c4\u8303\u4e86\u5168\u4e66\u7684\u7b26\u53f7\u5b9a\u4e49\n2. \u5c06\u82f1\u6587\u3001\u6570\u5b66\u5b57\u4f53\u66f4\u6362\u4e3aSTIX\u5b57\u4f53\n3. \u5c06\u4e2d\u6587\u5b57\u4f53\u66f4\u6362\u4e3a\u601d\u6e90\u5b8b\u4f53\n4. \u5c06\u5370\u5237\u7248\u548ciPad\u7edf\u4e00\u4e3a\u76f8\u540c\u7248\u5fc3\u5927\u5c0f\uff0c\u4ee5\u540e\u53ea\u53d1\u5e03\u4e00\u4e2a\u7248\u672c\n5. \u4fee\u6539typos\n\n2019-12-18 \u66f4\u65b0\u8bf4\u660e\uff1a\n\n1. \u4fee\u6539typos\n\n2019-12-16 \u66f4\u65b0\u8bf4\u660e\uff1a\n\n1. \u7b2c13\u7ae0 \u6df1\u5ea6\u751f\u6210\u6a21\u578b \u8c03\u6574\u4e86\u4ecb\u7ecd\u300113.1 \u6982\u7387\u751f\u6210\u6a21\u578b\u300113.2.4 \u6a21\u578b\u6c47\u603b\u300113.2.5 \u518d\u53c2\u6570\u5316\n2. \u4fee\u6539typos\n\n2019-11-21\u66f4\u65b0\u8bf4\u660e\uff1a\n\n1. \u4fee\u6539typos\n\n2019-10-26\u66f4\u65b0\u8bf4\u660e\uff1a\n\n1. \u4fee\u6539typos\n\n2019-10-25\u66f4\u65b0\u8bf4\u660e\uff1a\n\n1. \u4fee\u6539typos\n\n2019-10-07\u66f4\u65b0\u8bf4\u660e\uff1a\n\n1. \u6570\u5b66\u57fa\u7840 \u8c03\u6574\u5fae\u79ef\u5206\u4e00\u8282\uff0c\u589e\u52a0\u4e86\u6cf0\u52d2\u516c\u5f0f\uff0c\u79ef\u5206\u7684\u4ecb\u7ecd\n2. \u6570\u5b66\u57fa\u7840 \u589e\u52a0\u71b5\u7f16\u7801\u4e00\u8282\n3. \u6982\u7387\u56fe\u6a21\u578b \u8c03\u6574\u201c\u5b66\u4e60\u201d\u548c\u201c\u63a8\u65ad\u201d\u4e24\u8282\u7684\u6b21\u5e8f\n4. \u6982\u7387\u56fe\u6a21\u578b \u589e\u52a0\u201c\u53d8\u5206\u63a8\u65ad\u201c\u7684\u7b80\u5355\u4ecb\u7ecd\n\n2019-09-29\u66f4\u65b0\u8bf4\u660e\uff1a\n\n1. \u66f4\u65b0\u5411\u91cf\u7b26\u53f7\uff1a\u5c06\u6240\u6709\u7c97\u4f53\u82f1\u6587\u5b57\u6bcd\u6539\u4e3a\u7c97\u659c\u4f53\n2. \u8c03\u6574\u7eea\u8bba\u7684\u5c0f\u8282\u987a\u5e8f\n3. \u4fee\u6539issue\u4e2d\u7684\u90e8\u5206\u9519\u8bef\n4. \u7edf\u4e00\u66f4\u6539\u4e86\u53c2\u8003\u6587\u732e\u683c\u5f0f\n5. \u589e\u52a0\u4e86\u5434\u7acb\u5fb7\u8001\u5e08\u5e8f",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 46918116,
    "name": "CNTK",
    "full_name": "microsoft/CNTK",
    "description": "Microsoft Cognitive Toolkit (CNTK), an open source deep-learning toolkit",
    "html_url": "https://github.com/microsoft/CNTK",
    "clone_url": "https://github.com/microsoft/CNTK.git",
    "owner_login": "microsoft",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/6154722?v=4",
    "stargazers_count": 17591,
    "watchers_count": 17591,
    "forks_count": 4267,
    "open_issues_count": 840,
    "size": 935217,
    "language": "C++",
    "languages": {
      "C++": 10906168,
      "Jupyter Notebook": 4765537,
      "Python": 2396698,
      "Cuda": 762435,
      "C#": 201374,
      "Shell": 173589,
      "SWIG": 167469,
      "Bikeshed": 133021,
      "PowerShell": 131647,
      "Makefile": 84671,
      "C": 62917,
      "Batchfile": 43142,
      "CMake": 35987,
      "Dockerfile": 21103,
      "Java": 12743,
      "Perl": 12728,
      "BrighterScript": 10390,
      "MATLAB": 2932,
      "HTML": 2164,
      "Awk": 675
    },
    "topics": [
      "c-plus-plus",
      "c-sharp",
      "cntk",
      "cognitive-toolkit",
      "deep-learning",
      "deep-neural-networks",
      "distributed",
      "java",
      "machine-learning",
      "neural-network",
      "python"
    ],
    "license_name": "Other",
    "created_at": "2015-11-26T09:52:06+00:00",
    "updated_at": "2025-08-05T10:08:11+00:00",
    "pushed_at": "2023-03-11T07:31:35+00:00",
    "contributors_count": 100,
    "readme_length": 25519,
    "readme_content": "## CNTK\n\n| **Chat** | **Windows build status** | **Linux build status** |\n|-------------|-------------|---------------|\n| [![Join the chat at https://gitter.im/Microsoft/CNTK](https://badges.gitter.im/Microsoft/CNTK.svg)](https://gitter.im/Microsoft/CNTK?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge) | [![Build Status](https://aiinfra.visualstudio.com/_apis/public/build/definitions/a95b3960-90bb-440b-bd18-d3ec5d1cf8c3/126/badge)](https://cntk.ai/nightly-windows.html) | [![Build Status](https://aiinfra.visualstudio.com/_apis/public/build/definitions/a95b3960-90bb-440b-bd18-d3ec5d1cf8c3/127/badge)](https://cntk.ai/nightly-linux.html) |\n\nThe Microsoft Cognitive Toolkit (https://cntk.ai) is a unified deep learning toolkit that describes neural networks as a series of computational steps via a directed graph. In this directed graph, leaf nodes represent input values or network parameters, while other nodes represent matrix operations upon their inputs. CNTK allows users to easily realize and combine popular model types such as feed-forward DNNs, convolutional nets (CNNs), and recurrent networks (RNNs/LSTMs). It implements stochastic gradient descent (SGD, error backpropagation) learning with automatic differentiation and parallelization across multiple GPUs and servers. CNTK has been available under an open-source license since April 2015. It is our hope that the community will take advantage of CNTK to share ideas more quickly through the exchange of open source working code.\n\n## Installation\n\n* [Setup CNTK](https://docs.microsoft.com/en-us/cognitive-toolkit/Setup-CNTK-on-your-machine)\n    * Windows ([Python-only](https://docs.microsoft.com/en-us/cognitive-toolkit/setup-windows-python) / [Script-driven](https://docs.microsoft.com/en-us/cognitive-toolkit/setup-windows-binary-script) / [Manual](https://docs.microsoft.com/en-us/cognitive-toolkit/setup-windows-binary-manual))\n    * Linux ([Python-only](https://docs.microsoft.com/en-us/cognitive-toolkit/setup-linux-python) / [Script-driven](https://docs.microsoft.com/en-us/cognitive-toolkit/setup-linux-binary-script) / [Manual](https://docs.microsoft.com/en-us/cognitive-toolkit/setup-linux-binary-manual) / [Docker](https://docs.microsoft.com/en-us/cognitive-toolkit/cntk-docker-containers))\n* [CNTK backend for Keras](https://docs.microsoft.com/en-us/cognitive-toolkit/using-cntk-with-keras)\n* [Setup CNTK development environment](https://docs.microsoft.com/en-us/cognitive-toolkit/setup-development-environment)\n    * Windows ([Script-driven](https://docs.microsoft.com/en-us/cognitive-toolkit/setup-cntk-with-script-on-windows) / [Manual](https://docs.microsoft.com/en-us/cognitive-toolkit/setup-cntk-on-windows))\n    * Linux ([Manual](https://docs.microsoft.com/en-us/cognitive-toolkit/setup-cntk-on-linux))\n    \n### Installing nightly packages\n\nIf you prefer to use latest CNTK bits from master, use one of the CNTK nightly packages:\n\n* [Nightly packages for Windows](https://cntk.ai/nightly-windows.html)\n* [Nightly packages for Linux](https://cntk.ai/nightly-linux.html)\n\n## Learning CNTK\n\nYou can learn more about using and contributing to CNTK with the following resources:\n\n* [General documentation](https://docs.microsoft.com/en-us/cognitive-toolkit/)\n* [Python API documentation](https://cntk.ai/pythondocs/)\n* [Evaluation documentation (C++, C#/.NET, Python, Java)](https://docs.microsoft.com/en-us/cognitive-toolkit/CNTK-Evaluation-Overview)\n* [Manual](https://github.com/Microsoft/CNTK/tree/master/Manual)\n* [Tutorials](https://docs.microsoft.com/en-us/cognitive-toolkit/tutorials)\n* [Examples](https://docs.microsoft.com/en-us/cognitive-toolkit/Examples)\n* [Pretrained models](./PretrainedModels)\n* [Blog](https://www.microsoft.com/en-us/cognitive-toolkit/blog/)\n* [Presentations](https://docs.microsoft.com/en-us/cognitive-toolkit/Presentations)\n* [License](./LICENSE.md)\n\n## More information\n\n* [Contribute to CNTK](https://docs.microsoft.com/en-us/cognitive-toolkit/Contributing-to-CNTK)\n* [FAQ](https://docs.microsoft.com/en-us/cognitive-toolkit/CNTK-FAQ)\n* [Feedback](https://docs.microsoft.com/en-us/cognitive-toolkit/Feedback-Channels)\n\n## Disclaimer\n\nDear community, \n\nWith our ongoing contributions to ONNX and the ONNX Runtime, we have made it easier to interoperate within the AI framework ecosystem and to access high performance, cross-platform inferencing capabilities for both traditional ML models and deep neural networks. Over the last few years we have been privileged to develop such key open-source machine learning projects, including the Microsoft Cognitive Toolkit, which has enabled its users to leverage industry-wide advancements in deep learning at scale. \n\nToday\u2019s 2.7 release will be the last main release of CNTK. We may have some subsequent minor releases for bug fixes, but these will be evaluated on a case-by-case basis. There are no plans for new feature development post this release. \n\nThe CNTK 2.7 release has full support for ONNX 1.4.1, and we encourage those seeking to operationalize their CNTK models to take advantage of ONNX and the ONNX Runtime. Moving forward, users can continue to leverage evolving ONNX innovations via the number of frameworks that support it. For example, users can natively export ONNX models from PyTorch or convert TensorFlow models to ONNX with the TensorFlow-ONNX converter. \n\nWe are incredibly grateful for all the support we have received from contributors and users over the years since the initial open-source release of CNTK. CNTK has enabled both Microsoft teams and external users to execute complex and large-scale workloads in all manner of deep learning applications, such as historical breakthroughs in speech recognition achieved by Microsoft Speech researchers, the originators of the framework. \n\nAs ONNX is increasingly employed in serving models used across Microsoft products such as Bing and Office, we are dedicated to synthesizing innovations from research with the rigorous demands of production to progress the ecosystem forward. \n\nAbove all, our goal is to make innovations in deep learning across the software and hardware stacks as open and accessible as possible. We will be working hard to bring both the existing strengths of CNTK and new state-of-the-art research into other open-source projects to truly broaden the reach of such technologies. \n\nWith gratitude, \n\n-- The CNTK Team \n\n## Microsoft Open Source Code of Conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## News\n\n> You can find more news on [the official project feed](https://docs.microsoft.com/en-us/cognitive-toolkit/news)\n\n***2019-03-29.*** CNTK 2.7.0\n## Highlights of this release\n* Moved to CUDA 10 for both Windows and Linux.\n* Support advance RNN loop in ONNX export.\n* Export larger than 2GB models in ONNX format.\n* Support FP16 in Brain Script train action.\n\n## CNTK support for CUDA 10\n\n### CNTK now supports CUDA 10. This requires an update to build environment to Visual Studio 2017 v15.9 for Windows.\n\nTo setup build and runtime environment on Windows:\n* Install [Visual Studio 2017](https://www.visualstudio.com/downloads/). Note: going forward for CUDA 10 and beyond, it is no longer required to install and run with the specific VC Tools version 14.11.\n* Install [Nvidia CUDA 10](https://developer.nvidia.com/cuda-downloads?target_os=Windows&target_arch=x86_64)\n* From PowerShell, run:\n    [DevInstall.ps1](../Tools/devInstall/Windows/DevInstall.ps1)\n* Start Visual Studio 2017 and open [CNTK.sln](./CNTK.sln).\n\nTo setup build and runtime environment on Linux using docker, please build Unbuntu 16.04 docker image using Dockerfiles [here](./Tools/docker). For other Linux systems, please refer to the Dockerfiles to setup dependent libraries for CNTK.\n\n## Support advance RNN loop in ONNX export\nCNTK models with recursive loops can be exported to ONNX models with scan ops.\n\n## Export larger than 2GB models in ONNX format\nTo export models larger than 2GB in ONNX format, use cntk.Function API:\nsave(self, filename, format=ModelFormat.CNTKv2, use_external_files_to_store_parameters=False)\nwith 'format' set to ModelFormat.ONNX and use_external_files_to_store_parameters set to True.\nIn this case, model parameters are saved in external files. Exported models shall be used with external parameter files when doing model evaluation with onnxruntime.\n\n***2018-11-26.***  \n[Netron](https://github.com/lutzroeder/netron) now supports visualizing CNTK v1 and CNTK v2 `.model` files.\n\n<img src=https://cntk.ai/Images/netron/netron-cntk-dark-1.png alt=\"NetronCNTKDark1\" width=\"300\"> <img src=https://cntk.ai/Images/netron/netron-cntk-light-1.png alt=\"NetronCNTKLight1\" width=\"300\">\n\n\n### Project changelog\n\n***2018-09-17.*** CNTK 2.6.0\n## Efficient group convolution\nThe implementation of group convolution in CNTK has been updated. The updated implementation moves away from creating a sub-graph for group convolution (using slicing and splicing), and instead uses cuDNN7 and MKL2017 APIs directly. This improves the experience both in terms of performance and model size. \n\nAs an example, for a single group convolution op with the following attributes:\n\n- Input tensor (C, H, W) = (32, 128, 128)\n- Number of output channels = 32 (channel multiplier is 1)\n- Groups = 32 (depth wise convolution)\n- Kernel size = (5, 5)\n\nThe comparison numbers for this single node are as follows:\n\n| First Header  | GPU exec. time (in millisec., 1000 run avg.) | CPU exec. time (in millisec., 1000 run avg.) | Model Size (in KB, CNTK format)\n| ------------- | ------------- | ------------- | ------------- |\n| Old implementation  | 9.349  | 41.921  | 38  |\n| New implementation  | 6.581  | 9.963  | 5  |\n| Speedup/savings   Approx.  | 30%  Approx.  | 65-75%   Approx.  | 87% |\n\n## Sequential Convolution\nThe implementation of sequential convolution in CNTK has been updated. The updated implementation creates a separate sequential convolution layer. Different from regular convolution layer, this operation convolves also on the dynamic axis(sequence), and filter_shape[0] is applied to that axis. The updated implementation supports broader cases, such as where stride > 1 for the sequence axis.\n\nFor example, a sequential convolution over a batch of one-channel black-and-white images. The images have the same fixed height of 640, but each with width of variable lengths. The width is then represented by sequential axis. Padding is enabled, and strides for both width and height are 2.\n\n     >>> f = SequentialConvolution((3,3), reduction_rank=0, pad=True, strides=(2,2), activation=C.relu)\n     >>> x = C.input_variable(**Sequence[Tensor[640]])\n     >>> x.shape\n         (640,)\n     >>> h = f(x)\n     >>> h.shape\n         (320,)\n     >>> f.W.shape\n         (1, 1, 3, 3)\n\n## Operators\n### depth_to_space and space_to_depth\nThere is a breaking change in the **depth_to_space** and **space_to_depth** operators. These have been updated to match ONNX specification, specifically\nthe permutation for how the depth dimension is placed as blocks in the spatial dimensions, and vice-versa, has been changed. Please refer to the updated doc\nexamples for these two ops to see the change.\n\n### Tan and Atan\nAdded support for trigonometric ops `Tan` and `Atan`.\n\n### ELU\nAdded support for `alpha` attribute in ELU op.\n\n### Convolution\nUpdated auto padding algorithms of `Convolution` to produce symmetric padding at best effort on CPU, without affecting the final convolution output values. This update increases the range of cases that could be covered by MKL API and improves the performance, E.g. ResNet50.\n\n## Default arguments order\nThere is a breaking change in the **arguments** property in CNTK python API. The default behavior has been updated to return arguments in python order instead of in C++ order. This way it will return arguments in the same order as they are fed into ops. If you wish to still get arguments in C++ order, you can simply override the global option. This change should only affect the following ops: Times, TransposeTimes, and Gemm(internal). \n\n## Bug fixes\n- Updated doc for Convolution layer to include group and dilation arguments.\n- Added improved input validation for group convolution.\n- Updated `LogSoftMax` to use more numerically stable implementation.\n- Fixed Gather op's incorrect gradient value.\n- Added validation for 'None' node in python clone substitution.\n- Added validation for padding channel axis in convolution.\n- Added CNTK native default lotusIR logger to fix the \"Attempt to use DefaultLogger\" error when loading some ONNX models.\n- Added proper initialization for ONNX TypeStrToProtoMap.\n- Updated python doctest to handle different print format for newer version numpy(version >= 1.14).\n- Fixed Pooling(CPU) to produce correct output values when kernel center is on padded input cells.\n\n## ONNX\n### Updates\n- Updated CNTK's ONNX import/export to use ONNX 1.2 spec.\n- Major update to how batch and sequence axes are handled in export and import. As a result, the complex scenarios and edge cases are handled accurately.\n- Updated CNTK's ONNX `BatchNormalization` op export/import to latest spec.\n- Added model domain to ONNX model export.\n- Improved error reporting during import and export of ONNX models.\n- Updated `DepthToSpace` and `SpaceToDepth` ops to match ONNX spec on the permutation for how the depth dimension is placed as block dimension.\n- Added support for exporting `alpha` attribute in `ELU` ONNX op.\n- Major overhaul to `Convolution` and `Pooling` export. Unlike before, these ops do not export an explicit `Pad` op in any situation.\n- Major overhaul to `ConvolutionTranspose` export and import. Attributes such as `output_shape`, `output_padding`, and `pads` are fully supported.\n- Added support for CNTK's `StopGradient` as a no-op.\n- Added ONNX support for TopK op.\n- Added ONNX support for sequence ops: sequence.slice, sequence.first, sequence.last, sequence.reduce_sum, sequence.reduce_max, sequence.softmax. For these ops, there is no need to expand ONNX spec. CNTK ONNX exporter just builds computation equivalent graphs for these sequence ops.\n- Added full support for Softmax op.\n- Made CNTK broadcast ops compatible with ONNX specification.\n- Handle to_batch, to_sequence, unpack_batch, sequence.unpack ops in CNTK ONNX exporter.\n- ONNX tests to export ONNX test cases for other toolkits to run and to validate.\n- Fixed `Hardmax`/`Softmax`/`LogSoftmax` import/export.\n- Added support for `Select` op export.\n- Added import/export support for several trigonometric ops.\n- Updated CNTK support for ONNX `MatMul` op.\n- Updated CNTK support for ONNX `Gemm` op.\n- Updated CNTK's ONNX `MeanVarianceNormalization` op export/import to latest spec.\n- Updated CNTK's ONNX `LayerNormalization` op export/import to latest spec.\n- Updated CNTK's ONNX `PRelu` op export/import to latest spec.\n- Updated CNTK's ONNX `Gather` op export/import to latest spec.\n- Updated CNTK's ONNX `ImageScaler` op export/import to latest spec.\n- Updated CNTK's ONNX `Reduce` ops export/import to latest spec.\n- Updated CNTK's ONNX `Flatten` op export/import to latest spec.\n- Added CNTK support for ONNX `Unsqueeze` op.\n\n### Bug or minor fixes:\n- Updated LRN op to match ONNX 1.2 spec where the `size` attribute has the semantics of diameter, not radius. Added validation if LRN kernel size is larger than channel size.\n- Updated `Min`/`Max` import implementation to handle variadic inputs.\n- Fixed possible file corruption when resaving on top of existing ONNX model file.\n\n## .Net Support\nThe Cntk.Core.Managed library has officially been converted to .Net Standard and supports .Net Core and .Net Framework applications on both Windows and Linux. Starting from this release, .Net developers should be able to restore CNTK Nuget packages using new .Net SDK style project file with package management format set to PackageReference.\n\nThe following C# code now works on both Windows and Linux:\n\n     >>> var weightParameterName = \"weight\";\n\t >>> var biasParameterName = \"bias\";\n\t >>> var inputName = \"input\";\n\t >>> var outputDim = 2;\n\t >>> var inputDim = 3;\n\t >>> Variable inputVariable = Variable.InputVariable(new int[] { inputDim }, DataType.Float, inputName);\n\t >>> var weightParameter = new Parameter(new int[] { outputDim, inputDim }, DataType.Float, 1, device, weightParameterName);\n\t >>> var biasParameter = new Parameter(new int[] { outputDim }, DataType.Float, 0, device, biasParameterName);\n\t >>> \n     >>> Function modelFunc = CNTKLib.Times(weightParameter, inputVariable) + biasParameter;\n\nFor example, simply adding an ItemGroup clause in the .csproj file of a .Net Core application is sufficient:\n     >>> <Project Sdk=\"Microsoft.NET.Sdk\">\n     >>>\n     >>>   <PropertyGroup>\n     >>>     <TargetFramework>netcoreapp2.1</TargetFramework>\n     >>>     <Platforms>x64</Platforms>\n     >>>   </PropertyGroup>\n     >>>\n     >>>   <ItemGroup>\n     >>>     <PackageReference Include=\"CNTK.GPU\" Version=\"2.6.0\" />\n     >>>   </ItemGroup>\n     >>>\n     >>> </Project>\n\n### Bug or minor fixes:\n- Fixed C# string and char to native wstring and wchar UTF conversion issues on Linux.\n- Fixed multibyte and wide character conversions across the codebase.\n- Fixed Nuget package mechanism to pack for .Net Standard.\n- Fixed a memory leak issue in Value class in C# API where Dispose was not called upon object destruction.\n\n## Misc\n\n\n***2018-04-16.*** CNTK 2.5.1\n\nRepack CNTK 2.5 with third party libraries included in the bundles (Python wheel packages)\n\n---\n\n***2018-03-15.*** CNTK 2.5\n\nChange profiler details output format to be `chrome://tracing`\n\nEnable per-node timing. Working example [here](/Examples/Image/Classification/MLP/Python/SimpleMNIST.py)\n* per-node timing creates items in profiler details when profiler is enabled.\n* usage in Python:\n\n```python\nimport cntk as C\nC.debugging.debug.set_node_timing(True)\nC.debugging.start_profiler() # optional\nC.debugging.enable_profiler() # optional\n#<trainer|evaluator|function> executions\n<trainer|evaluator|function>.print_node_timing()\nC.debugging.stop_profiler()\n```\n\nExample profiler details view in `chrome://tracing`\n![ProfilerDetailWithNodeTiming](https://cntk.ai/Images/ProfilerDetailWithNodeTiming.jpg)\n\nCPU inference performance improvements using MKL\n* Accelerates some common tensor ops in Intel CPU inference for float32, especially for fully connected networks\n* Can be turned on/off by `cntk.cntk_py.enable_cpueval_optimization()/cntk.cntk_py.disable_cpueval_optimization()`\n\n1BitSGD incorporated into CNTK\n* `1BitSGD` source code is now available with CNTK license (MIT license) under `Source/1BitSGD/`\n* `1bitsgd` build target was merged into existing gpu target\n\nNew loss function: hierarchical softmax\n* Thanks @yaochengji for the contribution!\n\nDistributed Training with Multiple Learners\n* Trainer now accepts multiple parameter learners for distributed training. With this change, different parameters of a network can be learned by different learners in a single training session. This also facilitates distributed training for GANs. For more information, please refer to the [Basic_GAN_Distributed.py](/Examples/Image/GAN/Basic_GAN_Distributed.py) and the [cntk.learners.distributed_multi_learner_test.py](/bindings/python/cntk/learners/tests/distributed_multi_learner_test.py)\n\nOperators\n* Added `MeanVarianceNormalization` operator. \n\nBug fixes\n* Fixed convergence issue in Tutorial 201B\n* Fixed pooling/unpooling to support free dimension for sequences\n* Fixed crash in `CNTKBinaryFormat` deserializer when crossing sweep boundary\n* Fixed shape inference bug in RNN step function for scalar broadcasting\n* Fixed a build bug when `mpi=no`\n* Improved distributed training aggregation speed by increasing packing threshold, and expose the knob in V2\n* Fixed a memory leak in MKL layout\n* Fixed a bug in `cntk.convert` API in `misc.converter.py`, which prevents converting complex networks.\n\nONNX\n* Updates\n    * CNTK exported ONNX models are now `ONNX.checker` compliant. \n    * Added ONNX support for CNTK\u2019s `OptimizedRNNStack` operator (LSTM only).\n    * Added support for LSTM and GRU operators\n    * Added support for experimental ONNX op `MeanVarianceNormalization`.\n    * Added support for experimental ONNX op `Identity`.\n    * Added support for exporting CNTK\u2019s `LayerNormalization` layer using ONNX `MeanVarianceNormalization` op.\n* Bug or minor fixes:\n    * Axis attribute is optional in CNTK\u2019s ONNX `Concat` operator.\n    * Bug fix in ONNX broadcasting for scalars.\n    * Bug fix in ONNX ConvTranspose operator. \n    * Backward compatibility bug fix in `LeakyReLu` (argument \u2018alpha\u2019 reverted to type double).\n\nMisc\n* Added a new API `find_by_uid()` under `cntk.logging.graph`. \n\n---\n\n***2018-02-28.*** CNTK supports nightly build\n\nIf you prefer to use latest CNTK bits from master, use one of the CNTK nightly package.\n* [Nightly packages for Windows](https://cntk.ai/nightly-windows.html)\n* [Nightly packages for Linux](https://cntk.ai/nightly-linux.html)\n\nAlternatively, you can also click corresponding build badge to land to nightly build page.\n\n---\n\n***2018-01-31.* CNTK 2.4**\n\nHighlights:\n* Moved to CUDA9, cuDNN 7 and Visual Studio 2017.\n* Removed Python 3.4 support.\n* Added Volta GPU and FP16 support.\n* Better ONNX support.\n* CPU perf improvement.\n* More OPs.\n\nOPs\n* `top_k` operation: in the forward pass it computes the top (largest) k values and corresponding indices along the specified axis. In the backward pass the gradient is scattered to the top k elements (an element not in the top k gets a zero gradient).\n* `gather` operation now supports an axis argument\n* `squeeze` and `expand_dims` operations for easily removing and adding singleton axes\n* `zeros_like` and `ones_like` operations. In many situations you can just rely on CNTK correctly broadcasting a simple 0 or 1 but sometimes you need the actual tensor.\n* `depth_to_space`: Rearranges elements in the input tensor from the depth dimension into spatial blocks. Typical use of this operation is for implementing sub-pixel convolution for some image super-resolution models.\n* `space_to_depth`: Rearranges elements in the input tensor from the spatial dimensions to the depth dimension. It is largely the inverse of DepthToSpace.\n* `sum` operation: Create a new Function instance that computes element-wise sum of input tensors.\n* `softsign` operation: Create a new Function instance that computes the element-wise softsign of a input tensor.\n* `asinh` operation: Create a new Function instance that computes the element-wise asinh of a input tensor.\n* `log_softmax` operation: Create a new Function instance that computes the logsoftmax normalized values of a input tensor.\n* `hard_sigmoid` operation: Create a new Function instance that computes the hard_sigmoid normalized values of a input tensor.\n* `element_and`, `element_not`, `element_or`, `element_xor` element-wise logic operations\n* `reduce_l1` operation: Computes the L1 norm of the input tensor's element along the provided axes.\n* `reduce_l2` operation: Computes the L2 norm of the input tensor's element along the provided axes.\n* `reduce_sum_square` operation: Computes the sum square of the input tensor's element along the provided axes.\n* `image_scaler` operation: Alteration of image by scaling its individual values.\n\nONNX\n* There have been several improvements to ONNX support in CNTK.\n* Updates\n  * Updated ONNX `Reshape` op to handle `InferredDimension`.\n  * Adding `producer_name` and `producer_version` fields to ONNX models.\n  * Handling the case when neither `auto_pad` nor `pads` atrribute is specified in ONNX `Conv` op.\n* Bug fixes\n  * Fixed bug in ONNX `Pooling` op serialization\n  * Bug fix to create ONNX `InputVariable` with only one batch axis.\n  * Bug fixes and updates to implementation of ONNX `Transpose` op to match updated spec.\n  * Bug fixes and updates to implementation of ONNX `Conv`, `ConvTranspose`, and `Pooling` ops to match updated spec.\n\nOperators\n* Group convolution\n  * Fixed bug in group convolution. Output of CNTK `Convolution` op will change for groups > 1. More optimized implementation of group convolution is expected in the next release.\n  * Better error reporting for group convolution in `Convolution` layer.\n\nHalide Binary Convolution\n- The CNTK build can now use optional [Halide](http://halide-lang.org/) libraries to build `Cntk.BinaryConvolution.so/dll` library that can be used with the `netopt` module. The library contains optimized binary convolution operators that perform better than the python based binarized convolution operators. To enable Halide in the build, please download [Halide release](https://github.com/halide/Halide/releases) and set `HALIDE_PATH` environment varibale before starting a build. In Linux, you can use `./configure --with-halide[=directory]` to enable it. For more information on how to use this feature, please refer to [How_to_use_network_optimization](https://github.com/Microsoft/CNTK/blob/master/Manual/Manual_How_to_use_network_optimizations.ipynb).\n\nSee more in the [Release Notes](https://docs.microsoft.com/en-us/cognitive-toolkit/ReleaseNotes/CNTK_2_4_Release_Notes).\nGet the Release from the [CNTK Releases page](https://github.com/Microsoft/CNTK/releases).\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 6671099,
    "name": "neural-networks-and-deep-learning",
    "full_name": "mnielsen/neural-networks-and-deep-learning",
    "description": "Code samples for my book \"Neural Networks and Deep Learning\"",
    "html_url": "https://github.com/mnielsen/neural-networks-and-deep-learning",
    "clone_url": "https://github.com/mnielsen/neural-networks-and-deep-learning.git",
    "owner_login": "mnielsen",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/52171?v=4",
    "stargazers_count": 16915,
    "watchers_count": 16915,
    "forks_count": 6874,
    "open_issues_count": 7,
    "size": 20916,
    "language": "Python",
    "languages": {
      "Python": 99389
    },
    "topics": [],
    "license_name": null,
    "created_at": "2012-11-13T13:38:24+00:00",
    "updated_at": "2025-08-05T23:14:26+00:00",
    "pushed_at": "2024-06-02T11:18:37+00:00",
    "contributors_count": 11,
    "readme_length": 1925,
    "readme_content": "# Code samples for \"Neural Networks and Deep Learning\"\r\n\r\nThis repository contains code samples for my book on [\"Neural Networks\r\nand Deep Learning\"](http://neuralnetworksanddeeplearning.com).\r\n\r\nThe code is written for Python 2.6 or 2.7. There is a version for \r\nPython 3.8-3.10 [here](https://github.com/unexploredtest/neural-networks-and-deep-learning). \r\nI will not be updating the current repository for Python 3 compatibility.\r\n\r\nThe program `src/network3.py` uses version 0.6 or 0.7 of the Theano\r\nlibrary.  It needs modification for compatibility with later versions\r\nof the library.  I will not be making such modifications.\r\n\r\nAs the code is written to accompany the book, I don't intend to add\r\nnew features. However, bug reports are welcome, and you should feel\r\nfree to fork and modify the code.\r\n\r\n## License\r\n\r\nMIT License\r\n\r\nCopyright (c) 2012-2022 Michael Nielsen\r\n\r\nPermission is hereby granted, free of charge, to any person obtaining\r\na copy of this software and associated documentation files (the\r\n\"Software\"), to deal in the Software without restriction, including\r\nwithout limitation the rights to use, copy, modify, merge, publish,\r\ndistribute, sublicense, and/or sell copies of the Software, and to\r\npermit persons to whom the Software is furnished to do so, subject to\r\nthe following conditions:\r\n\r\nThe above copyright notice and this permission notice shall be\r\nincluded in all copies or substantial portions of the Software.\r\n\r\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\r\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\r\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\r\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\r\nLIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\r\nOF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\r\nWITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\r\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 589831718,
    "name": "ComfyUI",
    "full_name": "comfyanonymous/ComfyUI",
    "description": "The most powerful and modular diffusion model GUI, api and backend with a graph/nodes interface.",
    "html_url": "https://github.com/comfyanonymous/ComfyUI",
    "clone_url": "https://github.com/comfyanonymous/ComfyUI.git",
    "owner_login": "comfyanonymous",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/121283862?v=4",
    "stargazers_count": 84523,
    "watchers_count": 84523,
    "forks_count": 9377,
    "open_issues_count": 2638,
    "size": 73762,
    "language": "Python",
    "languages": {
      "Python": 3988561,
      "Batchfile": 936,
      "Mako": 689
    },
    "topics": [
      "ai",
      "python",
      "pytorch",
      "stable-diffusion"
    ],
    "license_name": "GNU General Public License v3.0",
    "created_at": "2023-01-17T03:15:56+00:00",
    "updated_at": "2025-08-06T02:10:53+00:00",
    "pushed_at": "2025-08-06T01:02:47+00:00",
    "contributors_count": 100,
    "readme_length": 26638,
    "readme_content": "<div align=\"center\">\n\n# ComfyUI\n**The most powerful and modular visual AI engine and application.**\n\n\n[![Website][website-shield]][website-url]\n[![Dynamic JSON Badge][discord-shield]][discord-url]\n[![Twitter][twitter-shield]][twitter-url]\n[![Matrix][matrix-shield]][matrix-url]\n<br>\n[![][github-release-shield]][github-release-link]\n[![][github-release-date-shield]][github-release-link]\n[![][github-downloads-shield]][github-downloads-link]\n[![][github-downloads-latest-shield]][github-downloads-link]\n\n[matrix-shield]: https://img.shields.io/badge/Matrix-000000?style=flat&logo=matrix&logoColor=white\n[matrix-url]: https://app.element.io/#/room/%23comfyui_space%3Amatrix.org\n[website-shield]: https://img.shields.io/badge/ComfyOrg-4285F4?style=flat\n[website-url]: https://www.comfy.org/\n<!-- Workaround to display total user from https://github.com/badges/shields/issues/4500#issuecomment-2060079995 -->\n[discord-shield]: https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fdiscord.com%2Fapi%2Finvites%2Fcomfyorg%3Fwith_counts%3Dtrue&query=%24.approximate_member_count&logo=discord&logoColor=white&label=Discord&color=green&suffix=%20total\n[discord-url]: https://www.comfy.org/discord\n[twitter-shield]: https://img.shields.io/twitter/follow/ComfyUI\n[twitter-url]: https://x.com/ComfyUI\n\n[github-release-shield]: https://img.shields.io/github/v/release/comfyanonymous/ComfyUI?style=flat&sort=semver\n[github-release-link]: https://github.com/comfyanonymous/ComfyUI/releases\n[github-release-date-shield]: https://img.shields.io/github/release-date/comfyanonymous/ComfyUI?style=flat\n[github-downloads-shield]: https://img.shields.io/github/downloads/comfyanonymous/ComfyUI/total?style=flat\n[github-downloads-latest-shield]: https://img.shields.io/github/downloads/comfyanonymous/ComfyUI/latest/total?style=flat&label=downloads%40latest\n[github-downloads-link]: https://github.com/comfyanonymous/ComfyUI/releases\n\n![ComfyUI Screenshot](https://github.com/user-attachments/assets/7ccaf2c1-9b72-41ae-9a89-5688c94b7abe)\n</div>\n\nComfyUI lets you design and execute advanced stable diffusion pipelines using a graph/nodes/flowchart based interface. Available on Windows, Linux, and macOS.\n\n## Get Started\n\n#### [Desktop Application](https://www.comfy.org/download)\n- The easiest way to get started. \n- Available on Windows & macOS.\n\n#### [Windows Portable Package](#installing)\n- Get the latest commits and completely portable.\n- Available on Windows.\n\n#### [Manual Install](#manual-install-windows-linux)\nSupports all operating systems and GPU types (NVIDIA, AMD, Intel, Apple Silicon, Ascend).\n\n## [Examples](https://comfyanonymous.github.io/ComfyUI_examples/)\nSee what ComfyUI can do with the [example workflows](https://comfyanonymous.github.io/ComfyUI_examples/).\n\n## Features\n- Nodes/graph/flowchart interface to experiment and create complex Stable Diffusion workflows without needing to code anything.\n- Image Models\n   - SD1.x, SD2.x ([unCLIP](https://comfyanonymous.github.io/ComfyUI_examples/unclip/))\n   - [SDXL](https://comfyanonymous.github.io/ComfyUI_examples/sdxl/), [SDXL Turbo](https://comfyanonymous.github.io/ComfyUI_examples/sdturbo/)\n   - [Stable Cascade](https://comfyanonymous.github.io/ComfyUI_examples/stable_cascade/)\n   - [SD3 and SD3.5](https://comfyanonymous.github.io/ComfyUI_examples/sd3/)\n   - Pixart Alpha and Sigma\n   - [AuraFlow](https://comfyanonymous.github.io/ComfyUI_examples/aura_flow/)\n   - [HunyuanDiT](https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_dit/)\n   - [Flux](https://comfyanonymous.github.io/ComfyUI_examples/flux/)\n   - [Lumina Image 2.0](https://comfyanonymous.github.io/ComfyUI_examples/lumina2/)\n   - [HiDream](https://comfyanonymous.github.io/ComfyUI_examples/hidream/)\n   - [Cosmos Predict2](https://comfyanonymous.github.io/ComfyUI_examples/cosmos_predict2/)\n   - [Qwen Image](https://comfyanonymous.github.io/ComfyUI_examples/qwen_image/)\n- Image Editing Models\n   - [Omnigen 2](https://comfyanonymous.github.io/ComfyUI_examples/omnigen/)\n   - [Flux Kontext](https://comfyanonymous.github.io/ComfyUI_examples/flux/#flux-kontext-image-editing-model)\n   - [HiDream E1.1](https://comfyanonymous.github.io/ComfyUI_examples/hidream/#hidream-e11)\n- Video Models\n   - [Stable Video Diffusion](https://comfyanonymous.github.io/ComfyUI_examples/video/)\n   - [Mochi](https://comfyanonymous.github.io/ComfyUI_examples/mochi/)\n   - [LTX-Video](https://comfyanonymous.github.io/ComfyUI_examples/ltxv/)\n   - [Hunyuan Video](https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_video/)\n   - [Nvidia Cosmos](https://comfyanonymous.github.io/ComfyUI_examples/cosmos/) and [Cosmos Predict2](https://comfyanonymous.github.io/ComfyUI_examples/cosmos_predict2/)\n   - [Wan 2.1](https://comfyanonymous.github.io/ComfyUI_examples/wan/)\n   - [Wan 2.2](https://comfyanonymous.github.io/ComfyUI_examples/wan22/)\n- Audio Models\n   - [Stable Audio](https://comfyanonymous.github.io/ComfyUI_examples/audio/)\n   - [ACE Step](https://comfyanonymous.github.io/ComfyUI_examples/audio/)\n- 3D Models\n   - [Hunyuan3D 2.0](https://docs.comfy.org/tutorials/3d/hunyuan3D-2)\n- Asynchronous Queue system\n- Many optimizations: Only re-executes the parts of the workflow that changes between executions.\n- Smart memory management: can automatically run large models on GPUs with as low as 1GB vram with smart offloading.\n- Works even if you don't have a GPU with: ```--cpu``` (slow)\n- Can load ckpt and safetensors: All in one checkpoints or standalone diffusion models, VAEs and CLIP models.\n- Safe loading of ckpt, pt, pth, etc.. files.\n- Embeddings/Textual inversion\n- [Loras (regular, locon and loha)](https://comfyanonymous.github.io/ComfyUI_examples/lora/)\n- [Hypernetworks](https://comfyanonymous.github.io/ComfyUI_examples/hypernetworks/)\n- Loading full workflows (with seeds) from generated PNG, WebP and FLAC files.\n- Saving/Loading workflows as Json files.\n- Nodes interface can be used to create complex workflows like one for [Hires fix](https://comfyanonymous.github.io/ComfyUI_examples/2_pass_txt2img/) or much more advanced ones.\n- [Area Composition](https://comfyanonymous.github.io/ComfyUI_examples/area_composition/)\n- [Inpainting](https://comfyanonymous.github.io/ComfyUI_examples/inpaint/) with both regular and inpainting models.\n- [ControlNet and T2I-Adapter](https://comfyanonymous.github.io/ComfyUI_examples/controlnet/)\n- [Upscale Models (ESRGAN, ESRGAN variants, SwinIR, Swin2SR, etc...)](https://comfyanonymous.github.io/ComfyUI_examples/upscale_models/)\n- [GLIGEN](https://comfyanonymous.github.io/ComfyUI_examples/gligen/)\n- [Model Merging](https://comfyanonymous.github.io/ComfyUI_examples/model_merging/)\n- [LCM models and Loras](https://comfyanonymous.github.io/ComfyUI_examples/lcm/)\n- Latent previews with [TAESD](#how-to-show-high-quality-previews)\n- Works fully offline: core will never download anything unless you want to.\n- Optional API nodes to use paid models from external providers through the online [Comfy API](https://docs.comfy.org/tutorials/api-nodes/overview).\n- [Config file](extra_model_paths.yaml.example) to set the search paths for models.\n\nWorkflow examples can be found on the [Examples page](https://comfyanonymous.github.io/ComfyUI_examples/)\n\n## Release Process\n\nComfyUI follows a weekly release cycle targeting Friday but this regularly changes because of model releases or large changes to the codebase. There are three interconnected repositories:\n\n1. **[ComfyUI Core](https://github.com/comfyanonymous/ComfyUI)**\n   - Releases a new stable version (e.g., v0.7.0)\n   - Serves as the foundation for the desktop release\n\n2. **[ComfyUI Desktop](https://github.com/Comfy-Org/desktop)**\n   - Builds a new release using the latest stable core version\n\n3. **[ComfyUI Frontend](https://github.com/Comfy-Org/ComfyUI_frontend)**\n   - Weekly frontend updates are merged into the core repository\n   - Features are frozen for the upcoming core release\n   - Development continues for the next release cycle\n\n## Shortcuts\n\n| Keybind                            | Explanation                                                                                                        |\n|------------------------------------|--------------------------------------------------------------------------------------------------------------------|\n| `Ctrl` + `Enter`                      | Queue up current graph for generation                                                                              |\n| `Ctrl` + `Shift` + `Enter`              | Queue up current graph as first for generation                                                                     |\n| `Ctrl` + `Alt` + `Enter`                | Cancel current generation                                                                                          |\n| `Ctrl` + `Z`/`Ctrl` + `Y`                 | Undo/Redo                                                                                                          |\n| `Ctrl` + `S`                          | Save workflow                                                                                                      |\n| `Ctrl` + `O`                          | Load workflow                                                                                                      |\n| `Ctrl` + `A`                          | Select all nodes                                                                                                   |\n| `Alt `+ `C`                           | Collapse/uncollapse selected nodes                                                                                 |\n| `Ctrl` + `M`                          | Mute/unmute selected nodes                                                                                         |\n| `Ctrl` + `B`                           | Bypass selected nodes (acts like the node was removed from the graph and the wires reconnected through)            |\n| `Delete`/`Backspace`                   | Delete selected nodes                                                                                              |\n| `Ctrl` + `Backspace`                   | Delete the current graph                                                                                           |\n| `Space`                              | Move the canvas around when held and moving the cursor                                                             |\n| `Ctrl`/`Shift` + `Click`                 | Add clicked node to selection                                                                                      |\n| `Ctrl` + `C`/`Ctrl` + `V`                  | Copy and paste selected nodes (without maintaining connections to outputs of unselected nodes)                     |\n| `Ctrl` + `C`/`Ctrl` + `Shift` + `V`          | Copy and paste selected nodes (maintaining connections from outputs of unselected nodes to inputs of pasted nodes) |\n| `Shift` + `Drag`                       | Move multiple selected nodes at the same time                                                                      |\n| `Ctrl` + `D`                           | Load default graph                                                                                                 |\n| `Alt` + `+`                          | Canvas Zoom in                                                                                                     |\n| `Alt` + `-`                          | Canvas Zoom out                                                                                                    |\n| `Ctrl` + `Shift` + LMB + Vertical drag | Canvas Zoom in/out                                                                                                 |\n| `P`                                  | Pin/Unpin selected nodes                                                                                           |\n| `Ctrl` + `G`                           | Group selected nodes                                                                                               |\n| `Q`                                 | Toggle visibility of the queue                                                                                     |\n| `H`                                  | Toggle visibility of history                                                                                       |\n| `R`                                  | Refresh graph                                                                                                      |\n| `F`                                  | Show/Hide menu                                                                                                      |\n| `.`                                  | Fit view to selection (Whole graph when nothing is selected)                                                        |\n| Double-Click LMB                   | Open node quick search palette                                                                                     |\n| `Shift` + Drag                       | Move multiple wires at once                                                                                        |\n| `Ctrl` + `Alt` + LMB                   | Disconnect all wires from clicked slot                                                                             |\n\n`Ctrl` can also be replaced with `Cmd` instead for macOS users\n\n# Installing\n\n## Windows Portable\n\nThere is a portable standalone build for Windows that should work for running on Nvidia GPUs or for running on your CPU only on the [releases page](https://github.com/comfyanonymous/ComfyUI/releases).\n\n### [Direct link to download](https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia.7z)\n\nSimply download, extract with [7-Zip](https://7-zip.org) and run. Make sure you put your Stable Diffusion checkpoints/models (the huge ckpt/safetensors files) in: ComfyUI\\models\\checkpoints\n\nIf you have trouble extracting it, right click the file -> properties -> unblock\n\n#### How do I share models between another UI and ComfyUI?\n\nSee the [Config file](extra_model_paths.yaml.example) to set the search paths for models. In the standalone windows build you can find this file in the ComfyUI directory. Rename this file to extra_model_paths.yaml and edit it with your favorite text editor.\n\n\n## [comfy-cli](https://docs.comfy.org/comfy-cli/getting-started)\n\nYou can install and start ComfyUI using comfy-cli:\n```bash\npip install comfy-cli\ncomfy install\n```\n\n## Manual Install (Windows, Linux)\n\npython 3.13 is supported but using 3.12 is recommended because some custom nodes and their dependencies might not support it yet.\n\nGit clone this repo.\n\nPut your SD checkpoints (the huge ckpt/safetensors files) in: models/checkpoints\n\nPut your VAE in: models/vae\n\n\n### AMD GPUs (Linux only)\nAMD users can install rocm and pytorch with pip if you don't have it already installed, this is the command to install the stable version:\n\n```pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.3```\n\nThis is the command to install the nightly with ROCm 6.4 which might have some performance improvements:\n\n```pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.4```\n\n### Intel GPUs (Windows and Linux)\n\n(Option 1) Intel Arc GPU users can install native PyTorch with torch.xpu support using pip (currently available in PyTorch nightly builds). More information can be found [here](https://pytorch.org/docs/main/notes/get_start_xpu.html)\n  \n1. To install PyTorch nightly, use the following command:\n\n```pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/xpu```\n\n2. Launch ComfyUI by running `python main.py`\n\n\n(Option 2) Alternatively, Intel GPUs supported by Intel Extension for PyTorch (IPEX) can leverage IPEX for improved performance.\n\n1. For Intel\u00ae Arc\u2122 A-Series Graphics utilizing IPEX, create a conda environment and use the commands below:\n\n```\nconda install libuv\npip install torch==2.3.1.post0+cxx11.abi torchvision==0.18.1.post0+cxx11.abi torchaudio==2.3.1.post0+cxx11.abi intel-extension-for-pytorch==2.3.110.post0+xpu --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/ --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/cn/\n```\n\nFor other supported Intel GPUs with IPEX, visit [Installation](https://intel.github.io/intel-extension-for-pytorch/index.html#installation?platform=gpu) for more information.\n\nAdditional discussion and help can be found [here](https://github.com/comfyanonymous/ComfyUI/discussions/476).\n\n### NVIDIA\n\nNvidia users should install stable pytorch using this command:\n\n```pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu128```\n\nThis is the command to install pytorch nightly instead which might have performance improvements.\n\n```pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu129```\n\n#### Troubleshooting\n\nIf you get the \"Torch not compiled with CUDA enabled\" error, uninstall torch with:\n\n```pip uninstall torch```\n\nAnd install it again with the command above.\n\n### Dependencies\n\nInstall the dependencies by opening your terminal inside the ComfyUI folder and:\n\n```pip install -r requirements.txt```\n\nAfter this you should have everything installed and can proceed to running ComfyUI.\n\n### Others:\n\n#### Apple Mac silicon\n\nYou can install ComfyUI in Apple Mac silicon (M1 or M2) with any recent macOS version.\n\n1. Install pytorch nightly. For instructions, read the [Accelerated PyTorch training on Mac](https://developer.apple.com/metal/pytorch/) Apple Developer guide (make sure to install the latest pytorch nightly).\n1. Follow the [ComfyUI manual installation](#manual-install-windows-linux) instructions for Windows and Linux.\n1. Install the ComfyUI [dependencies](#dependencies). If you have another Stable Diffusion UI [you might be able to reuse the dependencies](#i-already-have-another-ui-for-stable-diffusion-installed-do-i-really-have-to-install-all-of-these-dependencies).\n1. Launch ComfyUI by running `python main.py`\n\n> **Note**: Remember to add your models, VAE, LoRAs etc. to the corresponding Comfy folders, as discussed in [ComfyUI manual installation](#manual-install-windows-linux).\n\n#### DirectML (AMD Cards on Windows)\n\nThis is very badly supported and is not recommended. There are some unofficial builds of pytorch ROCm on windows that exist that will give you a much better experience than this. This readme will be updated once official pytorch ROCm builds for windows come out.\n\n```pip install torch-directml``` Then you can launch ComfyUI with: ```python main.py --directml```\n\n#### Ascend NPUs\n\nFor models compatible with Ascend Extension for PyTorch (torch_npu). To get started, ensure your environment meets the prerequisites outlined on the [installation](https://ascend.github.io/docs/sources/ascend/quick_install.html) page. Here's a step-by-step guide tailored to your platform and installation method:\n\n1. Begin by installing the recommended or newer kernel version for Linux as specified in the Installation page of torch-npu, if necessary.\n2. Proceed with the installation of Ascend Basekit, which includes the driver, firmware, and CANN, following the instructions provided for your specific platform.\n3. Next, install the necessary packages for torch-npu by adhering to the platform-specific instructions on the [Installation](https://ascend.github.io/docs/sources/pytorch/install.html#pytorch) page.\n4. Finally, adhere to the [ComfyUI manual installation](#manual-install-windows-linux) guide for Linux. Once all components are installed, you can run ComfyUI as described earlier.\n\n#### Cambricon MLUs\n\nFor models compatible with Cambricon Extension for PyTorch (torch_mlu). Here's a step-by-step guide tailored to your platform and installation method:\n\n1. Install the Cambricon CNToolkit by adhering to the platform-specific instructions on the [Installation](https://www.cambricon.com/docs/sdk_1.15.0/cntoolkit_3.7.2/cntoolkit_install_3.7.2/index.html)\n2. Next, install the PyTorch(torch_mlu) following the instructions on the [Installation](https://www.cambricon.com/docs/sdk_1.15.0/cambricon_pytorch_1.17.0/user_guide_1.9/index.html)\n3. Launch ComfyUI by running `python main.py`\n\n#### Iluvatar Corex\n\nFor models compatible with Iluvatar Extension for PyTorch. Here's a step-by-step guide tailored to your platform and installation method:\n\n1. Install the Iluvatar Corex Toolkit by adhering to the platform-specific instructions on the [Installation](https://support.iluvatar.com/#/DocumentCentre?id=1&nameCenter=2&productId=520117912052801536)\n2. Launch ComfyUI by running `python main.py`\n\n# Running\n\n```python main.py```\n\n### For AMD cards not officially supported by ROCm\n\nTry running it with this command if you have issues:\n\nFor 6700, 6600 and maybe other RDNA2 or older: ```HSA_OVERRIDE_GFX_VERSION=10.3.0 python main.py```\n\nFor AMD 7600 and maybe other RDNA3 cards: ```HSA_OVERRIDE_GFX_VERSION=11.0.0 python main.py```\n\n### AMD ROCm Tips\n\nYou can enable experimental memory efficient attention on recent pytorch in ComfyUI on some AMD GPUs using this command, it should already be enabled by default on RDNA3. If this improves speed for you on latest pytorch on your GPU please report it so that I can enable it by default.\n\n```TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1 python main.py --use-pytorch-cross-attention```\n\nYou can also try setting this env variable `PYTORCH_TUNABLEOP_ENABLED=1` which might speed things up at the cost of a very slow initial run.\n\n# Notes\n\nOnly parts of the graph that have an output with all the correct inputs will be executed.\n\nOnly parts of the graph that change from each execution to the next will be executed, if you submit the same graph twice only the first will be executed. If you change the last part of the graph only the part you changed and the part that depends on it will be executed.\n\nDragging a generated png on the webpage or loading one will give you the full workflow including seeds that were used to create it.\n\nYou can use () to change emphasis of a word or phrase like: (good code:1.2) or (bad code:0.8). The default emphasis for () is 1.1. To use () characters in your actual prompt escape them like \\\\( or \\\\).\n\nYou can use {day|night}, for wildcard/dynamic prompts. With this syntax \"{wild|card|test}\" will be randomly replaced by either \"wild\", \"card\" or \"test\" by the frontend every time you queue the prompt. To use {} characters in your actual prompt escape them like: \\\\{ or \\\\}.\n\nDynamic prompts also support C-style comments, like `// comment` or `/* comment */`.\n\nTo use a textual inversion concepts/embeddings in a text prompt put them in the models/embeddings directory and use them in the CLIPTextEncode node like this (you can omit the .pt extension):\n\n```embedding:embedding_filename.pt```\n\n\n## How to show high-quality previews?\n\nUse ```--preview-method auto``` to enable previews.\n\nThe default installation includes a fast latent preview method that's low-resolution. To enable higher-quality previews with [TAESD](https://github.com/madebyollin/taesd), download the [taesd_decoder.pth, taesdxl_decoder.pth, taesd3_decoder.pth and taef1_decoder.pth](https://github.com/madebyollin/taesd/) and place them in the `models/vae_approx` folder. Once they're installed, restart ComfyUI and launch it with `--preview-method taesd` to enable high-quality previews.\n\n## How to use TLS/SSL?\nGenerate a self-signed certificate (not appropriate for shared/production use) and key by running the command: `openssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -sha256 -days 3650 -nodes -subj \"/C=XX/ST=StateName/L=CityName/O=CompanyName/OU=CompanySectionName/CN=CommonNameOrHostname\"`\n\nUse `--tls-keyfile key.pem --tls-certfile cert.pem` to enable TLS/SSL, the app will now be accessible with `https://...` instead of `http://...`.\n\n> Note: Windows users can use [alexisrolland/docker-openssl](https://github.com/alexisrolland/docker-openssl) or one of the [3rd party binary distributions](https://wiki.openssl.org/index.php/Binaries) to run the command example above. \n<br/><br/>If you use a container, note that the volume mount `-v` can be a relative path so `... -v \".\\:/openssl-certs\" ...` would create the key & cert files in the current directory of your command prompt or powershell terminal.\n\n## Support and dev channel\n\n[Discord](https://comfy.org/discord): Try the #help or #feedback channels.\n\n[Matrix space: #comfyui_space:matrix.org](https://app.element.io/#/room/%23comfyui_space%3Amatrix.org) (it's like discord but open source).\n\nSee also: [https://www.comfy.org/](https://www.comfy.org/)\n\n## Frontend Development\n\nAs of August 15, 2024, we have transitioned to a new frontend, which is now hosted in a separate repository: [ComfyUI Frontend](https://github.com/Comfy-Org/ComfyUI_frontend). This repository now hosts the compiled JS (from TS/Vue) under the `web/` directory.\n\n### Reporting Issues and Requesting Features\n\nFor any bugs, issues, or feature requests related to the frontend, please use the [ComfyUI Frontend repository](https://github.com/Comfy-Org/ComfyUI_frontend). This will help us manage and address frontend-specific concerns more efficiently.\n\n### Using the Latest Frontend\n\nThe new frontend is now the default for ComfyUI. However, please note:\n\n1. The frontend in the main ComfyUI repository is updated fortnightly.\n2. Daily releases are available in the separate frontend repository.\n\nTo use the most up-to-date frontend version:\n\n1. For the latest daily release, launch ComfyUI with this command line argument:\n\n   ```\n   --front-end-version Comfy-Org/ComfyUI_frontend@latest\n   ```\n\n2. For a specific version, replace `latest` with the desired version number:\n\n   ```\n   --front-end-version Comfy-Org/ComfyUI_frontend@1.2.2\n   ```\n\nThis approach allows you to easily switch between the stable fortnightly release and the cutting-edge daily updates, or even specific versions for testing purposes.\n\n### Accessing the Legacy Frontend\n\nIf you need to use the legacy frontend for any reason, you can access it using the following command line argument:\n\n```\n--front-end-version Comfy-Org/ComfyUI_legacy_frontend@latest\n```\n\nThis will use a snapshot of the legacy frontend preserved in the [ComfyUI Legacy Frontend repository](https://github.com/Comfy-Org/ComfyUI_legacy_frontend).\n\n# QA\n\n### Which GPU should I buy for this?\n\n[See this page for some recommendations](https://github.com/comfyanonymous/ComfyUI/wiki/Which-GPU-should-I-buy-for-ComfyUI)\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 83119431,
    "name": "ailearning",
    "full_name": "apachecn/ailearning",
    "description": "AiLearning\uff1a\u6570\u636e\u5206\u6790+\u673a\u5668\u5b66\u4e60\u5b9e\u6218+\u7ebf\u6027\u4ee3\u6570+PyTorch+NLTK+TF2",
    "html_url": "https://github.com/apachecn/ailearning",
    "clone_url": "https://github.com/apachecn/ailearning.git",
    "owner_login": "apachecn",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/24802038?v=4",
    "stargazers_count": 41217,
    "watchers_count": 41217,
    "forks_count": 11578,
    "open_issues_count": 3,
    "size": 171378,
    "language": "Python",
    "languages": {
      "Python": 909354,
      "JavaScript": 23773,
      "CSS": 22379,
      "Jupyter Notebook": 17703,
      "HTML": 3927,
      "Shell": 469,
      "Dockerfile": 49
    },
    "topics": [
      "adaboost",
      "apriori",
      "deeplearning",
      "dnn",
      "fp-growth",
      "kmeans",
      "logistic",
      "lstm",
      "mahchine-leaning",
      "naivebayes",
      "nlp",
      "pca",
      "python",
      "recommendedsystem",
      "regression",
      "rnn",
      "scikit-learn",
      "sklearn",
      "svd",
      "svm"
    ],
    "license_name": "Other",
    "created_at": "2017-02-25T08:53:02+00:00",
    "updated_at": "2025-08-06T00:58:25+00:00",
    "pushed_at": "2024-11-12T16:21:55+00:00",
    "contributors_count": 12,
    "readme_length": 15777,
    "readme_content": "<p align=\"center\">\n    <a href=\"https://www.apachecn.org\">\n        <img width=\"200\" src=\"docs/img/logo.jpg\">\n    </a>\n    <br >\n    <a href=\"https://www.apachecn.org/\"><img src=\"https://img.shields.io/badge/%3E-HOME-green.svg\"></a>\n    <a href=\"https://home.apachecn.org/about/\"><img src=\"https://img.shields.io/badge/%3E-ABOUT-green.svg\"></a>\n    <a href=\"mailto:apache@163.com\"><img src=\"https://img.shields.io/badge/%3E-Email-green.svg\"></a>\n</p>\n\n<h1 align=\"center\"><a href=\"https://github.com/apachecn/AiLearning\">AI learning</a></h1>\n\n> \u534f\u8bae\uff1a[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh)\n> \n> \u4e00\u79cd\u65b0\u6280\u672f\u4e00\u65e6\u5f00\u59cb\u6d41\u884c\uff0c\u4f60\u8981\u4e48\u5750\u4e0a\u538b\u8def\u673a\uff0c\u8981\u4e48\u6210\u4e3a\u94fa\u8def\u77f3\u3002\u2014\u2014Stewart Brand\n\n* [\u5728\u7ebf\u9605\u8bfb](https://ailearning.apachecn.org)\n* [\u5728\u7ebf\u9605\u8bfb\uff08v1\uff09](https://alv1.apachecn.org/)\n* [QuantLearning](https://qlearn.apachecn.org/#/)\n* [ApacheCN \u4e2d\u6587\u7ffb\u8bd1\u7ec4 713436582](https://qm.qq.com/cgi-bin/qm/qr?k=5u_aAU-YlY3fH-m8meXTJzBEo2boQIUs&jump_from=webapi&authKey=CVZcReMt/vKdTXZBQ8ly+jWncXiSzzWOlrx5hybX5pSrKu6s0fvGX54+vHHlgYNt)\n* [ApacheCN \u5b66\u4e60\u8d44\u6e90](https://www.apachecn.org/)\n* \u6ce8: \u5e7f\u544a\u4f4d\u5408\u4f5c(\u7269\u7f8e\u4ef7\u5ec9)\uff0c\u8bf7\u8054\u7cfb <apachecn@163.com>\n\n# \u8def\u7ebf\u56fe\n\n* \u5165\u95e8\u53ea\u770b: \u6b65\u9aa4 1 => 2 => 3\uff0c\u4f60\u53ef\u4ee5\u5f53\u5927\u725b\uff01\n* \u4e2d\u7ea7\u8865\u5145 - \u8d44\u6599\u5e93: <https://github.com/apachecn/ai-roadmap>\n\n> \u8865\u5145\n\n* \u7b97\u6cd5\u5237\u9898: <https://www.ixigua.com/pseries/6822642486343631363/>\n* \u9762\u8bd5\u6c42\u804c: <https://www.ixigua.com/pseries/6822563009391493636/>\n* \u673a\u5668\u5b66\u4e60\u5b9e\u6218: <https://www.ixigua.com/pseries/6822816341615968772/>\n* NLP\u6559\u5b66\u89c6\u9891: <https://www.ixigua.com/pseries/6828241431295951373/>\n* **AI\u5e38\u7528\u51fd\u6570\u8bf4\u660e**: <https://github.com/apachecn/AiLearning/tree/master/AI\u5e38\u7528\u51fd\u6570\u8bf4\u660e.md>\n\n## 1.\u673a\u5668\u5b66\u4e60 - \u57fa\u7840\n\n> \u652f\u6301\u7248\u672c \n\n| Version | Supported          |\n| ------- | ------------------ |\n| 3.6.x   | :x:                |\n| 2.7.x   | :white_check_mark: |\n\n\u6ce8\u610f\u4e8b\u9879: \n\n- \u673a\u5668\u5b66\u4e60\u5b9e\u6218: \u4ec5\u4ec5\u53ea\u662f\u5b66\u4e60\uff0c\u8bf7\u4f7f\u7528 python 2.7.x \u7248\u672c \uff083.6.x \u53ea\u662f\u4fee\u6539\u4e86\u90e8\u5206\uff09\n\n### \u57fa\u672c\u4ecb\u7ecd\n\n* \u8d44\u6599\u6765\u6e90: Machine Learning in Action(\u673a\u5668\u5b66\u4e60\u5b9e\u6218-\u4e2a\u4eba\u7b14\u8bb0)\n* \u7edf\u4e00\u6570\u636e\u5730\u5740: <https://github.com/apachecn/data>\n  * \u767e\u5ea6\u4e91\u6253\u5305\u5730\u5740: <https://github.com/apachecn/data/issues/3>\n* \u4e66\u7c4d\u4e0b\u8f7d\u5730\u5740: <https://github.com/apachecn/data/tree/master/book>\n* \u673a\u5668\u5b66\u4e60\u4e0b\u8f7d\u5730\u5740: <https://github.com/apachecn/data/tree/master/\u673a\u5668\u5b66\u4e60>\n* \u6df1\u5ea6\u5b66\u4e60\u6570\u636e\u5730\u5740: <https://github.com/apachecn/data/tree/master/\u6df1\u5ea6\u5b66\u4e60>\n* \u63a8\u8350\u7cfb\u7edf\u6570\u636e\u5730\u5740: <https://github.com/apachecn/data/tree/master/\u63a8\u8350\u7cfb\u7edf>\n* \u89c6\u9891\u7f51\u7ad9: \u4f18\u9177 \uff0fbilibili / Acfun / \u7f51\u6613\u4e91\u8bfe\u5802\uff0c\u53ef\u76f4\u63a5\u5728\u7ebf\u64ad\u653e\u3002\uff08\u6700\u4e0b\u65b9\u6709\u76f8\u5e94\u94fe\u63a5\uff09\n* -- \u63a8\u8350 [\u7ea2\u8272\u77f3\u5934](https://github.com/RedstoneWill): [\u53f0\u6e7e\u5927\u5b66\u6797\u8f69\u7530\u673a\u5668\u5b66\u4e60\u7b14\u8bb0](https://github.com/apachecn/ntu-hsuantienlin-ml)\n* -- \u63a8\u8350 [\u673a\u5668\u5b66\u4e60\u7b14\u8bb0](https://feisky.xyz/machine-learning): https://feisky.xyz/machine-learning\n\n### \u5b66\u4e60\u6587\u6863\n\n| \u6a21\u5757 | \u7ae0\u8282 | \u7c7b\u578b | \u8d1f\u8d23\u4eba(GitHub) | QQ |\n| --- | --- | --- | --- | --- |\n| \u673a\u5668\u5b66\u4e60\u5b9e\u6218 | [\u7b2c 1 \u7ae0: \u673a\u5668\u5b66\u4e60\u57fa\u7840](/docs/ml/1.md) | \u4ecb\u7ecd | [@\u6bdb\u7ea2\u52a8](https://github.com/ElmaDavies) | 1306014226 |\n| \u673a\u5668\u5b66\u4e60\u5b9e\u6218 | [\u7b2c 2 \u7ae0: KNN \u8fd1\u90bb\u7b97\u6cd5](/docs/ml/2.md) | \u5206\u7c7b | [@\u5c24\u6c38\u6c5f](https://github.com/youyj521) | 279393323 |\n| \u673a\u5668\u5b66\u4e60\u5b9e\u6218 | [\u7b2c 3 \u7ae0: \u51b3\u7b56\u6811](/docs/ml/3.md) | \u5206\u7c7b | [@\u666f\u6d9b](https://github.com/jingwangfei) | 844300439 |\n| \u673a\u5668\u5b66\u4e60\u5b9e\u6218 | [\u7b2c 4 \u7ae0: \u6734\u7d20\u8d1d\u53f6\u65af](/docs/ml/4.md) | \u5206\u7c7b | [@wnma3mz](https://github.com/wnma3mz)<br/>[@\u5206\u6790](https://github.com/kailian) | 1003324213<br/>244970749 |\n| \u673a\u5668\u5b66\u4e60\u5b9e\u6218 | [\u7b2c 5 \u7ae0: Logistic\u56de\u5f52](/docs/ml/5.md) | \u5206\u7c7b | [@\u5fae\u5149\u540c\u5c18](https://github.com/DataMonk2017) | 529925688 |\n| \u673a\u5668\u5b66\u4e60\u5b9e\u6218 | [\u7b2c 6 \u7ae0: SVM \u652f\u6301\u5411\u91cf\u673a](/docs/ml/6.md) | \u5206\u7c7b | [@\u738b\u5fb7\u7ea2](https://github.com/VPrincekin) | 934969547 |\n| \u7f51\u4e0a\u7ec4\u5408\u5185\u5bb9 | [\u7b2c 7 \u7ae0: \u96c6\u6210\u65b9\u6cd5\uff08\u968f\u673a\u68ee\u6797\u548c AdaBoost\uff09](/docs/ml/7.md) | \u5206\u7c7b | [@\u7247\u523b](https://github.com/jiangzhonglian) | 529815144 |\n| \u673a\u5668\u5b66\u4e60\u5b9e\u6218 | [\u7b2c 8 \u7ae0: \u56de\u5f52](/docs/ml/8.md) | \u56de\u5f52 | [@\u5fae\u5149\u540c\u5c18](https://github.com/DataMonk2017) | 529925688 |\n| \u673a\u5668\u5b66\u4e60\u5b9e\u6218 | [\u7b2c 9 \u7ae0: \u6811\u56de\u5f52](/docs/ml/9.md) | \u56de\u5f52 | [@\u5fae\u5149\u540c\u5c18](https://github.com/DataMonk2017) | 529925688 |\n| \u673a\u5668\u5b66\u4e60\u5b9e\u6218 | [\u7b2c 10 \u7ae0: K-Means \u805a\u7c7b](/docs/ml/10.md) | \u805a\u7c7b | [@\u5f90\u662d\u6e05](https://github.com/xuzhaoqing) | 827106588 |\n| \u673a\u5668\u5b66\u4e60\u5b9e\u6218 | [\u7b2c 11 \u7ae0: \u5229\u7528 Apriori \u7b97\u6cd5\u8fdb\u884c\u5173\u8054\u5206\u6790](/docs/ml/11.md) | \u9891\u7e41\u9879\u96c6 | [@\u5218\u6d77\u98de](https://github.com/WindZQ) | 1049498972 |\n| \u673a\u5668\u5b66\u4e60\u5b9e\u6218 | [\u7b2c 12 \u7ae0: FP-growth \u9ad8\u6548\u53d1\u73b0\u9891\u7e41\u9879\u96c6](/docs/ml/12.md) | \u9891\u7e41\u9879\u96c6 | [@\u7a0b\u5a01](https://github.com/mikechengwei) | 842725815 |\n| \u673a\u5668\u5b66\u4e60\u5b9e\u6218 | [\u7b2c 13 \u7ae0: \u5229\u7528 PCA \u6765\u7b80\u5316\u6570\u636e](/docs/ml/13.md) | \u5de5\u5177 | [@\u5ed6\u7acb\u5a1f](https://github.com/lljuan330) | 835670618 |\n| \u673a\u5668\u5b66\u4e60\u5b9e\u6218 | [\u7b2c 14 \u7ae0: \u5229\u7528 SVD \u6765\u7b80\u5316\u6570\u636e](/docs/ml/14.md) | \u5de5\u5177 | [@\u5f20\u4fca\u7693](https://github.com/marsjhao) | 714974242 |\n| \u673a\u5668\u5b66\u4e60\u5b9e\u6218 | [\u7b2c 15 \u7ae0: \u5927\u6570\u636e\u4e0e MapReduce](/docs/ml/15.md) | \u5de5\u5177 | [@wnma3mz](https://github.com/wnma3mz) | 1003324213 |\n| Ml\u9879\u76ee\u5b9e\u6218 | [\u7b2c 16 \u7ae0: \u63a8\u8350\u7cfb\u7edf\uff08\u5df2\u8fc1\u79fb\uff09](/docs/ml/16.md) | \u9879\u76ee | [\u63a8\u8350\u7cfb\u7edf\uff08\u8fc1\u79fb\u540e\u5730\u5740\uff09](https://github.com/apachecn/RecommenderSystems)  |  |\n| \u7b2c\u4e00\u671f\u7684\u603b\u7ed3 | [2017-04-08: \u7b2c\u4e00\u671f\u7684\u603b\u7ed3](/docs/report/2017-04-08.md) | \u603b\u7ed3 | \u603b\u7ed3 | 529815144 |\n\n### \u7f51\u7ad9\u89c6\u9891\n\n> [\u77e5\u4e4e\u95ee\u7b54-\u7206\u70b8\u5566-\u673a\u5668\u5b66\u4e60\u8be5\u600e\u4e48\u5165\u95e8\uff1f](https://www.zhihu.com/question/20691338/answer/248678328)\n\n\u5f53\u7136\u6211\u77e5\u9053\uff0c\u7b2c\u4e00\u53e5\u5c31\u4f1a\u88ab\u5410\u69fd\uff0c\u56e0\u4e3a\u79d1\u73ed\u51fa\u8eab\u7684\u4eba\uff0c\u4e0d\u5c51\u7684\u5410\u4e86\u4e00\u53e3\u553e\u6cab\uff0c\u8bf4\u50bbX\uff0c\u8fd8\u8bc4\u8bba Andrew Ng \u7684\u89c6\u9891\u3002\u3002\n\n\u6211\u8fd8\u77e5\u9053\u8fd8\u6709\u4e00\u90e8\u5206\u4eba\uff0c\u770b Andrew Ng \u7684\u89c6\u9891\u5c31\u662f\u770b\u4e0d\u61c2\uff0c\u90a3\u795e\u79d8\u7684\u6570\u5b66\u63a8\u5bfc\uff0c\u90a3\u8ff7\u4e4b\u5fae\u7b11\u7684\u82f1\u6587\u7248\u7684\u6559\u5b66\uff0c\u6211\u4f55\u5c1d\u53c8\u4e0d\u662f\u8fd9\u6837\u8d70\u8fc7\u6765\u7684\uff1f\uff1f \u6211\u7684\u5fc3\u53ef\u80fd\u6bd4\u4f60\u4eec\u90fd\u75db\uff0c\u56e0\u4e3a\u6211\u5728\u7f51\u4e0a\u6536\u85cf\u8fc7\u4e0a10\u90e8\u300a\u673a\u5668\u5b66\u4e60\u300b\u76f8\u5173\u89c6\u9891\uff0c\u5916\u52a0\u56fd\u5185\u672c\u571f\u98ce\u683c\u7684\u6559\u7a0b: 7\u6708+\u5c0f\u8c61 \u7b49\u7b49\uff0c\u6211\u90fd\u5f88\u96be\u53bb\u542c\u61c2\uff0c\u76f4\u5230\u6709\u4e00\u5929\uff0c\u88ab\u4e00\u4e2a\u767e\u5ea6\u7684\u9ad8\u7ea7\u7b97\u6cd5\u5206\u6790\u5e08\u63a8\u8350\u8bf4: \u300a\u673a\u5668\u5b66\u4e60\u5b9e\u6218\u300b\u8fd8\u4e0d\u9519\uff0c\u901a\u4fd7\u6613\u61c2\uff0c\u4f60\u53bb\u8bd5\u8bd5\uff1f\uff1f\n\n\u6211\u8bd5\u4e86\u8bd5\uff0c\u8fd8\u597d\u6211\u7684Python\u57fa\u7840\u548c\u8c03\u8bd5\u80fd\u529b\u8fd8\u4e0d\u9519\uff0c\u57fa\u672c\u4e0a\u4ee3\u7801\u90fd\u8c03\u8bd5\u8fc7\u4e00\u904d\uff0c\u5f88\u591a\u9ad8\u5927\u4e0a\u7684 \"\u7406\u8bba+\u63a8\u5bfc\"\uff0c\u5728\u6211\u773c\u4e2d\u53d8\u6210\u4e86\u51e0\u4e2a \"\u52a0\u51cf\u4e58\u9664+\u5faa\u73af\"\uff0c\u6211\u60f3\u8fd9\u4e0d\u5c31\u662f\u50cf\u6211\u8fd9\u6837\u7684\u7a0b\u5e8f\u5458\u60f3\u8981\u7684\u5165\u95e8\u6559\u7a0b\u4e48\uff1f\n\n\u5f88\u591a\u7a0b\u5e8f\u5458\u8bf4\u673a\u5668\u5b66\u4e60 TM \u592a\u96be\u5b66\u4e86\uff0c\u662f\u7684\uff0c\u771f TM \u96be\u5b66\uff0c\u6211\u60f3\u6700\u96be\u7684\u662f: \u6ca1\u6709\u4e00\u672c\u50cf\u300a\u673a\u5668\u5b66\u4e60\u5b9e\u6218\u300b\u90a3\u6837\u7684\u4f5c\u8005\u613f\u610f\u4ee5\u7a0b\u5e8f\u5458 Coding \u89d2\u5ea6\u53bb\u7ed9\u5927\u5bb6\u8bb2\u89e3\uff01\uff01\n\n\u6700\u8fd1\u51e0\u5929\uff0cGitHub \u6da8\u4e86 300\u9897 star\uff0c\u52a0\u7fa4\u7684200\u4eba\uff0c \u73b0\u5728\u8fd8\u5728\u4e0d\u65ad\u7684\u589e\u52a0++\uff0c\u6211\u60f3\u5927\u5bb6\u53ef\u80fd\u90fd\u662f\u611f\u540c\u8eab\u53d7\u5427\uff01\n\n\u5f88\u591a\u60f3\u5165\u95e8\u65b0\u624b\u5c31\u662f\u88ab\u5ffd\u60a0\u7740\u6536\u85cf\u6536\u85cf\u518d\u6536\u85cf\uff0c\u4f46\u662f\u6700\u540e\u8fd8\u662f\u4ec0\u4e48\u90fd\u6ca1\u6709\u5b66\u5230\uff0c\u4e5f\u5c31\u662f\"\u8d44\u6e90\u6536\u85cf\u5bb6\"\uff0c\u4e5f\u8bb8\u65b0\u624b\u8981\u7684\u5c31\u662f [MachineLearning(\u673a\u5668\u5b66\u4e60) \u5b66\u4e60\u8def\u7ebf\u56fe](https:/docs.apachecn.org/map)\u3002\u6ca1\u9519\uff0c\u6211\u53ef\u4ee5\u7ed9\u4f60\u4eec\u7684\u4e00\u4efd\uff0c\u56e0\u4e3a\u6211\u4eec\u8fd8\u901a\u8fc7\u89c6\u9891\u8bb0\u5f55\u4e0b\u6765\u6211\u4eec\u7684\u5b66\u4e60\u8fc7\u7a0b\u3002\u6c34\u5e73\u5f53\u7136\u4e5f\u6709\u9650\uff0c\u4e0d\u8fc7\u5bf9\u4e8e\u65b0\u624b\u5165\u95e8\uff0c\u7edd\u5bf9\u6ca1\u95ee\u9898\uff0c\u5982\u679c\u4f60\u8fd8\u4e0d\u4f1a\uff0c\u90a3\u7b97\u6211\u8f93\uff01\uff01\n\n> \u89c6\u9891\u600e\u4e48\u770b\uff1f\n\n![](img/ApacheCN-ML-bilibili-compare.jpg)\n\n1. \u7406\u8bba\u79d1\u73ed\u51fa\u8eab-\u5efa\u8bae\u53bb\u5b66\u4e60 Andrew Ng \u7684\u89c6\u9891\uff08Ng \u7684\u89c6\u9891\u7edd\u5bf9\u662f\u6743\u5a01\uff0c\u8fd9\u4e2a\u6bcb\u5eb8\u7f6e\u7591\uff09\n2. \u7f16\u7801\u80fd\u529b\u5f3a - \u5efa\u8bae\u770b\u6211\u4eec\u7684[\u300a\u673a\u5668\u5b66\u4e60\u5b9e\u6218-\u6559\u5b66\u7248\u300b](https://space.bilibili.com/97678687/channel/collectiondetail?sid=707585)\n3. \u7f16\u7801\u80fd\u529b\u5f31 - \u5efa\u8bae\u770b\u6211\u4eec\u7684[\u300a\u673a\u5668\u5b66\u4e60\u5b9e\u6218-\u8ba8\u8bba\u7248\u300b](https://space.bilibili.com/97678687/channel/collectiondetail?sid=707596)\uff0c\u4e0d\u8fc7\u5728\u770b\u7406\u8bba\u7684\u65f6\u5019\uff0c\u770b \u6559\u5b66\u7248-\u7406\u8bba\u90e8\u5206\uff1b\u8ba8\u8bba\u7248\u7684\u5e9f\u8bdd\u592a\u591a\uff0c\u4e0d\u8fc7\u5728\u8bb2\u89e3\u4ee3\u7801\u7684\u65f6\u5019\u662f\u4e00\u884c\u4e00\u884c\u8bb2\u89e3\u7684\uff1b\u6240\u4ee5\uff0c\u6839\u636e\u81ea\u5df1\u7684\u9700\u6c42\uff0c\u81ea\u7531\u7684\u7ec4\u5408\u3002\n\n> \u3010\u514d\u8d39\u3011\u6570\u5b66\u6559\u5b66\u89c6\u9891 - \u53ef\u6c57\u5b66\u9662 \u5165\u95e8\u7bc7\n\n* [@\u4e8e\u632f\u6893]() \u63a8\u8350: \u53ef\u6c57\u5b66\u9662-\u7f51\u6613\u516c\u5f00\u8bfe\n\n| \u6982\u7387 | \u7edf\u8ba1 | \u7ebf\u6027\u4ee3\u6570 |\n| - | - | - |\n| [\u53ef\u6c57\u5b66\u9662(\u6982\u7387)](http://open.163.com/special/Khan/probability.html)  | [\u53ef\u6c57\u5b66\u9662(\u7edf\u8ba1\u5b66)](http://open.163.com/special/Khan/khstatistics.html)| [\u53ef\u6c57\u5b66\u9662(\u7ebf\u6027\u4ee3\u6570)](http://open.163.com/special/Khan/linearalgebra.html)\n\n> \u673a\u5668\u5b66\u4e60\u89c6\u9891 - ApacheCN \u6559\u5b66\u7248\n\n|||\n| - | - |\n| AcFun | B\u7ad9 |\n| <a title=\"AcFun\uff08\u673a\u5668\u5b66\u4e60\u89c6\u9891\uff09\" href=\"http://www.acfun.cn/u/12540256.aspx#page=1\" target=\"_blank\"><img width=\"290\" src=\"/docs/img/ApacheCN-ML-AcFun.jpg\"></a> | <a title=\"bilibili\uff08\u673a\u5668\u5b66\u4e60\u89c6\u9891\uff09\" href=\"https://space.bilibili.com/97678687/channel/collectiondetail?sid=707585\" target=\"_blank\"><img width=\"290\" src=\"/docs/img/ApacheCN-ML-bilibili.jpg\"></a> |\n| \u4f18\u9177 | \u7f51\u6613\u4e91\u8bfe\u5802 |\n| <a title=\"YouKu\uff08\u673a\u5668\u5b66\u4e60\u89c6\u9891\uff09\" href=\"http://i.youku.com/apachecn\" target=\"_blank\"><img width=\"290\" src=\"/docs/img/ApacheCM-ML-youku.jpg\"></a> | <a title=\"WangYiYunKeTang\uff08\u673a\u5668\u5b66\u4e60\u89c6\u9891\uff09\" href=\"http://study.163.com/course/courseMain.htm?courseId=1004582003\" target=\"_blank\"><img width=\"290\" src=\"/docs/img/ApacheCM-ML-WangYiYunKeTang.png\"></a> |\n\n> \u3010\u514d\u8d39\u3011\u673a\u5668/\u6df1\u5ea6\u5b66\u4e60\u89c6\u9891 - \u5434\u6069\u8fbe\n\n| \u673a\u5668\u5b66\u4e60 | \u6df1\u5ea6\u5b66\u4e60 |\n| - | - |\n| [\u5434\u6069\u8fbe\u673a\u5668\u5b66\u4e60](http://study.163.com/course/courseMain.htm?courseId=1004570029) | [\u795e\u7ecf\u7f51\u7edc\u548c\u6df1\u5ea6\u5b66\u4e60](http://mooc.study.163.com/course/2001281002?tid=2001392029) |\n\n\n## 2.\u6df1\u5ea6\u5b66\u4e60\n\n> \u652f\u6301\u7248\u672c \n\n| Version | Supported          |\n| ------- | ------------------ |\n| 3.6.x   | :white_check_mark: |\n| 2.7.x   | :x:                |\n\n### \u5165\u95e8\u57fa\u7840\n\n1. [\u53cd\u5411\u4f20\u9012](/docs/dl/\u53cd\u5411\u4f20\u9012.md): https://www.cnblogs.com/charlotte77/p/5629865.html\n2. [CNN\u539f\u7406](/docs/dl/CNN\u539f\u7406.md): http://www.cnblogs.com/charlotte77/p/7759802.html\n3. [RNN\u539f\u7406](/docs/dl/RNN\u539f\u7406.md): https://blog.csdn.net/qq_39422642/article/details/78676567\n4. [LSTM\u539f\u7406](/docs/dl/LSTM\u539f\u7406.md): https://blog.csdn.net/weixin_42111770/article/details/80900575\n\n### Pytorch - \u6559\u7a0b\n\n-- \u5f85\u66f4\u65b0\n\n### TensorFlow 2.0 - \u6559\u7a0b\n\n-- \u5f85\u66f4\u65b0\n\n> \u76ee\u5f55\u7ed3\u6784:\n\n* [\u5b89\u88c5\u6307\u5357](/docs/TensorFlow2.x/\u5b89\u88c5\u6307\u5357.md)\n* [Keras \u5feb\u901f\u5165\u95e8](/docs/TensorFlow2.x/Keras\u5feb\u901f\u5165\u95e8.md)\n* [\u5b9e\u6218\u9879\u76ee 1 \u7535\u5f71\u60c5\u611f\u5206\u7c7b](/docs/TensorFlow2.x/\u5b9e\u6218\u9879\u76ee_1_\u7535\u5f71\u60c5\u611f\u5206\u7c7b.md)\n* [\u5b9e\u6218\u9879\u76ee 2 \u6c7d\u8f66\u71c3\u6cb9\u6548\u7387](/docs/TensorFlow2.x/\u5b9e\u6218\u9879\u76ee_2_\u6c7d\u8f66\u71c3\u6cb9\u6548\u7387.md)\n* [\u5b9e\u6218\u9879\u76ee 3 \u4f18\u5316 \u8fc7\u62df\u5408\u548c\u6b20\u62df\u5408](/docs/TensorFlow2.x/\u5b9e\u6218\u9879\u76ee_3_\u4f18\u5316_\u8fc7\u62df\u5408\u548c\u6b20\u62df\u5408.md)\n* [\u5b9e\u6218\u9879\u76ee 4 \u53e4\u8bd7\u8bcd\u81ea\u52a8\u751f\u6210](/docs/TensorFlow2.x/\u5b9e\u6218\u9879\u76ee_4_\u53e4\u8bd7\u8bcd\u81ea\u52a8\u751f\u6210.md)\n\n\u5207\u5206\uff08\u5206\u8bcd\uff09\n\n\u8bcd\u6027\u6807\u6ce8\n\n\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\n\n\u53e5\u6cd5\u5206\u6790\n\nWordNet\u53ef\u4ee5\u88ab\u770b\u4f5c\u662f\u4e00\u4e2a\u540c\u4e49\u8bcd\u8bcd\u5178\n\n\u8bcd\u5e72\u63d0\u53d6\uff08stemming\uff09\u4e0e\u8bcd\u5f62\u8fd8\u539f\uff08lemmatization\uff09\n\n* https://www.biaodianfu.com/nltk.html/amp\n\nTensorFlow 2.0\u5b66\u4e60\u7f51\u5740\n* https://github.com/lyhue1991/eat_tensorflow2_in_30_days\n\n## 3.\u81ea\u7136\u8bed\u8a00\u5904\u7406\n\n> \u652f\u6301\u7248\u672c \n\n| Version | Supported          |\n| ------- | ------------------ |\n| 3.6.x   | :white_check_mark: |\n| 2.7.x   | :x:                |\n\n\u5b66\u4e60\u8fc7\u7a0b\u4e2d-\u5185\u5fc3\u590d\u6742\u7684\u53d8\u5316\uff01\uff01\uff01\n\n```python\n\u81ea\u4ece\u5b66\u4e60NLP\u4ee5\u540e\uff0c\u624d\u53d1\u73b0\u56fd\u5185\u4e0e\u56fd\u5916\u7684\u5178\u578b\u533a\u522b:\n1. \u5bf9\u8d44\u6e90\u7684\u6001\u5ea6\u662f\u5b8c\u5168\u76f8\u53cd\u7684:\n  1) \u56fd\u5185: \u5c31\u597d\u50cf\u4e3a\u4e86\u540d\u6c14\uff0c\u4e3e\u529e\u5de5\u4f5c\u88c5\u903c\u7684\u4f1a\u8bae\uff0c\u5c31\u662f\u6ca1\u6709\u5e72\u8d27\uff0c\u5168\u90e8\u90fd\u662f\u8c61\u5f81\u6027\u7684PPT\u4ecb\u7ecd\uff0c\u4e0d\u662f\u9488\u5bf9\u5728\u505a\u7684\u5404\u4f4d\n  2\uff09\u56fd\u5916: \u5c31\u597d\u50cf\u662f\u4e3a\u4e86\u63a8\u52a8nlp\u8fdb\u6b65\u4e00\u6837\uff0c\u5206\u4eab\u8005\u5404\u79cd\u5e72\u8d27\u8d44\u6599\u548c\u5177\u4f53\u7684\u5b9e\u73b0\u3002\uff08\u7279\u522b\u662f: python\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff09\n2. \u8bba\u6587\u7684\u5b9e\u73b0: \n  1) \u5404\u79cd\u9ad8\u5927\u4e0a\u7684\u8bba\u6587\u5b9e\u73b0\uff0c\u5374\u8fd8\u662f\u6ca1\u770b\u5230\u4e00\u4e2a\u50cf\u6837\u7684GitHub\u9879\u76ee\uff01\uff08\u53ef\u80fd\u6211\u7684\u641c\u7d22\u80fd\u529b\u5dee\u4e86\u70b9\uff0c\u4e00\u76f4\u6ca1\u627e\u5230\uff09\n  2\uff09\u56fd\u5916\u5c31\u4e0d\u4e3e\u4f8b\u4e86\uff0c\u6211\u770b\u4e0d\u61c2\uff01\n3. \u5f00\u6e90\u7684\u6846\u67b6\n  1\uff09\u56fd\u5916\u7684\u5f00\u6e90\u6846\u67b6:  tensorflow/pytorch \u6587\u6863+\u6559\u7a0b+\u89c6\u9891\uff08\u5b98\u65b9\u63d0\u4f9b\uff09\n  2) \u56fd\u5185\u7684\u5f00\u6e90\u6846\u67b6: \u989d\u989d\uff0c\u8fd8\u771f\u4e3e\u4f8b\u4e0d\u51fa\u6765\uff01\u4f46\u662f\u725b\u903c\u5439\u5f97\u4e0d\u6bd4\u56fd\u5916\u5dee\uff01\uff08MXNet\u867d\u7136\u6709\u4f17\u591a\u56fd\u4eba\u53c2\u4e0e\u5f00\u53d1\uff0c\u4f46\u4e0d\u80fd\u7b97\u662f\u56fd\u5185\u5f00\u6e90\u6846\u67b6\u3002\u57fa\u4e8eMXNet\u7684\u52a8\u624b\u5b66\u6df1\u5ea6\u5b66\u4e60(http://zh.d2l.ai & https://discuss.gluon.ai/t/topic/753)\u4e2d\u6587\u6559\u7a0b,\u5df2\u7ecf\u7531\u6c90\u795e(\u674e\u6c90)\u4ee5\u53ca\u963f\u65af\u987f\u00b7\u5f20\u8bb2\u6388\u5f55\u5236\uff0c\u516c\u5f00\u53d1\u5e03(\u6587\u6863+\u7b2c\u4e00\u5b63\u6559\u7a0b+\u89c6\u9891\uff09\u3002)\n\u6bcf\u4e00\u6b21\u6df1\u5165\u90fd\u8981\u53bb\u7ffb\u5899\uff0c\u6bcf\u4e00\u6b21\u6df1\u5165\u90fd\u8981Google\uff0c\u6bcf\u4e00\u6b21\u770b\u7740\u56fd\u5185\u7684\u8bf4: \u54c8\u5de5\u5927\u3001\u8baf\u98de\u3001\u4e2d\u79d1\u5927\u3001\u767e\u5ea6\u3001\u963f\u91cc\u591a\u725b\u903c\uff0c\u4f46\u662f\u8d44\u6599\u8fd8\u662f\u5f97\u56fd\u5916\u53bb\u627e\uff01\n\u6709\u65f6\u5019\u771f\u7684\u633a\u6068\u7684\uff01\u771f\u7684\u6709\u70b9\u77a7\u4e0d\u8d77\u81ea\u5df1\u56fd\u5185\u7684\u6280\u672f\u73af\u5883\uff01\n\n\u5f53\u7136\u8c22\u8c22\u56fd\u5185\u5f88\u591a\u535a\u5ba2\u5927\u4f6c\uff0c\u7279\u522b\u662f\u4e00\u4e9b\u5165\u95e8\u7684Demo\u548c\u57fa\u672c\u6982\u5ff5\u3002\u3010\u6df1\u5165\u7684\u6c34\u5e73\u6709\u9650\uff0c\u6ca1\u770b\u61c2\u3011\n```\n\n![](nlp/img/F94581F64C21A1094A473397DFA42F9C.jpg)\n\n* **\u3010\u5165\u95e8\u987b\u77e5\u3011\u5fc5\u987b\u4e86\u89e3**: <https://github.com/apachecn/AiLearning/tree/master/nlp>\n* **\u3010\u5165\u95e8\u6559\u7a0b\u3011\u5f3a\u70c8\u63a8\u8350: PyTorch \u81ea\u7136\u8bed\u8a00\u5904\u7406**: <https://github.com/apachecn/NLP-with-PyTorch>\n* Python \u81ea\u7136\u8bed\u8a00\u5904\u7406 \u7b2c\u4e8c\u7248: <https://usyiyi.github.io/nlp-py-2e-zh>\n* \u63a8\u8350\u4e00\u4e2a[liuhuanyong\u5927\u4f6c](https://github.com/liuhuanyong)\u6574\u7406\u7684nlp\u5168\u9762\u77e5\u8bc6\u4f53\u7cfb: <https://liuhuanyong.github.io>\n* \u5f00\u6e90 - \u8bcd\u5411\u91cf\u5e93\u96c6\u5408: \n  * <https://www.cnblogs.com/Darwin2000/p/5786984.html>\n  * <https://ai.tencent.com/ailab/nlp/embedding.html>\n  * <https://blog.csdn.net/xiezj007/article/details/85073890>\n  * <https://github.com/Embedding/Chinese-Word-Vectors>\n  * <https://github.com/brightmart/nlp_chinese_corpus>\n  * <https://github.com/codemayq/chinese_chatbot_corpus>\n  * <https://github.com/candlewill/Dialog_Corpus>\n\n\n### 1.\u4f7f\u7528\u573a\u666f \uff08\u767e\u5ea6\u516c\u5f00\u8bfe\uff09\n\n> \u7b2c\u4e00\u90e8\u5206 \u5165\u95e8\u4ecb\u7ecd\n\n* 1.) [\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5165\u95e8\u4ecb\u7ecd](/docs/nlp/1.\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5165\u95e8\u4ecb\u7ecd.md)\n\n> \u7b2c\u4e8c\u90e8\u5206 \u673a\u5668\u7ffb\u8bd1\n\n* 2.) [\u673a\u5668\u7ffb\u8bd1](/nlp/2.\u673a\u5668\u7ffb\u8bd1.md)\n\n> \u7b2c\u4e09\u90e8\u5206 \u7bc7\u7ae0\u5206\u6790\n\n* 3.1.) [\u7bc7\u7ae0\u5206\u6790-\u5185\u5bb9\u6982\u8ff0](/docs/nlp/3.1.\u7bc7\u7ae0\u5206\u6790-\u5185\u5bb9\u6982\u8ff0.md)\n* 3.2.) [\u7bc7\u7ae0\u5206\u6790-\u5185\u5bb9\u6807\u7b7e](/docs/nlp/3.2.\u7bc7\u7ae0\u5206\u6790-\u5185\u5bb9\u6807\u7b7e.md)\n* 3.3.) [\u7bc7\u7ae0\u5206\u6790-\u60c5\u611f\u5206\u6790](/docs/nlp/3.3.\u7bc7\u7ae0\u5206\u6790-\u60c5\u611f\u5206\u6790.md)\n* 3.4.) [\u7bc7\u7ae0\u5206\u6790-\u81ea\u52a8\u6458\u8981](/docs/nlp/3.4.\u7bc7\u7ae0\u5206\u6790-\u81ea\u52a8\u6458\u8981.md)\n\n> \u7b2c\u56db\u90e8\u5206 UNIT-\u8bed\u8a00\u7406\u89e3\u4e0e\u4ea4\u4e92\u6280\u672f\n\n* 4.) [UNIT-\u8bed\u8a00\u7406\u89e3\u4e0e\u4ea4\u4e92\u6280\u672f](/docs/nlp/4.UNIT-\u8bed\u8a00\u7406\u89e3\u4e0e\u4ea4\u4e92\u6280\u672f.md)\n\n### \u5e94\u7528\u9886\u57df\n\n#### \u4e2d\u6587\u5206\u8bcd: \n\n* \u6784\u5efaDAG\u56fe\n* \u52a8\u6001\u89c4\u5212\u67e5\u627e\uff0c\u7efc\u5408\u6b63\u53cd\u5411\uff08\u6b63\u5411\u52a0\u6743\u53cd\u5411\u8f93\u51fa\uff09\u6c42\u5f97DAG\u6700\u5927\u6982\u7387\u8def\u5f84\n* \u4f7f\u7528\u4e86SBME\u8bed\u6599\u8bad\u7ec3\u4e86\u4e00\u5957 HMM + Viterbi \u6a21\u578b\uff0c\u89e3\u51b3\u672a\u767b\u5f55\u8bcd\u95ee\u9898\n\n#### 1.\u6587\u672c\u5206\u7c7b\uff08Text Classification\uff09\n\n\u6587\u672c\u5206\u7c7b\u662f\u6307\u6807\u8bb0\u53e5\u5b50\u6216\u6587\u6863\uff0c\u4f8b\u5982\u7535\u5b50\u90ae\u4ef6\u5783\u573e\u90ae\u4ef6\u5206\u7c7b\u548c\u60c5\u611f\u5206\u6790\u3002\n\n\u4e0b\u9762\u662f\u4e00\u4e9b\u5f88\u597d\u7684\u521d\u5b66\u8005\u6587\u672c\u5206\u7c7b\u6570\u636e\u96c6\u3002\n\n1. [\u8def\u900f\u793eNewswire\u4e3b\u9898\u5206\u7c7b](http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html)\uff08\u8def\u900f\u793e-21578\uff09\u30021987\u5e74\u8def\u900f\u793e\u51fa\u73b0\u7684\u4e00\u7cfb\u5217\u65b0\u95fb\u6587\u4ef6\uff0c\u6309\u7c7b\u522b\u7f16\u5236\u7d22\u5f15\u3002[\u53e6\u89c1RCV1\uff0cRCV2\u548cTRC2](http://trec.nist.gov/data/reuters/reuters.html)\u3002\n2. [IMDB\u7535\u5f71\u8bc4\u8bba\u60c5\u611f\u5206\u7c7b\uff08\u65af\u5766\u798f\uff09](http://ai.stanford.edu/~amaas/data/sentiment)\u3002\u6765\u81ea\u7f51\u7ad9imdb.com\u7684\u4e00\u7cfb\u5217\u7535\u5f71\u8bc4\u8bba\u53ca\u5176\u79ef\u6781\u6216\u6d88\u6781\u7684\u60c5\u7eea\u3002\n3. [\u65b0\u95fb\u7ec4\u7535\u5f71\u8bc4\u8bba\u60c5\u611f\u5206\u7c7b\uff08\u5eb7\u5948\u5c14\uff09](http://www.cs.cornell.edu/people/pabo/movie-review-data/)\u3002\u6765\u81ea\u7f51\u7ad9imdb.com\u7684\u4e00\u7cfb\u5217\u7535\u5f71\u8bc4\u8bba\u53ca\u5176\u79ef\u6781\u6216\u6d88\u6781\u7684\u60c5\u7eea\u3002\n\n\u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u5e16\u5b50: \n[\u5355\u6807\u7b7e\u6587\u672c\u5206\u7c7b\u7684\u6570\u636e\u96c6](http://ana.cachopo.org/datasets-for-single-label-text-categorization)\u3002\n\n> \u60c5\u611f\u5206\u6790\n\n\u6bd4\u8d5b\u5730\u5740: https://www.kaggle.com/c/word2vec-nlp-tutorial\n\n* \u65b9\u6848\u4e00(0.86): WordCount + \u6734\u7d20 Bayes\n* \u65b9\u6848\u4e8c(0.94): LDA + \u5206\u7c7b\u6a21\u578b\uff08knn/\u51b3\u7b56\u6811/\u903b\u8f91\u56de\u5f52/svm/xgboost/\u968f\u673a\u68ee\u6797\uff09\n  * a) \u51b3\u7b56\u6811\u6548\u679c\u4e0d\u662f\u5f88\u597d\uff0c\u8fd9\u79cd\u8fde\u7eed\u7279\u5f81\u4e0d\u592a\u9002\u5408\u7684\n  * b) \u901a\u8fc7\u53c2\u6570\u8c03\u6574 200 \u4e2atopic\uff0c\u4fe1\u606f\u91cf\u4fdd\u5b58\u6548\u679c\u8f83\u4f18\uff08\u8ba1\u7b97\u4e3b\u9898\uff09\n* \u65b9\u6848\u4e09(0.72): word2vec + CNN\n  * \u8bf4\u5b9e\u8bdd: \u6ca1\u6709\u4e00\u4e2a\u597d\u7684\u673a\u5668\uff0c\u662f\u8c03\u4e0d\u51fa\u6765\u4e00\u4e2a\u597d\u7684\u7ed3\u679c (: \u9003\n\n**\u901a\u8fc7AUC \u6765\u8bc4\u4f30\u6a21\u578b\u7684\u6548\u679c**\n\n#### 2.\u8bed\u8a00\u6a21\u578b\uff08Language Modeling\uff09\n\n\u8bed\u8a00\u5efa\u6a21\u6d89\u53ca\u5f00\u53d1\u4e00\u79cd\u7edf\u8ba1\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u53e5\u5b50\u4e2d\u7684\u4e0b\u4e00\u4e2a\u5355\u8bcd\u6216\u4e00\u4e2a\u5355\u8bcd\u4e2d\u7684\u4e0b\u4e00\u4e2a\u5355\u8bcd\u3002\u5b83\u662f\u8bed\u97f3\u8bc6\u522b\u548c\u673a\u5668\u7ffb\u8bd1\u7b49\u4efb\u52a1\u4e2d\u7684\u524d\u7f6e\u4efb\u52a1\u3002\n\n\u5b83\u662f\u8bed\u97f3\u8bc6\u522b\u548c\u673a\u5668\u7ffb\u8bd1\u7b49\u4efb\u52a1\u4e2d\u7684\u524d\u7f6e\u4efb\u52a1\u3002\n\n\u4e0b\u9762\u662f\u4e00\u4e9b\u5f88\u597d\u7684\u521d\u5b66\u8005\u8bed\u8a00\u5efa\u6a21\u6570\u636e\u96c6\u3002\n\n1. [\u53e4\u817e\u5821\u9879\u76ee](https://www.gutenberg.org/)\uff0c\u4e00\u7cfb\u5217\u514d\u8d39\u4e66\u7c4d\uff0c\u53ef\u4ee5\u7528\u7eaf\u6587\u672c\u68c0\u7d22\u5404\u79cd\u8bed\u8a00\u3002\n2. \u8fd8\u6709\u66f4\u591a\u6b63\u5f0f\u7684\u8bed\u6599\u5e93\u5f97\u5230\u4e86\u5f88\u597d\u7684\u7814\u7a76; \u4f8b\u5982: \n    [\u5e03\u6717\u5927\u5b66\u73b0\u4ee3\u7f8e\u56fd\u82f1\u8bed\u6807\u51c6\u8bed\u6599\u5e93](https://en.wikipedia.org/wiki/Brown_Corpus)\u3002\u5927\u91cf\u82f1\u8bed\u5355\u8bcd\u6837\u672c\u3002\n    [\u8c37\u6b4c10\u4ebf\u5b57\u8bed\u6599\u5e93](https://github.com/ciprian-chelba/1-billion-word-language-modeling-benchmark)\u3002\n\n> \u65b0\u8bcd\u53d1\u73b0\n\n* \u4e2d\u6587\u5206\u8bcd\u65b0\u8bcd\u53d1\u73b0\n* python3\u5229\u7528\u4e92\u4fe1\u606f\u548c\u5de6\u53f3\u4fe1\u606f\u71b5\u7684\u4e2d\u6587\u5206\u8bcd\u65b0\u8bcd\u53d1\u73b0\n* <https://github.com/zhanzecheng/Chinese_segment_augment>\n\n> \u53e5\u5b50\u76f8\u4f3c\u5ea6\u8bc6\u522b\n\n* \u9879\u76ee\u5730\u5740: https://www.kaggle.com/c/quora-question-pairs\n* \u89e3\u51b3\u65b9\u6848: word2vec + Bi-GRU\n\n> \u6587\u672c\u7ea0\u9519\n\n* bi-gram + levenshtein\n\n#### 3.\u56fe\u50cf\u5b57\u5e55\uff08Image Captioning\uff09\n\nmage\u5b57\u5e55\u662f\u4e3a\u7ed9\u5b9a\u56fe\u50cf\u751f\u6210\u6587\u672c\u63cf\u8ff0\u7684\u4efb\u52a1\u3002\n\n\u4e0b\u9762\u662f\u4e00\u4e9b\u5f88\u597d\u7684\u521d\u5b66\u8005\u56fe\u50cf\u5b57\u5e55\u6570\u636e\u96c6\u3002\n\n1. [\u4e0a\u4e0b\u6587\u4e2d\u7684\u516c\u5171\u5bf9\u8c61\uff08COCO\uff09](http://mscoco.org/dataset/#overview)\u3002\u5305\u542b\u8d85\u8fc712\u4e07\u5f20\u5e26\u63cf\u8ff0\u7684\u56fe\u50cf\u7684\u96c6\u5408\n2. [Flickr 8K](http://nlp.cs.illinois.edu/HockenmaierGroup/8k-pictures.html)\u3002\u4eceflickr.com\u83b7\u53d6\u76848\u5343\u4e2a\u63cf\u8ff0\u56fe\u50cf\u7684\u96c6\u5408\u3002\n3. [Flickr 30K](http://shannon.cs.illinois.edu/DenotationGraph/)\u3002\u4eceflickr.com\u83b7\u53d6\u76843\u4e07\u4e2a\u63cf\u8ff0\u56fe\u50cf\u7684\u96c6\u5408\u3002\n    \u6b32\u4e86\u89e3\u66f4\u591a\uff0c\u8bf7\u770b\u5e16\u5b50: \n\n[\u63a2\u7d22\u56fe\u50cf\u5b57\u5e55\u6570\u636e\u96c6\uff0c2016\u5e74](http://sidgan.me/technical/2016/01/09/Exploring-Datasets)\n\n#### 4.\u673a\u5668\u7ffb\u8bd1\uff08Machine Translation\uff09\n\n\u673a\u5668\u7ffb\u8bd1\u662f\u5c06\u6587\u672c\u4ece\u4e00\u79cd\u8bed\u8a00\u7ffb\u8bd1\u6210\u53e6\u4e00\u79cd\u8bed\u8a00\u7684\u4efb\u52a1\u3002\n\n\u4e0b\u9762\u662f\u4e00\u4e9b\u5f88\u597d\u7684\u521d\u5b66\u8005\u673a\u5668\u7ffb\u8bd1\u6570\u636e\u96c6\u3002\n\n1. [\u52a0\u62ff\u5927\u7b2c36\u5c4a\u8bae\u4f1a\u7684\u534f\u8c03\u56fd\u4f1a\u8bae\u5458](https://www.isi.edu/natural-language/download/hansard/)\u3002\u6210\u5bf9\u7684\u82f1\u8bed\u548c\u6cd5\u8bed\u53e5\u5b50\u3002\n2. [\u6b27\u6d32\u8bae\u4f1a\u8bc9\u8bbc\u5e73\u884c\u8bed\u6599\u5e931996-2011](http://www.statmt.org/europarl/)\u3002\u53e5\u5b50\u5bf9\u4e00\u5957\u6b27\u6d32\u8bed\u8a00\u3002\n    \u6709\u5927\u91cf\u6807\u51c6\u6570\u636e\u96c6\u7528\u4e8e\u5e74\u5ea6\u673a\u5668\u7ffb\u8bd1\u6311\u6218; \u770b\u5230: \n\n[\u7edf\u8ba1\u673a\u5668\u7ffb\u8bd1](http://www.statmt.org/)\n\n> \u673a\u5668\u7ffb\u8bd1\n\n* Encoder + Decoder(Attention)\n* \u53c2\u8003\u6848\u4f8b: http://pytorch.apachecn.org/cn/tutorials/intermediate/seq2seq_translation_tutorial.html\n\n#### 5.\u95ee\u7b54\u7cfb\u7edf\uff08Question Answering\uff09\n\n\u95ee\u7b54\u662f\u4e00\u9879\u4efb\u52a1\uff0c\u5176\u4e2d\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53e5\u5b50\u6216\u6587\u672c\u6837\u672c\uff0c\u4ece\u4e2d\u63d0\u51fa\u95ee\u9898\u5e76\u4e14\u5fc5\u987b\u56de\u7b54\u95ee\u9898\u3002\n\n\u4e0b\u9762\u662f\u4e00\u4e9b\u5f88\u597d\u7684\u521d\u5b66\u8005\u95ee\u9898\u56de\u7b54\u6570\u636e\u96c6\u3002\n\n1. [\u65af\u5766\u798f\u95ee\u9898\u56de\u7b54\u6570\u636e\u96c6\uff08SQuAD\uff09](https://rajpurkar.github.io/SQuAD-explorer/)\u3002\u56de\u7b54\u6709\u5173\u7ef4\u57fa\u767e\u79d1\u6587\u7ae0\u7684\u95ee\u9898\u3002\n2. [Deepmind\u95ee\u9898\u56de\u7b54\u8bed\u6599\u5e93](https://github.com/deepmind/rc-data)\u3002\u4ece\u6bcf\u65e5\u90ae\u62a5\u56de\u7b54\u6709\u5173\u65b0\u95fb\u6587\u7ae0\u7684\u95ee\u9898\u3002\n3. [\u4e9a\u9a6c\u900a\u95ee\u7b54\u6570\u636e](http://jmcauley.ucsd.edu/data/amazon/qa/)\u3002\u56de\u7b54\u6709\u5173\u4e9a\u9a6c\u900a\u4ea7\u54c1\u7684\u95ee\u9898\u3002\n    \u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u5e16\u5b50: \n\n[\u6570\u636e\u96c6: \u6211\u5982\u4f55\u83b7\u5f97\u95ee\u7b54\u7f51\u7ad9\u7684\u8bed\u6599\u5e93\uff0c\u5982Quora\u6216Yahoo Answers\u6216Stack Overflow\u6765\u5206\u6790\u7b54\u6848\u8d28\u91cf\uff1f](https://www.quora.com/Datasets-How-can-I-get-corpus-of-a-question-answering-website-like-Quora-or-Yahoo-Answers-or-Stack-Overflow-for-analyzing-answer-quality)\n\n#### 6.\u8bed\u97f3\u8bc6\u522b\uff08Speech Recognition\uff09\n\n\u8bed\u97f3\u8bc6\u522b\u662f\u5c06\u53e3\u8bed\u7684\u97f3\u9891\u8f6c\u6362\u4e3a\u4eba\u7c7b\u53ef\u8bfb\u6587\u672c\u7684\u4efb\u52a1\u3002\n\n\u4e0b\u9762\u662f\u4e00\u4e9b\u5f88\u597d\u7684\u521d\u5b66\u8005\u8bed\u97f3\u8bc6\u522b\u6570\u636e\u96c6\u3002\n\n1. [TIMIT\u58f0\u5b66 - \u8bed\u97f3\u8fde\u7eed\u8bed\u97f3\u8bed\u6599\u5e93](https://catalog.ldc.upenn.edu/LDC93S1)\u3002\u4e0d\u662f\u514d\u8d39\u7684\uff0c\u4f46\u56e0\u5176\u5e7f\u6cdb\u4f7f\u7528\u800c\u4e0a\u5e02\u3002\u53e3\u8bed\u7f8e\u56fd\u82f1\u8bed\u548c\u76f8\u5173\u7684\u8f6c\u5f55\u3002\n2. [VoxForge](http://voxforge.org/)\u3002\u7528\u4e8e\u6784\u5efa\u7528\u4e8e\u8bed\u97f3\u8bc6\u522b\u7684\u5f00\u6e90\u6570\u636e\u5e93\u7684\u9879\u76ee\u3002\n3. [LibriSpeech ASR\u8bed\u6599\u5e93](http://www.openslr.org/12/)\u3002\u4eceLibriVox\u6536\u96c6\u7684\u5927\u91cf\u82f1\u8bed\u6709\u58f0\u8bfb\u7269\u3002\n\n#### 7.\u81ea\u52a8\u6587\u6458\uff08Document Summarization\uff09\n\n\u6587\u6863\u6458\u8981\u662f\u521b\u5efa\u8f83\u5927\u6587\u6863\u7684\u7b80\u77ed\u6709\u610f\u4e49\u63cf\u8ff0\u7684\u4efb\u52a1\u3002\n\n\u4e0b\u9762\u662f\u4e00\u4e9b\u5f88\u597d\u7684\u521d\u5b66\u8005\u6587\u6863\u6458\u8981\u6570\u636e\u96c6\u3002\n\n1. [\u6cd5\u5f8b\u6848\u4f8b\u62a5\u544a\u6570\u636e\u96c6](https://archive.ics.uci.edu/ml/datasets/Legal+Case+Reports)\u3002\u6536\u96c6\u4e864000\u4efd\u6cd5\u5f8b\u6848\u4ef6\u53ca\u5176\u6458\u8981\u3002\n2. [TIPSTER\u6587\u672c\u6458\u8981\u8bc4\u4f30\u4f1a\u8bae\u8bed\u6599\u5e93](http://www-nlpir.nist.gov/related_projects/tipster_summac/cmp_lg.html)\u3002\u6536\u96c6\u4e86\u8fd1200\u4efd\u6587\u4ef6\u53ca\u5176\u6458\u8981\u3002\n3. [\u82f1\u8bed\u65b0\u95fb\u6587\u672c\u7684AQUAINT\u8bed\u6599\u5e93](https://catalog.ldc.upenn.edu/LDC2002T31)\u3002\u4e0d\u662f\u514d\u8d39\u7684\uff0c\u800c\u662f\u5e7f\u6cdb\u4f7f\u7528\u7684\u3002\u65b0\u95fb\u6587\u7ae0\u7684\u8bed\u6599\u5e93\u3002\n    \u6b32\u4e86\u89e3\u66f4\u591a\u4fe1\u606f: \n\n[\u6587\u6863\u7406\u89e3\u4f1a\u8bae\uff08DUC\uff09\u4efb\u52a1](http://www-nlpir.nist.gov/projects/duc/data.html)\u3002\n[\u5728\u54ea\u91cc\u53ef\u4ee5\u627e\u5230\u7528\u4e8e\u6587\u672c\u6458\u8981\u7684\u826f\u597d\u6570\u636e\u96c6\uff1f](https://www.quora.com/Where-can-I-find-good-data-sets-for-text-summarization)\n\n> \u547d\u540d\u5b9e\u4f53\u8bc6\u522b\n\n* Bi-LSTM CRF\n* \u53c2\u8003\u6848\u4f8b: http://pytorch.apachecn.org/cn/tutorials/beginner/nlp/advanced_tutorial.html\n* CRF\u63a8\u8350\u6587\u6863: https://www.jianshu.com/p/55755fc649b1\n\n> \u6587\u672c\u6458\u8981\n\n* **\u62bd\u53d6\u5f0f**\n* word2vec + textrank\n* word2vec\u63a8\u8350\u6587\u6863: https://www.zhihu.com/question/44832436/answer/266068967\n* textrank\u63a8\u8350\u6587\u6863: https://blog.csdn.net/BaiHuaXiu123/article/details/77847232\n\n\n## Graph\u56fe\u8ba1\u7b97\u3010\u6162\u6162\u66f4\u65b0\u3011\n\n* \u6570\u636e\u96c6: [https://github.com/apachecn/data/tree/master/graph](https://github.com/apachecn/data/tree/master/graph)\n* \u5b66\u4e60\u8d44\u6599: spark graphX\u5b9e\u6218.pdf \u3010\u6587\u4ef6\u592a\u5927\u4e0d\u65b9\u4fbf\u63d0\u4f9b\uff0c\u81ea\u5df1\u767e\u5ea6\u3011\n\n## \u77e5\u8bc6\u56fe\u8c31\n\n* \u77e5\u8bc6\u56fe\u8c31\uff0c\u6211\u53ea\u8ba4 [SimmerChan](https://www.zhihu.com/people/simmerchan): [\u3010\u77e5\u8bc6\u56fe\u8c31-\u7ed9AI\u88c5\u4e2a\u5927\u8111\u3011](https://zhuanlan.zhihu.com/knowledgegraph)\n* \u8bf4\u5b9e\u8bdd\uff0c\u6211\u662f\u770b\u8fd9\u535a\u4e3b\u8001\u54e5\u5199\u7684\u535a\u5ba2\u957f\u5927\u7684\uff0c\u5199\u7684\u771f\u7684\u662f\u6df1\u5165\u6d45\u51fa\u3002\u6211\u5f88\u559c\u6b22\uff0c\u6240\u4ee5\u5c31\u5206\u4eab\u7ed9\u5927\u5bb6\uff0c\u5e0c\u671b\u4f60\u4eec\u4e5f\u559c\u6b22\u3002\n\n### \u8fdb\u4e00\u6b65\u9605\u8bfb\n\n\u5982\u679c\u60a8\u5e0c\u671b\u66f4\u6df1\u5165\uff0c\u672c\u8282\u63d0\u4f9b\u4e86\u5176\u4ed6\u6570\u636e\u96c6\u5217\u8868\u3002\n\n1. [\u7ef4\u57fa\u767e\u79d1\u7814\u7a76\u4e2d\u4f7f\u7528\u7684\u6587\u672c\u6570\u636e\u96c6](https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research#Text_data)\n2. [\u6570\u636e\u96c6: \u8ba1\u7b97\u8bed\u8a00\u5b66\u5bb6\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7814\u7a76\u4eba\u5458\u4f7f\u7528\u7684\u4e3b\u8981\u6587\u672c\u8bed\u6599\u5e93\u662f\u4ec0\u4e48\uff1f](https://www.quora.com/Datasets-What-are-the-major-text-corpora-used-by-computational-linguists-and-natural-language-processing-researchers-and-what-are-the-characteristics-biases-of-each-corpus)\n3. [\u65af\u5766\u798f\u7edf\u8ba1\u81ea\u7136\u8bed\u8a00\u5904\u7406\u8bed\u6599\u5e93](https://nlp.stanford.edu/links/statnlp.html#Corpora)\n4. [\u6309\u5b57\u6bcd\u987a\u5e8f\u6392\u5217\u7684NLP\u6570\u636e\u96c6\u5217\u8868](https://github.com/niderhoff/nlp-datasets)\n5. [\u8be5\u673a\u6784NLTK](http://www.nltk.org/nltk_data/)\n6. [\u5728DL4J\u4e0a\u6253\u5f00\u6df1\u5ea6\u5b66\u4e60\u6570\u636e](https://deeplearning4j.org/opendata)\n7. [NLP\u6570\u636e\u96c6](https://github.com/caesar0301/awesome-public-datasets#natural-language)\n8. \u56fd\u5185\u5f00\u653e\u6570\u636e\u96c6: https://bosonnlp.com/dev/resource\n\n\n## \u53c2\u8003\n\n* [\u6bd4\u8d5b\u6536\u96c6\u5e73\u53f0](https://github.com/iphysresearch/DataSciComp)\n* [pbharrin/machinelearninginaction](https://github.com/pbharrin/machinelearninginaction)\n* [ML Mastery](https://machinelearningmastery.com/datasets-natural-language-processing)\n\n## \u81f4\u8c22\n\n\u6700\u8fd1\u65e0\u610f\u6536\u5230\u7fa4\u53cb\u63a8\u9001\u7684\u94fe\u63a5\uff0c\u53d1\u73b0\u5f97\u5230\u5927\u4f6c\u9ad8\u5ea6\u7684\u8ba4\u53ef\uff0c\u5e76\u5728\u70ed\u5fc3\u7684\u63a8\u5e7f\u3002\u5728\u6b64\u611f\u8c22:\n\n* [\u91cf\u5b50\u4f4d](https://www.zhihu.com/question/20472776/answer/691646493)\n* [\u4eba\u5de5\u667a\u80fd\u524d\u6cbf\u8bb2\u4e60](https://mp.weixin.qq.com/s/f2dqulxOPkt7k5hqPsydyQ)\n\n## \u8d5e\u52a9\u6211\u4eec\n\n<img src=\"http://data.apachecn.org/img/about/donate.jpg\" alt=\"\u5fae\u4fe1&\u652f\u4ed8\u5b9d\" />\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 51117837,
    "name": "models",
    "full_name": "tensorflow/models",
    "description": "Models and examples built with TensorFlow",
    "html_url": "https://github.com/tensorflow/models",
    "clone_url": "https://github.com/tensorflow/models.git",
    "owner_login": "tensorflow",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/15658638?v=4",
    "stargazers_count": 77623,
    "watchers_count": 77623,
    "forks_count": 45524,
    "open_issues_count": 1259,
    "size": 659995,
    "language": "Python",
    "languages": {
      "Python": 23723811,
      "Jupyter Notebook": 3190918,
      "C++": 526197,
      "Shell": 102376,
      "Starlark": 84913,
      "SCSS": 33845,
      "CSS": 28240,
      "HTML": 19607,
      "JavaScript": 18631,
      "Dockerfile": 10147
    },
    "topics": [],
    "license_name": "Other",
    "created_at": "2016-02-05T01:15:20+00:00",
    "updated_at": "2025-08-05T21:34:54+00:00",
    "pushed_at": "2025-08-05T20:38:43+00:00",
    "contributors_count": 100,
    "readme_length": 4923,
    "readme_content": "<div align=\"center\">\n  <img src=\"https://storage.googleapis.com/tf_model_garden/tf_model_garden_logo.png\">\n</div>\n\n[![Python](https://img.shields.io/pypi/pyversions/tensorflow.svg?style=plastic)](https://badge.fury.io/py/tensorflow)\n[![tf-models-official PyPI](https://badge.fury.io/py/tf-models-official.svg)](https://badge.fury.io/py/tf-models-official)\n\n\n# Welcome to the Model Garden for TensorFlow\n\nThe TensorFlow Model Garden is a repository with a number of different\nimplementations of state-of-the-art (SOTA) models and modeling solutions for\nTensorFlow users. We aim to demonstrate the best practices for modeling so that\nTensorFlow users can take full advantage of TensorFlow for their research and\nproduct development.\n\nTo improve the transparency and reproducibility of our models, training logs on\n[TensorBoard.dev](https://tensorboard.dev) are also provided for models to the\nextent possible though not all models are suitable.\n\n| Directory | Description |\n|-----------|-------------|\n| [official](official) | \u2022 A collection of example implementations for SOTA models using the latest TensorFlow 2's high-level APIs<br />\u2022 Officially maintained, supported, and kept up to date with the latest TensorFlow 2 APIs by TensorFlow<br />\u2022 Reasonably optimized for fast performance while still being easy to read<br /> For more details on the capabilities, check the guide on the [Model-garden](https://www.tensorflow.org/tfmodels)|\n| [research](research) | \u2022 A collection of research model implementations in TensorFlow 1 or 2 by researchers<br />\u2022 Maintained and supported by researchers |\n| [community](community) | \u2022 A curated list of the GitHub repositories with machine learning models and implementations powered by TensorFlow 2 |\n| [orbit](orbit) | \u2022 A flexible and lightweight library that users can easily use or fork when writing customized training loop code in TensorFlow 2.x. It seamlessly integrates with `tf.distribute` and supports running on different device types (CPU, GPU, and TPU). |\n\n## Installation\n\nTo install the current release of tensorflow-models, please follow any one of the methods described below.\n\n#### Method 1: Install the TensorFlow Model Garden pip package\n\n<details>\n\n**tf-models-official** is the stable Model Garden package. Please check out the [releases](https://github.com/tensorflow/models/releases) to see what are available modules.\n\npip3 will install all models and dependencies automatically.\n\n```shell\npip3 install tf-models-official\n```\n\nPlease check out our examples:\n  - [basic library import](https://github.com/tensorflow/models/blob/master/tensorflow_models/tensorflow_models_pypi.ipynb)\n  - [nlp model building](https://github.com/tensorflow/models/blob/master/docs/nlp/index.ipynb)\nto learn how to use a PIP package.\n\nNote that **tf-models-official** may not include the latest changes in the master branch of this\ngithub repo. To include latest changes, you may install **tf-models-nightly**,\nwhich is the nightly Model Garden package created daily automatically.\n\n```shell\npip3 install tf-models-nightly\n```\n\n</details>\n\n\n#### Method 2: Clone the source\n\n<details>\n\n1. Clone the GitHub repository:\n\n```shell\ngit clone https://github.com/tensorflow/models.git\n```\n\n2. Add the top-level ***/models*** folder to the Python path.\n\n```shell\nexport PYTHONPATH=$PYTHONPATH:/path/to/models\n```\n\nIf you are using in a Windows environment, you may need to use the following command with PowerShell:\n```shell\n$env:PYTHONPATH += \":\\path\\to\\models\"\n```\n\nIf you are using a Colab notebook, please set the Python path with os.environ.\n\n```python\nimport os\nos.environ['PYTHONPATH'] += \":/path/to/models\"\n```\n\n3. Install other dependencies\n\n```shell\npip3 install --user -r models/official/requirements.txt\n```\n\nFinally, if you are using nlp packages, please also install\n**tensorflow-text-nightly**:\n\n```shell\npip3 install tensorflow-text-nightly\n```\n\n</details>\n\n\n## Announcements\n\nPlease check [this page](https://github.com/tensorflow/models/wiki/Announcements) for recent announcements.\n\n## Contributions\n\n[![help wanted:paper implementation](https://img.shields.io/github/issues/tensorflow/models/help%20wanted%3Apaper%20implementation)](https://github.com/tensorflow/models/labels/help%20wanted%3Apaper%20implementation)\n\nIf you want to contribute, please review the [contribution guidelines](https://github.com/tensorflow/models/wiki/How-to-contribute).\n\n## License\n\n[Apache License 2.0](LICENSE)\n\n## Citing TensorFlow Model Garden\n\nIf you use TensorFlow Model Garden in your research, please cite this repository.\n\n```\n@misc{tensorflowmodelgarden2020,\n  author = {Hongkun Yu and Chen Chen and Xianzhi Du and Yeqing Li and Abdullah Rashwan and Le Hou and Pengchong Jin and Fan Yang\n            and Frederick Liu and Jaeyoun Kim and Jing Li},\n  title = {{TensorFlow Model Garden}},\n  howpublished = {\\url{https://github.com/tensorflow/models}},\n  year = {2020}\n}\n```\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 154747577,
    "name": "bert",
    "full_name": "google-research/bert",
    "description": "TensorFlow code and pre-trained models for BERT",
    "html_url": "https://github.com/google-research/bert",
    "clone_url": "https://github.com/google-research/bert.git",
    "owner_login": "google-research",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/43830688?v=4",
    "stargazers_count": 39393,
    "watchers_count": 39393,
    "forks_count": 9695,
    "open_issues_count": 885,
    "size": 317,
    "language": "Python",
    "languages": {
      "Python": 214335,
      "Jupyter Notebook": 66488
    },
    "topics": [
      "google",
      "natural-language-processing",
      "natural-language-understanding",
      "nlp",
      "tensorflow"
    ],
    "license_name": "Apache License 2.0",
    "created_at": "2018-10-25T22:57:34+00:00",
    "updated_at": "2025-08-05T14:21:52+00:00",
    "pushed_at": "2024-07-23T23:39:41+00:00",
    "contributors_count": 25,
    "readme_length": 50505,
    "readme_content": "# BERT\n\n**\\*\\*\\*\\*\\* New March 11th, 2020: Smaller BERT Models \\*\\*\\*\\*\\***\n\nThis is a release of 24 smaller BERT models (English only, uncased, trained with WordPiece masking) referenced in [Well-Read Students Learn Better: On the Importance of Pre-training Compact Models](https://arxiv.org/abs/1908.08962).\n\nWe have shown that the standard BERT recipe (including model architecture and training objective) is effective on a wide range of model sizes, beyond BERT-Base and BERT-Large. The smaller BERT models are intended for environments with restricted computational resources. They can be fine-tuned in the same manner as the original BERT models. However, they are most effective in the context of knowledge distillation, where the fine-tuning labels are produced by a larger and more accurate teacher.\n\nOur goal is to enable research in institutions with fewer computational resources and encourage the community to seek directions of innovation alternative to increasing model capacity.\n\nYou can download all 24 from [here][all], or individually from the table below:\n\n|   |H=128|H=256|H=512|H=768|\n|---|:---:|:---:|:---:|:---:|\n| **L=2**  |[**2/128 (BERT-Tiny)**][2_128]|[2/256][2_256]|[2/512][2_512]|[2/768][2_768]|\n| **L=4**  |[4/128][4_128]|[**4/256 (BERT-Mini)**][4_256]|[**4/512 (BERT-Small)**][4_512]|[4/768][4_768]|\n| **L=6**  |[6/128][6_128]|[6/256][6_256]|[6/512][6_512]|[6/768][6_768]|\n| **L=8**  |[8/128][8_128]|[8/256][8_256]|[**8/512 (BERT-Medium)**][8_512]|[8/768][8_768]|\n| **L=10** |[10/128][10_128]|[10/256][10_256]|[10/512][10_512]|[10/768][10_768]|\n| **L=12** |[12/128][12_128]|[12/256][12_256]|[12/512][12_512]|[**12/768 (BERT-Base)**][12_768]|\n\nNote that the BERT-Base model in this release is included for completeness only; it was re-trained under the same regime as the original model.\n\nHere are the corresponding GLUE scores on the test set:\n\n|Model|Score|CoLA|SST-2|MRPC|STS-B|QQP|MNLI-m|MNLI-mm|QNLI(v2)|RTE|WNLI|AX|\n|---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n|BERT-Tiny|64.2|0.0|83.2|81.1/71.1|74.3/73.6|62.2/83.4|70.2|70.3|81.5|57.2|62.3|21.0|\n|BERT-Mini|65.8|0.0|85.9|81.1/71.8|75.4/73.3|66.4/86.2|74.8|74.3|84.1|57.9|62.3|26.1|\n|BERT-Small|71.2|27.8|89.7|83.4/76.2|78.8/77.0|68.1/87.0|77.6|77.0|86.4|61.8|62.3|28.6|\n|BERT-Medium|73.5|38.0|89.6|86.6/81.6|80.4/78.4|69.6/87.9|80.0|79.1|87.7|62.2|62.3|30.5|\n\nFor each task, we selected the best fine-tuning hyperparameters from the lists below, and trained for 4 epochs:\n- batch sizes: 8, 16, 32, 64, 128\n- learning rates: 3e-4, 1e-4, 5e-5, 3e-5\n\nIf you use these models, please cite the following paper:\n\n```\n@article{turc2019,\n  title={Well-Read Students Learn Better: On the Importance of Pre-training Compact Models},\n  author={Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},\n  journal={arXiv preprint arXiv:1908.08962v2 },\n  year={2019}\n}\n```\n\n[2_128]: https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-2_H-128_A-2.zip\n[2_256]: https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-2_H-256_A-4.zip\n[2_512]: https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-2_H-512_A-8.zip\n[2_768]: https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-2_H-768_A-12.zip\n[4_128]: https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-4_H-128_A-2.zip\n[4_256]: https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-4_H-256_A-4.zip\n[4_512]: https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-4_H-512_A-8.zip\n[4_768]: https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-4_H-768_A-12.zip\n[6_128]: https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-6_H-128_A-2.zip\n[6_256]: https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-6_H-256_A-4.zip\n[6_512]: https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-6_H-512_A-8.zip\n[6_768]: https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-6_H-768_A-12.zip\n[8_128]: https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-8_H-128_A-2.zip\n[8_256]: https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-8_H-256_A-4.zip\n[8_512]: https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-8_H-512_A-8.zip\n[8_768]: https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-8_H-768_A-12.zip\n[10_128]: https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-10_H-128_A-2.zip\n[10_256]: https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-10_H-256_A-4.zip\n[10_512]: https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-10_H-512_A-8.zip\n[10_768]: https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-10_H-768_A-12.zip\n[12_128]: https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-12_H-128_A-2.zip\n[12_256]: https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-12_H-256_A-4.zip\n[12_512]: https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-12_H-512_A-8.zip\n[12_768]: https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-12_H-768_A-12.zip\n[all]: https://storage.googleapis.com/bert_models/2020_02_20/all_bert_models.zip\n\n**\\*\\*\\*\\*\\* New May 31st, 2019: Whole Word Masking Models \\*\\*\\*\\*\\***\n\nThis is a release of several new models which were the result of an improvement\nthe pre-processing code.\n\nIn the original pre-processing code, we randomly select WordPiece tokens to\nmask. For example:\n\n`Input Text: the man jumped up , put his basket on phil ##am ##mon ' s head`\n`Original Masked Input: [MASK] man [MASK] up , put his [MASK] on phil\n[MASK] ##mon ' s head`\n\nThe new technique is called Whole Word Masking. In this case, we always mask\n*all* of the the tokens corresponding to a word at once. The overall masking\nrate remains the same.\n\n`Whole Word Masked Input: the man [MASK] up , put his basket on [MASK] [MASK]\n[MASK] ' s head`\n\nThe training is identical -- we still predict each masked WordPiece token\nindependently. The improvement comes from the fact that the original prediction\ntask was too 'easy' for words that had been split into multiple WordPieces.\n\nThis can be enabled during data generation by passing the flag\n`--do_whole_word_mask=True` to `create_pretraining_data.py`.\n\nPre-trained models with Whole Word Masking are linked below. The data and\ntraining were otherwise identical, and the models have identical structure and\nvocab to the original models. We only include BERT-Large models. When using\nthese models, please make it clear in the paper that you are using the Whole\nWord Masking variant of BERT-Large.\n\n*   **[`BERT-Large, Uncased (Whole Word Masking)`](https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip)**:\n    24-layer, 1024-hidden, 16-heads, 340M parameters\n\n*   **[`BERT-Large, Cased (Whole Word Masking)`](https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip)**:\n    24-layer, 1024-hidden, 16-heads, 340M parameters\n\nModel                                    | SQUAD 1.1 F1/EM | Multi NLI Accuracy\n---------------------------------------- | :-------------: | :----------------:\nBERT-Large, Uncased (Original)           | 91.0/84.3       | 86.05\nBERT-Large, Uncased (Whole Word Masking) | 92.8/86.7       | 87.07\nBERT-Large, Cased (Original)             | 91.5/84.8       | 86.09\nBERT-Large, Cased (Whole Word Masking)   | 92.9/86.7       | 86.46\n\n**\\*\\*\\*\\*\\* New February 7th, 2019: TfHub Module \\*\\*\\*\\*\\***\n\nBERT has been uploaded to [TensorFlow Hub](https://tfhub.dev). See\n`run_classifier_with_tfhub.py` for an example of how to use the TF Hub module,\nor run an example in the browser on\n[Colab](https://colab.sandbox.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb).\n\n**\\*\\*\\*\\*\\* New November 23rd, 2018: Un-normalized multilingual model + Thai +\nMongolian \\*\\*\\*\\*\\***\n\nWe uploaded a new multilingual model which does *not* perform any normalization\non the input (no lower casing, accent stripping, or Unicode normalization), and\nadditionally inclues Thai and Mongolian.\n\n**It is recommended to use this version for developing multilingual models,\nespecially on languages with non-Latin alphabets.**\n\nThis does not require any code changes, and can be downloaded here:\n\n*   **[`BERT-Base, Multilingual Cased`](https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip)**:\n    104 languages, 12-layer, 768-hidden, 12-heads, 110M parameters\n\n**\\*\\*\\*\\*\\* New November 15th, 2018: SOTA SQuAD 2.0 System \\*\\*\\*\\*\\***\n\nWe released code changes to reproduce our 83% F1 SQuAD 2.0 system, which is\ncurrently 1st place on the leaderboard by 3%. See the SQuAD 2.0 section of the\nREADME for details.\n\n**\\*\\*\\*\\*\\* New November 5th, 2018: Third-party PyTorch and Chainer versions of\nBERT available \\*\\*\\*\\*\\***\n\nNLP researchers from HuggingFace made a\n[PyTorch version of BERT available](https://github.com/huggingface/pytorch-pretrained-BERT)\nwhich is compatible with our pre-trained checkpoints and is able to reproduce\nour results. Sosuke Kobayashi also made a\n[Chainer version of BERT available](https://github.com/soskek/bert-chainer)\n(Thanks!) We were not involved in the creation or maintenance of the PyTorch\nimplementation so please direct any questions towards the authors of that\nrepository.\n\n**\\*\\*\\*\\*\\* New November 3rd, 2018: Multilingual and Chinese models available\n\\*\\*\\*\\*\\***\n\nWe have made two new BERT models available:\n\n*   **[`BERT-Base, Multilingual`](https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip)\n    (Not recommended, use `Multilingual Cased` instead)**: 102 languages,\n    12-layer, 768-hidden, 12-heads, 110M parameters\n*   **[`BERT-Base, Chinese`](https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip)**:\n    Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads, 110M\n    parameters\n\nWe use character-based tokenization for Chinese, and WordPiece tokenization for\nall other languages. Both models should work out-of-the-box without any code\nchanges. We did update the implementation of `BasicTokenizer` in\n`tokenization.py` to support Chinese character tokenization, so please update if\nyou forked it. However, we did not change the tokenization API.\n\nFor more, see the\n[Multilingual README](https://github.com/google-research/bert/blob/master/multilingual.md).\n\n**\\*\\*\\*\\*\\* End new information \\*\\*\\*\\*\\***\n\n## Introduction\n\n**BERT**, or **B**idirectional **E**ncoder **R**epresentations from\n**T**ransformers, is a new method of pre-training language representations which\nobtains state-of-the-art results on a wide array of Natural Language Processing\n(NLP) tasks.\n\nOur academic paper which describes BERT in detail and provides full results on a\nnumber of tasks can be found here:\n[https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805).\n\nTo give a few numbers, here are the results on the\n[SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/) question answering\ntask:\n\nSQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1\n------------------------------------- | :------: | :------:\n1st Place Ensemble - BERT             | **87.4** | **93.2**\n2nd Place Ensemble - nlnet            | 86.0     | 91.7\n1st Place Single Model - BERT         | **85.1** | **91.8**\n2nd Place Single Model - nlnet        | 83.5     | 90.1\n\nAnd several natural language inference tasks:\n\nSystem                  | MultiNLI | Question NLI | SWAG\n----------------------- | :------: | :----------: | :------:\nBERT                    | **86.7** | **91.1**     | **86.3**\nOpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0\n\nPlus many other tasks.\n\nMoreover, these results were all obtained with almost no task-specific neural\nnetwork architecture design.\n\nIf you already know what BERT is and you just want to get started, you can\n[download the pre-trained models](#pre-trained-models) and\n[run a state-of-the-art fine-tuning](#fine-tuning-with-bert) in only a few\nminutes.\n\n## What is BERT?\n\nBERT is a method of pre-training language representations, meaning that we train\na general-purpose \"language understanding\" model on a large text corpus (like\nWikipedia), and then use that model for downstream NLP tasks that we care about\n(like question answering). BERT outperforms previous methods because it is the\nfirst *unsupervised*, *deeply bidirectional* system for pre-training NLP.\n\n*Unsupervised* means that BERT was trained using only a plain text corpus, which\nis important because an enormous amount of plain text data is publicly available\non the web in many languages.\n\nPre-trained representations can also either be *context-free* or *contextual*,\nand contextual representations can further be *unidirectional* or\n*bidirectional*. Context-free models such as\n[word2vec](https://www.tensorflow.org/tutorials/representation/word2vec) or\n[GloVe](https://nlp.stanford.edu/projects/glove/) generate a single \"word\nembedding\" representation for each word in the vocabulary, so `bank` would have\nthe same representation in `bank deposit` and `river bank`. Contextual models\ninstead generate a representation of each word that is based on the other words\nin the sentence.\n\nBERT was built upon recent work in pre-training contextual representations \u2014\nincluding [Semi-supervised Sequence Learning](https://arxiv.org/abs/1511.01432),\n[Generative Pre-Training](https://blog.openai.com/language-unsupervised/),\n[ELMo](https://allennlp.org/elmo), and\n[ULMFit](http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html)\n\u2014 but crucially these models are all *unidirectional* or *shallowly\nbidirectional*. This means that each word is only contextualized using the words\nto its left (or right). For example, in the sentence `I made a bank deposit` the\nunidirectional representation of `bank` is only based on `I made a` but not\n`deposit`. Some previous work does combine the representations from separate\nleft-context and right-context models, but only in a \"shallow\" manner. BERT\nrepresents \"bank\" using both its left and right context \u2014 `I made a ... deposit`\n\u2014 starting from the very bottom of a deep neural network, so it is *deeply\nbidirectional*.\n\nBERT uses a simple approach for this: We mask out 15% of the words in the input,\nrun the entire sequence through a deep bidirectional\n[Transformer](https://arxiv.org/abs/1706.03762) encoder, and then predict only\nthe masked words. For example:\n\n```\nInput: the man went to the [MASK1] . he bought a [MASK2] of milk.\nLabels: [MASK1] = store; [MASK2] = gallon\n```\n\nIn order to learn relationships between sentences, we also train on a simple\ntask which can be generated from any monolingual corpus: Given two sentences `A`\nand `B`, is `B` the actual next sentence that comes after `A`, or just a random\nsentence from the corpus?\n\n```\nSentence A: the man went to the store .\nSentence B: he bought a gallon of milk .\nLabel: IsNextSentence\n```\n\n```\nSentence A: the man went to the store .\nSentence B: penguins are flightless .\nLabel: NotNextSentence\n```\n\nWe then train a large model (12-layer to 24-layer Transformer) on a large corpus\n(Wikipedia + [BookCorpus](http://yknzhu.wixsite.com/mbweb)) for a long time (1M\nupdate steps), and that's BERT.\n\nUsing BERT has two stages: *Pre-training* and *fine-tuning*.\n\n**Pre-training** is fairly expensive (four days on 4 to 16 Cloud TPUs), but is a\none-time procedure for each language (current models are English-only, but\nmultilingual models will be released in the near future). We are releasing a\nnumber of pre-trained models from the paper which were pre-trained at Google.\nMost NLP researchers will never need to pre-train their own model from scratch.\n\n**Fine-tuning** is inexpensive. All of the results in the paper can be\nreplicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU,\nstarting from the exact same pre-trained model. SQuAD, for example, can be\ntrained in around 30 minutes on a single Cloud TPU to achieve a Dev F1 score of\n91.0%, which is the single system state-of-the-art.\n\nThe other important aspect of BERT is that it can be adapted to many types of\nNLP tasks very easily. In the paper, we demonstrate state-of-the-art results on\nsentence-level (e.g., SST-2), sentence-pair-level (e.g., MultiNLI), word-level\n(e.g., NER), and span-level (e.g., SQuAD) tasks with almost no task-specific\nmodifications.\n\n## What has been released in this repository?\n\nWe are releasing the following:\n\n*   TensorFlow code for the BERT model architecture (which is mostly a standard\n    [Transformer](https://arxiv.org/abs/1706.03762) architecture).\n*   Pre-trained checkpoints for both the lowercase and cased version of\n    `BERT-Base` and `BERT-Large` from the paper.\n*   TensorFlow code for push-button replication of the most important\n    fine-tuning experiments from the paper, including SQuAD, MultiNLI, and MRPC.\n\nAll of the code in this repository works out-of-the-box with CPU, GPU, and Cloud\nTPU.\n\n## Pre-trained models\n\nWe are releasing the `BERT-Base` and `BERT-Large` models from the paper.\n`Uncased` means that the text has been lowercased before WordPiece tokenization,\ne.g., `John Smith` becomes `john smith`. The `Uncased` model also strips out any\naccent markers. `Cased` means that the true case and accent markers are\npreserved. Typically, the `Uncased` model is better unless you know that case\ninformation is important for your task (e.g., Named Entity Recognition or\nPart-of-Speech tagging).\n\nThese models are all released under the same license as the source code (Apache\n2.0).\n\nFor information about the Multilingual and Chinese model, see the\n[Multilingual README](https://github.com/google-research/bert/blob/master/multilingual.md).\n\n**When using a cased model, make sure to pass `--do_lower=False` to the training\nscripts. (Or pass `do_lower_case=False` directly to `FullTokenizer` if you're\nusing your own script.)**\n\nThe links to the models are here (right-click, 'Save link as...' on the name):\n\n*   **[`BERT-Large, Uncased (Whole Word Masking)`](https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip)**:\n    24-layer, 1024-hidden, 16-heads, 340M parameters\n*   **[`BERT-Large, Cased (Whole Word Masking)`](https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip)**:\n    24-layer, 1024-hidden, 16-heads, 340M parameters\n*   **[`BERT-Base, Uncased`](https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip)**:\n    12-layer, 768-hidden, 12-heads, 110M parameters\n*   **[`BERT-Large, Uncased`](https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip)**:\n    24-layer, 1024-hidden, 16-heads, 340M parameters\n*   **[`BERT-Base, Cased`](https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip)**:\n    12-layer, 768-hidden, 12-heads , 110M parameters\n*   **[`BERT-Large, Cased`](https://storage.googleapis.com/bert_models/2018_10_18/cased_L-24_H-1024_A-16.zip)**:\n    24-layer, 1024-hidden, 16-heads, 340M parameters\n*   **[`BERT-Base, Multilingual Cased (New, recommended)`](https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip)**:\n    104 languages, 12-layer, 768-hidden, 12-heads, 110M parameters\n*   **[`BERT-Base, Multilingual Uncased (Orig, not recommended)`](https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip)\n    (Not recommended, use `Multilingual Cased` instead)**: 102 languages,\n    12-layer, 768-hidden, 12-heads, 110M parameters\n*   **[`BERT-Base, Chinese`](https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip)**:\n    Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads, 110M\n    parameters\n\nEach .zip file contains three items:\n\n*   A TensorFlow checkpoint (`bert_model.ckpt`) containing the pre-trained\n    weights (which is actually 3 files).\n*   A vocab file (`vocab.txt`) to map WordPiece to word id.\n*   A config file (`bert_config.json`) which specifies the hyperparameters of\n    the model.\n\n## Fine-tuning with BERT\n\n**Important**: All results on the paper were fine-tuned on a single Cloud TPU,\nwhich has 64GB of RAM. It is currently not possible to re-produce most of the\n`BERT-Large` results on the paper using a GPU with 12GB - 16GB of RAM, because\nthe maximum batch size that can fit in memory is too small. We are working on\nadding code to this repository which allows for much larger effective batch size\non the GPU. See the section on [out-of-memory issues](#out-of-memory-issues) for\nmore details.\n\nThis code was tested with TensorFlow 1.11.0. It was tested with Python2 and\nPython3 (but more thoroughly with Python2, since this is what's used internally\nin Google).\n\nThe fine-tuning examples which use `BERT-Base` should be able to run on a GPU\nthat has at least 12GB of RAM using the hyperparameters given.\n\n### Fine-tuning with Cloud TPUs\n\nMost of the examples below assumes that you will be running training/evaluation\non your local machine, using a GPU like a Titan X or GTX 1080.\n\nHowever, if you have access to a Cloud TPU that you want to train on, just add\nthe following flags to `run_classifier.py` or `run_squad.py`:\n\n```\n  --use_tpu=True \\\n  --tpu_name=$TPU_NAME\n```\n\nPlease see the\n[Google Cloud TPU tutorial](https://cloud.google.com/tpu/docs/tutorials/mnist)\nfor how to use Cloud TPUs. Alternatively, you can use the Google Colab notebook\n\"[BERT FineTuning with Cloud TPUs](https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\".\n\nOn Cloud TPUs, the pretrained model and the output directory will need to be on\nGoogle Cloud Storage. For example, if you have a bucket named `some_bucket`, you\nmight use the following flags instead:\n\n```\n  --output_dir=gs://some_bucket/my_output_dir/\n```\n\nThe unzipped pre-trained model files can also be found in the Google Cloud\nStorage folder `gs://bert_models/2018_10_18`. For example:\n\n```\nexport BERT_BASE_DIR=gs://bert_models/2018_10_18/uncased_L-12_H-768_A-12\n```\n\n### Sentence (and sentence-pair) classification tasks\n\nBefore running this example you must download the\n[GLUE data](https://gluebenchmark.com/tasks) by running\n[this script](https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e)\nand unpack it to some directory `$GLUE_DIR`. Next, download the `BERT-Base`\ncheckpoint and unzip it to some directory `$BERT_BASE_DIR`.\n\nThis example code fine-tunes `BERT-Base` on the Microsoft Research Paraphrase\nCorpus (MRPC) corpus, which only contains 3,600 examples and can fine-tune in a\nfew minutes on most GPUs.\n\n```shell\nexport BERT_BASE_DIR=/path/to/bert/uncased_L-12_H-768_A-12\nexport GLUE_DIR=/path/to/glue\n\npython run_classifier.py \\\n  --task_name=MRPC \\\n  --do_train=true \\\n  --do_eval=true \\\n  --data_dir=$GLUE_DIR/MRPC \\\n  --vocab_file=$BERT_BASE_DIR/vocab.txt \\\n  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\n  --max_seq_length=128 \\\n  --train_batch_size=32 \\\n  --learning_rate=2e-5 \\\n  --num_train_epochs=3.0 \\\n  --output_dir=/tmp/mrpc_output/\n```\n\nYou should see output like this:\n\n```\n***** Eval results *****\n  eval_accuracy = 0.845588\n  eval_loss = 0.505248\n  global_step = 343\n  loss = 0.505248\n```\n\nThis means that the Dev set accuracy was 84.55%. Small sets like MRPC have a\nhigh variance in the Dev set accuracy, even when starting from the same\npre-training checkpoint. If you re-run multiple times (making sure to point to\ndifferent `output_dir`), you should see results between 84% and 88%.\n\nA few other pre-trained models are implemented off-the-shelf in\n`run_classifier.py`, so it should be straightforward to follow those examples to\nuse BERT for any single-sentence or sentence-pair classification task.\n\nNote: You might see a message `Running train on CPU`. This really just means\nthat it's running on something other than a Cloud TPU, which includes a GPU.\n\n#### Prediction from classifier\n\nOnce you have trained your classifier you can use it in inference mode by using\nthe --do_predict=true command. You need to have a file named test.tsv in the\ninput folder. Output will be created in file called test_results.tsv in the\noutput folder. Each line will contain output for each sample, columns are the\nclass probabilities.\n\n```shell\nexport BERT_BASE_DIR=/path/to/bert/uncased_L-12_H-768_A-12\nexport GLUE_DIR=/path/to/glue\nexport TRAINED_CLASSIFIER=/path/to/fine/tuned/classifier\n\npython run_classifier.py \\\n  --task_name=MRPC \\\n  --do_predict=true \\\n  --data_dir=$GLUE_DIR/MRPC \\\n  --vocab_file=$BERT_BASE_DIR/vocab.txt \\\n  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n  --init_checkpoint=$TRAINED_CLASSIFIER \\\n  --max_seq_length=128 \\\n  --output_dir=/tmp/mrpc_output/\n```\n\n### SQuAD 1.1\n\nThe Stanford Question Answering Dataset (SQuAD) is a popular question answering\nbenchmark dataset. BERT (at the time of the release) obtains state-of-the-art\nresults on SQuAD with almost no task-specific network architecture modifications\nor data augmentation. However, it does require semi-complex data pre-processing\nand post-processing to deal with (a) the variable-length nature of SQuAD context\nparagraphs, and (b) the character-level answer annotations which are used for\nSQuAD training. This processing is implemented and documented in `run_squad.py`.\n\nTo run on SQuAD, you will first need to download the dataset. The\n[SQuAD website](https://rajpurkar.github.io/SQuAD-explorer/) does not seem to\nlink to the v1.1 datasets any longer, but the necessary files can be found here:\n\n*   [train-v1.1.json](https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json)\n*   [dev-v1.1.json](https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json)\n*   [evaluate-v1.1.py](https://github.com/allenai/bi-att-flow/blob/master/squad/evaluate-v1.1.py)\n\nDownload these to some directory `$SQUAD_DIR`.\n\nThe state-of-the-art SQuAD results from the paper currently cannot be reproduced\non a 12GB-16GB GPU due to memory constraints (in fact, even batch size 1 does\nnot seem to fit on a 12GB GPU using `BERT-Large`). However, a reasonably strong\n`BERT-Base` model can be trained on the GPU with these hyperparameters:\n\n```shell\npython run_squad.py \\\n  --vocab_file=$BERT_BASE_DIR/vocab.txt \\\n  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\n  --do_train=True \\\n  --train_file=$SQUAD_DIR/train-v1.1.json \\\n  --do_predict=True \\\n  --predict_file=$SQUAD_DIR/dev-v1.1.json \\\n  --train_batch_size=12 \\\n  --learning_rate=3e-5 \\\n  --num_train_epochs=2.0 \\\n  --max_seq_length=384 \\\n  --doc_stride=128 \\\n  --output_dir=/tmp/squad_base/\n```\n\nThe dev set predictions will be saved into a file called `predictions.json` in\nthe `output_dir`:\n\n```shell\npython $SQUAD_DIR/evaluate-v1.1.py $SQUAD_DIR/dev-v1.1.json ./squad/predictions.json\n```\n\nWhich should produce an output like this:\n\n```shell\n{\"f1\": 88.41249612335034, \"exact_match\": 81.2488174077578}\n```\n\nYou should see a result similar to the 88.5% reported in the paper for\n`BERT-Base`.\n\nIf you have access to a Cloud TPU, you can train with `BERT-Large`. Here is a\nset of hyperparameters (slightly different than the paper) which consistently\nobtain around 90.5%-91.0% F1 single-system trained only on SQuAD:\n\n```shell\npython run_squad.py \\\n  --vocab_file=$BERT_LARGE_DIR/vocab.txt \\\n  --bert_config_file=$BERT_LARGE_DIR/bert_config.json \\\n  --init_checkpoint=$BERT_LARGE_DIR/bert_model.ckpt \\\n  --do_train=True \\\n  --train_file=$SQUAD_DIR/train-v1.1.json \\\n  --do_predict=True \\\n  --predict_file=$SQUAD_DIR/dev-v1.1.json \\\n  --train_batch_size=24 \\\n  --learning_rate=3e-5 \\\n  --num_train_epochs=2.0 \\\n  --max_seq_length=384 \\\n  --doc_stride=128 \\\n  --output_dir=gs://some_bucket/squad_large/ \\\n  --use_tpu=True \\\n  --tpu_name=$TPU_NAME\n```\n\nFor example, one random run with these parameters produces the following Dev\nscores:\n\n```shell\n{\"f1\": 90.87081895814865, \"exact_match\": 84.38978240302744}\n```\n\nIf you fine-tune for one epoch on\n[TriviaQA](http://nlp.cs.washington.edu/triviaqa/) before this the results will\nbe even better, but you will need to convert TriviaQA into the SQuAD json\nformat.\n\n### SQuAD 2.0\n\nThis model is also implemented and documented in `run_squad.py`.\n\nTo run on SQuAD 2.0, you will first need to download the dataset. The necessary\nfiles can be found here:\n\n*   [train-v2.0.json](https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json)\n*   [dev-v2.0.json](https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json)\n*   [evaluate-v2.0.py](https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/)\n\nDownload these to some directory `$SQUAD_DIR`.\n\nOn Cloud TPU you can run with BERT-Large as follows:\n\n```shell\npython run_squad.py \\\n  --vocab_file=$BERT_LARGE_DIR/vocab.txt \\\n  --bert_config_file=$BERT_LARGE_DIR/bert_config.json \\\n  --init_checkpoint=$BERT_LARGE_DIR/bert_model.ckpt \\\n  --do_train=True \\\n  --train_file=$SQUAD_DIR/train-v2.0.json \\\n  --do_predict=True \\\n  --predict_file=$SQUAD_DIR/dev-v2.0.json \\\n  --train_batch_size=24 \\\n  --learning_rate=3e-5 \\\n  --num_train_epochs=2.0 \\\n  --max_seq_length=384 \\\n  --doc_stride=128 \\\n  --output_dir=gs://some_bucket/squad_large/ \\\n  --use_tpu=True \\\n  --tpu_name=$TPU_NAME \\\n  --version_2_with_negative=True\n```\n\nWe assume you have copied everything from the output directory to a local\ndirectory called ./squad/. The initial dev set predictions will be at\n./squad/predictions.json and the differences between the score of no answer (\"\")\nand the best non-null answer for each question will be in the file\n./squad/null_odds.json\n\nRun this script to tune a threshold for predicting null versus non-null answers:\n\npython $SQUAD_DIR/evaluate-v2.0.py $SQUAD_DIR/dev-v2.0.json\n./squad/predictions.json --na-prob-file ./squad/null_odds.json\n\nAssume the script outputs \"best_f1_thresh\" THRESH. (Typical values are between\n-1.0 and -5.0). You can now re-run the model to generate predictions with the\nderived threshold or alternatively you can extract the appropriate answers from\n./squad/nbest_predictions.json.\n\n```shell\npython run_squad.py \\\n  --vocab_file=$BERT_LARGE_DIR/vocab.txt \\\n  --bert_config_file=$BERT_LARGE_DIR/bert_config.json \\\n  --init_checkpoint=$BERT_LARGE_DIR/bert_model.ckpt \\\n  --do_train=False \\\n  --train_file=$SQUAD_DIR/train-v2.0.json \\\n  --do_predict=True \\\n  --predict_file=$SQUAD_DIR/dev-v2.0.json \\\n  --train_batch_size=24 \\\n  --learning_rate=3e-5 \\\n  --num_train_epochs=2.0 \\\n  --max_seq_length=384 \\\n  --doc_stride=128 \\\n  --output_dir=gs://some_bucket/squad_large/ \\\n  --use_tpu=True \\\n  --tpu_name=$TPU_NAME \\\n  --version_2_with_negative=True \\\n  --null_score_diff_threshold=$THRESH\n```\n\n### Out-of-memory issues\n\nAll experiments in the paper were fine-tuned on a Cloud TPU, which has 64GB of\ndevice RAM. Therefore, when using a GPU with 12GB - 16GB of RAM, you are likely\nto encounter out-of-memory issues if you use the same hyperparameters described\nin the paper.\n\nThe factors that affect memory usage are:\n\n*   **`max_seq_length`**: The released models were trained with sequence lengths\n    up to 512, but you can fine-tune with a shorter max sequence length to save\n    substantial memory. This is controlled by the `max_seq_length` flag in our\n    example code.\n\n*   **`train_batch_size`**: The memory usage is also directly proportional to\n    the batch size.\n\n*   **Model type, `BERT-Base` vs. `BERT-Large`**: The `BERT-Large` model\n    requires significantly more memory than `BERT-Base`.\n\n*   **Optimizer**: The default optimizer for BERT is Adam, which requires a lot\n    of extra memory to store the `m` and `v` vectors. Switching to a more memory\n    efficient optimizer can reduce memory usage, but can also affect the\n    results. We have not experimented with other optimizers for fine-tuning.\n\nUsing the default training scripts (`run_classifier.py` and `run_squad.py`), we\nbenchmarked the maximum batch size on single Titan X GPU (12GB RAM) with\nTensorFlow 1.11.0:\n\nSystem       | Seq Length | Max Batch Size\n------------ | ---------- | --------------\n`BERT-Base`  | 64         | 64\n...          | 128        | 32\n...          | 256        | 16\n...          | 320        | 14\n...          | 384        | 12\n...          | 512        | 6\n`BERT-Large` | 64         | 12\n...          | 128        | 6\n...          | 256        | 2\n...          | 320        | 1\n...          | 384        | 0\n...          | 512        | 0\n\nUnfortunately, these max batch sizes for `BERT-Large` are so small that they\nwill actually harm the model accuracy, regardless of the learning rate used. We\nare working on adding code to this repository which will allow much larger\neffective batch sizes to be used on the GPU. The code will be based on one (or\nboth) of the following techniques:\n\n*   **Gradient accumulation**: The samples in a minibatch are typically\n    independent with respect to gradient computation (excluding batch\n    normalization, which is not used here). This means that the gradients of\n    multiple smaller minibatches can be accumulated before performing the weight\n    update, and this will be exactly equivalent to a single larger update.\n\n*   [**Gradient checkpointing**](https://github.com/openai/gradient-checkpointing):\n    The major use of GPU/TPU memory during DNN training is caching the\n    intermediate activations in the forward pass that are necessary for\n    efficient computation in the backward pass. \"Gradient checkpointing\" trades\n    memory for compute time by re-computing the activations in an intelligent\n    way.\n\n**However, this is not implemented in the current release.**\n\n## Using BERT to extract fixed feature vectors (like ELMo)\n\nIn certain cases, rather than fine-tuning the entire pre-trained model\nend-to-end, it can be beneficial to obtained *pre-trained contextual\nembeddings*, which are fixed contextual representations of each input token\ngenerated from the hidden layers of the pre-trained model. This should also\nmitigate most of the out-of-memory issues.\n\nAs an example, we include the script `extract_features.py` which can be used\nlike this:\n\n```shell\n# Sentence A and Sentence B are separated by the ||| delimiter for sentence\n# pair tasks like question answering and entailment.\n# For single sentence inputs, put one sentence per line and DON'T use the\n# delimiter.\necho 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt\n\npython extract_features.py \\\n  --input_file=/tmp/input.txt \\\n  --output_file=/tmp/output.jsonl \\\n  --vocab_file=$BERT_BASE_DIR/vocab.txt \\\n  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\n  --layers=-1,-2,-3,-4 \\\n  --max_seq_length=128 \\\n  --batch_size=8\n```\n\nThis will create a JSON file (one line per line of input) containing the BERT\nactivations from each Transformer layer specified by `layers` (-1 is the final\nhidden layer of the Transformer, etc.)\n\nNote that this script will produce very large output files (by default, around\n15kb for every input token).\n\nIf you need to maintain alignment between the original and tokenized words (for\nprojecting training labels), see the [Tokenization](#tokenization) section\nbelow.\n\n**Note:** You may see a message like `Could not find trained model in model_dir:\n/tmp/tmpuB5g5c, running initialization to predict.` This message is expected, it\njust means that we are using the `init_from_checkpoint()` API rather than the\nsaved model API. If you don't specify a checkpoint or specify an invalid\ncheckpoint, this script will complain.\n\n## Tokenization\n\nFor sentence-level tasks (or sentence-pair) tasks, tokenization is very simple.\nJust follow the example code in `run_classifier.py` and `extract_features.py`.\nThe basic procedure for sentence-level tasks is:\n\n1.  Instantiate an instance of `tokenizer = tokenization.FullTokenizer`\n\n2.  Tokenize the raw text with `tokens = tokenizer.tokenize(raw_text)`.\n\n3.  Truncate to the maximum sequence length. (You can use up to 512, but you\n    probably want to use shorter if possible for memory and speed reasons.)\n\n4.  Add the `[CLS]` and `[SEP]` tokens in the right place.\n\nWord-level and span-level tasks (e.g., SQuAD and NER) are more complex, since\nyou need to maintain alignment between your input text and output text so that\nyou can project your training labels. SQuAD is a particularly complex example\nbecause the input labels are *character*-based, and SQuAD paragraphs are often\nlonger than our maximum sequence length. See the code in `run_squad.py` to show\nhow we handle this.\n\nBefore we describe the general recipe for handling word-level tasks, it's\nimportant to understand what exactly our tokenizer is doing. It has three main\nsteps:\n\n1.  **Text normalization**: Convert all whitespace characters to spaces, and\n    (for the `Uncased` model) lowercase the input and strip out accent markers.\n    E.g., `John Johanson's, \u2192 john johanson's,`.\n\n2.  **Punctuation splitting**: Split *all* punctuation characters on both sides\n    (i.e., add whitespace around all punctuation characters). Punctuation\n    characters are defined as (a) Anything with a `P*` Unicode class, (b) any\n    non-letter/number/space ASCII character (e.g., characters like `$` which are\n    technically not punctuation). E.g., `john johanson's, \u2192 john johanson ' s ,`\n\n3.  **WordPiece tokenization**: Apply whitespace tokenization to the output of\n    the above procedure, and apply\n    [WordPiece](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/text_encoder.py)\n    tokenization to each token separately. (Our implementation is directly based\n    on the one from `tensor2tensor`, which is linked). E.g., `john johanson ' s\n    , \u2192 john johan ##son ' s ,`\n\nThe advantage of this scheme is that it is \"compatible\" with most existing\nEnglish tokenizers. For example, imagine that you have a part-of-speech tagging\ntask which looks like this:\n\n```\nInput:  John Johanson 's   house\nLabels: NNP  NNP      POS NN\n```\n\nThe tokenized output will look like this:\n\n```\nTokens: john johan ##son ' s house\n```\n\nCrucially, this would be the same output as if the raw text were `John\nJohanson's house` (with no space before the `'s`).\n\nIf you have a pre-tokenized representation with word-level annotations, you can\nsimply tokenize each input word independently, and deterministically maintain an\noriginal-to-tokenized alignment:\n\n```python\n### Input\norig_tokens = [\"John\", \"Johanson\", \"'s\",  \"house\"]\nlabels      = [\"NNP\",  \"NNP\",      \"POS\", \"NN\"]\n\n### Output\nbert_tokens = []\n\n# Token map will be an int -> int mapping between the `orig_tokens` index and\n# the `bert_tokens` index.\norig_to_tok_map = []\n\ntokenizer = tokenization.FullTokenizer(\n    vocab_file=vocab_file, do_lower_case=True)\n\nbert_tokens.append(\"[CLS]\")\nfor orig_token in orig_tokens:\n  orig_to_tok_map.append(len(bert_tokens))\n  bert_tokens.extend(tokenizer.tokenize(orig_token))\nbert_tokens.append(\"[SEP]\")\n\n# bert_tokens == [\"[CLS]\", \"john\", \"johan\", \"##son\", \"'\", \"s\", \"house\", \"[SEP]\"]\n# orig_to_tok_map == [1, 2, 4, 6]\n```\n\nNow `orig_to_tok_map` can be used to project `labels` to the tokenized\nrepresentation.\n\nThere are common English tokenization schemes which will cause a slight mismatch\nbetween how BERT was pre-trained. For example, if your input tokenization splits\noff contractions like `do n't`, this will cause a mismatch. If it is possible to\ndo so, you should pre-process your data to convert these back to raw-looking\ntext, but if it's not possible, this mismatch is likely not a big deal.\n\n## Pre-training with BERT\n\nWe are releasing code to do \"masked LM\" and \"next sentence prediction\" on an\narbitrary text corpus. Note that this is *not* the exact code that was used for\nthe paper (the original code was written in C++, and had some additional\ncomplexity), but this code does generate pre-training data as described in the\npaper.\n\nHere's how to run the data generation. The input is a plain text file, with one\nsentence per line. (It is important that these be actual sentences for the \"next\nsentence prediction\" task). Documents are delimited by empty lines. The output\nis a set of `tf.train.Example`s serialized into `TFRecord` file format.\n\nYou can perform sentence segmentation with an off-the-shelf NLP toolkit such as\n[spaCy](https://spacy.io/). The `create_pretraining_data.py` script will\nconcatenate segments until they reach the maximum sequence length to minimize\ncomputational waste from padding (see the script for more details). However, you\nmay want to intentionally add a slight amount of noise to your input data (e.g.,\nrandomly truncate 2% of input segments) to make it more robust to non-sentential\ninput during fine-tuning.\n\nThis script stores all of the examples for the entire input file in memory, so\nfor large data files you should shard the input file and call the script\nmultiple times. (You can pass in a file glob to `run_pretraining.py`, e.g.,\n`tf_examples.tf_record*`.)\n\nThe `max_predictions_per_seq` is the maximum number of masked LM predictions per\nsequence. You should set this to around `max_seq_length` * `masked_lm_prob` (the\nscript doesn't do that automatically because the exact value needs to be passed\nto both scripts).\n\n```shell\npython create_pretraining_data.py \\\n  --input_file=./sample_text.txt \\\n  --output_file=/tmp/tf_examples.tfrecord \\\n  --vocab_file=$BERT_BASE_DIR/vocab.txt \\\n  --do_lower_case=True \\\n  --max_seq_length=128 \\\n  --max_predictions_per_seq=20 \\\n  --masked_lm_prob=0.15 \\\n  --random_seed=12345 \\\n  --dupe_factor=5\n```\n\nHere's how to run the pre-training. Do not include `init_checkpoint` if you are\npre-training from scratch. The model configuration (including vocab size) is\nspecified in `bert_config_file`. This demo code only pre-trains for a small\nnumber of steps (20), but in practice you will probably want to set\n`num_train_steps` to 10000 steps or more. The `max_seq_length` and\n`max_predictions_per_seq` parameters passed to `run_pretraining.py` must be the\nsame as `create_pretraining_data.py`.\n\n```shell\npython run_pretraining.py \\\n  --input_file=/tmp/tf_examples.tfrecord \\\n  --output_dir=/tmp/pretraining_output \\\n  --do_train=True \\\n  --do_eval=True \\\n  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\n  --train_batch_size=32 \\\n  --max_seq_length=128 \\\n  --max_predictions_per_seq=20 \\\n  --num_train_steps=20 \\\n  --num_warmup_steps=10 \\\n  --learning_rate=2e-5\n```\n\nThis will produce an output like this:\n\n```\n***** Eval results *****\n  global_step = 20\n  loss = 0.0979674\n  masked_lm_accuracy = 0.985479\n  masked_lm_loss = 0.0979328\n  next_sentence_accuracy = 1.0\n  next_sentence_loss = 3.45724e-05\n```\n\nNote that since our `sample_text.txt` file is very small, this example training\nwill overfit that data in only a few steps and produce unrealistically high\naccuracy numbers.\n\n### Pre-training tips and caveats\n\n*   **If using your own vocabulary, make sure to change `vocab_size` in\n    `bert_config.json`. If you use a larger vocabulary without changing this,\n    you will likely get NaNs when training on GPU or TPU due to unchecked\n    out-of-bounds access.**\n*   If your task has a large domain-specific corpus available (e.g., \"movie\n    reviews\" or \"scientific papers\"), it will likely be beneficial to run\n    additional steps of pre-training on your corpus, starting from the BERT\n    checkpoint.\n*   The learning rate we used in the paper was 1e-4. However, if you are doing\n    additional steps of pre-training starting from an existing BERT checkpoint,\n    you should use a smaller learning rate (e.g., 2e-5).\n*   Current BERT models are English-only, but we do plan to release a\n    multilingual model which has been pre-trained on a lot of languages in the\n    near future (hopefully by the end of November 2018).\n*   Longer sequences are disproportionately expensive because attention is\n    quadratic to the sequence length. In other words, a batch of 64 sequences of\n    length 512 is much more expensive than a batch of 256 sequences of\n    length 128. The fully-connected/convolutional cost is the same, but the\n    attention cost is far greater for the 512-length sequences. Therefore, one\n    good recipe is to pre-train for, say, 90,000 steps with a sequence length of\n    128 and then for 10,000 additional steps with a sequence length of 512. The\n    very long sequences are mostly needed to learn positional embeddings, which\n    can be learned fairly quickly. Note that this does require generating the\n    data twice with different values of `max_seq_length`.\n*   If you are pre-training from scratch, be prepared that pre-training is\n    computationally expensive, especially on GPUs. If you are pre-training from\n    scratch, our recommended recipe is to pre-train a `BERT-Base` on a single\n    [preemptible Cloud TPU v2](https://cloud.google.com/tpu/docs/pricing), which\n    takes about 2 weeks at a cost of about $500 USD (based on the pricing in\n    October 2018). You will have to scale down the batch size when only training\n    on a single Cloud TPU, compared to what was used in the paper. It is\n    recommended to use the largest batch size that fits into TPU memory.\n\n### Pre-training data\n\nWe will **not** be able to release the pre-processed datasets used in the paper.\nFor Wikipedia, the recommended pre-processing is to download\n[the latest dump](https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2),\nextract the text with\n[`WikiExtractor.py`](https://github.com/attardi/wikiextractor), and then apply\nany necessary cleanup to convert it into plain text.\n\nUnfortunately the researchers who collected the\n[BookCorpus](http://yknzhu.wixsite.com/mbweb) no longer have it available for\npublic download. The\n[Project Guttenberg Dataset](https://web.eecs.umich.edu/~lahiri/gutenberg_dataset.html)\nis a somewhat smaller (200M word) collection of older books that are public\ndomain.\n\n[Common Crawl](http://commoncrawl.org/) is another very large collection of\ntext, but you will likely have to do substantial pre-processing and cleanup to\nextract a usable corpus for pre-training BERT.\n\n### Learning a new WordPiece vocabulary\n\nThis repository does not include code for *learning* a new WordPiece vocabulary.\nThe reason is that the code used in the paper was implemented in C++ with\ndependencies on Google's internal libraries. For English, it is almost always\nbetter to just start with our vocabulary and pre-trained models. For learning\nvocabularies of other languages, there are a number of open source options\navailable. However, keep in mind that these are not compatible with our\n`tokenization.py` library:\n\n*   [Google's SentencePiece library](https://github.com/google/sentencepiece)\n\n*   [tensor2tensor's WordPiece generation script](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/text_encoder_build_subword.py)\n\n*   [Rico Sennrich's Byte Pair Encoding library](https://github.com/rsennrich/subword-nmt)\n\n## Using BERT in Colab\n\nIf you want to use BERT with [Colab](https://colab.research.google.com), you can\nget started with the notebook\n\"[BERT FineTuning with Cloud TPUs](https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\".\n**At the time of this writing (October 31st, 2018), Colab users can access a\nCloud TPU completely for free.** Note: One per user, availability limited,\nrequires a Google Cloud Platform account with storage (although storage may be\npurchased with free credit for signing up with GCP), and this capability may not\nlonger be available in the future. Click on the BERT Colab that was just linked\nfor more information.\n\n## FAQ\n\n#### Is this code compatible with Cloud TPUs? What about GPUs?\n\nYes, all of the code in this repository works out-of-the-box with CPU, GPU, and\nCloud TPU. However, GPU training is single-GPU only.\n\n#### I am getting out-of-memory errors, what is wrong?\n\nSee the section on [out-of-memory issues](#out-of-memory-issues) for more\ninformation.\n\n#### Is there a PyTorch version available?\n\nThere is no official PyTorch implementation. However, NLP researchers from\nHuggingFace made a\n[PyTorch version of BERT available](https://github.com/huggingface/pytorch-pretrained-BERT)\nwhich is compatible with our pre-trained checkpoints and is able to reproduce\nour results. We were not involved in the creation or maintenance of the PyTorch\nimplementation so please direct any questions towards the authors of that\nrepository.\n\n#### Is there a Chainer version available?\n\nThere is no official Chainer implementation. However, Sosuke Kobayashi made a\n[Chainer version of BERT available](https://github.com/soskek/bert-chainer)\nwhich is compatible with our pre-trained checkpoints and is able to reproduce\nour results. We were not involved in the creation or maintenance of the Chainer\nimplementation so please direct any questions towards the authors of that\nrepository.\n\n#### Will models in other languages be released?\n\nYes, we plan to release a multi-lingual BERT model in the near future. We cannot\nmake promises about exactly which languages will be included, but it will likely\nbe a single model which includes *most* of the languages which have a\nsignificantly-sized Wikipedia.\n\n#### Will models larger than `BERT-Large` be released?\n\nSo far we have not attempted to train anything larger than `BERT-Large`. It is\npossible that we will release larger models if we are able to obtain significant\nimprovements.\n\n#### What license is this library released under?\n\nAll code *and* models are released under the Apache 2.0 license. See the\n`LICENSE` file for more information.\n\n#### How do I cite BERT?\n\nFor now, cite [the Arxiv paper](https://arxiv.org/abs/1810.04805):\n\n```\n@article{devlin2018bert,\n  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},\n  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},\n  journal={arXiv preprint arXiv:1810.04805},\n  year={2018}\n}\n```\n\nIf we submit the paper to a conference or journal, we will update the BibTeX.\n\n## Disclaimer\n\nThis is not an official Google product.\n\n## Contact information\n\nFor help or issues using BERT, please submit a GitHub issue.\n\nFor personal communication related to BERT, please contact Jacob Devlin\n(`jacobdevlin@google.com`), Ming-Wei Chang (`mingweichang@google.com`), or\nKenton Lee (`kentonl@google.com`).\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 119160553,
    "name": "photoprism",
    "full_name": "photoprism/photoprism",
    "description": "AI-Powered Photos App for the Decentralized Web \ud83c\udf08\ud83d\udc8e\u2728",
    "html_url": "https://github.com/photoprism/photoprism",
    "clone_url": "https://github.com/photoprism/photoprism.git",
    "owner_login": "photoprism",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/32436079?v=4",
    "stargazers_count": 38056,
    "watchers_count": 38056,
    "forks_count": 2121,
    "open_issues_count": 427,
    "size": 292719,
    "language": "Go",
    "languages": {
      "Go": 7626761,
      "JavaScript": 1035298,
      "Vue": 853301,
      "Dockerfile": 153403,
      "CSS": 107565,
      "Shell": 59312,
      "Makefile": 52254,
      "Mermaid": 20059,
      "Batchfile": 2247
    },
    "topics": [
      "ai",
      "golang",
      "google-photos",
      "machine-learning",
      "photography",
      "private-cloud",
      "self-hosted",
      "tensorflow"
    ],
    "license_name": "Other",
    "created_at": "2018-01-27T12:00:15+00:00",
    "updated_at": "2025-08-06T01:16:31+00:00",
    "pushed_at": "2025-08-05T16:02:32+00:00",
    "contributors_count": 100,
    "readme_length": 14330,
    "readme_content": "PhotoPrism: Browse Your Life in Pictures\n========================================\n\n[![License: AGPL](https://img.shields.io/badge/license-AGPL%203.0-454377.svg)](https://docs.photoprism.app/license/agpl/)\n[![Documentation](https://img.shields.io/badge/read-the%20docs-4d6a91.svg)](https://docs.photoprism.app/)\n[![Community Chat](https://img.shields.io/badge/chat-on%20gitter-4d6a91.svg)](https://link.photoprism.app/chat)\n[![GitHub Discussions](https://img.shields.io/badge/ask-%20on%20github-4d6a91.svg)](https://link.photoprism.app/discussions)\n[![Bluesky Social](https://dl.photoprism.app/img/badges/badge-bluesky.svg)](https://bsky.app/profile/photoprism.app)\n[![Mastodon](https://dl.photoprism.app/img/badges/badge-floss-social.svg)](https://floss.social/@photoprism)\n\nPhotoPrism\u00ae is an AI-Powered Photos App for the [Decentralized Web](https://en.wikipedia.org/wiki/Decentralized_web).\nIt makes use of the latest technologies to tag and find pictures automatically without getting in your way.\nYou can run it at home, on a private server, or in the cloud.\n\n![](https://dl.photoprism.app/img/ui/2025/desktop-search.jpg)\n\nTo get a first impression, you are welcome to play with our [public demo](https://try.photoprism.app/). Please be careful not to upload any private, unlawful or offensive pictures.\n\n## Feature Overview ##\n\n**Our mission is to provide the most user- and privacy-friendly solution to keep your pictures organized and accessible.** That's why PhotoPrism was built from the ground up to run wherever you need it, without compromising freedom, privacy, or functionality:\n\n<img align=\"right\" height=\"270\" src=\"https://dl.photoprism.app/img/ui/2025/iphone-crocus-540px.png\">\n\n* Browse [all your pictures](https://docs.photoprism.app/user-guide/organize/browse/) without worrying about [RAW images](https://www.photoprism.app/kb/file-formats) or [video formats](https://docs.photoprism.app/user-guide/organize/video/)\n* Whether you're using a phone, tablet, or desktop computer, our [intuitive PWA](https://try.photoprism.app/) provides a native app-like experience and can be [easily installed](https://docs.photoprism.app/user-guide/pwa/) on your home screen\n* Quickly find specific photos and videos with [powerful search filters](https://docs.photoprism.app/user-guide/search/filters/) that can be combined and are available for [many different properties](https://docs.photoprism.app/user-guide/search/filters/#filter-reference), including [labels](https://try.photoprism.app/library/labels), [location](https://try.photoprism.app/library/places?q=s2:47a85a63f764), [resolution](https://try.photoprism.app/library/browse?view=cards&q=mp:4), [color](https://try.photoprism.app/library/browse?view=cards&q=color:red), [chroma](https://try.photoprism.app/library/browse?view=cards&q=mono%3Atrue), and [quality](https://try.photoprism.app/library/review)\n* [Automatically labels your pictures](https://try.photoprism.app/library/labels) based on content and location, and recognizes the faces of [your family and friends](https://try.photoprism.app/library/people/new)\n* [Live Photos](https://try.photoprism.app/library/live) start playing when you [hover over them](https://try.photoprism.app/library/browse?view=cards&q=type%3Alive) and when viewing a slideshow\n* Six high-resolution [World Maps](https://try.photoprism.app/library/places) and our [privacy-preserving geocoding service](https://docs.photoprism.app/getting-started/#maps-places) help bring back memories of your favorite trips and let you explore the world\n* Metadata can be extracted and merged from Exif, XMP, and other sources like Google Photos\n* [Use compatible apps](https://docs.photoprism.app/user-guide/native-apps/) like [PhotoSync](https://link.photoprism.app/photosync) to back up iOS and Android phones in the background\n* WebDAV clients such as [Microsoft's Windows Explorer](https://docs.photoprism.app/user-guide/sync/webdav/#__tabbed_1_2) and [Apple's Finder](https://docs.photoprism.app/user-guide/sync/webdav/#connect-to-a-webdav-server) can [connect directly to PhotoPrism](https://docs.photoprism.app/user-guide/sync/webdav/), allowing you to open, edit, and delete files from your computer as if they were local\n\nBeing completely [**self-funded and independent**](https://link.photoprism.app/membership), we can promise you that we will [never sell your data](https://www.photoprism.app/privacy) and that we will [always be transparent](https://www.photoprism.app/terms) about our software and services. Your data will never be shared with Google, Amazon, Microsoft or Apple unless you intentionally upload files to one of their services. \ud83d\udd12\n\n## Getting Started ##\n\nStep-by-step [installation instructions](https://docs.photoprism.app/getting-started/) for our self-hosted [community edition](https://link.photoprism.app/personal-editions) can be found on [docs.photoprism.app](https://docs.photoprism.app/getting-started/) - all you need is a Web browser and [Docker](https://docs.docker.com/get-docker/) to run the server. It is available for Mac, Linux, and Windows.\n\nThe [stable releases](https://docs.photoprism.app/release-notes/) and [development preview](https://docs.photoprism.app/getting-started/updates/#development-preview) are available as a [multi-arch image](https://link.photoprism.app/docker-hub) for 64-bit AMD, Intel, and ARM processors.\nThat means, [Raspberry Pi](https://docs.photoprism.app/getting-started/raspberry-pi/) and Apple Silicon users enjoy the exact same functionality and can follow the same [installation steps](https://docs.photoprism.app/getting-started/docker-compose/).\n\nSee our [Getting Started FAQ](https://docs.photoprism.app/getting-started/faq/#how-can-i-install-photoprism-without-docker) for alternative installation methods, for example using the [*tar.gz* packages](https://dl.photoprism.app/pkg/linux/README.html) we provide.\n\n## Support Our Mission \ud83d\udc8e ##\n\n**PhotoPrism is 100% self-funded and independent.** Your [continued support](https://link.photoprism.app/membership) helps us [provide more features to the public](https://www.photoprism.app/oss/faq#what-functionality-is-generally-available), release [regular updates](https://docs.photoprism.app/release-notes/), and remain independent!\n\nOur members [enjoy additional features](https://www.photoprism.app/kb/personal), including access to [interactive world maps](https://try.photoprism.app/library/places), and can join our private chat room to [connect with our team](https://www.photoprism.app/about/team). We currently have the following membership options:\n\n- You can [sign up directly on our website](https://link.photoprism.app/membership) and pay with credit card or SEPA through Stripe, so you don't need to [link an external account](https://www.photoprism.app/kb/activation) and can easily upgrade or downgrade at any time\n- Alternatively, [Patreon](https://link.photoprism.app/patreon) also supports PayPal, additional currencies, and lets you choose between monthly and annual billing for all tiers\n\nIf you currently support us through [GitHub Sponsors](https://link.photoprism.app/sponsor), you can also [register on our website](https://my.photoprism.app/register) and use the *Activate GitHub Sponsors Membership* button to link your account. For details on this and how to [link your Patreon account](https://www.patreon.com/pledges), see our [Activation Guide](https://www.photoprism.app/kb/activation).\n\nYou are [welcome to contact us](https://www.photoprism.app/contact) for change requests, membership questions, and business partnerships.\n\n[View Membership FAQ \u203a](https://www.photoprism.app/kb/membership)\u2003[Sign Up \u203a](https://link.photoprism.app/membership)\n\n### Why Your Support Matters ###\n\n- Your continued support helps us provide regular updates and remain independent, so we can fulfill our mission and protect your privacy\n- Sustained funding is key to quickly releasing new features requested by you and other community members\n- Being self-funded and independent, we can personally promise you that we will never sell your data and that we will always be transparent about our software and services\n\nPlease also leave [a star](https://github.com/photoprism/photoprism/stargazers) on GitHub if you like this project. It provides additional motivation to keep going.\n\n**A big thank you to all current and past sponsors, whose generous support has been and continues to be essential to the success of the project!**\n\n[View Sponsors \u203a](SPONSORS.md)\u2003[View Credits \u203a](https://docs.photoprism.app/credits/)\n\n## Getting Support ##\n\nVisit [docs.photoprism.app/user-guide](https://docs.photoprism.app/user-guide/) to learn how to [sync](https://docs.photoprism.app/user-guide/sync/webdav/), [organize](https://docs.photoprism.app/user-guide/library/), and [share](https://docs.photoprism.app/user-guide/share/) your pictures. If you need help installing our software at home, you are welcome to post your question in [GitHub Discussions](https://link.photoprism.app/discussions) or ask in our [Community Chat](https://link.photoprism.app/chat).\nCommon problems can be quickly diagnosed and solved using our [Troubleshooting Checklists](https://docs.photoprism.app/getting-started/troubleshooting/). Eligible [members](https://link.photoprism.app/membership) are also welcome to email us for technical support and advice.\n\n## Upcoming Features and Enhancements ##\n\n<a href=\"https://github.com/orgs/photoprism/projects/5\"><img align=\"right\" height=\"240\" src=\"https://dl.photoprism.app/img/ui/2025/upcoming-features-240px.png\"></a>\n\nOur [Project Roadmap](https://link.photoprism.app/roadmap) shows what tasks are in progress and what features will be implemented next. You are invited to give ideas you like a thumbs-up, so we know what's most popular.\n\nBe aware that we have a zero-bug policy and do our best to help users when they need support or have other questions. This comes at a price though, as we can't give exact release dates for new features. Our team receives many more requests than can be implemented, so we want to emphasize that we are in no way obligated to implement the features, enhancements, or other changes you request. We do, however, appreciate your feedback and carefully consider all requests.\n\n**Because sustained funding is key to quickly releasing new features, we encourage you to support our mission by [signing up for a personal membership](https://link.photoprism.app/membership) or [purchasing a commercial license](https://www.photoprism.app/teams#compare).**\n\n[Become a Member \u203a](https://link.photoprism.app/membership)\n\n## GitHub Issues \u26a0\ufe0f ##\n\nWe kindly ask you not to report bugs via GitHub Issues **unless you are certain to have found a fully reproducible and previously unreported issue** that must be fixed directly in the app. Thank you for your careful consideration!\n\n- When browsing issues, please note that **our team and all issue subscribers receive an email notification** from GitHub whenever a new comment is added, so these should only be used for sharing important information and not for [discussions, questions](https://github.com/photoprism/photoprism/discussions), or [expressing personal opinions](https://www.photoprism.app/code-of-conduct)\n- In order for us to investigate [new bug reports](https://www.photoprism.app/kb/reporting-bugs), they must include **a complete list of steps to reproduce the problem**, the software versions used and information about the environment in which the problem occurred, such as [browser type, browser version, browser plug-ins](https://docs.photoprism.app/getting-started/troubleshooting/browsers/), operating system, [storage type](https://docs.photoprism.app/getting-started/troubleshooting/performance/#storage), [processor type](https://docs.photoprism.app/getting-started/troubleshooting/performance/#server-cpu), and [memory size](https://docs.photoprism.app/getting-started/troubleshooting/performance/#memory)\n- [Contact us](https://www.photoprism.app/contact) or [a community member](https://link.photoprism.app/discussions) if you need help, it could be a local configuration problem, or a misunderstanding in how the software works\n- This gives us the opportunity to [improve our documentation](https://docs.photoprism.app/getting-started/troubleshooting/) and provide best-in-class support instead of dealing with unclear/duplicate bug reports or triggering a flood of notifications by replying to comments\n\n## Connect with the Community ##\n\n<a href=\"https://link.photoprism.app/chat\"><img align=\"right\" width=\"144\" height=\"144\" src=\"https://dl.photoprism.app/img/brands/element-logo.svg\"></a>\n\nFollow us on [Mastodon](https://floss.social/@photoprism), [Bluesky](https://bsky.app/profile/photoprism.app), or join the [Community Chat](https://link.photoprism.app/chat) to get regular updates, connect with other users, and discuss your ideas. Our [Code of Conduct](https://www.photoprism.app/code-of-conduct) explains the \"dos and don\u2019ts\" when interacting with other community members.\n\nAs a [contributor](CONTRIBUTING.md), you are also welcome to [contact us directly](https://www.photoprism.app/contact) if you have something on your mind that you don't want to discuss publicly. Please note, however, that due to the high volume of emails we receive, our team may be unable to get back to you immediately. We do our best to respond within five business days or less.\n\n## Every Contribution Makes a Difference ##\n\nWe welcome [contributions](CONTRIBUTING.md) of any kind, including blog posts, tutorials, translations, testing, writing documentation, and pull requests. Our [Developer Guide](https://docs.photoprism.app/developer-guide/) contains all the information necessary for you to get started.\n\n----\n\n*PhotoPrism\u00ae is a [registered trademark](https://www.photoprism.app/trademark). By using the software and services we provide, you agree to our [Terms of Service](https://www.photoprism.app/terms), [Privacy Policy](https://www.photoprism.app/privacy), and [Code of Conduct](https://www.photoprism.app/code-of-conduct). Docs are [available](https://link.photoprism.app/github-docs) under the [CC BY-NC-SA 4.0 License](https://creativecommons.org/licenses/by-nc-sa/4.0/); [additional terms](https://github.com/photoprism/photoprism/blob/develop/assets/README.md) may apply.*\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 45512989,
    "name": "gold-miner",
    "full_name": "xitu/gold-miner",
    "description": "\ud83e\udd47\u6398\u91d1\u7ffb\u8bd1\u8ba1\u5212\uff0c\u53ef\u80fd\u662f\u4e16\u754c\u6700\u5927\u6700\u597d\u7684\u82f1\u8bd1\u4e2d\u6280\u672f\u793e\u533a\uff0c\u6700\u61c2\u8bfb\u8005\u548c\u8bd1\u8005\u7684\u7ffb\u8bd1\u5e73\u53f0\uff1a",
    "html_url": "https://github.com/xitu/gold-miner",
    "clone_url": "https://github.com/xitu/gold-miner.git",
    "owner_login": "xitu",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/10482599?v=4",
    "stargazers_count": 34226,
    "watchers_count": 34226,
    "forks_count": 5047,
    "open_issues_count": 10,
    "size": 80499,
    "language": null,
    "languages": {},
    "topics": [
      "ai",
      "android",
      "frontend",
      "ios",
      "javascript",
      "react",
      "swift",
      "tensorflow",
      "translation",
      "tutorials"
    ],
    "license_name": null,
    "created_at": "2015-11-04T03:29:13+00:00",
    "updated_at": "2025-08-06T01:46:58+00:00",
    "pushed_at": "2024-04-17T09:44:37+00:00",
    "contributors_count": 100,
    "readme_length": 10551,
    "readme_content": "# \u6398\u91d1\u7ffb\u8bd1\u8ba1\u5212\n\n[![xitu](https://camo.githubusercontent.com/c9c9db0a39b56738a62332f0791d58b1522fdf82/68747470733a2f2f7261776769742e636f6d2f616c65656e34322f6261646765732f6d61737465722f7372632f786974752e737667)](https://github.com/xitu/gold-miner)\n[![\u6398\u91d1\u7ffb\u8bd1\u8ba1\u5212](https://rawgit.com/aleen42/badges/master/src/juejin_translation.svg)](https://github.com/xitu/gold-miner/)\n[![](https://img.shields.io/badge/weibo-%E6%8E%98%E9%87%91%E7%BF%BB%E8%AF%91%E8%AE%A1%E5%88%92-brightgreen.svg)](http://weibo.com/juejinfanyi)\n[![](https://img.shields.io/badge/%E7%9F%A5%E4%B9%8E%E4%B8%93%E6%A0%8F-%E6%8E%98%E9%87%91%E7%BF%BB%E8%AF%91%E8%AE%A1%E5%88%92-blue.svg)](https://zhuanlan.zhihu.com/juejinfanyi)\n\n[\u6398\u91d1\u7ffb\u8bd1\u8ba1\u5212](https://juejin.im/tag/%E6%8E%98%E9%87%91%E7%BF%BB%E8%AF%91%E8%AE%A1%E5%88%92) \u662f\u4e00\u4e2a\u7ffb\u8bd1\u4f18\u8d28\u4e92\u8054\u7f51\u6280\u672f\u6587\u7ae0\u7684\u793e\u533a\uff0c\u6587\u7ae0\u6765\u6e90\u4e3a [\u6398\u91d1](https://juejin.im) \u4e0a\u7684\u82f1\u6587\u5206\u4eab\u6587\u7ae0\u3002\u5185\u5bb9\u8986\u76d6[\u533a\u5757\u94fe](#\u533a\u5757\u94fe)\u3001[\u4eba\u5de5\u667a\u80fd](#ai--deep-learning--machine-learning)\u3001[Android](#android)\u3001[iOS](#ios)\u3001[\u524d\u7aef](#\u524d\u7aef)\u3001[\u540e\u7aef](#\u540e\u7aef)\u3001[\u8bbe\u8ba1](#\u8bbe\u8ba1)\u3001[\u4ea7\u54c1](#\u4ea7\u54c1)\u3001[\u7b97\u6cd5](https://github.com/xitu/gold-miner/blob/master/algorithm.md)\u548c[\u5176\u4ed6](#\u5176\u4ed6)\u7b49\u9886\u57df\uff0c\u4ee5\u53ca\u5404\u5927\u578b\u4f18\u8d28 [\u5b98\u65b9\u6587\u6863\u53ca\u624b\u518c](#\u5b98\u65b9\u6587\u6863\u53ca\u624b\u518c)\uff0c\u8bfb\u8005\u4e3a\u70ed\u7231\u65b0\u6280\u672f\u7684\u65b0\u9510\u5f00\u53d1\u8005\u3002\n\n\u6398\u91d1\u7ffb\u8bd1\u8ba1\u5212\u76ee\u524d\u7ffb\u8bd1\u5b8c\u6210 [4000](#\u8fd1\u671f\u6587\u7ae0\u5217\u8868) \u4f59\u7bc7\u6587\u7ae0\uff0c\u5b98\u65b9\u6587\u6863\u53ca\u624b\u518c [13](#\u5b98\u65b9\u6587\u6863\u53ca\u624b\u518c) \u4e2a\uff0c\u5171\u6709 [1500](https://github.com/xitu/gold-miner/wiki/%E8%AF%91%E8%80%85%E7%A7%AF%E5%88%86%E8%A1%A8) \u4f59\u540d\u8bd1\u8005\u8d21\u732e\u7ffb\u8bd1\u548c\u6821\u5bf9\u3002\n\n> ## [\ud83e\udd47\u6398\u91d1\u7ffb\u8bd1\u8ba1\u5212 \u2014 \u533a\u5757\u94fe\u5206\u8235](https://github.com/xitu/blockchain-miner)\n\n# \u5b98\u65b9\u6307\u5357\n\n[**\u63a8\u8350\u4f18\u8d28\u82f1\u6587\u6587\u7ae0\u5230\u6398\u91d1\u7ffb\u8bd1\u8ba1\u5212**](https://github.com/xitu/gold-miner/issues/new/choose)\n\n<!--\nhttps://github.com/xitu/gold-miner/issues/new?title=\u63a8\u8350\u4f18\u79c0\u82f1\u6587\u6587\u7ae0&body=-%20\u539f\u6587\u94fe\u63a5\uff1a\u63a8\u8350\u6587\u7ae0\u524d%20Google%20\u4e00\u4e0b\uff0c\u5c3d\u91cf\u4fdd\u8bc1\u672c\u6587\u672a\u88ab\u7ffb\u8bd1%0A-%20\u7b80\u8981\u4ecb\u7ecd\uff1a\u4ecb\u7ecd\u4e00\u4e0b\u597d\u4e0d\u597d\u5566\uff0c\u6bd5\u7adf\u5c0f\u7f16\u4e5f\u770b\u4e0d\u592a\u61c2\u54ce_(:\u0437\u300d\u2220)_)\n-->\n\n### \u7ffb\u8bd1\u8ba1\u5212\u8bd1\u8005\u6559\u7a0b\n\n1. [\u5982\u4f55\u53c2\u4e0e\u7ffb\u8bd1](https://github.com/xitu/gold-miner/wiki/%E5%A6%82%E4%BD%95%E5%8F%82%E4%B8%8E%E7%BF%BB%E8%AF%91)\n2. [\u5173\u4e8e\u5982\u4f55\u63d0\u4ea4\u7ffb\u8bd1\u4ee5\u53ca\u540e\u7eed\u66f4\u65b0\u7684\u6559\u7a0b](https://github.com/xitu/gold-miner/wiki/%E5%85%B3%E4%BA%8E%E5%A6%82%E4%BD%95%E6%8F%90%E4%BA%A4%E7%BF%BB%E8%AF%91%E4%BB%A5%E5%8F%8A%E5%90%8E%E7%BB%AD%E6%9B%B4%E6%96%B0%E7%9A%84%E6%95%99%E7%A8%8B)\n3. [\u5982\u4f55\u53c2\u4e0e\u6821\u5bf9\u53ca\u6821\u5bf9\u7684\u6b63\u786e\u59ff\u52bf](https://github.com/xitu/gold-miner/wiki/%E5%8F%82%E4%B8%8E%E6%A0%A1%E5%AF%B9%E7%9A%84%E6%AD%A3%E7%A1%AE%E5%A7%BF%E5%8A%BF)\n4. [\u6587\u7ae0\u5206\u4eab\u5230\u6398\u91d1\u6307\u5357](https://github.com/xitu/gold-miner/wiki/%E5%88%86%E4%BA%AB%E5%88%B0%E6%8E%98%E9%87%91%E6%8C%87%E5%8D%97)\n5. [\u8bd1\u6587\u6392\u7248\u89c4\u5219\u6307\u5317](https://github.com/xitu/gold-miner/wiki/%E8%AF%91%E6%96%87%E6%8E%92%E7%89%88%E8%A7%84%E5%88%99%E6%8C%87%E5%8C%97)\n\n\n# \u8fd1\u671f\u6587\u7ae0\u5217\u8868\n\n## \u5b98\u65b9\u6587\u6863\u53ca\u624b\u518c\n\n* [\u5e74\u5ea6\u603b\u7ed3\u7cfb\u5217](https://github.com/xitu/Annual-Survey)\n* [TensorFlow \u4e2d\u6587\u6587\u6863](https://github.com/xitu/tensorflow-docs)\n* [The JavaScript Tutorial](https://github.com/xitu/javascript-tutorial-zh)\n* [ML Kit \u4e2d\u6587\u6587\u6863](https://github.com/Quorafind/MLkit-CN)\n* [GraphQL \u4e2d\u6587\u6587\u6863](https://github.com/xitu/graphql.github.io)\n* [Under-the-hood-ReactJS \u7cfb\u5217\u6559\u7a0b](https://github.com/xitu/Under-the-hood-ReactJS)\n* [\u7cfb\u7edf\u8bbe\u8ba1\u5165\u95e8\u6559\u7a0b](https://github.com/xitu/system-design-primer)\n* [Google Interview University \u9762\u8bd5\u6307\u5317](https://github.com/xitu/google-interview-university)\n* [\u524d\u7aef\u5f00\u53d1\u8005\u6307\u5357\uff082017\uff09](https://github.com/xitu/front-end-handbook-2017)\n* [\u524d\u7aef\u5f00\u53d1\u8005\u6307\u5357\uff082018\uff09](https://github.com/xitu/front-end-handbook-2018)\n* [Awesome Flutter](https://github.com/xitu/awesome-flutter)\n* [macOS Security and Privacy Guide](https://github.com/xitu/macOS-Security-and-Privacy-Guide)\n* [State of Vue.js report 2017 \u4e2d\u6587\u7248](https://github.com/xitu/gold-miner/blob/master/TODO/state-of-vue-report-2017.md)\n* [Next.js \u8f7b\u91cf\u7ea7 React \u670d\u52a1\u7aef\u6e32\u67d3\u5e94\u7528\u6846\u67b6\u4e2d\u6587\u6587\u6863](http://nextjs.frontendx.cn/)\n\n## \u533a\u5757\u94fe\n\n* [\u5c5e\u4e8e JavaScript \u5f00\u53d1\u8005\u7684 Crypto \u7b80\u4ecb](https://juejin.im/post/5ce0c39a51882525f07ef0fa) ([Xuyuey](https://github.com/Xuyuey) \u7ffb\u8bd1)\n* [\u6211\u4eec\u4e3a\u4ec0\u4e48\u770b\u597d\u52a0\u5bc6\u6536\u85cf\u54c1\uff08NFT\uff09\u7684\u524d\u666f](https://juejin.im/post/5cb87819518825329e7ea61e) ([portandbridge](https://github.com/portandbridge) \u7ffb\u8bd1)\n* [2019 \u533a\u5757\u94fe\u5e73\u53f0\u4e0e\u6280\u672f\u5c55\u671b](https://juejin.im/post/5c613e6e6fb9a049e4132ba5) ([gs666](https://github.com/gs666) \u7ffb\u8bd1)\n* [\u4ee5\u592a\u574a\u5165\u95e8\u6307\u5357](https://juejin.im/post/5c1080fbe51d452b307969a3) ([gs666](https://github.com/gs666) \u7ffb\u8bd1)\n* [\u4ee5\u592a\u574a\u5165\u95e8\uff1a\u4e92\u8054\u7f51\u653f\u5e9c](https://juejin.im/post/5c03c68851882551236eaa82) ([newraina](https://github.com/newraina) \u7ffb\u8bd1)\n* [\u6240\u6709\u533a\u5757\u94fe\u8bd1\u6587>>](https://github.com/xitu/gold-miner/blob/master/blockchain.md)\n\n## \u4eba\u5de5\u667a\u80fd\n\n* [\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u8bbe\u8ba1\u76f8\u5173\u9762\u8bd5\u95ee\u9898\u7684\u5256\u6790](https://juejin.cn/post/7109306303285051406)\uff08[caiyundong](https://github.com/caiyundong) \u7ffb\u8bd1\uff09\n* [\u5982\u4f55\u4f7f\u7528 Python \u7ba1\u9053 Pipe \u9ad8\u6548\u7f16\u7801](https://juejin.cn/post/7051051681357758494)\uff08[zenblofe](https://github.com/zenblofe) \u7ffb\u8bd1\uff09\n* [\u4f7f\u7528\u4eba\u5de5\u667a\u80fd/\u673a\u5668\u5b66\u4e60\u6784\u5efa\u6587\u7ae0\u63a8\u8350\u5f15\u64ce](https://juejin.cn/post/7001479252163952670)\uff08[jaredliw](https://github.com/jaredliw) \u7ffb\u8bd1\uff09\n* [AI \u662f\u5426\u5df2\u7ecf\u6210\u4e3a\u5185\u5bb9\u8425\u9500\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff1f](https://juejin.cn/post/6964280632801394724)\uff08[5Reasons](https://github.com/5Reasons) \u7ffb\u8bd1\uff09\n* [Google \u7684 Apollo \u82af\u7247\u8bbe\u8ba1\u4eba\u5de5\u667a\u80fd\u6846\u67b6\u5c06\u6df1\u5ea6\u5b66\u4e60\u82af\u7247\u7684\u6027\u80fd\u63d0\u9ad8\u4e86 25\uff05](https://juejin.cn/post/6952819856429285407)\uff08[PingHGao](https://github.com/PingHGao) \u7ffb\u8bd1\uff09\n* [\u6240\u6709 AI \u8bd1\u6587>>](https://github.com/xitu/gold-miner/blob/master/AI.md)\n\n## Android\n\n* [6 \u6761 Jetpack Compose \u6307\u5357\u5e2e\u4f60\u4f18\u5316 App \u6027\u80fd](https://juejin.cn/post/7153803045418041358)\uff08[Quincy-Ye](https://github.com/Quincy-Ye) \u7ffb\u8bd1\uff09\n* [React Native \u5f00\u53d1\u8005\u7684\u6d41\u884c\u5b58\u50a8\u65b9\u6848](https://juejin.cn/post/7008020729832669191)\uff08[KimYangOfCat](https://github.com/KimYangOfCat) \u7ffb\u8bd1\uff09\n* [Jetpack Compose\uff1a\u6837\u5f0f\u548c\u4e3b\u9898\uff08\u7b2c\u4e8c\u90e8\u5206\uff09](https://juejin.cn/post/6995419287435345934)\uff08[Kimhooo](https://github.com/Kimhooo) \u7ffb\u8bd1\uff09\n* [\u63a2\u7d22 ANDROID 12\uff1a\u542f\u52a8\u753b\u9762](https://juejin.cn/post/6983942336824737822)\uff08[Kimhooo](https://github.com/Kimhooo) \u7ffb\u8bd1\uff09\n* [Jetpack Compose\uff1a\u66f4\u7b80\u4fbf\u7684 RecyclerView\uff08\u7b2c\u4e00\u90e8\u5206\uff09](https://juejin.cn/post/6970858140824764424)\uff08[Kimhooo](https://github.com/Kimhooo) \u7ffb\u8bd1\uff09\n* [\u6240\u6709 Android \u8bd1\u6587>>](https://github.com/xitu/gold-miner/blob/master/android.md)\n\n## iOS\n\n* [2021 \u7684 SwiftUI\uff1a\u597d\u5904\u3001\u574f\u5904\u4ee5\u53ca\u4e11\u5904](https://juejin.cn/post/7140825514108780580)\uff08[earthaYan](https://github.com/earthaYan) \u7ffb\u8bd1\uff09\n* [4 \u4e2a\u9c9c\u4e3a\u4eba\u77e5\u7684 Swift \u7279\u6027](https://juejin.cn/post/7069326429397205005)\uff08[jaredliw](https://github.com/jaredliw) \u7ffb\u8bd1\uff09\n* [React Native \u5f00\u53d1\u8005\u7684\u6d41\u884c\u5b58\u50a8\u65b9\u6848](https://juejin.cn/post/7008020729832669191)\uff08[KimYangOfCat](https://github.com/KimYangOfCat) \u7ffb\u8bd1\uff09\n* [\u9006\u5411 `.car` \u6587\u4ef6\uff08\u5df2\u7f16\u8bd1\u7684 Asset Catalogs\uff09](https://juejin.cn/post/7002491722550919198)\uff08[LoneyIsError](https://github.com/LoneyIsError) \u7ffb\u8bd1\uff09\n* [Swift \u4e2d\u7684\u5185\u5b58\u5e03\u5c40](https://juejin.cn/post/6986520506002472973)\uff08[LoneyIsError](https://github.com/LoneyIsError) \u7ffb\u8bd1\uff09\n* [\u6240\u6709 iOS \u8bd1\u6587>>](https://github.com/xitu/gold-miner/blob/master/ios.md)\n\n## \u524d\u7aef\n\n* [\u5168\u9762\u5228\u6790 CSS-in-JS](https://juejin.cn/post/7172360607201493029)\uff08[Tong-H](https://github.com/Tong-H) \u7ffb\u8bd1\uff09\n* [WebRTC \u4e0e WebSockets \u6559\u7a0b \u2014 Web \u7aef\u7684\u5b9e\u65f6\u901a\u4fe1](https://juejin.cn/post/7138015673850003493)\uff08[DylanXie123](https://github.com/DylanXie123) \u7ffb\u8bd1\uff09\n* [ES2022 \u6709\u4ec0\u4e48\u65b0\u7279\u6027\uff1f](https://juejin.cn/post/7114676836851777566)\uff08[CarlosChenN](https://github.com/CarlosChenN) \u7ffb\u8bd1\uff09\n* [\u4f5c\u4e3a\u4e00\u540d\u524d\u7aef\u5de5\u7a0b\u5e08\u6211\u6d6a\u8d39\u65f6\u95f4\u5b66\u4e60\u4e86\u8fd9\u4e9b\u6280\u672f](https://juejin.cn/post/7086019601372282888)\uff08[airfri](https://github.com/airfri) \u7ffb\u8bd1\uff09\n* [\u8fc7\u5ea6\u4f7f\u7528\u61d2\u52a0\u8f7d\u5bf9 Web \u6027\u80fd\u7684\u5f71\u54cd](https://juejin.cn/post/7074759905197948935)\uff08[Tong-H](https://github.com/Tong-H) \u7ffb\u8bd1\uff09\n* [\u5982\u4f55\u5728\u7f51\u9875\u4e2d\u4f7f\u7528\u54cd\u5e94\u5f0f\u56fe\u50cf](https://juejin.cn/post/7074199947477778439)\uff08[zenblofe](https://github.com/zenblofe) \u7ffb\u8bd1\uff09\n* [\u5982\u4f55\u7f16\u5199\u66f4\u7b80\u6d01\u4f18\u96c5\u7684 React \u4ee3\u7801](https://juejin.cn/post/7070479272380465166)\uff08[zenblofe](https://github.com/zenblofe) \u7ffb\u8bd1\uff09\n* [\u7528 PNPM Workspaces \u66ff\u6362 Lerna + Yarn](https://juejin.cn/post/7071992448511279141)\uff08[CarlosChenN](https://github.com/CarlosChenN) \u7ffb\u8bd1\uff09\n* [\u6240\u6709\u524d\u7aef\u8bd1\u6587>>](https://github.com/xitu/gold-miner/blob/master/front-end.md)\n\n## \u540e\u7aef\n\n* [\u5b9e\u73b0 Bitcask \uff0c\u4e00\u79cd\u65e5\u5fd7\u7ed3\u6784\u7684\u54c8\u5e0c\u8868](https://juejin.cn/post/7174345557861728292)\uff08[wangxuanni](https://github.com/wangxuanni) \u7ffb\u8bd1\uff09\n* [\u7528 Isabelle/HOL \u9a8c\u8bc1\u5206\u5e03\u5f0f\u7cfb\u7edf](https://juejin.cn/post/7166450887626326030)\uff08[wangxuanni](https://github.com/wangxuanni) \u7ffb\u8bd1\uff09\n* [\u5341\u5927 Java \u8bed\u8a00\u7279\u6027](https://juejin.cn/post/7140097107000000520)\uff08[jaredliw](https://github.com/jaredliw) \u7ffb\u8bd1\uff09\n* [\u4f7f\u7528\u4ee4\u724c\u6876\u548c\u7194\u65ad\u5668\u8fdb\u884c\u91cd\u8bd5](https://juejin.cn/post/7153093426446237727)\uff08[wangxuanni](https://github.com/wangxuanni) \u7ffb\u8bd1\uff09\n* [WebRTC \u4e0e WebSockets \u6559\u7a0b \u2014 Web \u7aef\u7684\u5b9e\u65f6\u901a\u4fe1](https://juejin.cn/post/7138015673850003493)\uff08[DylanXie123](https://github.com/DylanXie123) \u7ffb\u8bd1\uff09\n* [\u5fae\u670d\u52a1\u67b6\u6784\u4f55\u65f6\u4f1a\u662f\u4e00\u79cd\u574f\u9009\u62e9](https://juejin.cn/post/7135364257918484488)\uff08[DylanXie123](https://github.com/DylanXie123) \u7ffb\u8bd1\uff09\n* [\u5982\u4f55\u4f7f\u7528 Python \u4e2d\u7684 PyPA setuptools \u6253\u5305\u548c\u90e8\u7f72 CLI \u5e94\u7528\u7a0b\u5e8f](https://juejin.cn/post/7125323312321789989)\uff08[haiyang-tju](https://github.com/haiyang-tju) \u7ffb\u8bd1\uff09\n* [10 \u4e2a\u6700\u96be\u7684 Python \u95ee\u9898](https://juejin.cn/post/7124285689717325831)\uff08[jaredliw](https://github.com/jaredliw) \u7ffb\u8bd1\uff09\n* [\u6240\u6709\u540e\u7aef\u8bd1\u6587>>](https://github.com/xitu/gold-miner/blob/master/backend.md)\n\n## \u8bbe\u8ba1\n\n* [5\u4e2a\u5173\u4e8e UI \u8bbe\u8ba1\u7cfb\u7edf\u7684\u8bef\u89e3](https://juejin.cn/post/7086291006286462990)\uff08[CarlosChenN](https://github.com/CarlosChenN) \u7ffb\u8bd1\uff09\n* [\u522b\u8ba9\u8f6e\u64ad\u6bc1\u4e86\u4f60\u7684\u5e94\u7528\u7a0b\u5e8f](https://juejin.cn/post/7003637296050225189)\uff08[jaredliw](https://github.com/jaredliw) \u7ffb\u8bd1\uff09\n* [\u4e3a Web \u5f00\u53d1\u540c\u5b66\u51c6\u5907\u7684 11 \u4e2a\u7b80\u5355\u5b9e\u7528\u7684 UI \u8bbe\u8ba1\u5c0f\u6280\u5de7](https://juejin.cn/post/6960922956876742669)\uff08[5Reasons](https://github.com/5Reasons) \u7ffb\u8bd1\uff09\n* [\u4f60\u6709\u8bbe\u8ba1\u4f5c\u54c1\u7684\u4f5c\u54c1\u96c6\u5417\uff1f\u633a\u597d\u7684\uff0c\u4f46\u8fd9\u8fd8\u4e0d\u591f](https://juejin.cn/post/6934328263011467277)\uff08[PassionPenguin](https://github.com/PassionPenguin) \u7ffb\u8bd1\uff09\n* [\u6784\u5efa\u8bbe\u8ba1\u7cfb\u7edf\u548c\u7ec4\u4ef6\u5e93](https://juejin.cn/post/6924152501805678606)\uff08[Charlo-O](https://github.com/Charlo-O) \u7ffb\u8bd1\uff09\n* [\u6240\u6709\u8bbe\u8ba1\u8bd1\u6587>>](https://github.com/xitu/gold-miner/blob/master/design.md)\n\n## \u4ea7\u54c1\n\n* [Github Actions \u662f\u5982\u4f55\u6e32\u67d3\u8d85\u5927\u65e5\u5fd7\u7684](https://juejin.cn/post/6966082485226569759)\uff08[felixliao](https://github.com/felixliao) \u7ffb\u8bd1\uff09\n* [\u7b97\u6cd5\u4e0d\u662f\u4ea7\u54c1](https://juejin.im/post/5e398e806fb9a07cb52bb462)\uff08[fireairforce](https://github.com/fireairforce) \u7ffb\u8bd1\uff09\n* [\u5229\u7528 84 \u79cd\u8ba4\u77e5\u504f\u89c1\u8bbe\u8ba1\u66f4\u597d\u7684\u4ea7\u54c1 \u2014\u2014 \u7b2c\u4e09\u90e8\u5206](https://juejin.im/post/5d568c9ce51d453bc64801cd)\uff08[JalanJiang](https://github.com/JalanJiang) \u7ffb\u8bd1\uff09\n* [\u60f3\u5e2e\u52a9\u7528\u6237\u505a\u51b3\u5b9a\uff1f\u4f60\u7684 APP \u53ef\u4ee5\u8fd9\u6837\u8bbe\u8ba1\uff01](https://juejin.im/post/5a7194986fb9a01c9f5bbbb2)\uff08[pthtc](https://github.com/pthtc) \u7ffb\u8bd1\uff09\n* [\u5229\u7528 84 \u79cd\u8ba4\u77e5\u504f\u89c1\u8bbe\u8ba1\u66f4\u597d\u7684\u4ea7\u54c1 \u2014\u2014 \u7b2c\u4e8c\u90e8\u5206](https://juejin.im/post/5d37e1816fb9a07ee1696a4e)\uff08[JalanJiang](https://github.com/JalanJiang) \u7ffb\u8bd1\uff09\n* [\u6240\u6709\u4ea7\u54c1\u8bd1\u6587>>](https://github.com/xitu/gold-miner/blob/master/product.md)\n\n## \u5176\u4ed6\n\n* [\u81ea\u52a8\u5316\u6d4b\u8bd5\uff1a\u4f60\u5e94\u5f53\u4e86\u89e3\u7684\u4e00\u5207](https://juejin.cn/post/7084071159821500447)\uff08[samyu2000](https://github.com/samyu2000) \u7ffb\u8bd1\uff09\n* [\u4f7f\u7528\u4e86\u4e09\u4e2a\u6708\u7684 Github Copilot\uff0c\u8fd9\u662f\u6211\u7684\u4e00\u4e9b\u770b\u6cd5\u2026\u2026](https://juejin.cn/post/7067817036738461732)\uff08[jaredliw](https://github.com/jaredliw) \u7ffb\u8bd1\uff09\n* [5 \u4e2a\u6709\u8da3\u7684\u539f\u56e0\u544a\u8bc9\u4f60\uff1a\u627e\u5bf9\u8c61\u5c31\u5f97\u627e\u7a0b\u5e8f\u5458\uff01](https://juejin.cn/post/7053326045352558599)\uff08[jaredliw](https://github.com/jaredliw) \u7ffb\u8bd1\uff09\n* [WasmEdge \u7684\u5b89\u88c5\u4e0e\u5378\u8f7d](https://github.com/xitu/gold-miner/blob/master/article/2022/Install-and-uninstall-WasmEdge.md)\uff08[jaredliw](https://github.com/jaredliw) \u7ffb\u8bd1\uff09\n* [\u4f7f\u7528 Python \u6a21\u62df\u5b9e\u73b0\u884c\u661f\u9645\u7a7a\u95f4\u65c5\u884c](https://juejin.cn/post/7047685861365776414)\uff08[zenblofe](https://github.com/zenblofe) \u7ffb\u8bd1\uff09\n* [\u6240\u6709\u5176\u4ed6\u5206\u7c7b\u8bd1\u6587>>](https://github.com/xitu/gold-miner/blob/master/others.md)\n\n# Copyright\n\n> **\u7248\u6743\u58f0\u660e\uff1a**[\u6398\u91d1\u7ffb\u8bd1\u8ba1\u5212](https://github.com/xitu/gold-miner)\u8bd1\u6587\u4ec5\u7528\u4e8e\u5b66\u4e60\u3001\u7814\u7a76\u548c\u4ea4\u6d41\u3002\u7248\u6743\u5f52[\u6398\u91d1\u7ffb\u8bd1\u8ba1\u5212](https://github.com/xitu/gold-miner/)\u3001\u6587\u7ae0\u4f5c\u8005\u548c\u8bd1\u8005\u6240\u6709\uff0c\u6b22\u8fce\u975e\u5546\u4e1a\u8f6c\u8f7d\u3002\u8f6c\u8f7d\u524d\u8bf7\u8054\u7cfb\u8bd1\u8005\u6216[\u7ba1\u7406\u5458](https://user-images.githubusercontent.com/8282645/118856035-10a49d80-b909-11eb-8561-00a5a16bd58a.png)\u83b7\u53d6\u6388\u6743\uff0c\u5e76\u5728\u6587\u7ae0\u5f00\u5934\u660e\u663e\u4f4d\u7f6e\u6ce8\u660e\u672c\u6587\u51fa\u5904\u3001\u8bd1\u8005\u3001\u6821\u5bf9\u8005\u548c\u6398\u91d1\u7ffb\u8bd1\u8ba1\u5212\u7684\u5b8c\u6574\u94fe\u63a5\uff0c\u8fdd\u8005\u5fc5\u7a76\u3002\n\n# \u5408\u4f5c\u4f19\u4f34\n\n<a href=\"http://www.ituring.com.cn/\" target=\"_blank\"><img src=\"https://i.loli.net/2018/03/21/5ab1c8723d6de.jpg\" width=\"130px;\"/></a>\n\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 164554832,
    "name": "handson-ml2",
    "full_name": "ageron/handson-ml2",
    "description": "A series of Jupyter notebooks that walk you through the fundamentals of Machine Learning and Deep Learning in Python using Scikit-Learn, Keras and TensorFlow 2.",
    "html_url": "https://github.com/ageron/handson-ml2",
    "clone_url": "https://github.com/ageron/handson-ml2.git",
    "owner_login": "ageron",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/76661?v=4",
    "stargazers_count": 29144,
    "watchers_count": 29144,
    "forks_count": 13123,
    "open_issues_count": 229,
    "size": 154110,
    "language": "Jupyter Notebook",
    "languages": {
      "Jupyter Notebook": 33519254,
      "Python": 7813,
      "Dockerfile": 3286,
      "Shell": 749,
      "Makefile": 263
    },
    "topics": [],
    "license_name": "Apache License 2.0",
    "created_at": "2019-01-08T03:49:07+00:00",
    "updated_at": "2025-08-06T00:25:34+00:00",
    "pushed_at": "2024-06-13T08:10:24+00:00",
    "contributors_count": 68,
    "readme_length": 5531,
    "readme_content": "Machine Learning Notebooks\n==========================\n\n# \u26a0 The 3rd edition of my book will be released in October 2022. The notebooks are available at [ageron/handson-ml3](https://github.com/ageron/handson-ml3) and contain more up-to-date code.\n\nThis project aims at teaching you the fundamentals of Machine Learning in\npython. It contains the example code and solutions to the exercises in the second edition of my O'Reilly book [Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/):\n\n<img src=\"https://images-na.ssl-images-amazon.com/images/I/51aqYc1QyrL._SX379_BO1,204,203,200_.jpg\" title=\"book\" width=\"150\" />\n\n**Note**: If you are looking for the first edition notebooks, check out [ageron/handson-ml](https://github.com/ageron/handson-ml). For the third edition, check out [ageron/handson-ml3](https://github.com/ageron/handson-ml3).\n\n## Quick Start\n\n### Want to play with these notebooks online without having to install anything?\nUse any of the following services (I recommended Colab or Kaggle, since they offer free GPUs and TPUs).\n\n**WARNING**: _Please be aware that these services provide temporary environments: anything you do will be deleted after a while, so make sure you download any data you care about._\n\n* <a href=\"https://colab.research.google.com/github/ageron/handson-ml2/blob/master/\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n* <a href=\"https://homl.info/kaggle/\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\" /></a>\n\n* <a href=\"https://mybinder.org/v2/gh/ageron/handson-ml2/HEAD?filepath=%2Findex.ipynb\"><img src=\"https://mybinder.org/badge_logo.svg\" alt=\"Launch binder\" /></a>\n\n* <a href=\"https://homl.info/deepnote/\"><img src=\"https://deepnote.com/buttons/launch-in-deepnote-small.svg\" alt=\"Launch in Deepnote\" /></a>\n\n### Just want to quickly look at some notebooks, without executing any code?\n\n* <a href=\"https://nbviewer.jupyter.org/github/ageron/handson-ml2/blob/master/index.ipynb\"><img src=\"https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg\" alt=\"Render nbviewer\" /></a>\n\n* [github.com's notebook viewer](https://github.com/ageron/handson-ml2/blob/master/index.ipynb) also works but it's not ideal: it's slower, the math equations are not always displayed correctly, and large notebooks often fail to open.\n\n### Want to run this project using a Docker image?\nRead the [Docker instructions](https://github.com/ageron/handson-ml2/tree/master/docker).\n\n### Want to install this project on your own machine?\n\nStart by installing [Anaconda](https://www.anaconda.com/distribution/) (or [Miniconda](https://docs.conda.io/en/latest/miniconda.html)), [git](https://git-scm.com/downloads), and if you have a TensorFlow-compatible GPU, install the [GPU driver](https://www.nvidia.com/Download/index.aspx), as well as the appropriate version of CUDA and cuDNN (see TensorFlow's documentation for more details).\n\nNext, clone this project by opening a terminal and typing the following commands (do not type the first `$` signs on each line, they just indicate that these are terminal commands):\n\n    $ git clone https://github.com/ageron/handson-ml2.git\n    $ cd handson-ml2\n\nNext, run the following commands:\n\n    $ conda env create -f environment.yml\n    $ conda activate tf2\n    $ python -m ipykernel install --user --name=python3\n\nFinally, start Jupyter:\n\n    $ jupyter notebook\n\nIf you need further instructions, read the [detailed installation instructions](INSTALL.md).\n\n# FAQ\n\n**Which Python version should I use?**\n\nI recommend Python 3.8. If you follow the installation instructions above, that's the version you will get. Most code will work with other versions of Python 3, but some libraries do not support Python 3.9 or 3.10 yet, which is why I recommend Python 3.8.\n\n**I'm getting an error when I call `load_housing_data()`**\n\nMake sure you call `fetch_housing_data()` *before* you call `load_housing_data()`. If you're getting an HTTP error, make sure you're running the exact same code as in the notebook (copy/paste it if needed). If the problem persists, please check your network configuration.\n\n**I'm getting an SSL error on MacOSX**\n\nYou probably need to install the SSL certificates (see this [StackOverflow question](https://stackoverflow.com/questions/27835619/urllib-and-ssl-certificate-verify-failed-error)). If you downloaded Python from the official website, then run `/Applications/Python\\ 3.8/Install\\ Certificates.command` in a terminal (change `3.8` to whatever version you installed). If you installed Python using MacPorts, run `sudo port install curl-ca-bundle` in a terminal.\n\n**I've installed this project locally. How do I update it to the latest version?**\n\nSee [INSTALL.md](INSTALL.md)\n\n**How do I update my Python libraries to the latest versions, when using Anaconda?**\n\nSee [INSTALL.md](INSTALL.md)\n\n## Contributors\nI would like to thank everyone [who contributed to this project](https://github.com/ageron/handson-ml2/graphs/contributors), either by providing useful feedback, filing issues or submitting Pull Requests. Special thanks go to Haesun Park and Ian Beauregard who reviewed every notebook and submitted many PRs, including help on some of the exercise solutions. Thanks as well to Steven Bunkley and Ziembla who created the `docker` directory, and to github user SuperYorio who helped on some exercise solutions.\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 29749635,
    "name": "data-science-ipython-notebooks",
    "full_name": "donnemartin/data-science-ipython-notebooks",
    "description": "Data science Python notebooks: Deep learning (TensorFlow, Theano, Caffe, Keras), scikit-learn, Kaggle, big data (Spark, Hadoop MapReduce, HDFS), matplotlib, pandas, NumPy, SciPy, Python essentials, AWS, and various command lines.",
    "html_url": "https://github.com/donnemartin/data-science-ipython-notebooks",
    "clone_url": "https://github.com/donnemartin/data-science-ipython-notebooks.git",
    "owner_login": "donnemartin",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/5458997?v=4",
    "stargazers_count": 28443,
    "watchers_count": 28443,
    "forks_count": 8011,
    "open_issues_count": 40,
    "size": 49025,
    "language": "Python",
    "languages": {
      "Python": 19488744,
      "CSS": 1612,
      "Makefile": 684,
      "Dockerfile": 191
    },
    "topics": [
      "aws",
      "big-data",
      "caffe",
      "data-science",
      "deep-learning",
      "hadoop",
      "kaggle",
      "keras",
      "machine-learning",
      "mapreduce",
      "matplotlib",
      "numpy",
      "pandas",
      "python",
      "scikit-learn",
      "scipy",
      "spark",
      "tensorflow",
      "theano"
    ],
    "license_name": "Other",
    "created_at": "2015-01-23T19:38:29+00:00",
    "updated_at": "2025-08-06T00:37:35+00:00",
    "pushed_at": "2024-03-20T13:52:34+00:00",
    "contributors_count": 12,
    "readme_length": 43957,
    "readme_content": "<br/>\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/README_1200x800.gif\">\n</p>\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/coversmall_alt.png\">\n  <br/>\n</p>\n\n# data-science-ipython-notebooks\n\n## Index\n\n* [deep-learning](#deep-learning)\n    * [tensorflow](#tensor-flow-tutorials)\n    * [theano](#theano-tutorials)\n    * [keras](#keras-tutorials)\n    * [caffe](#deep-learning-misc)\n* [scikit-learn](#scikit-learn)\n* [statistical-inference-scipy](#statistical-inference-scipy)\n* [pandas](#pandas)\n* [matplotlib](#matplotlib)\n* [numpy](#numpy)\n* [python-data](#python-data)\n* [kaggle-and-business-analyses](#kaggle-and-business-analyses)\n* [spark](#spark)\n* [mapreduce-python](#mapreduce-python)\n* [amazon web services](#aws)\n* [command lines](#commands)\n* [misc](#misc)\n* [notebook-installation](#notebook-installation)\n* [credits](#credits)\n* [contributing](#contributing)\n* [contact-info](#contact-info)\n* [license](#license)\n\n<br/>\n<p align=\"center\">\n  <img src=\"http://i.imgur.com/ZhKXrKZ.png\">\n</p>\n\n## deep-learning\n\nIPython Notebook(s) demonstrating deep learning functionality.\n\n<br/>\n<p align=\"center\">\n  <img src=\"https://avatars0.githubusercontent.com/u/15658638?v=3&s=100\">\n</p>\n\n### tensor-flow-tutorials\n\nAdditional TensorFlow tutorials:\n\n* [pkmital/tensorflow_tutorials](https://github.com/pkmital/tensorflow_tutorials)\n* [nlintz/TensorFlow-Tutorials](https://github.com/nlintz/TensorFlow-Tutorials)\n* [alrojo/tensorflow-tutorial](https://github.com/alrojo/tensorflow-tutorial)\n* [BinRoot/TensorFlow-Book](https://github.com/BinRoot/TensorFlow-Book)\n* [tuanavu/tensorflow-basic-tutorials](https://github.com/tuanavu/tensorflow-basic-tutorials)\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [tsf-basics](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/1_intro/basic_operations.ipynb) | Learn basic operations in TensorFlow, a library for various kinds of perceptual and language understanding tasks from Google. |\n| [tsf-linear](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/2_basic_classifiers/linear_regression.ipynb) | Implement linear regression in TensorFlow. |\n| [tsf-logistic](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/2_basic_classifiers/logistic_regression.ipynb) | Implement logistic regression in TensorFlow. |\n| [tsf-nn](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/2_basic_classifiers/nearest_neighbor.ipynb) | Implement nearest neighboars in TensorFlow. |\n| [tsf-alex](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/3_neural_networks/alexnet.ipynb) | Implement AlexNet in TensorFlow. |\n| [tsf-cnn](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/3_neural_networks/convolutional_network.ipynb) | Implement convolutional neural networks in TensorFlow. |\n| [tsf-mlp](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/3_neural_networks/multilayer_perceptron.ipynb) | Implement multilayer perceptrons in TensorFlow. |\n| [tsf-rnn](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/3_neural_networks/recurrent_network.ipynb) | Implement recurrent neural networks in TensorFlow. |\n| [tsf-gpu](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/4_multi_gpu/multigpu_basics.ipynb) | Learn about basic multi-GPU computation in TensorFlow. |\n| [tsf-gviz](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/5_ui/graph_visualization.ipynb) | Learn about graph visualization in TensorFlow. |\n| [tsf-lviz](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/5_ui/loss_visualization.ipynb) | Learn about loss visualization in TensorFlow. |\n\n### tensor-flow-exercises\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [tsf-not-mnist](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-exercises/1_notmnist.ipynb) | Learn simple data curation by creating a pickle with formatted datasets for training, development and testing in TensorFlow. |\n| [tsf-fully-connected](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-exercises/2_fullyconnected.ipynb) | Progressively train deeper and more accurate models using logistic regression and neural networks in TensorFlow. |\n| [tsf-regularization](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-exercises/3_regularization.ipynb) | Explore regularization techniques by training fully connected networks to classify notMNIST characters in TensorFlow. |\n| [tsf-convolutions](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-exercises/4_convolutions.ipynb) | Create convolutional neural networks in TensorFlow. |\n| [tsf-word2vec](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-exercises/5_word2vec.ipynb) | Train a skip-gram model over Text8 data in TensorFlow. |\n| [tsf-lstm](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-exercises/6_lstm.ipynb) | Train a LSTM character model over Text8 data in TensorFlow. |\n\n<br/>\n<p align=\"center\">\n  <img src=\"http://www.deeplearning.net/software/theano/_static/theano_logo_allblue_200x46.png\">\n</p>\n\n### theano-tutorials\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [theano-intro](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/theano-tutorial/intro_theano/intro_theano.ipynb) | Intro to Theano, which allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. It can use GPUs and perform efficient symbolic differentiation. |\n| [theano-scan](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/theano-tutorial/scan_tutorial/scan_tutorial.ipynb) | Learn scans, a mechanism to perform loops in a Theano graph. |\n| [theano-logistic](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/theano-tutorial/intro_theano/logistic_regression.ipynb) | Implement logistic regression in Theano. |\n| [theano-rnn](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/theano-tutorial/rnn_tutorial/simple_rnn.ipynb) | Implement recurrent neural networks in Theano. |\n| [theano-mlp](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/theano-tutorial/theano_mlp/theano_mlp.ipynb) | Implement multilayer perceptrons in Theano. |\n\n<br/>\n<p align=\"center\">\n  <img src=\"http://i.imgur.com/L45Q8c2.jpg\">\n</p>\n\n### keras-tutorials\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| keras | Keras is an open source neural network library written in Python. It is capable of running on top of either Tensorflow or Theano. |\n| [setup](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/0.%20Preamble.ipynb) | Learn about the tutorial goals and how to set up your Keras environment. |\n| [intro-deep-learning-ann](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/1.1%20Introduction%20-%20Deep%20Learning%20and%20ANN.ipynb) | Get an intro to deep learning with Keras and Artificial Neural Networks (ANN). |\n| [theano](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/1.2%20Introduction%20-%20Theano.ipynb) | Learn about Theano by working with weights matrices and gradients. |\n| [keras-otto](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/1.3%20Introduction%20-%20Keras.ipynb) | Learn about Keras by looking at the Kaggle Otto challenge. |\n| [ann-mnist](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/1.4%20%28Extra%29%20A%20Simple%20Implementation%20of%20ANN%20for%20MNIST.ipynb) | Review a simple implementation of ANN for MNIST using Keras. |\n| [conv-nets](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/2.1%20Supervised%20Learning%20-%20ConvNets.ipynb) | Learn about Convolutional Neural Networks (CNNs) with Keras. |\n| [conv-net-1](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/2.2.1%20Supervised%20Learning%20-%20ConvNet%20HandsOn%20Part%20I.ipynb) | Recognize handwritten digits from MNIST using Keras - Part 1. |\n| [conv-net-2](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/2.2.2%20Supervised%20Learning%20-%20ConvNet%20HandsOn%20Part%20II.ipynb) | Recognize handwritten digits from MNIST using Keras - Part 2. |\n| [keras-models](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/2.3%20Supervised%20Learning%20-%20Famous%20Models%20with%20Keras.ipynb) | Use pre-trained models such as VGG16, VGG19, ResNet50, and Inception v3 with Keras. |\n| [auto-encoders](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/3.1%20Unsupervised%20Learning%20-%20AutoEncoders%20and%20Embeddings.ipynb) | Learn about Autoencoders with Keras. |\n| [rnn-lstm](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/3.2%20RNN%20and%20LSTM.ipynb) | Learn about Recurrent Neural Networks (RNNs) with Keras. |\n| [lstm-sentence-gen](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/3.3%20%28Extra%29%20LSTM%20for%20Sentence%20Generation.ipynb) |  Learn about RNNs using Long Short Term Memory (LSTM) networks with Keras. |\n\n### deep-learning-misc\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [deep-dream](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/deep-dream/dream.ipynb) | Caffe-based computer vision program which uses a convolutional neural network to find and enhance patterns in images. |\n\n<br/>\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/scikitlearn.png\">\n</p>\n\n## scikit-learn\n\nIPython Notebook(s) demonstrating scikit-learn functionality.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [intro](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scikit-learn/scikit-learn-intro.ipynb) | Intro notebook to scikit-learn.  Scikit-learn adds Python support for large, multi-dimensional arrays and matrices, along with a large library of high-level mathematical functions to operate on these arrays. |\n| [knn](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scikit-learn/scikit-learn-intro.ipynb#K-Nearest-Neighbors-Classifier) | Implement k-nearest neighbors in scikit-learn. |\n| [linear-reg](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scikit-learn/scikit-learn-linear-reg.ipynb) | Implement linear regression in scikit-learn. |\n| [svm](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scikit-learn/scikit-learn-svm.ipynb) | Implement support vector machine classifiers with and without kernels in scikit-learn. |\n| [random-forest](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scikit-learn/scikit-learn-random-forest.ipynb) | Implement random forest classifiers and regressors in scikit-learn. |\n| [k-means](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scikit-learn/scikit-learn-k-means.ipynb) | Implement k-means clustering in scikit-learn. |\n| [pca](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scikit-learn/scikit-learn-pca.ipynb) | Implement principal component analysis in scikit-learn. |\n| [gmm](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scikit-learn/scikit-learn-gmm.ipynb) | Implement Gaussian mixture models in scikit-learn. |\n| [validation](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scikit-learn/scikit-learn-validation.ipynb) | Implement validation and model selection in scikit-learn. |\n\n<br/>\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/scipy.png\">\n</p>\n\n## statistical-inference-scipy\n\nIPython Notebook(s) demonstrating statistical inference with SciPy functionality.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| scipy | SciPy is a collection of mathematical algorithms and convenience functions built on the Numpy extension of Python. It adds significant power to the interactive Python session by providing the user with high-level commands and classes for manipulating and visualizing data. |\n| [effect-size](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scipy/effect_size.ipynb) | Explore statistics that quantify effect size by analyzing the difference in height between men and women.  Uses data from the Behavioral Risk Factor Surveillance System (BRFSS) to estimate the mean and standard deviation of height for adult women and men in the United States. |\n| [sampling](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scipy/sampling.ipynb) | Explore random sampling by analyzing the average weight of men and women in the United States using BRFSS data. |\n| [hypothesis](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scipy/hypothesis.ipynb) | Explore hypothesis testing by analyzing the difference of first-born babies compared with others. |\n\n<br/>\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/pandas.png\">\n</p>\n\n## pandas\n\nIPython Notebook(s) demonstrating pandas functionality.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [pandas](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/pandas.ipynb) | Software library written for data manipulation and analysis in Python. Offers data structures and operations for manipulating numerical tables and time series. |\n| [github-data-wrangling](https://github.com/donnemartin/viz/blob/master/githubstats/data_wrangling.ipynb) | Learn how to load, clean, merge, and feature engineer by analyzing GitHub data from the [`Viz`](https://github.com/donnemartin/viz) repo. |\n| [Introduction-to-Pandas](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.00-Introduction-to-Pandas.ipynb) | Introduction to Pandas. |\n| [Introducing-Pandas-Objects](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.01-Introducing-Pandas-Objects.ipynb) | Learn about Pandas objects. |\n| [Data Indexing and Selection](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.02-Data-Indexing-and-Selection.ipynb) | Learn about data indexing and selection in Pandas. |\n| [Operations-in-Pandas](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.03-Operations-in-Pandas.ipynb) | Learn about operating on data in Pandas. |\n| [Missing-Values](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.04-Missing-Values.ipynb) | Learn about handling missing data in Pandas. |\n| [Hierarchical-Indexing](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.05-Hierarchical-Indexing.ipynb) | Learn about hierarchical indexing in Pandas. |\n| [Concat-And-Append](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.06-Concat-And-Append.ipynb) | Learn about combining datasets: concat and append in Pandas. |\n| [Merge-and-Join](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.07-Merge-and-Join.ipynb) | Learn about combining datasets: merge and join in Pandas. |\n| [Aggregation-and-Grouping](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.08-Aggregation-and-Grouping.ipynb) | Learn about aggregation and grouping in Pandas. |\n| [Pivot-Tables](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.09-Pivot-Tables.ipynb) | Learn about pivot tables in Pandas. |\n| [Working-With-Strings](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.10-Working-With-Strings.ipynb) | Learn about vectorized string operations in Pandas. |\n| [Working-with-Time-Series](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.11-Working-with-Time-Series.ipynb) | Learn about working with time series in pandas. |\n| [Performance-Eval-and-Query](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.12-Performance-Eval-and-Query.ipynb) | Learn about high-performance Pandas: eval() and query() in Pandas. |\n\n<br/>\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/matplotlib.png\">\n</p>\n\n## matplotlib\n\nIPython Notebook(s) demonstrating matplotlib functionality.\n\n| Notebook | Description |\n|-----------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [matplotlib](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/matplotlib.ipynb) | Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. |\n| [matplotlib-applied](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/matplotlib-applied.ipynb) | Apply matplotlib visualizations to Kaggle competitions for exploratory data analysis.  Learn how to create bar plots, histograms, subplot2grid, normalized plots, scatter plots, subplots, and kernel density estimation plots. |\n| [Introduction-To-Matplotlib](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.00-Introduction-To-Matplotlib.ipynb) | Introduction to Matplotlib. |\n| [Simple-Line-Plots](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.01-Simple-Line-Plots.ipynb) | Learn about simple line plots in Matplotlib. |\n| [Simple-Scatter-Plots](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.02-Simple-Scatter-Plots.ipynb) | Learn about simple scatter plots in Matplotlib. |\n| [Errorbars.ipynb](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.03-Errorbars.ipynb) | Learn about visualizing errors in Matplotlib. |\n| [Density-and-Contour-Plots](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.04-Density-and-Contour-Plots.ipynb) | Learn about density and contour plots in Matplotlib. |\n| [Histograms-and-Binnings](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.05-Histograms-and-Binnings.ipynb) | Learn about histograms, binnings, and density in Matplotlib. |\n| [Customizing-Legends](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.06-Customizing-Legends.ipynb) | Learn about customizing plot legends in Matplotlib. |\n| [Customizing-Colorbars](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.07-Customizing-Colorbars.ipynb) | Learn about customizing colorbars in Matplotlib. |\n| [Multiple-Subplots](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.08-Multiple-Subplots.ipynb) | Learn about multiple subplots in Matplotlib. |\n| [Text-and-Annotation](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.09-Text-and-Annotation.ipynb) | Learn about text and annotation in Matplotlib. |\n| [Customizing-Ticks](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.10-Customizing-Ticks.ipynb) | Learn about customizing ticks in Matplotlib. |\n| [Settings-and-Stylesheets](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.11-Settings-and-Stylesheets.ipynb) | Learn about customizing Matplotlib: configurations and stylesheets. |\n| [Three-Dimensional-Plotting](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.12-Three-Dimensional-Plotting.ipynb) | Learn about three-dimensional plotting in Matplotlib. |\n| [Geographic-Data-With-Basemap](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.13-Geographic-Data-With-Basemap.ipynb) | Learn about geographic data with basemap in Matplotlib. |\n| [Visualization-With-Seaborn](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.14-Visualization-With-Seaborn.ipynb) | Learn about visualization with Seaborn. |\n\n<br/>\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/numpy.png\">\n</p>\n\n## numpy\n\nIPython Notebook(s) demonstrating NumPy functionality.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [numpy](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/numpy.ipynb) | Adds Python support for large, multi-dimensional arrays and matrices, along with a large library of high-level mathematical functions to operate on these arrays. |\n| [Introduction-to-NumPy](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.00-Introduction-to-NumPy.ipynb) | Introduction to NumPy. |\n| [Understanding-Data-Types](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.01-Understanding-Data-Types.ipynb) | Learn about data types in Python. |\n| [The-Basics-Of-NumPy-Arrays](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.02-The-Basics-Of-NumPy-Arrays.ipynb) | Learn about the basics of NumPy arrays. |\n| [Computation-on-arrays-ufuncs](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.03-Computation-on-arrays-ufuncs.ipynb) | Learn about computations on NumPy arrays: universal functions. |\n| [Computation-on-arrays-aggregates](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.04-Computation-on-arrays-aggregates.ipynb) | Learn about aggregations: min, max, and everything in between in NumPy. |\n| [Computation-on-arrays-broadcasting](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.05-Computation-on-arrays-broadcasting.ipynb) | Learn about computation on arrays: broadcasting in NumPy. |\n| [Boolean-Arrays-and-Masks](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.06-Boolean-Arrays-and-Masks.ipynb) | Learn about comparisons, masks, and boolean logic in NumPy. |\n| [Fancy-Indexing](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.07-Fancy-Indexing.ipynb) | Learn about fancy indexing in NumPy. |\n| [Sorting](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.08-Sorting.ipynb) | Learn about sorting arrays in NumPy. |\n| [Structured-Data-NumPy](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.09-Structured-Data-NumPy.ipynb) | Learn about structured data: NumPy's structured arrays. |\n\n<br/>\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/python.png\">\n</p>\n\n## python-data\n\nIPython Notebook(s) demonstrating Python functionality geared towards data analysis.\n\n| Notebook | Description |\n|-----------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------|\n| [data structures](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/python-data/structs.ipynb) | Learn Python basics with tuples, lists, dicts, sets. |\n| [data structure utilities](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/python-data/structs_utils.ipynb) | Learn Python operations such as slice, range, xrange, bisect, sort, sorted, reversed, enumerate, zip, list comprehensions. |\n| [functions](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/python-data/functions.ipynb) | Learn about more advanced Python features: Functions as objects, lambda functions, closures, *args, **kwargs currying, generators, generator expressions, itertools. |\n| [datetime](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/python-data/datetime.ipynb) | Learn how to work with Python dates and times: datetime, strftime, strptime, timedelta. |\n| [logging](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/python-data/logs.ipynb) | Learn about Python logging with RotatingFileHandler and TimedRotatingFileHandler. |\n| [pdb](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/python-data/pdb.ipynb) | Learn how to debug in Python with the interactive source code debugger. |\n| [unit tests](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/python-data/unit_tests.ipynb) | Learn how to test in Python with Nose unit tests. |\n\n<br/>\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/kaggle.png\">\n</p>\n\n## kaggle-and-business-analyses\n\nIPython Notebook(s) used in [kaggle](https://www.kaggle.com/) competitions and business analyses.\n\n| Notebook | Description |\n|-------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------|\n| [titanic](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/kaggle/titanic.ipynb) | Predict survival on the Titanic.  Learn data cleaning, exploratory data analysis, and machine learning. |\n| [churn-analysis](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/analyses/churn.ipynb) | Predict customer churn.  Exercise logistic regression, gradient boosting classifers, support vector machines, random forests, and k-nearest-neighbors.  Includes discussions of confusion matrices, ROC plots, feature importances, prediction probabilities, and calibration/descrimination.|\n\n<br/>\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/spark.png\">\n</p>\n\n## spark\n\nIPython Notebook(s) demonstrating spark and HDFS functionality.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------|\n| [spark](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/spark/spark.ipynb) | In-memory cluster computing framework, up to 100 times faster for certain applications and is well suited for machine learning algorithms. |\n| [hdfs](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/spark/hdfs.ipynb) | Reliably stores very large files across machines in a large cluster. |\n\n<br/>\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/mrjob.png\">\n</p>\n\n## mapreduce-python\n\nIPython Notebook(s) demonstrating Hadoop MapReduce with mrjob functionality.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------|\n| [mapreduce-python](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/mapreduce/mapreduce-python.ipynb) | Runs MapReduce jobs in Python, executing jobs locally or on Hadoop clusters. Demonstrates Hadoop Streaming in Python code with unit test and [mrjob](https://github.com/Yelp/mrjob) config file to analyze Amazon S3 bucket logs on Elastic MapReduce.  [Disco](https://github.com/discoproject/disco/) is another python-based alternative.|\n\n<br/>\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/aws.png\">\n</p>\n\n## aws\n\nIPython Notebook(s) demonstrating Amazon Web Services (AWS) and AWS tools functionality.\n\n\nAlso check out:\n\n* [SAWS](https://github.com/donnemartin/saws): A Supercharged AWS command line interface (CLI).\n* [Awesome AWS](https://github.com/donnemartin/awesome-aws): A curated list of libraries, open source repos, guides, blogs, and other resources.\n\n| Notebook | Description |\n|------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [boto](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/aws/aws.ipynb#Boto) | Official AWS SDK for Python. |\n| [s3cmd](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/aws/aws.ipynb#s3cmd) | Interacts with S3 through the command line. |\n| [s3distcp](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/aws/aws.ipynb#s3distcp) | Combines smaller files and aggregates them together by taking in a pattern and target file.  S3DistCp can also be used to transfer large volumes of data from S3 to your Hadoop cluster. |\n| [s3-parallel-put](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/aws/aws.ipynb#s3-parallel-put) | Uploads multiple files to S3 in parallel. |\n| [redshift](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/aws/aws.ipynb#redshift) | Acts as a fast data warehouse built on top of technology from massive parallel processing (MPP). |\n| [kinesis](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/aws/aws.ipynb#kinesis) | Streams data in real time with the ability to process thousands of data streams per second. |\n| [lambda](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/aws/aws.ipynb#lambda) | Runs code in response to events, automatically managing compute resources. |\n\n<br/>\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/commands.png\">\n</p>\n\n## commands\n\nIPython Notebook(s) demonstrating various command lines for Linux, Git, etc.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [linux](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/commands/linux.ipynb) | Unix-like and mostly POSIX-compliant computer operating system.  Disk usage, splitting files, grep, sed, curl, viewing running processes, terminal syntax highlighting, and Vim.|\n| [anaconda](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/commands/misc.ipynb#anaconda) | Distribution of the Python programming language for large-scale data processing, predictive analytics, and scientific computing, that aims to simplify package management and deployment. |\n| [ipython notebook](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/commands/misc.ipynb#ipython-notebook) | Web-based interactive computational environment where you can combine code execution, text, mathematics, plots and rich media into a single document. |\n| [git](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/commands/misc.ipynb#git) | Distributed revision control system with an emphasis on speed, data integrity, and support for distributed, non-linear workflows. |\n| [ruby](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/commands/misc.ipynb#ruby) | Used to interact with the AWS command line and for Jekyll, a blog framework that can be hosted on GitHub Pages. |\n| [jekyll](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/commands/misc.ipynb#jekyll) | Simple, blog-aware, static site generator for personal, project, or organization sites.  Renders Markdown or Textile and Liquid templates, and produces a complete, static website ready to be served by Apache HTTP Server, Nginx or another web server. |\n| [pelican](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/commands/misc.ipynb#pelican) | Python-based alternative to Jekyll. |\n| [django](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/commands/misc.ipynb#django) | High-level Python Web framework that encourages rapid development and clean, pragmatic design. It can be useful to share reports/analyses and for blogging. Lighter-weight alternatives include [Pyramid](https://github.com/Pylons/pyramid), [Flask](https://github.com/pallets/flask), [Tornado](https://github.com/tornadoweb/tornado), and [Bottle](https://github.com/bottlepy/bottle).\n\n## misc\n\nIPython Notebook(s) demonstrating miscellaneous functionality.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [regex](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/misc/regex.ipynb) | Regular expression cheat sheet useful in data wrangling.|\n[algorithmia](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/misc/Algorithmia.ipynb) | Algorithmia is a marketplace for algorithms. This notebook showcases 4 different algorithms: Face Detection, Content Summarizer, Latent Dirichlet Allocation and Optical Character Recognition.|\n\n## notebook-installation\n\n### anaconda\n\nAnaconda is a free distribution of the Python programming language for large-scale data processing, predictive analytics, and scientific computing that aims to simplify package management and deployment.\n\nFollow instructions to install [Anaconda](https://docs.continuum.io/anaconda/install) or the more lightweight [miniconda](http://conda.pydata.org/miniconda.html).\n\n### dev-setup\n\nFor detailed instructions, scripts, and tools to set up your development environment for data analysis, check out the [dev-setup](https://github.com/donnemartin/dev-setup) repo.\n\n### running-notebooks\n\nTo view interactive content or to modify elements within the IPython notebooks, you must first clone or download the repository then run the notebook.  More information on IPython Notebooks can be found [here.](http://ipython.org/notebook.html)\n\n    $ git clone https://github.com/donnemartin/data-science-ipython-notebooks.git\n    $ cd data-science-ipython-notebooks\n    $ jupyter notebook\n\nNotebooks tested with Python 2.7.x.\n\n## credits\n\n* [Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython](http://www.amazon.com/Python-Data-Analysis-Wrangling-IPython/dp/1449319793) by Wes McKinney\n* [PyCon 2015 Scikit-learn Tutorial](https://github.com/jakevdp/sklearn_pycon2015) by Jake VanderPlas\n* [Python Data Science Handbook](https://github.com/jakevdp/PythonDataScienceHandbook) by Jake VanderPlas\n* [Parallel Machine Learning with scikit-learn and IPython](https://github.com/ogrisel/parallel_ml_tutorial) by Olivier Grisel\n* [Statistical Interference Using Computational Methods in Python](https://github.com/AllenDowney/CompStats) by Allen Downey\n* [TensorFlow Examples](https://github.com/aymericdamien/TensorFlow-Examples) by Aymeric Damien\n* [TensorFlow Tutorials](https://github.com/pkmital/tensorflow_tutorials) by Parag K Mital\n* [TensorFlow Tutorials](https://github.com/nlintz/TensorFlow-Tutorials) by Nathan Lintz\n* [TensorFlow Tutorials](https://github.com/alrojo/tensorflow-tutorial) by Alexander R Johansen\n* [TensorFlow Book](https://github.com/BinRoot/TensorFlow-Book) by Nishant Shukla\n* [Summer School 2015](https://github.com/mila-udem/summerschool2015) by mila-udem\n* [Keras tutorials](https://github.com/leriomaggio/deep-learning-keras-tensorflow) by Valerio Maggio\n* [Kaggle](https://www.kaggle.com/)\n* [Yhat Blog](http://blog.yhat.com/)\n\n## contributing\n\nContributions are welcome!  For bug reports or requests please [submit an issue](https://github.com/donnemartin/data-science-ipython-notebooks/issues).\n\n## contact-info\n\nFeel free to contact me to discuss any issues, questions, or comments.\n\n* Email: [donne.martin@gmail.com](mailto:donne.martin@gmail.com)\n* Twitter: [@donne_martin](https://twitter.com/donne_martin)\n* GitHub: [donnemartin](https://github.com/donnemartin)\n* LinkedIn: [donnemartin](https://www.linkedin.com/in/donnemartin)\n* Website: [donnemartin.com](http://donnemartin.com)\n\n## license\n\nThis repository contains a variety of content; some developed by Donne Martin, and some from third-parties.  The third-party content is distributed under the license provided by those parties.\n\nThe content developed by Donne Martin is distributed under the following license:\n\n*I am providing code and resources in this repository to you under an open source license.  Because this is my personal repository, the license you receive to my code and resources is from me and not my employer (Facebook).*\n\n    Copyright 2015 Donne Martin\n\n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 211124697,
    "name": "spleeter",
    "full_name": "deezer/spleeter",
    "description": "Deezer source separation library including pretrained models.",
    "html_url": "https://github.com/deezer/spleeter",
    "clone_url": "https://github.com/deezer/spleeter.git",
    "owner_login": "deezer",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/4393583?v=4",
    "stargazers_count": 27201,
    "watchers_count": 27201,
    "forks_count": 2976,
    "open_issues_count": 270,
    "size": 9630,
    "language": "Python",
    "languages": {
      "Python": 130340,
      "Dockerfile": 7944,
      "TeX": 5650,
      "Jupyter Notebook": 3967,
      "Shell": 386
    },
    "topics": [
      "audio-processing",
      "bass",
      "deep-learning",
      "deezer",
      "drums",
      "model",
      "pretrained-models",
      "python",
      "tensorflow",
      "vocals"
    ],
    "license_name": "MIT License",
    "created_at": "2019-09-26T15:40:46+00:00",
    "updated_at": "2025-08-06T02:12:28+00:00",
    "pushed_at": "2025-04-02T16:22:20+00:00",
    "contributors_count": 15,
    "readme_length": 8683,
    "readme_content": "<img src=\"https://github.com/deezer/spleeter/raw/master/images/spleeter_logo.png\" height=\"80\" />\n\n[![Github actions](https://github.com/deezer/spleeter/workflows/pytest/badge.svg)](https://github.com/deezer/spleeter/actions) ![PyPI - Python Version](https://img.shields.io/pypi/pyversions/spleeter) [![PyPI version](https://badge.fury.io/py/spleeter.svg)](https://badge.fury.io/py/spleeter) [![Conda](https://img.shields.io/conda/vn/deezer-research/spleeter)](https://anaconda.org/deezer-research/spleeter) [![Docker Pulls](https://img.shields.io/docker/pulls/deezer/spleeter)](https://hub.docker.com/r/deezer/spleeter) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deezer/spleeter/blob/master/spleeter.ipynb) [![Gitter chat](https://badges.gitter.im/gitterHQ/gitter.png)](https://gitter.im/spleeter/community) [![status](https://joss.theoj.org/papers/259e5efe669945a343bad6eccb89018b/status.svg)](https://joss.theoj.org/papers/259e5efe669945a343bad6eccb89018b)\n\n> :warning: [Spleeter 2.1.0](https://pypi.org/project/spleeter/) release introduces some breaking changes, including new CLI option naming for input, and the drop\n> of dedicated GPU package. Please read [CHANGELOG](CHANGELOG.md) for more details.\n\n## About\n\n**Spleeter** is [Deezer](https://www.deezer.com/) source separation library with pretrained models\nwritten in [Python](https://www.python.org/) and uses [Tensorflow](https://tensorflow.org/). It makes it easy\nto train source separation model (assuming you have a dataset of isolated sources), and provides\nalready trained state of the art model for performing various flavour of separation :\n\n* Vocals (singing voice) / accompaniment separation ([2 stems](https://github.com/deezer/spleeter/wiki/2.-Getting-started#using-2stems-model))\n* Vocals / drums / bass / other separation ([4 stems](https://github.com/deezer/spleeter/wiki/2.-Getting-started#using-4stems-model))\n* Vocals / drums / bass / piano / other separation ([5 stems](https://github.com/deezer/spleeter/wiki/2.-Getting-started#using-5stems-model))\n\n2 stems and 4 stems models have [high performances](https://github.com/deezer/spleeter/wiki/Separation-Performances) on the [musdb](https://sigsep.github.io/datasets/musdb.html) dataset. **Spleeter** is also very fast as it can perform separation of audio files to 4 stems 100x faster than real-time when run on a GPU.\n\nWe designed **Spleeter** so you can use it straight from [command line](https://github.com/deezer/spleeter/wiki/2.-Getting-started#usage)\nas well as directly in your own development pipeline as a [Python library](https://github.com/deezer/spleeter/wiki/4.-API-Reference#separator). It can be installed with [pip](https://github.com/deezer/spleeter/wiki/1.-Installation#using-pip) or be used with\n[Docker](https://github.com/deezer/spleeter/wiki/2.-Getting-started#using-docker-image).\n\n### Projects and Softwares using **Spleeter**\n\nSince it's been released, there are multiple forks exposing **Spleeter** through either a Guided User Interface (GUI) or a standalone free or paying website. Please note that we do not host, maintain or directly support any of these initiatives.\n\nThat being said, many cool projects have been built on top of ours. Notably the porting to the *Ableton Live* ecosystem through the [Spleeter 4 Max](https://github.com/diracdeltas/spleeter4max#spleeter-for-max) project.\n\n**Spleeter** pre-trained models have also been used by professionnal audio softwares. Here's a non-exhaustive list:\n\n* [iZotope](https://www.izotope.com/en/shop/rx-8-standard.html) in its *Music Rebalance* feature within **RX 8**\n* [SpectralLayers](https://new.steinberg.net/spectralayers/) in its *Unmix* feature in **SpectralLayers 7**\n* [Acon Digital](https://acondigital.com/products/acoustica-audio-editor/) within **Acoustica 7**\n* [VirtualDJ](https://www.virtualdj.com/stems/) in their stem isolation feature\n* [Algoriddim](https://www.algoriddim.com/apps) in their **NeuralMix** and **djayPRO** app suite\n\n\ud83c\udd95 **Spleeter** is a baseline in the ongoing [Music Demixing Challenge](https://www.aicrowd.com/challenges/music-demixing-challenge-ismir-2021)!\n\n## Spleeter Pro (Commercial version)\n\nCheck out our commercial version : [Spleeter Pro](https://www.deezer-techservices.com/solutions/spleeter/). Benefit from our expertise for precise audio separation, faster processing speeds, and dedicated professional support. \n\n## Quick start\n\nWant to try it out but don't want to install anything ? We have set up a [Google Colab](https://colab.research.google.com/github/deezer/spleeter/blob/master/spleeter.ipynb).\n\nReady to dig into it ? In a few lines you can install **Spleeter**  and separate the vocal and accompaniment parts from an example audio file.\nYou need first to install `ffmpeg` and `libsndfile`. It can be done on most platform using [Conda](https://github.com/deezer/spleeter/wiki/1.-Installation#using-conda):\n\n```bash\n# install dependencies using conda\nconda install -c conda-forge ffmpeg libsndfile\n# install spleeter with pip\npip install spleeter\n# download an example audio file (if you don't have wget, use another tool for downloading)\nwget https://github.com/deezer/spleeter/raw/master/audio_example.mp3\n# separate the example audio into two components\nspleeter separate -p spleeter:2stems -o output audio_example.mp3\n```\n\n> :warning: Note that we no longer recommend using `conda` for installing spleeter.\n\n> \u26a0\ufe0f There are known issues with Apple M1 chips, mostly due to TensorFlow compatibility. Until these are fixed, you can use [this workaround](https://github.com/deezer/spleeter/issues/607#issuecomment-1021669444).\n\nYou should get two separated audio files (`vocals.wav` and `accompaniment.wav`) in the `output/audio_example` folder.\n\nFor a detailed documentation, please check the [repository wiki](https://github.com/deezer/spleeter/wiki/1.-Installation)\n\n## Development and Testing\n\nThis project is managed using [Poetry](https://python-poetry.org/docs/basic-usage/), to run test suite you\ncan execute the following set of commands:\n\n```bash\n# Clone spleeter repository\ngit clone https://github.com/Deezer/spleeter && cd spleeter\n# Install poetry\npip install poetry\n# Install spleeter dependencies\npoetry install\n# Run unit test suite\npoetry run pytest tests/\n```\n\n## Reference\n\n* Deezer Research - Source Separation Engine Story - deezer.io blog post:\n  * [English version](https://deezer.io/releasing-spleeter-deezer-r-d-source-separation-engine-2b88985e797e)\n  * [Japanese version](http://dzr.fm/splitterjp)\n* [Music Source Separation tool with pre-trained models / ISMIR2019 extended abstract](http://archives.ismir.net/ismir2019/latebreaking/000036.pdf)\n\nIf you use **Spleeter** in your work, please cite:\n\n```BibTeX\n@article{spleeter2020,\n  doi = {10.21105/joss.02154},\n  url = {https://doi.org/10.21105/joss.02154},\n  year = {2020},\n  publisher = {The Open Journal},\n  volume = {5},\n  number = {50},\n  pages = {2154},\n  author = {Romain Hennequin and Anis Khlif and Felix Voituret and Manuel Moussallam},\n  title = {Spleeter: a fast and efficient music source separation tool with pre-trained models},\n  journal = {Journal of Open Source Software},\n  note = {Deezer Research}\n}\n```\n\n## License\n\nThe code of **Spleeter** is [MIT-licensed](LICENSE).\n\n## Disclaimer\n\nIf you plan to use **Spleeter** on copyrighted material, make sure you get proper authorization from right owners beforehand.\n\n## Troubleshooting\n\n**Spleeter** is a complex piece of software and although we continously try to improve and test it you may encounter unexpected issues running it. If that's the case please check the [FAQ page](https://github.com/deezer/spleeter/wiki/5.-FAQ) first as well as the list of [currently open issues](https://github.com/deezer/spleeter/issues)\n\n### Windows users\n\n   It appears that sometimes the shortcut command `spleeter` does not work properly on windows. This is a known issue that we will hopefully fix soon. In the meantime replace `spleeter separate` by `python -m spleeter separate` in command line and it should work.\n\n## Contributing\n\nIf you would like to participate in the development of **Spleeter** you are more than welcome to do so. Don't hesitate to throw us a pull request and we'll do our best to examine it quickly. Please check out our [guidelines](.github/CONTRIBUTING.md) first.\n\n## Note\n\nThis repository include a demo audio file `audio_example.mp3` which is an excerpt\nfrom Slow Motion Dream by Steven M Bryant (c) copyright 2011 Licensed under a Creative\nCommons Attribution (3.0) [license](http://dig.ccmixter.org/files/stevieb357/34740)\nFt: CSoul,Alex Beroza & Robert Siekawitch\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 60273704,
    "name": "DeepSpeech",
    "full_name": "mozilla/DeepSpeech",
    "description": "DeepSpeech is an open source embedded (offline, on-device) speech-to-text engine which can run in real time on devices ranging from a Raspberry Pi 4 to high power GPU servers.",
    "html_url": "https://github.com/mozilla/DeepSpeech",
    "clone_url": "https://github.com/mozilla/DeepSpeech.git",
    "owner_login": "mozilla",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/131524?v=4",
    "stargazers_count": 26553,
    "watchers_count": 26553,
    "forks_count": 4072,
    "open_issues_count": 151,
    "size": 50588,
    "language": "C++",
    "languages": {
      "C++": 1052035,
      "Python": 479417,
      "C": 251606,
      "Shell": 241110,
      "C#": 62013,
      "Swift": 39402,
      "Java": 29528,
      "Makefile": 20327,
      "CMake": 20205,
      "TypeScript": 12607,
      "SWIG": 10334,
      "Starlark": 8680,
      "JavaScript": 5803,
      "Awk": 2632,
      "Ruby": 974,
      "Objective-C": 321
    },
    "topics": [
      "deep-learning",
      "deepspeech",
      "embedded",
      "machine-learning",
      "neural-networks",
      "offline",
      "on-device",
      "speech-recognition",
      "speech-to-text",
      "tensorflow"
    ],
    "license_name": "Mozilla Public License 2.0",
    "created_at": "2016-06-02T15:04:53+00:00",
    "updated_at": "2025-08-06T01:32:36+00:00",
    "pushed_at": "2025-06-19T12:52:51+00:00",
    "contributors_count": 100,
    "readme_length": 1590,
    "readme_content": "Status\n======\n\nThis project is now discontinued.\n\nProject DeepSpeech\n==================\n\n.. image:: https://readthedocs.org/projects/deepspeech/badge/?version=latest\n   :target: https://deepspeech.readthedocs.io/?badge=latest\n   :alt: Documentation\n\n\n.. image:: https://github.com/mozilla/DeepSpeech/actions/workflows/macOS-amd64.yml/badge.svg\n   :target: https://github.com/mozilla/DeepSpeech/actions/workflows/macOS-amd64.yml\n   :alt: macOS builds\n\n.. image:: https://github.com/mozilla/DeepSpeech/actions/workflows/lint.yml/badge.svg\n   :target: https://github.com/mozilla/DeepSpeech/actions/workflows/lint.yml\n   :alt: Linters\n\n.. image:: https://github.com/mozilla/DeepSpeech/actions/workflows/docker.yml/badge.svg\n   :target: https://github.com/mozilla/DeepSpeech/actions/workflows/docker.yml\n   :alt: Docker Images\n\n\nDeepSpeech is an open-source Speech-To-Text engine, using a model trained by machine learning techniques based on `Baidu's Deep Speech research paper <https://arxiv.org/abs/1412.5567>`_. Project DeepSpeech uses Google's `TensorFlow <https://www.tensorflow.org/>`_ to make the implementation easier.\n\nDocumentation for installation, usage, and training models are available on `deepspeech.readthedocs.io <https://deepspeech.readthedocs.io/?badge=latest>`_.\n\nFor the latest release, including pre-trained models and checkpoints, `see the latest release on GitHub <https://github.com/mozilla/DeepSpeech/releases/latest>`_.\n\nFor contribution guidelines, see `CONTRIBUTING.rst <CONTRIBUTING.rst>`_.\n\nFor contact and support information, see `SUPPORT.rst <SUPPORT.rst>`_.\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 152166877,
    "name": "d2l-en",
    "full_name": "d2l-ai/d2l-en",
    "description": "Interactive deep learning book with multi-framework code, math, and discussions. Adopted at 500 universities from 70 countries including Stanford, MIT, Harvard, and Cambridge.",
    "html_url": "https://github.com/d2l-ai/d2l-en",
    "clone_url": "https://github.com/d2l-ai/d2l-en.git",
    "owner_login": "d2l-ai",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/43974506?v=4",
    "stargazers_count": 26496,
    "watchers_count": 26496,
    "forks_count": 4700,
    "open_issues_count": 166,
    "size": 317904,
    "language": "Python",
    "languages": {
      "Python": 435529,
      "TeX": 167397,
      "HTML": 91223,
      "Shell": 11224,
      "CSS": 86
    },
    "topics": [
      "book",
      "computer-vision",
      "data-science",
      "deep-learning",
      "gaussian-processes",
      "hyperparameter-optimization",
      "jax",
      "kaggle",
      "keras",
      "machine-learning",
      "mxnet",
      "natural-language-processing",
      "notebook",
      "python",
      "pytorch",
      "recommender-system",
      "reinforcement-learning",
      "tensorflow"
    ],
    "license_name": "Other",
    "created_at": "2018-10-09T01:04:37+00:00",
    "updated_at": "2025-08-05T21:16:52+00:00",
    "pushed_at": "2024-08-18T08:02:36+00:00",
    "contributors_count": 100,
    "readme_length": 4754,
    "readme_content": "<div align=\"left\">\n  <img src=\"https://raw.githubusercontent.com/d2l-ai/d2l-en/master/static/logo-with-text.png\" width=\"350\">\n</div>\n\n# D2L.ai: Interactive Deep Learning Book with Multi-Framework Code, Math, and Discussions\n\n[![Continuous Integration](https://github.com/d2l-ai/d2l-en/actions/workflows/ci.yml/badge.svg)](https://github.com/d2l-ai/d2l-en/actions/workflows/ci.yml)\n\n[Book website](https://d2l.ai/) | [STAT 157 Course at UC Berkeley](http://courses.d2l.ai/berkeley-stat-157/index.html)\n\n<h5 align=\"center\"><i>The best way to understand deep learning is learning by doing.</i></h5>\n\n<p align=\"center\">\n  <img width=\"200\"  src=\"static/frontpage/_images/eq.jpg\">\n  <img width=\"200\"  src=\"static/frontpage/_images/figure.jpg\">\n  <img width=\"200\"  src=\"static/frontpage/_images/code.jpg\">\n  <img width=\"200\"  src=\"static/frontpage/_images/notebook.gif\">\n</p>\n\nThis open-source book represents our attempt to make deep learning approachable, teaching you the concepts, the context, and the code. The entire book is drafted in Jupyter notebooks, seamlessly integrating exposition figures, math, and interactive examples with self-contained code.\n\nOur goal is to offer a resource that could\n1. be freely available for everyone;\n1. offer sufficient technical depth to provide a starting point on the path to actually becoming an applied machine learning scientist;\n1. include runnable code, showing readers how to solve problems in practice;\n1. allow for rapid updates, both by us and also by the community at large;\n1. be complemented by a forum for interactive discussion of technical details and to answer questions.\n\n## Universities Using D2L\n<p align=\"center\">\n  <img width=\"600\"  src=\"static/frontpage/_images/map.png\">\n</p>\n\n\n\nIf you find this book useful, please star (\u2605) this repository or cite this book using the following bibtex entry:\n\n```\n@book{zhang2023dive,\n    title={Dive into Deep Learning},\n    author={Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},\n    publisher={Cambridge University Press},\n    note={\\url{https://D2L.ai}},\n    year={2023}\n}\n```\n\n\n## Endorsements\n\n> <p>\"In less than a decade, the AI revolution has swept from research labs to broad industries to every corner of our daily life.  Dive into Deep Learning is an excellent text on deep learning and deserves attention from anyone who wants to learn why deep learning has ignited the AI revolution: the most powerful technology force of our time.\"</p>\n> <b>&mdash; Jensen Huang, Founder and CEO, NVIDIA</b>\n\n> <p>\"This is a timely, fascinating book, providing with not only a comprehensive overview of deep learning principles but also detailed algorithms with hands-on programming code, and moreover, a state-of-the-art introduction to deep learning in computer vision and natural language processing. Dive into this book if you want to dive into deep learning!\"</p>\n> <b>&mdash; Jiawei Han, Michael Aiken Chair Professor, University of Illinois at Urbana-Champaign</b>\n\n> <p>\"This is a highly welcome addition to the machine learning literature, with a focus on hands-on experience implemented via the integration of Jupyter notebooks. Students of deep learning should find this invaluable to become proficient in this field.\"</p>\n> <b>&mdash; Bernhard Sch\u00f6lkopf, Director, Max Planck Institute for Intelligent Systems</b>\n\n> <p>\"Dive into Deep Learning strikes an excellent balance between hands-on learning and in-depth explanation. I've used it in my deep learning course and recommend it to anyone who wants to develop a thorough and practical understanding of deep learning.\"</p>\n> <b>&mdash; Colin Raffel, Assistant Professor, University of North Carolina, Chapel Hill</b>\n\n## Contributing ([Learn How](https://d2l.ai/chapter_appendix-tools-for-deep-learning/contributing.html))\n\nThis open source book has benefited from pedagogical suggestions, typo corrections, and other improvements from community contributors. Your help is valuable for making the book better for everyone.\n\n**Dear [D2L contributors](https://github.com/d2l-ai/d2l-en/graphs/contributors), please email your GitHub ID and name to d2lbook.en AT gmail DOT com so your name will appear on the [acknowledgments](https://d2l.ai/chapter_preface/index.html#acknowledgments). Thanks.**\n\n\n## License Summary\n\nThis open source book is made available under the Creative Commons Attribution-ShareAlike 4.0 International License. See [LICENSE](LICENSE) file.\n\nThe sample and reference code within this open source book is made available under a modified MIT license. See the [LICENSE-SAMPLECODE](LICENSE-SAMPLECODE) file.\n\n[Chinese version](https://github.com/d2l-ai/d2l-zh) | [Discuss and report issues](https://discuss.d2l.ai/) | [Code of conduct](CODE_OF_CONDUCT.md)\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 107595270,
    "name": "Mask_RCNN",
    "full_name": "matterport/Mask_RCNN",
    "description": "Mask R-CNN for object detection and instance segmentation on Keras and TensorFlow",
    "html_url": "https://github.com/matterport/Mask_RCNN",
    "clone_url": "https://github.com/matterport/Mask_RCNN.git",
    "owner_login": "matterport",
    "owner_type": "Organization",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/4206481?v=4",
    "stargazers_count": 25266,
    "watchers_count": 25266,
    "forks_count": 11708,
    "open_issues_count": 2025,
    "size": 122597,
    "language": "Python",
    "languages": {
      "Python": 199107
    },
    "topics": [
      "instance-segmentation",
      "keras",
      "mask-rcnn",
      "object-detection",
      "tensorflow"
    ],
    "license_name": "Other",
    "created_at": "2017-10-19T20:28:34+00:00",
    "updated_at": "2025-08-05T21:20:56+00:00",
    "pushed_at": "2024-06-07T02:36:21+00:00",
    "contributors_count": 37,
    "readme_length": 13769,
    "readme_content": "# Mask R-CNN for Object Detection and Segmentation\n\nThis is an implementation of [Mask R-CNN](https://arxiv.org/abs/1703.06870) on Python 3, Keras, and TensorFlow. The model generates bounding boxes and segmentation masks for each instance of an object in the image. It's based on Feature Pyramid Network (FPN) and a ResNet101 backbone.\n\n![Instance Segmentation Sample](assets/street.png)\n\nThe repository includes:\n* Source code of Mask R-CNN built on FPN and ResNet101.\n* Training code for MS COCO\n* Pre-trained weights for MS COCO\n* Jupyter notebooks to visualize the detection pipeline at every step\n* ParallelModel class for multi-GPU training\n* Evaluation on MS COCO metrics (AP)\n* Example of training on your own dataset\n\n\nThe code is documented and designed to be easy to extend. If you use it in your research, please consider citing this repository (bibtex below). If you work on 3D vision, you might find our recently released [Matterport3D](https://matterport.com/blog/2017/09/20/announcing-matterport3d-research-dataset/) dataset useful as well.\nThis dataset was created from 3D-reconstructed spaces captured by our customers who agreed to make them publicly available for academic use. You can see more examples [here](https://matterport.com/gallery/).\n\n# Getting Started\n* [demo.ipynb](samples/demo.ipynb) Is the easiest way to start. It shows an example of using a model pre-trained on MS COCO to segment objects in your own images.\nIt includes code to run object detection and instance segmentation on arbitrary images.\n\n* [train_shapes.ipynb](samples/shapes/train_shapes.ipynb) shows how to train Mask R-CNN on your own dataset. This notebook introduces a toy dataset (Shapes) to demonstrate training on a new dataset.\n\n* ([model.py](mrcnn/model.py), [utils.py](mrcnn/utils.py), [config.py](mrcnn/config.py)): These files contain the main Mask RCNN implementation. \n\n\n* [inspect_data.ipynb](samples/coco/inspect_data.ipynb). This notebook visualizes the different pre-processing steps\nto prepare the training data.\n\n* [inspect_model.ipynb](samples/coco/inspect_model.ipynb) This notebook goes in depth into the steps performed to detect and segment objects. It provides visualizations of every step of the pipeline.\n\n* [inspect_weights.ipynb](samples/coco/inspect_weights.ipynb)\nThis notebooks inspects the weights of a trained model and looks for anomalies and odd patterns.\n\n\n# Step by Step Detection\nTo help with debugging and understanding the model, there are 3 notebooks \n([inspect_data.ipynb](samples/coco/inspect_data.ipynb), [inspect_model.ipynb](samples/coco/inspect_model.ipynb),\n[inspect_weights.ipynb](samples/coco/inspect_weights.ipynb)) that provide a lot of visualizations and allow running the model step by step to inspect the output at each point. Here are a few examples:\n\n\n\n## 1. Anchor sorting and filtering\nVisualizes every step of the first stage Region Proposal Network and displays positive and negative anchors along with anchor box refinement.\n![](assets/detection_anchors.png)\n\n## 2. Bounding Box Refinement\nThis is an example of final detection boxes (dotted lines) and the refinement applied to them (solid lines) in the second stage.\n![](assets/detection_refinement.png)\n\n## 3. Mask Generation\nExamples of generated masks. These then get scaled and placed on the image in the right location.\n\n![](assets/detection_masks.png)\n\n## 4.Layer activations\nOften it's useful to inspect the activations at different layers to look for signs of trouble (all zeros or random noise).\n\n![](assets/detection_activations.png)\n\n## 5. Weight Histograms\nAnother useful debugging tool is to inspect the weight histograms. These are included in the inspect_weights.ipynb notebook.\n\n![](assets/detection_histograms.png)\n\n## 6. Logging to TensorBoard\nTensorBoard is another great debugging and visualization tool. The model is configured to log losses and save weights at the end of every epoch.\n\n![](assets/detection_tensorboard.png)\n\n## 6. Composing the different pieces into a final result\n\n![](assets/detection_final.png)\n\n\n# Training on MS COCO\nWe're providing pre-trained weights for MS COCO to make it easier to start. You can\nuse those weights as a starting point to train your own variation on the network.\nTraining and evaluation code is in `samples/coco/coco.py`. You can import this\nmodule in Jupyter notebook (see the provided notebooks for examples) or you\ncan run it directly from the command line as such:\n\n```\n# Train a new model starting from pre-trained COCO weights\npython3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=coco\n\n# Train a new model starting from ImageNet weights\npython3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=imagenet\n\n# Continue training a model that you had trained earlier\npython3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=/path/to/weights.h5\n\n# Continue training the last model you trained. This will find\n# the last trained weights in the model directory.\npython3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=last\n```\n\nYou can also run the COCO evaluation code with:\n```\n# Run COCO evaluation on the last trained model\npython3 samples/coco/coco.py evaluate --dataset=/path/to/coco/ --model=last\n```\n\nThe training schedule, learning rate, and other parameters should be set in `samples/coco/coco.py`.\n\n\n# Training on Your Own Dataset\n\nStart by reading this [blog post about the balloon color splash sample](https://engineering.matterport.com/splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46). It covers the process starting from annotating images to training to using the results in a sample application.\n\nIn summary, to train the model on your own dataset you'll need to extend two classes:\n\n```Config```\nThis class contains the default configuration. Subclass it and modify the attributes you need to change.\n\n```Dataset```\nThis class provides a consistent way to work with any dataset. \nIt allows you to use new datasets for training without having to change \nthe code of the model. It also supports loading multiple datasets at the\nsame time, which is useful if the objects you want to detect are not \nall available in one dataset. \n\nSee examples in `samples/shapes/train_shapes.ipynb`, `samples/coco/coco.py`, `samples/balloon/balloon.py`, and `samples/nucleus/nucleus.py`.\n\n## Differences from the Official Paper\nThis implementation follows the Mask RCNN paper for the most part, but there are a few cases where we deviated in favor of code simplicity and generalization. These are some of the differences we're aware of. If you encounter other differences, please do let us know.\n\n* **Image Resizing:** To support training multiple images per batch we resize all images to the same size. For example, 1024x1024px on MS COCO. We preserve the aspect ratio, so if an image is not square we pad it with zeros. In the paper the resizing is done such that the smallest side is 800px and the largest is trimmed at 1000px.\n* **Bounding Boxes**: Some datasets provide bounding boxes and some provide masks only. To support training on multiple datasets we opted to ignore the bounding boxes that come with the dataset and generate them on the fly instead. We pick the smallest box that encapsulates all the pixels of the mask as the bounding box. This simplifies the implementation and also makes it easy to apply image augmentations that would otherwise be harder to apply to bounding boxes, such as image rotation.\n\n    To validate this approach, we compared our computed bounding boxes to those provided by the COCO dataset.\nWe found that ~2% of bounding boxes differed by 1px or more, ~0.05% differed by 5px or more, \nand only 0.01% differed by 10px or more.\n\n* **Learning Rate:** The paper uses a learning rate of 0.02, but we found that to be\ntoo high, and often causes the weights to explode, especially when using a small batch\nsize. It might be related to differences between how Caffe and TensorFlow compute \ngradients (sum vs mean across batches and GPUs). Or, maybe the official model uses gradient\nclipping to avoid this issue. We do use gradient clipping, but don't set it too aggressively.\nWe found that smaller learning rates converge faster anyway so we go with that.\n\n## Citation\nUse this bibtex to cite this repository:\n```\n@misc{matterport_maskrcnn_2017,\n  title={Mask R-CNN for object detection and instance segmentation on Keras and TensorFlow},\n  author={Waleed Abdulla},\n  year={2017},\n  publisher={Github},\n  journal={GitHub repository},\n  howpublished={\\url{https://github.com/matterport/Mask_RCNN}},\n}\n```\n\n## Contributing\nContributions to this repository are welcome. Examples of things you can contribute:\n* Speed Improvements. Like re-writing some Python code in TensorFlow or Cython.\n* Training on other datasets.\n* Accuracy Improvements.\n* Visualizations and examples.\n\nYou can also [join our team](https://matterport.com/careers/) and help us build even more projects like this one.\n\n## Requirements\nPython 3.4, TensorFlow 1.3, Keras 2.0.8 and other common packages listed in `requirements.txt`.\n\n### MS COCO Requirements:\nTo train or test on MS COCO, you'll also need:\n* pycocotools (installation instructions below)\n* [MS COCO Dataset](http://cocodataset.org/#home)\n* Download the 5K [minival](https://dl.dropboxusercontent.com/s/o43o90bna78omob/instances_minival2014.json.zip?dl=0)\n  and the 35K [validation-minus-minival](https://dl.dropboxusercontent.com/s/s3tw5zcg7395368/instances_valminusminival2014.json.zip?dl=0)\n  subsets. More details in the original [Faster R-CNN implementation](https://github.com/rbgirshick/py-faster-rcnn/blob/master/data/README.md).\n\nIf you use Docker, the code has been verified to work on\n[this Docker container](https://hub.docker.com/r/waleedka/modern-deep-learning/).\n\n\n## Installation\n1. Clone this repository\n2. Install dependencies\n   ```bash\n   pip3 install -r requirements.txt\n   ```\n3. Run setup from the repository root directory\n    ```bash\n    python3 setup.py install\n    ``` \n3. Download pre-trained COCO weights (mask_rcnn_coco.h5) from the [releases page](https://github.com/matterport/Mask_RCNN/releases).\n4. (Optional) To train or test on MS COCO install `pycocotools` from one of these repos. They are forks of the original pycocotools with fixes for Python3 and Windows (the official repo doesn't seem to be active anymore).\n\n    * Linux: https://github.com/waleedka/coco\n    * Windows: https://github.com/philferriere/cocoapi.\n    You must have the Visual C++ 2015 build tools on your path (see the repo for additional details)\n\n# Projects Using this Model\nIf you extend this model to other datasets or build projects that use it, we'd love to hear from you.\n\n### [4K Video Demo](https://www.youtube.com/watch?v=OOT3UIXZztE) by Karol Majek.\n[![Mask RCNN on 4K Video](assets/4k_video.gif)](https://www.youtube.com/watch?v=OOT3UIXZztE)\n\n### [Images to OSM](https://github.com/jremillard/images-to-osm): Improve OpenStreetMap by adding baseball, soccer, tennis, football, and basketball fields.\n\n![Identify sport fields in satellite images](assets/images_to_osm.png)\n\n### [Splash of Color](https://engineering.matterport.com/splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46). A blog post explaining how to train this model from scratch and use it to implement a color splash effect.\n![Balloon Color Splash](assets/balloon_color_splash.gif)\n\n\n### [Segmenting Nuclei in Microscopy Images](samples/nucleus). Built for the [2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018)\nCode is in the `samples/nucleus` directory.\n\n![Nucleus Segmentation](assets/nucleus_segmentation.png)\n\n### [Detection and Segmentation for Surgery Robots](https://github.com/SUYEgit/Surgery-Robot-Detection-Segmentation) by the NUS Control & Mechatronics Lab.\n![Surgery Robot Detection and Segmentation](https://github.com/SUYEgit/Surgery-Robot-Detection-Segmentation/raw/master/assets/video.gif)\n\n### [Reconstructing 3D buildings from aerial LiDAR](https://medium.com/geoai/reconstructing-3d-buildings-from-aerial-lidar-with-ai-details-6a81cb3079c0)\nA proof of concept project by [Esri](https://www.esri.com/), in collaboration with Nvidia and Miami-Dade County. Along with a great write up and code by Dmitry Kudinov, Daniel Hedges, and Omar Maher.\n![3D Building Reconstruction](assets/project_3dbuildings.png)\n\n### [Usiigaci: Label-free Cell Tracking in Phase Contrast Microscopy](https://github.com/oist/usiigaci)\nA project from Japan to automatically track cells in a microfluidics platform. Paper is pending, but the source code is released.\n\n![](assets/project_usiigaci1.gif) ![](assets/project_usiigaci2.gif)\n\n### [Characterization of Arctic Ice-Wedge Polygons in Very High Spatial Resolution Aerial Imagery](http://www.mdpi.com/2072-4292/10/9/1487)\nResearch project to understand the complex processes between degradations in the Arctic and climate change. By Weixing Zhang, Chandi Witharana, Anna Liljedahl, and Mikhail Kanevskiy.\n![image](assets/project_ice_wedge_polygons.png)\n\n### [Mask-RCNN Shiny](https://github.com/huuuuusy/Mask-RCNN-Shiny)\nA computer vision class project by HU Shiyu to apply the color pop effect on people with beautiful results.\n![](assets/project_shiny1.jpg)\n\n### [Mapping Challenge](https://github.com/crowdAI/crowdai-mapping-challenge-mask-rcnn): Convert satellite imagery to maps for use by humanitarian organisations.\n![Mapping Challenge](assets/mapping_challenge.png)\n\n### [GRASS GIS Addon](https://github.com/ctu-geoforall-lab/i.ann.maskrcnn) to generate vector masks from geospatial imagery. Based on a [Master's thesis](https://github.com/ctu-geoforall-lab-projects/dp-pesek-2018) by Ond\u0159ej Pe\u0161ek.\n![GRASS GIS Image](assets/project_grass_gis.png)\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  },
  {
    "id": 167694194,
    "name": "frigate",
    "full_name": "blakeblackshear/frigate",
    "description": "NVR with realtime local object detection for IP cameras",
    "html_url": "https://github.com/blakeblackshear/frigate",
    "clone_url": "https://github.com/blakeblackshear/frigate.git",
    "owner_login": "blakeblackshear",
    "owner_type": "User",
    "owner_avatar_url": "https://avatars.githubusercontent.com/u/569905?v=4",
    "stargazers_count": 24539,
    "watchers_count": 24539,
    "forks_count": 2283,
    "open_issues_count": 168,
    "size": 95137,
    "language": "TypeScript",
    "languages": {
      "TypeScript": 1785457,
      "Python": 1616294,
      "CSS": 29455,
      "Shell": 20027,
      "Dockerfile": 16210,
      "JavaScript": 9458,
      "Makefile": 6148,
      "HCL": 4033,
      "Jupyter Notebook": 2384,
      "HTML": 2323
    },
    "topics": [
      "ai",
      "camera",
      "google-coral",
      "home-assistant",
      "home-automation",
      "homeautomation",
      "mqtt",
      "nvr",
      "object-detection",
      "realtime",
      "rtsp",
      "tensorflow"
    ],
    "license_name": "MIT License",
    "created_at": "2019-01-26T13:52:38+00:00",
    "updated_at": "2025-08-06T02:06:17+00:00",
    "pushed_at": "2025-08-05T22:35:12+00:00",
    "contributors_count": 100,
    "readme_length": 2918,
    "readme_content": "<p align=\"center\">\n  <img align=\"center\" alt=\"logo\" src=\"docs/static/img/frigate.png\">\n</p>\n\n# Frigate - NVR With Realtime Object Detection for IP Cameras\n\n<a href=\"https://hosted.weblate.org/engage/frigate-nvr/\">\n<img src=\"https://hosted.weblate.org/widget/frigate-nvr/language-badge.svg\" alt=\"Translation status\" />\n</a>\n\n\\[English\\] | [\u7b80\u4f53\u4e2d\u6587](https://github.com/blakeblackshear/frigate/blob/dev/README_CN.md)\n\nA complete and local NVR designed for [Home Assistant](https://www.home-assistant.io) with AI object detection. Uses OpenCV and Tensorflow to perform realtime object detection locally for IP cameras.\n\nUse of a GPU or AI accelerator such as a [Google Coral](https://coral.ai/products/) or [Hailo](https://hailo.ai/) is highly recommended. AI accelerators will outperform even the best CPUs with very little overhead.\n\n- Tight integration with Home Assistant via a [custom component](https://github.com/blakeblackshear/frigate-hass-integration)\n- Designed to minimize resource use and maximize performance by only looking for objects when and where it is necessary\n- Leverages multiprocessing heavily with an emphasis on realtime over processing every frame\n- Uses a very low overhead motion detection to determine where to run object detection\n- Object detection with TensorFlow runs in separate processes for maximum FPS\n- Communicates over MQTT for easy integration into other systems\n- Records video with retention settings based on detected objects\n- 24/7 recording\n- Re-streaming via RTSP to reduce the number of connections to your camera\n- WebRTC & MSE support for low-latency live view\n\n## Documentation\n\nView the documentation at https://docs.frigate.video\n\n## Donations\n\nIf you would like to make a donation to support development, please use [Github Sponsors](https://github.com/sponsors/blakeblackshear).\n\n## Screenshots\n\n### Live dashboard\n\n<div>\n<img width=\"800\" alt=\"Live dashboard\" src=\"https://github.com/blakeblackshear/frigate/assets/569905/5e713cb9-9db5-41dc-947a-6937c3bc376e\">\n</div>\n\n### Streamlined review workflow\n\n<div>\n<img width=\"800\" alt=\"Streamlined review workflow\" src=\"https://github.com/blakeblackshear/frigate/assets/569905/6fed96e8-3b18-40e5-9ddc-31e6f3c9f2ff\">\n</div>\n\n### Multi-camera scrubbing\n\n<div>\n<img width=\"800\" alt=\"Multi-camera scrubbing\" src=\"https://github.com/blakeblackshear/frigate/assets/569905/d6788a15-0eeb-4427-a8d4-80b93cae3d74\">\n</div>\n\n### Built-in mask and zone editor\n\n<div>\n<img width=\"800\" alt=\"Multi-camera scrubbing\" src=\"https://github.com/blakeblackshear/frigate/assets/569905/d7885fc3-bfe6-452f-b7d0-d957cb3e31f5\">\n</div>\n\n## Translations\n\nWe use [Weblate](https://hosted.weblate.org/projects/frigate-nvr/) to support language translations. Contributions are always welcome.\n\n<a href=\"https://hosted.weblate.org/engage/frigate-nvr/\">\n<img src=\"https://hosted.weblate.org/widget/frigate-nvr/multi-auto.svg\" alt=\"Translation status\" />\n</a>\n",
    "has_ci": false,
    "has_tests": false,
    "has_documentation": true,
    "momentum_score": 0.0,
    "quality_score": 0.0,
    "final_score": 0.0,
    "readme_embedding": null,
    "cluster_id": null
  }
]